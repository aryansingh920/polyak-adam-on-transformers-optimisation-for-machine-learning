{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:39.776594Z",
     "start_time": "2025-04-29T15:36:37.607622Z"
    }
   },
   "source": [
    "#Cell 1\n",
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import logging  # Added for metric logging\n",
    "\n",
    "from evaluation import load_model_and_metrics, evaluate_on_test_set\n",
    "from test_visualisation import (\n",
    "    create_loss_comparison_plot,\n",
    "    create_improvement_heatmap,\n",
    "    create_perplexity_comparison\n",
    ")\n",
    "from train_with_viz import test_configs_with_tracking, save_model_and_metrics\n",
    "from new_gpt import ModelConfig, GPTLanguageModel, load_data, get_batch, estimate_loss\n",
    "from gpt_downsizing import create_custom_config\n",
    "\n",
    "# Detect device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename='experiment_metrics.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s'\n",
    ")\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seed for reproducibility across multiple runs.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Default seed for single runs\n",
    "default_seed = 42\n",
    "set_seed(default_seed)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b5e9e2ef55ea40bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:42.235139Z",
     "start_time": "2025-04-29T15:36:42.229525Z"
    }
   },
   "source": [
    "def loss_function(x, minibatch):\n",
    "    \"\"\"\n",
    "    Given parameter x in R^2, compute the average of\n",
    "    min(13 * ||z||^2, ||z + [9,2]||^2)\n",
    "    over all w in the minibatch, where z = x - w - 1.\n",
    "    \"\"\"\n",
    "    y = 0.0\n",
    "    for w in minibatch:\n",
    "        z = x - w - 1  # z ∈ R^2\n",
    "        val1 = 13.0 * (z[0]**2 + z[1]**2)\n",
    "        val2 = (z[0] + 9.0)**2 + (z[1] + 2.0)**2\n",
    "        y += min(val1, val2)\n",
    "    return y / len(minibatch)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "b6f9304898bae5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:42.927146Z",
     "start_time": "2025-04-29T15:36:42.924759Z"
    }
   },
   "source": [
    "def gradient_function(x, minibatch):\n",
    "    \"\"\"\n",
    "    Piecewise gradient of the loss_function with respect to x.\n",
    "    If 13*||z||^2 < ||z + [9,2]||^2, use gradient of 13*||z||^2,\n",
    "    else use gradient of ||z + [9,2]||^2.  Averaged over minibatch.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    for w in minibatch:\n",
    "        z = x - w - 1\n",
    "        val1 = 13.0 * (z[0]**2 + z[1]**2)\n",
    "        val2 = (z[0] + 9.0)**2 + (z[1] + 2.0)**2\n",
    "\n",
    "        if val1 < val2:\n",
    "            # ∇(13||z||^2) = 26 z\n",
    "            grad[0] += 26.0 * z[0]\n",
    "            grad[1] += 26.0 * z[1]\n",
    "        else:\n",
    "            # ∇||z + [9,2]||^2 = 2*(z + [9,2])\n",
    "            grad[0] += 2.0 * (z[0] + 9.0)\n",
    "            grad[1] += 2.0 * (z[1] + 2.0)\n",
    "\n",
    "    return grad / len(minibatch)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3146a9fef407c56f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:43.497353Z",
     "start_time": "2025-04-29T15:36:43.493788Z"
    }
   },
   "source": [
    "# Cell 4\n",
    "\n",
    "def sgd_optimizer(\n",
    "    data,\n",
    "    x_init,\n",
    "    loss_fn,\n",
    "    grad_fn,\n",
    "    method='constant',\n",
    "    alpha=0.01,\n",
    "    batch_size=8,\n",
    "    max_epochs=100,\n",
    "    beta1=0.9,         # for Heavy Ball and Adam\n",
    "    beta2=0.999,       # for RMSProp and Adam\n",
    "    epsilon=1e-8,      # for RMSProp and Adam\n",
    "    polyak_f_star=0.0  # for Polyak steps\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform mini-batch SGD on the given data using various step-size methods.\n",
    "\n",
    "    :param data: array of shape (n, 2) representing n training points in R^2.\n",
    "    :param x_init: initial parameter vector in R^2.\n",
    "    :param loss_fn: f(x, minibatch) -> scalar.\n",
    "    :param grad_fn: grad_f(x, minibatch) -> R^2 gradient.\n",
    "    :param method: one of 'constant', 'polyak', 'rmsprop', 'heavy_ball', 'adam'.\n",
    "    :param alpha: base step size (learning rate).\n",
    "    :param batch_size: size of each mini-batch.\n",
    "    :param max_epochs: number of passes through the data.\n",
    "    :param beta1: momentum parameter (heavy ball, Adam).\n",
    "    :param beta2: second-moment decay (RMSProp, Adam).\n",
    "    :param epsilon: small constant to avoid division by zero.\n",
    "    :param polyak_f_star: known (or estimated) minimal value of f.\n",
    "    :return: (x, losses) final parameters and list of losses per epoch.\n",
    "    \"\"\"\n",
    "    n = data.shape[0]\n",
    "    x = x_init.copy()\n",
    "    losses = []\n",
    "    m = np.zeros_like(x)  # momentum / first moment\n",
    "    v = np.zeros_like(x)  # second moment (for RMSProp/Adam)\n",
    "    iteration = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Shuffle data each epoch\n",
    "        indices = np.random.permutation(n)\n",
    "        data_shuffled = data[indices]\n",
    "\n",
    "        # Mini-batch loop\n",
    "        for start_idx in range(0, n, batch_size):\n",
    "            minibatch = data_shuffled[start_idx:start_idx + batch_size]\n",
    "            grad = grad_fn(x, minibatch)\n",
    "\n",
    "            if method == 'constant':\n",
    "                x -= alpha * grad\n",
    "\n",
    "            elif method == 'polyak':\n",
    "                current_loss = loss_fn(x, minibatch)\n",
    "                denom = np.dot(grad, grad) + 1e-12\n",
    "                alpha_k = (current_loss - polyak_f_star) / denom\n",
    "                if alpha_k < alpha: \n",
    "                    print(f\"[WARN] α_k={alpha_k:.2e}<α_min\"); \n",
    "                    alpha_k = alpha\n",
    "                x -= alpha_k * grad\n",
    "\n",
    "            elif method == 'rmsprop':\n",
    "                v = beta2 * v + (1 - beta2) * (grad * grad)\n",
    "                x -= alpha * grad / (np.sqrt(v) + epsilon)\n",
    "\n",
    "            elif method == 'heavy_ball':\n",
    "                m = beta1 * m + alpha * grad\n",
    "                x -= m\n",
    "\n",
    "            elif method == 'adam':\n",
    "                iteration += 1\n",
    "                m = beta1 * m + (1 - beta1) * grad\n",
    "                v = beta2 * v + (1 - beta2) * (grad * grad)\n",
    "                m_hat = m / (1 - beta1**iteration)\n",
    "                v_hat = v / (1 - beta2**iteration)\n",
    "                x -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        # Record full-dataset loss at end of epoch\n",
    "        epoch_loss = loss_fn(x, data)\n",
    "        losses.append(epoch_loss)\n",
    "\n",
    "    return x, losses"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "70d44383ea05e610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:44.176892Z",
     "start_time": "2025-04-29T15:36:44.173358Z"
    }
   },
   "source": [
    "def train_sgd(model, dataloader, loss_fn,\n",
    "              num_epochs=20,\n",
    "              step_method='constant',\n",
    "              alpha=1e-2,\n",
    "              f_star=0.0,\n",
    "              alpha_max=1.0):\n",
    "    model.to(device).train()\n",
    "    history = []\n",
    "    convergence_epoch = None\n",
    "    threshold = 0.05  # Loss threshold for convergence\n",
    "\n",
    "    # NEW: Log alpha values for Polyak\n",
    "    alpha_log = []  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for Xb, yb in dataloader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            out = model(Xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            grads = torch.cat([p.grad.view(-1) for p in model.parameters() if p.grad is not None])\n",
    "\n",
    "            if step_method == 'constant':\n",
    "                lr = alpha\n",
    "            else:  # 'polyak'\n",
    "                loss_val = loss.item()\n",
    "                denom = grads.dot(grads).item() + 1e-12\n",
    "                lr = (loss_val - f_star) / denom\n",
    "                if lr < 0:\n",
    "                    lr = alpha\n",
    "                lr = max(alpha, min(lr, alpha_max))\n",
    "                \n",
    "                # NEW: Log alpha_k\n",
    "                alpha_log.append(lr)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        p.data -= lr * p.grad\n",
    "            running_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        history.append(epoch_loss)\n",
    "\n",
    "        if convergence_epoch is None and epoch_loss < threshold:\n",
    "            convergence_epoch = epoch + 1\n",
    "\n",
    "    metrics = {\n",
    "        'final_loss': history[-1],\n",
    "        'convergence_epoch': convergence_epoch or num_epochs\n",
    "    }\n",
    "\n",
    "    # NEW: If Polyak, store alpha_log inside metrics\n",
    "    if step_method == 'polyak':\n",
    "        metrics['alpha_log'] = alpha_log\n",
    "\n",
    "    logging.info(f\"train_sgd: step_method={step_method}, final_loss={history[-1]:.4f}, \"\n",
    "                 f\"convergence_epoch={metrics['convergence_epoch']}\")\n",
    "    \n",
    "    return history, metrics\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "eabf5054131a1c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:44.879632Z",
     "start_time": "2025-04-29T15:36:44.877622Z"
    }
   },
   "source": [
    "# simple logistic regression model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "fc134f78397e4568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:45.309192Z",
     "start_time": "2025-04-29T15:36:45.297835Z"
    }
   },
   "source": [
    "\n",
    "# generate toy binary classification data\n",
    "np.random.seed(0)\n",
    "N = 1000\n",
    "X = np.random.randn(N, 2)\n",
    "w_true = np.array([2.0, -3.0])\n",
    "b_true = 0.5\n",
    "logits = X.dot(w_true) + b_true\n",
    "probs = 1/(1+np.exp(-logits))\n",
    "y = (probs > 0.5).astype(np.float32)\n",
    "\n",
    "# wrap in DataLoader\n",
    "X_t = torch.tensor(X, dtype=torch.float32)\n",
    "y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "ds = TensorDataset(X_t, y_t)\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True)\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "8636280ed76e99f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:55.974470Z",
     "start_time": "2025-04-29T15:36:45.973069Z"
    }
   },
   "source": [
    "# Cell 8: Binary Classification for 1(c)\n",
    "# Generate toy binary classification data\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "N = 1000\n",
    "X = np.random.randn(N, 2)\n",
    "w_true = np.array([2.0, -3.0])\n",
    "b_true = 0.5\n",
    "logits = X.dot(w_true) + b_true\n",
    "probs = 1/(1+np.exp(-logits))\n",
    "y = (probs > 0.5).astype(np.float32)\n",
    "\n",
    "# Wrap in DataLoader\n",
    "X_t = torch.tensor(X, dtype=torch.float32)\n",
    "y_t = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "ds = TensorDataset(X_t, y_t)\n",
    "train_loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "bce = nn.BCELoss()\n",
    "\n",
    "# Run multiple trials for robustness\n",
    "num_trials = 3\n",
    "hist_const_all = []\n",
    "hist_poly_all = []\n",
    "for trial in range(num_trials):\n",
    "    set_seed(42 + trial)  # Different seed per trial\n",
    "    \n",
    "    # Constant-LR run\n",
    "    model_const = LogisticRegression(2).to(device)\n",
    "    hist_const, metrics_const = train_sgd(\n",
    "        model_const, train_loader, bce,\n",
    "        num_epochs=30,\n",
    "        step_method='constant',\n",
    "        alpha=0.1,\n",
    "        alpha_max=1.0\n",
    "    )\n",
    "    \n",
    "    # Polyak run\n",
    "    model_poly = LogisticRegression(2).to(device)\n",
    "    hist_poly, metrics_poly = train_sgd(\n",
    "        model_poly, train_loader, bce,\n",
    "        num_epochs=30,\n",
    "        step_method='polyak',\n",
    "        alpha=0.01,  # Lower bound\n",
    "        f_star=0.0,\n",
    "        alpha_max=1.0\n",
    "    )\n",
    "    \n",
    "    hist_const_all.append(hist_const)\n",
    "    hist_poly_all.append(hist_poly)\n",
    "    \n",
    "    # Log trial metrics\n",
    "    logging.info(f\"Trial {trial+1}: Constant LR - {metrics_const}\")\n",
    "    logging.info(f\"Trial {trial+1}: Polyak LR - {metrics_poly}\")\n",
    "\n",
    "# Average results\n",
    "hist_const = np.mean(hist_const_all, axis=0)\n",
    "hist_poly = np.mean(hist_poly_all, axis=0)\n",
    "\n",
    "# Plot training loss curves\n",
    "plt.figure()\n",
    "plt.plot(hist_const, label='Constant LR')\n",
    "plt.plot(hist_poly, label='Polyak LR')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Binary Classification: Constant vs Polyak')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "a8b829b9024af935",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:55.986274Z",
     "start_time": "2025-04-29T15:36:55.983878Z"
    }
   },
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        ])\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "1f90cd71be430240",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:56.000671Z",
     "start_time": "2025-04-29T15:36:55.999040Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2b7e91d9c832968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:36:56.093685Z",
     "start_time": "2025-04-29T15:36:56.090437Z"
    }
   },
   "source": [
    "# simple linear model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "b8a749f4dff0a638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T15:38:40.568552Z",
     "start_time": "2025-04-29T15:38:35.630119Z"
    }
   },
   "source": [
    "def train_sgd(model, loader, loss_fn, num_epochs=10, step_method='constant', \n",
    "           alpha=1e-3, alpha_max=1.0, f_star=None, return_step_sizes=False):\n",
    "    \"\"\"\n",
    "    Train a model using SGD with either constant or Polyak step sizes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        PyTorch model to train\n",
    "    loader : DataLoader\n",
    "        PyTorch data loader\n",
    "    loss_fn : callable\n",
    "        Loss function\n",
    "    num_epochs : int\n",
    "        Number of epochs to train\n",
    "    step_method : str\n",
    "        'constant' or 'polyak'\n",
    "    alpha : float\n",
    "        Learning rate for constant, or minimum step size for polyak\n",
    "    alpha_max : float\n",
    "        Maximum step size (for clamping)\n",
    "    f_star : float or None\n",
    "        Optimal loss value for Polyak step size\n",
    "    return_step_sizes : bool\n",
    "        Whether to return step sizes and clamp rates used during training\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    losses : list\n",
    "        Loss history\n",
    "    metrics : dict\n",
    "        Training metrics\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=alpha)\n",
    "    losses = []\n",
    "    \n",
    "    # For step size analysis\n",
    "    step_sizes = []\n",
    "    clamp_rates = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        epoch_steps = []\n",
    "        epoch_clamps = 0\n",
    "        total_steps = 0\n",
    "        \n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            epoch_losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply step size logic based on method\n",
    "            if step_method == 'constant':\n",
    "                # Default step size is already set in optimizer\n",
    "                step_size = alpha\n",
    "                \n",
    "                # Apply the step\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Record the step for analysis\n",
    "                epoch_steps.append(step_size)\n",
    "                epoch_clamps += int(step_size >= alpha_max)\n",
    "                total_steps += 1\n",
    "                \n",
    "            elif step_method == 'polyak':\n",
    "                # Implement Polyak step size: alpha_t = (f(w) - f*) / ||∇f(w)||²\n",
    "                if f_star is None:\n",
    "                    raise ValueError(\"f_star must be provided for Polyak step size\")\n",
    "                \n",
    "                # Compute squared gradient norm\n",
    "                grad_sq_norm = 0\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grad_sq_norm += torch.sum(param.grad ** 2).item()\n",
    "                \n",
    "                # Avoid division by zero\n",
    "                if grad_sq_norm > 0:\n",
    "                    # Polyak step size formula\n",
    "                    step_size = (loss.item() - f_star) / grad_sq_norm\n",
    "                    \n",
    "                    # Clamp step size\n",
    "                    original_step_size = step_size\n",
    "                    step_size = max(alpha, min(step_size, alpha_max))\n",
    "                    was_clamped = (step_size != original_step_size)\n",
    "                    \n",
    "                    # Set the learning rate\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = step_size\n",
    "                    \n",
    "                    # Apply the step\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Record the step for analysis\n",
    "                    epoch_steps.append(step_size)\n",
    "                    epoch_clamps += int(was_clamped)\n",
    "                    total_steps += 1\n",
    "                else:\n",
    "                    # Skip update if gradient is zero\n",
    "                    pass\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown step method: {step_method}\")\n",
    "        \n",
    "        # Compute mean loss for this epoch\n",
    "        mean_loss = np.mean(epoch_losses)\n",
    "        losses.append(mean_loss)\n",
    "        \n",
    "        # Compute mean step size and clamp rate for this epoch\n",
    "        if epoch_steps:\n",
    "            mean_step_size = np.mean(epoch_steps)\n",
    "            clamp_rate = (epoch_clamps / total_steps) * 100  # As percentage\n",
    "        else:\n",
    "            mean_step_size = 0\n",
    "            clamp_rate = 0\n",
    "            \n",
    "        step_sizes.append(mean_step_size)\n",
    "        clamp_rates.append(clamp_rate)\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    train_time = time.time() - start_time\n",
    "    metrics = {\n",
    "        'train_time': f\"{train_time:.2f}s\",\n",
    "        'final_loss': f\"{losses[-1]:.6f}\",\n",
    "        'avg_step_size': f\"{np.mean(step_sizes):.6f}\",\n",
    "        'avg_clamp_rate': f\"{np.mean(clamp_rates):.2f}%\"\n",
    "    }\n",
    "    \n",
    "    if return_step_sizes:\n",
    "        return (losses, step_sizes, clamp_rates), metrics\n",
    "    else:\n",
    "        return losses, metrics\n",
    "\n",
    "\n",
    "# Cell 12: Linear Regression for 1(a) with Tests and Step Size Analysis\n",
    "'''\n",
    "1a: Implementation and Testing of Polyak Step Size\n",
    "'''\n",
    "# Generate synthetic linear data y = 4x - 2 + noise\n",
    "torch.manual_seed(0)\n",
    "N = 200\n",
    "X = torch.linspace(-5, 5, N).unsqueeze(1)\n",
    "y = 4 * X - 2 + 0.5 * torch.randn_like(X)\n",
    "\n",
    "# Wrap in DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "mse = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Run multiple trials\n",
    "num_trials = 3\n",
    "hist_const_all = []\n",
    "hist_poly_all = []\n",
    "step_sizes_const_all = []  # Track constant step sizes\n",
    "step_sizes_poly_all = []   # Track Polyak step sizes\n",
    "clamp_rates_const_all = [] # Track clamp rates for constant LR\n",
    "clamp_rates_poly_all = []  # Track clamp rates for Polyak\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    set_seed(42 + trial)\n",
    "    \n",
    "    # Train with constant LR\n",
    "    model_const = LinearModel().to(device)\n",
    "    hist_const, metrics_const = train_sgd(\n",
    "        model_const, loader, mse,\n",
    "        num_epochs=50,\n",
    "        step_method='constant',\n",
    "        alpha=1e-2,\n",
    "        alpha_max=1.0,\n",
    "        return_step_sizes=True  # Add parameter to return step sizes\n",
    "    )\n",
    "    \n",
    "    # Train with Polyak step\n",
    "    model_poly = LinearModel().to(device)\n",
    "    hist_poly, metrics_poly = train_sgd(\n",
    "        model_poly, loader, mse,\n",
    "        num_epochs=50,\n",
    "        step_method='polyak',\n",
    "        alpha=1e-4,  # Lower bound\n",
    "        f_star=0.0,\n",
    "        alpha_max=1.0,\n",
    "        return_step_sizes=True  # Add parameter to return step sizes\n",
    "    )\n",
    "    \n",
    "    # Extract loss history and step size data from returned metrics\n",
    "    loss_hist_const, step_sizes_const, clamp_rates_const = hist_const\n",
    "    loss_hist_poly, step_sizes_poly, clamp_rates_poly = hist_poly\n",
    "    \n",
    "    # Store for averaging\n",
    "    hist_const_all.append(loss_hist_const)\n",
    "    hist_poly_all.append(loss_hist_poly)\n",
    "    step_sizes_const_all.append(step_sizes_const)\n",
    "    step_sizes_poly_all.append(step_sizes_poly)\n",
    "    clamp_rates_const_all.append(clamp_rates_const)\n",
    "    clamp_rates_poly_all.append(clamp_rates_poly)\n",
    "    \n",
    "    # Log trial metrics\n",
    "    logging.info(f\"Linear Trial {trial+1}: Constant - {metrics_const}\")\n",
    "    logging.info(f\"Linear Trial {trial+1}: Polyak - {metrics_poly}\")\n",
    "\n",
    "# Average results\n",
    "hist_const = np.mean(hist_const_all, axis=0)\n",
    "hist_poly = np.mean(hist_poly_all, axis=0)\n",
    "avg_step_sizes_const = np.mean(step_sizes_const_all, axis=0)\n",
    "avg_step_sizes_poly = np.mean(step_sizes_poly_all, axis=0)\n",
    "avg_clamp_rates_const = np.mean(clamp_rates_const_all, axis=0)\n",
    "avg_clamp_rates_poly = np.mean(clamp_rates_poly_all, axis=0)\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(hist_const, label='Constant LR')\n",
    "plt.plot(hist_poly, label='Polyak Step')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train MSE Loss')\n",
    "plt.title('Linear Regression: Constant vs Polyak Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Step Sizes\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(avg_step_sizes_const, label='Constant LR')\n",
    "plt.plot(avg_step_sizes_poly, label='Polyak Step')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Step Size')\n",
    "plt.title('Step Size vs Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Clamp Rates\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(avg_clamp_rates_const, label='Constant LR')\n",
    "plt.plot(avg_clamp_rates_poly, label='Polyak Step')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clamp Rate (% of steps clamped)')\n",
    "plt.title('Clamp Rate vs Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 4: Clamp Rate Heatmap\n",
    "plt.subplot(2, 2, 4)\n",
    "clamp_data = np.vstack([avg_clamp_rates_const, avg_clamp_rates_poly])\n",
    "plt.imshow(clamp_data, aspect='auto', cmap='hot')\n",
    "plt.colorbar(label='Clamp Rate (%)')\n",
    "plt.yticks([0, 1], ['Constant LR', 'Polyak Step'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Clamp Rate Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create tables for clamp rate analysis\n",
    "epochs_to_show = [0, 5, 10, 20, 30, 49]  # Select specific epochs to display\n",
    "clamp_data_table = pd.DataFrame({\n",
    "    'Epoch': epochs_to_show,\n",
    "    'Constant LR Step Size': [avg_step_sizes_const[i] for i in epochs_to_show],\n",
    "    'Constant LR Clamp Rate (%)': [avg_clamp_rates_const[i] for i in epochs_to_show],\n",
    "    'Polyak Step Size': [avg_step_sizes_poly[i] for i in epochs_to_show],\n",
    "    'Polyak Clamp Rate (%)': [avg_clamp_rates_poly[i] for i in epochs_to_show]\n",
    "})\n",
    "\n",
    "# Display the table\n",
    "print(\"\\nStep Size and Clamp Rate Analysis Table:\")\n",
    "print(clamp_data_table.to_string(index=False, float_format=lambda x: f\"{x:.5f}\"))\n",
    "\n",
    "# Save table to CSV for further analysis\n",
    "clamp_data_table.to_csv('linear_regression_step_analysis.csv', index=False)\n",
    "print(\"Analysis table saved to 'linear_regression_step_analysis.csv'\")\n",
    "\n",
    "# Print and log final loss values\n",
    "print(f\"\\nFinal Constant LR Loss: {hist_const[-1]:.4f}\")\n",
    "print(f\"Final Polyak Loss: {hist_poly[-1]:.4f}\")\n",
    "logging.info(f\"Linear Final: Constant LR Loss: {hist_const[-1]:.4f}\")\n",
    "logging.info(f\"Linear Final: Polyak Loss: {hist_poly[-1]:.4f}\")\n",
    "\n",
    "# Test assertions\n",
    "sigma = 0.5\n",
    "assert hist_const[-1] < sigma**2 * 2, f\"Constant LR MSE {hist_const[-1]} > {sigma**2 * 2}\"\n",
    "assert hist_poly[-1] < sigma**2 * 2, f\"Polyak MSE {hist_poly[-1]} > {sigma**2 * 2}\"\n",
    "print(\"Tests passed: Final MSE within expected range\")\n",
    "\n",
    "# Edge-case test: Tiny gradients\n",
    "X_edge = torch.linspace(-0.01, 0.01, N).unsqueeze(1)  # Small inputs\n",
    "y_edge = 4 * X_edge - 2 + 0.01 * torch.randn_like(X_edge)  # Low noise\n",
    "edge_loader = DataLoader(TensorDataset(X_edge, y_edge), batch_size=20, shuffle=True)\n",
    "\n",
    "model_edge = LinearModel().to(device)\n",
    "hist_edge, metrics_edge = train_sgd(\n",
    "    model_edge, edge_loader, mse,\n",
    "    num_epochs=50,\n",
    "    step_method='polyak',\n",
    "    alpha=1e-4,\n",
    "    f_star=0.0,\n",
    "    alpha_max=1.0,\n",
    "    return_step_sizes=True\n",
    ")\n",
    "\n",
    "loss_hist_edge, step_sizes_edge, clamp_rates_edge = hist_edge\n",
    "\n",
    "# Plot edge case step sizes and clamp rates\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(step_sizes_edge)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Step Size')\n",
    "plt.title('Edge Case: Polyak Step Size')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(clamp_rates_edge)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clamp Rate (%)')\n",
    "plt.title('Edge Case: Clamp Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "assert loss_hist_edge[-1] < 0.01, f\"Edge-case MSE {loss_hist_edge[-1]} too high\"\n",
    "logging.info(f\"Edge-case Test: Polyak MSE={loss_hist_edge[-1]:.4f}, {metrics_edge}\")\n",
    "print(f\"Edge-case Test Passed: Polyak MSE={loss_hist_edge[-1]:.4f}\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1200 with 5 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdEAAASlCAYAAABHkZBpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3QWYlOX6x/F7GxbYpbtLGhQULDBQLMRuUeRge+z6e+z2nGN3oGIcsAMDEwUllBCUEJHujoXt+V/3+84zsbuzO53fz3UNM8zMzr6T+8z93s/vSXM4HA4BAAAAAAAAAACVpFc+CwAAAAAAAAAAKIroAAAAAAAAAAD4QBEdAAAAAAAAAAAfKKIDAAAAAAAAAOADRXQAAAAAAAAAAHygiA4AAAAAAAAAgA8U0QEAAAAAAAAA8IEiOgAAAAAAAAAAPlBEBwAAAAAAAADAB4roQAQtX75c0tLS5LXXXov1piAEF154obRv3z7Wm4EUFI3PEH19161bN2K3DwAAAP/odw4dmyG6Y+3//Oc/sd4UAAmAIjoQJC1q6R/cX3/9VZLVXXfdZd1Hc8jKyrIGdv/85z9l+/btsd68lLd06VK55JJLpGPHjlKrVi3Jy8uTgw8+WJ544gnZu3dvTLft888/t14/0fDzzz9bvyteXpOHHXaY1/umYcOGsv/++8vYsWOlvLxckvEzYvPmzbHeFAAAkGLmz58vp512mrRr184aC7dq1UqOOuooeeqpp7yu98ADD8hHH30kibCtyVqk9nV46KGHYr2JAOC3TP+vCiBQOkjSYqYWnxPZc889Z3WqFhQUyLfffmsN9mbPni1Tp06VVPDSSy/FXfHzs88+k9NPP11ycnJk5MiR0qtXLykuLraekxtvvFH++OMPefHFF2NaRH/mmWeiUkjXIvrdd99tde3Ur19f4kHr1q3lwQcftE5v2rRJxo0bJ6NHj5Y///yTLwsAAABhGP8dfvjh0rZtWxkzZow0b95cVq1aJdOnT7caSq666iqvIroWsE866aS439bFixdLenry9TqeffbZctxxx1U6f999943J9gBAMCiiAxGke9e10yCe7dmzR3Jzc6u9jg46GzdubJ3WzuezzjpLJkyYIDNnzpQDDjggSlsqViFbC8XRfkzjbSfIsmXLrOdAd9J899130qJFC9dlV1xxhfz1119WkR2xk5+fL+edd57r//q+2WeffeTpp5+We++9N+5eUwAAAInk/vvvt8Zbv/zyS6Umio0bN0qibqs2yCSj/fbbz2tsDACJKPl2cQJxnmds8ofXrFljdUPo6SZNmsgNN9wgZWVllYrGjz/+uPTs2dMqHDdr1swqxm3bts3reh9//LEcf/zx0rJlS2vg1alTJ6tQV/H2NGZCO5ZnzZolgwcPtorn//d//xfw/Tr00ENdcSKeZsyYIcccc4w1SNTbHjJkiPz000+Vfn7y5MkyYMAA6z7ptr7wwguuWAhP+v8rr7xS3nrrLesx0Pv25ZdfWpfp43fRRRdZj4mer5drXEZF2jWvl+n2NGjQwPq9b7/9tuvyXbt2yTXXXGPF1OjtNG3a1JpaqZ321WWia1f+9ddfL23atLF+TgukmqXncDiqvA86hVQfe7Ot5n54WrRokaxcubLGx/+RRx6R3bt3yyuvvOJVQDc6d+4sV199tev/paWl1utBH2v9/Xpf9HkvKiry+jk9/4QTTrC62XXniD4/GhWjXdSeSkpKrM7vLl26WNdp1KiRHHLIIfL111+7Hi/tQjf33xwMfZwOOugg6+dq164t/fv3l/fee6/S/fDnsdPXjXbeqw4dOrh+l773qqK3p+853XlUVYeMdgaZ941GNQ0bNszagaTbqbevr7lg6Otv0KBB1utGO9PV33//bc0m0LgXc3lNOz9effVV6/7NmTOn0mXaZZWRkWG9N9SUKVOs29euJ33s9LV67bXX+hX1M3fuXOtzST8z9LUWKt3Zo58bderUsb48jhgxQhYuXOh1HX/ei0uWLJFTTz3Vep70tacd/7pDaceOHSFvIwAASBz6PUTHhVXNQtQxhKHjJh1/vf76665xomfmuD/fKfS7i/6cNhHpGFrHITqmOfHEE62O8nBta1WZ6NVFoXiOd/V7hDY+6bhSx0j6neeTTz6pdrt0TK/XHzVqVKXLdu7cad2Ofkf193tVqMx3ka+++kr69etn/f4ePXrIBx98UOm6/o6jCwsLre8LXbt2tW5Pvzudcsoplb7HKp3Fa74vaRSj7vQAAE90ogMxoEU6Lc4NHDjQKih+88038t///tf6o33ZZZe5rqcFcy3A68BGc8i1A1k7WbWApsVp082q19HC4HXXXWcda8HqjjvusAY///73v71+95YtW+TYY4+1Ck/aDaADxkCZAZsOngz9nXq7WhC98847rWmIWvA74ogjrGKe6VjXbddCuw5gtBCrj8U999xjFeyqorf7zjvvWMVPLWbq4GrDhg3WQMkUWfVnv/jiCysuQ++zFuJMDIs+bjqg1KKyDqLmzZtnFfvPOecc6zqXXnqpVcDV29FBmj4+WkTWAp92TFRFC+U6aP7++++t36mDvEmTJlnFXB2IP/bYY17X19vTwd/ll18u9erVkyeffNIqBGrBXAvJRvfu3a0dDzpQr86nn35qFbe1EO2Pf/zjH9YXB30ctPCv91+jRvQ+fvjhh17X1S52vZ7erwsuuMD6EqEDeX1eddCsdCCqP6+3q8+rPuZacNZipxY99XW7du1aq6j+xhtvVNoenbaqj9+5555rzSwYP368NQieOHGitTMokMdOB8EakfK///3PetzNjAlfr6czzzzTKvCbOBxDi+r6uOp91UK0dgUdffTR1u3ccsst1pcefd1XNYj3lw729bb1tvQ1rM+f/l59jep90edIHxd9PZ588slV3oY+NzrbQHcsVZz+qudp0VszNtW7775r3b5+pujt68wR/fKzevVq6zJf9AuDfj7pFyPdQac7EEKhn2/62aCvWX3taBFft0Pz+/U1Y3ZQ1fRe1NeKbpfu/NFpz/oFVt9v+rrRPHzdeQcAAFKDzsicNm2a/P7771azhS86FjVj1osvvtg6T79zKX+/U3h2lOt1b775ZmusqM1OQ4cOtZoPqhsv+butvra/on/961/W7zcLw2uMo46rdAyo41Yt8Ov3J23Yev/9932OK/W7pF6m41ttasrOznZdpk0sOubS74z+fq+qjo5Jq1pDR8fFmZmZXg0TOl7XcaF+F9Hvkzpm1yYa/Z6h/B1H6/dMLcprHKneD91ubdrQ7yj6XJjXgdKdAXqZfo/R51iblvR7ho7fmUEKwMUBICivvvqqthw7fvnlF5/XWbZsmXUdva5xwQUXWOfdc889Xtfdd999Hf3793f9f8qUKdb13nrrLa/rffnll5XO37NnT6Xffckllzhyc3MdhYWFrvOGDBli/ezzzz/v13288847resvXrzYsWnTJsfy5csdY8eOddSuXdvRpEkTR0FBgXW98vJyR5cuXRzDhg2zTntuV4cOHRxHHXWU67zhw4db27VmzRrXeUuWLHFkZmZav8uT/j89Pd3xxx9/eJ0/evRoR4sWLRybN2/2Ov+ss85y5Ofnux6PESNGOHr27FntfdTrX3HFFdVeR5+zdu3auf7/0UcfWdt23333eV3vtNNOc6SlpTn++usvr/uQnZ3tdd5vv/1mnf/UU09Vur/6HFVnx44d1vX0vvlj7ty51vX/8Y9/eJ1/ww03WOd/9913rvP0Pup5P/74o+u8jRs3OnJychzXX3+967y+ffs6jj/++Gp/rz6mvv7EVHy9FhcXO3r16uU44ogjvM7397H797//bZ2n77ea6OuzVatWjlNPPdXr/Hfeecfrvn/44Yc1vr990eewW7du1ntGDwsXLnT885//tG5PX//qmmuusf6v73Nj165d1vulffv2jrKyMp+fIWeffbajZcuWruuo2bNnV7peVZ8LDz74oPUaXbFihdfru06dOtbpqVOnOvLy8qzn1/Ozo6bPCL2fvvTr18/RtGlTx5YtW7yeR31vjxw50u/34pw5c6zf9e6779a4XQAAILl99dVXjoyMDOtw4IEHOm666SbHpEmTrHFlRTrO0fFORf5+p/j++++tMYiOIXfu3Flp/PjEE0+EbVt1PF7VthqPPPKI9TvHjRvnOu/II4909O7d22vspmPegw46yPqOVh3dDr29Tz/91Ov84447ztGxY0fX//35XlUVM5b1dZg2bZrXfdfz3n//fa/vPvoc6Xdlw99xtH5v1es9+uijlbbLfGc129eoUSPH1q1bXZd//PHHVT4uAFIbcS5AjOjedU8adaB7ug3tFNXOSt3jrnvtzUE7grXrQLugDc/OB92DrtfT29O98zq1z5NOT6tqyl51NKpEOzO0Y1SnO2pciHZpmCx17b7QrgHtQtDuUbOtOnXyyCOPlB9//NGKptFuAO1K1a4IjZ4x9Pa0U7Uq2pmtXamG1la1o2L48OHWac/HRrtUNdbBxD9oZ4N23VY3FU+vox0U2jkdyKKZ2lGsnQ+etMtbt0kfG0/aoeLZ6dCnTx/Jy8vzer7NfaupC127YpR2Zfu7rUpnKVTcVlVx2qM+1iauR+nzrs+/57bqY6YdL/qcB8Pz9arRRPqc6e/0jO0I9LHzl3aWaDeLPi6eMSU6PVe7dzSWRpnpttrlrFNdA6XvO33s9KAzDLTzWrvszfRg/f3aEWV+n9L3tXZIacf7ggULfN62LiSrr1fPzwDtQtfHVbv0q3qc9b2o7xHt2tHXWVVxMHp7+h7S96x2JIUjk3PdunXW54N2+Ot0W8/nUT/bzOvTn/ei6TTXWR9VxfEAAIDUoeMI7e7W7uPffvvN6hzWcYyO52qKMQn0O4XnGMxzDK5d2Tq71nM8E4lt9Ryr3XrrrdaMvPPPP986b+vWrdbM3TPOOMP1PVAP+p1Mf4eO103UX1V01rDO5NSxsOf4XLu1tSPc8Od7VXV0jKu3WfHg+T1P6XdEz855Hffr465j1/Xr1wc0jtbnV++b58KtRsUYUb2vnrOszfehYL9zAEhOFNGBGNA8topxE/pH2zPrXAc8OnjTnDxTjDMHLf55LkKjBU0dbGiRSQcaeh2zcEvFrGAdrHlO1fOHDkB0kKPT3HTKo/5uzwKdKabqlLuK2/ryyy9bUwF1O/TnNMpBi+YVVXWe0hxqT5onrdENmllX8XeZnQPmsdGpljqg0kGW5ndrDEbFjHYdxOp0Ps2L1utp3ERNg6UVK1ZYA7yKhWwtlprLPWkmdUUVn29/6fOrdJDsD90Wjdap+PhqFIYOhoPZVo3f0edAswV79+5txdjodE5/aWFaX0f6PtDCqj53zz33XJW51uF87DwHyfo6NF9a9P2kg3EtrpsBte680YK0Rg7p4FszvHU6acUceV90h5O+Z3SnkUaS6KBf77eJm9HHXXdOVOTrNVTxi5h+YdPCudIdVBpno9vo+ZrUyBtTvDZrL+j9UhUfa52Sq0V+jYjR6b+Bfkb4Yu6Hr/tqdrb5817UzwLdGaSfKfo46hdDjeYhDx0AgNSkudW641/HhRpbpwVmHSNrcbu6hoRAv1MY+n3Ck44bdYztay2ecG2r0gK2jmE1tuXRRx/1imLUnQC33357pfuhEZtV3Q9PGqWiY16N8DPjXN1ObSLxLKL7872qOvoz2hxT8WC+2xj6eFYscOt3DmUeZ3/H0Zp7rtfzjIvxpeJ3DlNQD+U7B4DkQyY6EAPaxVwTLYxpAd0UyioyRXgd/GlhTAcgWtzUrl0tTmrnhA529HY8BZNvrIuQmuKfdmto4VTzrHWBUi3Qmt+h+euaD14VHXRpoS5QFbfX/C7dSaBF+6pol6sZSC1evNgqXmqOnu4MePbZZ628eC2OKu3a0E4DzQbXRWz0Pjz88MPW4NFXd3y4nu+Ki5D6Q59nLeBrsTEQFQejoWyrvh50UKqDbX3MtKipeeTPP/+8lTlZHc3H1y4cvQ19LrQYrDmDWqCuamGicD52hhbwtcitxWKdPaFZ6FpU9/yioI+XZipOnz7duly7n3UWhq5doOeZDEpfNItSvxhEgj4mut2aTamPoX6B0e5ts+NM6awPLbZrd5J+DnTr1s3aJu1E0sJ6xc8F7To/7rjjrOdU3yuaHxlt/rwX9fHX7TevPZ0Novn8+pzoIqMAACD16M5/LVLrQQuuWgTXWb2miFyVQL5TxHpbdV0YLbbreE3Hr55FYXM/dAFQbTCoiq9mJUPzwjUTXWfT6oxh/R06duzbt6/rOv58r0pkkfjOASD5UEQH4pQWw7WLVbsNqit8a/yHTtfTQpMWJg1dhDQStHiogzwd8OkASwddJm5DC7zVFQ51p4AW+LVjoqKqzvO180C7bbVI6E+RUguHWhzVgw5AdYEYXRRIuz90W5QWcnXhSj1op4YuYqjX8VVE18WB9LnR7hHPzl8TnaOXR5IWOLVrRqeFHnjggdVeV7dFB9c6W8B0Z5gFeXQHTLDbqt3N+hrQg3Zy62tPO4dNEd1X0V4H3Pq4a1HaMy5Ei+jB8ncHQcWCrS5wqvE4On1Vi+paXK9Iz9ODvh60yK87j3Qh1Jp2FtREH3f9IlKRv68hndaqBWUt8OsXHn1feH5xmj9/vrXgqi6ypNc1tDve12OoO+y0m1078vU2dZHSUJn74eu+6s45fY8a/rwXdSeeHnRRrZ9//tn6jNQdOPfdd1/I2wsAABKbLoxuIuWqGysG+p1CVYwy1AKrfocJtthe1bZWRZsGNB5PIzKbNWvmdZku3K60KSXYBg4dx+sYTMfEGpGi8TC33XZbUN+rQmU66z2fMx3TKrMYvb/jaP2OqlGB2lXP4qAAwoE4FyBOaZFPB3X33ntvpctKS0utAqjnXnPPveQ6qNHOgEjRQqJ2fWqXqNKcdh2k/Oc///HKmfacLmm2VQd3utq7Z+6xDpYq5oj7orehUw61GFtVN7b5XUp3LlTs/NDcPX2sdDClj2/FKAgt9Gund3WxHdqxqz/79NNPe52v3dg64Au2g10HfhrBUZObbrrJGsRqIVeL4RVpl7gWiM22qscff9zrOmYaqEZ4BKri46o7VrTDxfMxM4VR8zr1fP70MdLHz9CpmfqaCJav31UdHfzr9mqRWbtp9P3mSaduVuw8MbMs/I10qY4+LzqVV3eEGBprojtH9AtCxXzIivTLmh50FoC+F3RnlmdXUlWfC3ravC6qou8P3RmnnVE640S3L1T6hUwfN32cPZ8ffe9qJ7l5ffrzXtQdHvrZ50mL6TobJhzPCQAASByaD15Vl7DJJ/eM+9CxYlVjUn+/Uxjjxo3zilTUWYtaAK9p7B/ItlakjSbaJa4RdhqlUpGOl7TxQa9TVTG+qvtRkY6ltNNdmzPeeOMNa7zlOUPTn+9V4aLfEXVWoqHjP33cdTypcZSBjKP1+dXowIrf2RQd5gCCQSc6ECJdKFCLcBVdffXVId2uRrRccsklVlSBdh4cffTR1h507YDQKX9aDNPBji4UqJltOg1RuxS0QKmDn0gODHQ79P5pFrbe92OOOcYq5ukAsmfPnlZ3smava3SEDhq1Q10HZUq7lbV4pt2jl112masY3atXL+t++uOhhx6ybnfgwIEyZswYa6CksRUaYaMd4npa6WOmgy39Xdq1sXDhQut3aeFYO090MK07A/Rx1OmKWgzWn9cFc7TL1xctMB5++OFWh4YWgPVn9T5pxMQ111zjtRBmILRTXJ/3mhYX1dvXrmgd3OrPaKexPn6680Q7c/X1oZEXSrdNXxs6qDTRPzro1KKmTtfU+xEofbx1sK47T7Qj/ddff7W+RFx55ZWu6+hlSl+T2iGtX1S00KuPvRbw9TWjkSTabaxfCrQIH0iuuifzu/T50N+hr099jjw7nCvSDmf9nfozWoCt+EVBHx/dEaVrDejjrV+YND5FX8um8BuKW265xcox1/eMPkb6OOrv1Bkk+mVOv8zURJ93nbqrPKNclE7B1e3Wy/V9qNutt1tTrqPOetFpurrIlG7bDz/8YL22aqLPqVlo2ND78H//939WLIvels6aGD16tBWdowut6hoO+nmg9PGt6b2oXVH6GtNOeZ3+rF/w9LPOfAkGAACpQxeL1IXGdaym4x4zDjYzDE2uuRkr6rhCxyu6g17XWdHvEf5+pzB0vKad2nrb2siiTSo6ntSfDde2etICsM7O0+3SGZxvvvmm1+V6ezre1bG0bpc2F+i2aHe6bp8WmTVLXRczrYmOhXV8pjOO9XY8Z7D6872qJvqYVtx+peNVz5m1OsbT8aKOAfX36HdtvS+es1b9HUfrWFkL8Lqmjn7/0dhALbbrc6uPq87ABICAOAAE5dVXX9Uqtc/DqlWrHMuWLbNO63WNCy64wFGnTp1Kt3fnnXda163oxRdfdPTv399Ru3ZtR7169Ry9e/d23HTTTY61a9e6rvPTTz85Bg0aZF2nZcuW1uWTJk2ybu/77793XW/IkCGOnj17+n0fzTZt2rSp0mU7duxw5OfnW7dpzJkzx3HKKac4GjVq5MjJyXG0a9fOccYZZzi+/fZbr5/V/++7776O7OxsR6dOnRwvv/yy4/rrr3fUqlXL63r6u6+44ooqt23Dhg3WZW3atHFkZWU5mjdv7jjyyCOtx8t44YUXHIMHD3Ztj/6uG2+80dp2VVRUZP2/b9++1mOrz4uefvbZZ71+lz5nel887dq1y3Httddaj7f+/i5dujj+/e9/O8rLy/26D3p7ersVr+v5eNbkzz//dIwZM8bRvn1767HU+3DwwQc7nnrqKUdhYaHreiUlJY67777b0aFDB2tb9TG79dZbva5jtun444+v9Ht0mzy367777nMccMABjvr161uvuW7dujnuv/9+R3Fxses6paWljquuusrRpEkTR1pamtdr+5VXXrEeL31O9Gf1/VHV6z+Qx+7ee+91tGrVypGenm79nL73anLbbbdZ1+3cuXOly2bPnu04++yzHW3btrW2s2nTpo4TTjjB8euvv9Z4u/6+z5YuXeo47bTTrMdRX/v6mE6cONHrOlV9hhjr1q1zZGRkOLp27Vrl7S9YsMAxdOhQR926dR2NGze2Xiu//fabX59JmzdvdvTo0cN6Xy1ZssTnfTDPW1UH3Tbjm2++sV6b+nrJy8tzDB8+3No+w5/34t9//+246KKLrPexPl4NGzZ0HH744dZtAwCA1PLFF19Y4wIdS+pYR8fCOqbT8ad+T/C0aNEi6zuBjkN0jOI5jvTnO4V+n9Kf+9///meNoXVcqLel4+YVK1aEdVs9x7lmHOjr4Dne1XHlyJEjre3X+6HjYh27vvfee349nvodRh8DvV0d61dU0/cqX2q6D57Phfkuot9j+/Tp4/qu8O677wY1jlZ79uyxxvzme5A+Pvpz+vOe26ff4yrS83WsCwBGmv4TWNkdAMJPu6L/+OOPSlmDAMRnd5LGpeiCTrfffnusNwcAACAp6SxRnb2psz111hwiQ7vydQakzooEgHhEJjqAqNM4B09aONdMwHAsZAikitdee82KQzr//PNjvSkAAAAAACQ1MtEBRJ3m9Glmtx6vWLFCnnvuOWtxGl0wE0D1NBt8wYIFcv/991szOLRrBwAAAAAARA5FdABRp4tK6mIw69evtxbJ0cVkHnjgAenSpUusNw2Ie/fcc4+1GJUu7KQLQAEAAAAAgMgiEx0AAAAAAAAAAB/IRAcAAAAAAAAAIFXjXMrLy2Xt2rVSr149SUtLi/XmAAAAIAHoZM1du3ZJy5YtJT2dvpNEwLgfAAAAkRr3J30RXQfSbdq0ifVmAAAAIAGtWrVKWrduHevNgB8Y9wMAACBS4/6kL6JrJ4p5IPLy8mK9OQAAAEgAO3futAqyZiyJ+Me4HwAAAJEa9yd9Ed1M5dSBNINpAAAABIJYkMTBuB8AAACRGvcT8AgAAAAgIM8884y0b99eatWqJQMHDpSZM2f69XPjx4+3vqCcdNJJlbIo77jjDmnRooXUrl1bhg4dKkuWLInQ1gMAAACBoYgOAAAAwG8TJkyQ6667Tu68806ZPXu29O3bV4YNGyYbN26s9ueWL18uN9xwgxx66KGVLnvkkUfkySeflOeff15mzJghderUsW6zsLAwgvcEAAAA8A9FdAAAAAB+e/TRR2XMmDEyatQo6dGjh1X4zs3NlbFjx/r8mbKyMjn33HPl7rvvlo4dO1bqQn/88cflX//6l4wYMUL69Okj48aNsxYK/eijj3zeZlFRkZVh6XkAAAAAIiHpM9EBAAAqFvNKSkpivRmIsaysLMnIyIj1ZiSc4uJimTVrltx6662u89LT0634lWnTpvn8uXvuuUeaNm0qo0ePlilTpnhdtmzZMlm/fr11G0Z+fr4VE6O3edZZZ1V5mw8++KBVlAcAAKgK436Ec9xPER0AAKQE7XbVQt327dtjvSmIE/Xr15fmzZuzeGgANm/ebH0hbdasmdf5+v9FixZV+TNTp06VV155RebOnVvl5fq+NLdR8TbNZVXRQr7Gyhjaid6mTZuA7g8AAEg+jPsRiXE/RXQAAJASzEBau2E1eoLCaWp/sdqzZ48rw1sXs0Rk7Nq1S84//3x56aWXpHHjxmG97ZycHOsAAADgiXE/IjHup4gOAACSnnbOmoF0o0aNYr05iAO1a9e2jnVAra8Lol38o4Vwfaw2bNjgdb7+X7t7Klq6dKm1oOjw4cNd55WXl1vHmZmZsnjxYtfP6W14frHR//fr1y+C9wYAACQbxv2I1LifhUUBAEDSM1mI2okCGOb1QFam/7Kzs6V///7y7bffehXF9f8HHnhgpet369ZN5s+fb0W5mMOJJ54ohx9+uHVa41c6dOhgFdI9b1OjWWbMmFHlbQIAAPjCuB+RGvfTiQ4AAFIGUznhiddDcDSH/IILLpABAwbIAQccII8//rgUFBTIqFGjrMtHjhwprVq1shb+rFWrlvTq1atSJqXyPP+aa66R++67T7p06WIV1W+//XZp2bKlnHTSSVG+dwAAIBkwzkO4Xw8U0QEAAAD47cwzz5RNmzbJHXfcYWWOauTKl19+6VoYdOXKlZKeHtiE15tuuskqxF988cXWFOxDDjnEuk0twgMAAACxlubQhPUkplNB8/PzZceOHZKXlxfrzQEAADFQWFgoy5YtszpcKcrBn9cFY8jEw3MGAAAY9yNS434y0QEAAAAAAAAA8IEiOgAAQBzTuIyrrrpKOnbsKDk5OdZCjMOHD/dahDHSLrzwwohlUx922GFWHnao19OcQ3PQDpL9999fPv744zBvLQAAABAZjPvje9xPER0AACBOLV++XPr37y/fffed/Pvf/5b58+dbOdGHH364XHHFFbHevLjz6quvyrp16+TXX3+Vgw8+WE477TTrMQMAAADiGeP++B/3U0QHAAApR5eE2VNcGpNDIMvRXH755VaHxcyZM+XUU0+Vrl27Ss+ePeW6666T6dOnu66nCzmOGDFC6tata3VjnHHGGbJhwwbX5XfddZe1+OMbb7wh7du3tzL/zjrrLNm1a5frOu+995707t1bateuLY0aNZKhQ4daCz3qz77++utWd4fp+Jg8ebL1MzfffLO1Tbm5uVbHzO233y4lJSV+/17tdPnhhx/kiSeecN22foEIVv369aV58+bWNt17771SWloq33//fdC3BwAAgMTGuJ9xf7hkhu2WAAAAEsTekjLpccekmPzuBfcMk9zsmodgW7dutbpP7r//fqlTp06VA0dVXl7uGkjrwFQHkNqtcuaZZ7oGvWrp0qXy0UcfycSJE2Xbtm3WgPuhhx6ybl+7OM4++2x55JFH5OSTT7YGu1OmTLEG/jfccIMsXLjQWnBHOz5Uw4YNreN69erJa6+9Ji1btrQ6P8aMGWOdd9NNN/n1e3UQ/eeff0qvXr3knnvusa7fpEmTkB9jfQxeeeUV63R2dnbItwcAAIDExLifcX/SFdH1Qb311lvl6quvlscff9y1cur1118v48ePl6KiIhk2bJg8++yz0qxZs1hvLgAAQET99ddf1mC2W7du1V5PMxJ1IKurzWtuoho3bpzVufLLL79YGYFm0K0DXx3sqvPPP9/6WTOY1gHoKaecIu3atbMu1+4UQ7tUdCym3R6e/vWvf7lOa8eJDrx13OY5mK7u92qHig52taOl4m0HQ78QZGRkyN69e63fq9ukg3cAAAAgXjHuT4xxf1wU0fWJfuGFF6RPnz5e51977bXy2Wefybvvvms92FdeeaX1JP/0008x21YAAJD4amdlWJ0hsfrd/vB3+qd2i+gg2gykVY8ePayOFb3MDKZ1YGkGtKpFixayceNG63Tfvn3lyCOPtAbQ2rRw9NFHW7mCDRo0qPZ3T5gwQZ588kmr62T37t3WgFynlXqq7veG22OPPWZNR/3777+tcaRum+meAQAAQOph3M+4P2ky0fWBP/fcc+Wll17yesJ27NhhteM/+uijcsQRR1jh+jqV4Oeff/bKAgIAAAiUZvDp1MpYHPR3+6NLly7WdRctWhSW+5yVlVXpMdCuDaVdHF9//bV88cUX1kD8qaeekn322cfqcvFl2rRp1hjuuOOOs6ZszpkzR2677TYpLi72+/eGm3a1dO7c2foyoONGndoaqYE7AAAA4h/jfsb9SVNE1+ye448/3tp74GnWrFlWQL3n+TqtoW3bttaT54tOOdDsHs9DtK3aukdembpMPpi9Ouq/GwAAJAftpNDukGeeecZa6Kei7du3W8fdu3eXVatWWQdjwYIF1uU6MPaXDnJ1Zfu7777bGhjrdMsPP/zQukxPl5WVeV1fGxt0CqgOoAcMGGAN/lesWBHw/azqtsPhgAMOsJowdPoogASzcaHIX9/EeisAAIgKxv2JMe6PaRFds3Nmz54tDz74YKXL1q9fbz24Jjzf0Dx0vcwXvS2NfjEHzykO0fL35gK5d+ICeXmK7704AAAANdGBtA40dWD4/vvvy5IlS6ypmjpd8cADD7Suow0HOh1Tu0N0XDVz5kwZOXKkDBkyxBrk+mPGjBnywAMPyK+//iorV66UDz74QDZt2mQN1M3UzHnz5snixYtl8+bNVqODDp71ujqe02mduk1m8B0IvW39/cuXL7duu7puFd2muXPneh02bNjg8/rXXHONFRm4Zs2agLcLQAxNOF/kzVNFti2P9ZYAABAVjPvjf9wfsyK67jXRRUTfeustqVWrVthuVxcn1SgYc/DcOxMt2Rn2w1pcFpkpCwAAIDV07NjRGiAffvjh1mLrupr9UUcdZS3Q89xzz7k6ST7++GMrFm/w4MHW4Fp/TnML/aV5hj/++KM1RbNr167WwkH//e9/5dhjj7UuHzNmjDXNUwfnTZo0sdanOfHEE638QV2zpl+/flaHyu233x7wfdRFiXRaqXbP6G3rAN2Xt99+W/bdd1+vg0YC+nLMMcdIhw4d6EYHEs0O53e47b4/DwAASCaM++N/3J/m8De9Psw++ugjOfnkk60Hz9A9LvqCSE9Pl0mTJlkvhm3btnl1o+v0Ad27oE+ePzTORTvStaBeMfA+Umav3CanPPuztGlYW6bcdERUficAAPCtsLDQyvnTgVU4d94jeV8XsRhDIjQ8Z0mitEjkvqb26dNeFel1Sqy3CACQQBj3I1Lj/kyJEV0Jdv78+V7njRo1yso9v/nmm60YFg2k1z0up556qnW5TiXQvRRmGkO8cnWil9KJDgAAAAB+K/RY02rPllhuCQAAQOyL6PXq1bOmJniqU6eONGrUyHX+6NGj5brrrrMC9nVPwFVXXWUV0AcNGiTxrFaWXUQvoogOAAAAAP4r3OE+TREdAACkehHdH4899pgV7aKd6EVFRdZKtc8++6zEu2xnRA2d6AAAAAAQAIroAAAgDsVVEX3y5Mle/9eMGl2dVg+JJIdOdAAAAAAIXJFHEb1gcyy3BAAAwMWu9iIimehl5Q7rAAAAAAAItBOdIjoAAIgPFNEjIDvT/bAS6QIAAAAAwRTRt8ZySwAAAFwookdAjkcRvai0LKbbAgAAAAAJo3Cn+zRxLgAAIE5QRI+AzIx0SU+zT9OJDgAAAABBLizqIB4TAADEHkX0CMnJzLCOWVwUAAAAAIIoopeXiBR5dKYDAADECEX0COeiU0QHAACx8tprr0n9+vXDepsXXnihnHTSSWG9TQCosoiuiHQBAKBGjPsjjyJ6hHPRyUQHAAChDFzT0tKsQ3Z2tnTu3FnuueceKS0tlUTyww8/yBFHHCENGzaU3Nxc6dKli1xwwQVSXFwcsUE/gARVsfOcxUUBACmAcX/8o4ge4U50MtEBAEAojjnmGFm3bp0sWbJErr/+ernrrrvk3//+tySKBQsWWPdhwIAB8uOPP8r8+fPlqaeesr4clJXRbACghk70PXSiAwBSA+P++EYRPUIoogMAEMd0obrigtgcAlwkLycnR5o3by7t2rWTyy67TIYOHSqffPKJddm2bdtk5MiR0qBBA6vT49hjj7UG3VVZvny5pKeny6+//up1/uOPP27ddnl5uTW4HT16tHTo0EFq164t++yzjzzxxBPVbt8vv/wiTZo0kYcffrjKy7/66itr+x955BHp1auXdOrUyRpcv/TSS9bvmDx5sowaNUp27Njh6r7RLwyqqKhIbrjhBmnVqpXUqVNHBg4caF3fMJ0sH330kdXlUqtWLRk2bJisWrUqoMcYQBwW0TNy3IuLAgAQLMb9Loz7Q5MZ4s/DBxYWBQAgjpXsEXmgZWx+9/+tFcmuE/SP6wB0y5YtrmmfOnjWwXVeXp7cfPPNctxxx1ldIFlZWV4/1759e2sg/uqrr1rdIYb+X29HB9olJSXSunVreffdd6VRo0by888/y8UXXywtWrSQM844o9K2fPfdd3LKKadYA2W9XlV0IK0dNdqNMnjw4EqXH3TQQdaA/o477pDFixdb59WtW9c6vvLKK637Mn78eGnZsqV8+OGH1kBcu1p08Kz27Nkj999/v4wbN87qcrn88svlrLPOkp9++inoxxhAHBTRG7QX2byYTHQAQGgY97vOZ9wfGjrRI4ROdAAAEE4Oh0O++eYbmTRpkpUzaAbRL7/8shx66KHSt29feeutt2TNmjVWh0ZV/vGPf8j//vc/q9NDzZ492xqYakeI0gH43XffbQ22tSvl3HPPtS575513Kt2WDmxHjBghL7zwgs+BtDr99NPl7LPPliFDhliD8pNPPlmefvpp2bnTzj3WAXB+fr7ViaIDbz3oYHrlypXWQF8H9nr/tJNFu1MOOeQQ63xDvwDo7R144IHSv39/ef31160vATNnzgz5MQcQA4XOTPRGnexj4lwAACmGcX+nuBz304ke8YVFKaIDABB3snLtzpBY/e4ATJw40Rpc6qBRp16ec8451rTHb7/9VjIzM62pjoZ2kehUzIULF1Z5WyeddJJcccUV1kBYuzZ0WuThhx9udasYzzzzjIwdO9YazO7du9daBKhfv35etzNjxgxru9577z3rNquTkZFhDX7vu+8+q4NFf/aBBx6wpoHqgFcH2FXRQb5OM+3atavX+fpFQO+noY/B/vvv7/p/t27drKme+hgccMAB1W4bgDhTVipSvMs+3bCjfczCogCAUDDuZ9wfJhTRI1xEL06C4HwAAJJOWlpIUyujSQe7zz33nNW5oVMbdfAYLL0NzVLUwa1Ox3z77be9sg91+qR2ffz3v/+1Ojzq1atnLWakA2BP2h2iA1oddB9//PGVppBWRfMNzz//fOtw7733WoPk559/3uqAqcru3butgfisWbOsY09m2ieAJFPk7EJXDTvYx8S5AABCwbifcX+YUESPdCd6CZ3oAAAgeLqwTufOnSud3717dyktLbUGupovqDQzUfMFe/To4fP2dGqnLvTz7LPPWj+vg2pD8wT1tjRf0Fi6dGml22jcuLF88MEHcthhh1mZiTrt058BtaELImknSkFBgWuQr90nnvbdd1/rvI0bN1rTOn3R+6CLJpnuE73/27dvtx4fAAlaRNfOvXrObjUWFgUApAjG/RvjetxPJnqkM9HLKKIDAIDw0wV2NJtwzJgxMnXqVPntt9/kvPPOszo/9HxfdJA5aNAgazEizSzUBYs8b1MHppq/+Oeff8rtt98uv/zyS5W307RpU2ua5qJFi6zb0UFtVTQ78bLLLpOvvvrKGpj/8ccf1u/W4+HDh1vX0Wml2oGiU1U3b95sLRqkHSuazagdNDpwX7ZsmTUN9MEHH5TPPvvMdfs6iL/qqqusLxXavaKLJen9I8oFSOBFRXPyRHKd07fJRAcApDjG/fEx7qeIHiHZGSwsCgAAIkunZ+qiOieccII1DVMXIfr8889r7A4ZPXq0lXl40UUXeZ1/ySWXWB0qZ555ppW5qB0unt0pFeliQDqg1hxDHfhW7CpROqjVgfKll14qPXv2tBYamj59urUIkp5W2gWjl+vvbdKkiTzyyCOu+6eD6euvv97KfNQcRh3ct23b1nX7ubm51uBcMyMPPvhga8rnhAkTAn4sAcRREb1WvkhuY/s0megAADDuj4Nxf5pDH/UkpivA6sqvO3bskLy8vKj93pvfmycTfl0lNw7bR644vPJUDAAAED2FhYVWR4OuPF+rVi1JdZpN+O6778q8efMkkekCSddcc401jTPcr4tYjSERPJ6zJLDwU5EJ54m0PkDknAkijzhz0f+1USQzJ9ZbBwBIAIz7vTHuD9+4n070CMe5FNGJDgAA4oR2hvz+++/y9NNPW1MhASCuFO50d6LXqi+S5lxcjFx0AAACwrg//CiiR3ph0dLK0xsAAABi4corr7SmgerCQBWndAJA/MS55Imkp3vkolNEBwAgEIz7w48ieqQXFqUTHQAAxNE0yKKiIis7MCPD2eGZwHQxoWCndAKI80x0ZYroBSwuCgBAIBj3hx9F9AjJybRfoMS5AAAAAIAfijziXFQds7gonegAACC2KKJHCJ3oAADEn/Jy/i7DjdcDEKed6DnORb1yG9rHFNEBAAFinIdwvx4yQ74FVIkiOgAA8SM7O1vS09Nl7dq10qRJE+v/aWlpsd4sxIjD4ZDi4mLZtGmT9brQ1wOAeIxzcXaiE+cCAPAT435EatxPET1CWFgUAID4oQOmDh06yLp166wBNaByc3Olbdu21usDQBwW0YlzAQAEiHE/IjXup4geIXSiAwAQX7TrQAdOpaWlUlbGTu5UpwssZWZm0pkExGURvb73wqJ76EQHAPiPcT8iMe6niB7xTnSK6AAAxAsdOGVlZVkHAEC8FtHzKhTRt8ZumwAACYlxP8KNuasRLqLTiQ4AAAAAwWSiO4voZKIDAIAYo4geITmZGdYxnegAAAAAUAOHQ6Rop49MdIroAAAgtiiiRwiZ6AAAAADgp+LdIg7nd6ccE+diiuhbRcr5XgUAAGKHInqki+hlDPYAAAAAwK8ol/Qskaza9unchvaxo0ykcHvstg0AAKQ8iuiRXli0hFWAAQAAAMDvPPS0NPt0Zo67K53FRQEAQAxRRI8QOtEBAAAAhEUqRJkUVshDN0w3OrnoAAAghiiiR3ph0ZIUGPACAAAAiIyPrxR5tJtIwRZJjU50Z+e5YXLRCyiiAwCA2KGIHuFO9CI60QEAAAAEa8lXIrs3iKyeKSkT5+KpjllcNMl3IgAAgLhGET3CmejFpeXicDhivTkAAAAAEo1+jzBZ4NtXSlIr8hXn0sg+Js4FAADEEEX0CHeiK3LRAQAAkEyeeeYZad++vdSqVUsGDhwoM2f67pL+4IMPZMCAAVK/fn2pU6eO9OvXT9544w2v61x44YWSlpbmdTjmmGOicE/iXHGBSHmJfXrbCklqhdtrKKKzsCgAAIidzBj+7pToRFdFpeWujHQAAAAgkU2YMEGuu+46ef75560C+uOPPy7Dhg2TxYsXS9OmTStdv2HDhnLbbbdJt27dJDs7WyZOnCijRo2yrqs/Z2jR/NVXX3X9PycnJ2r3KW7t9Sgcb1+RGnEuOXlVF9HJRAcAADFEJ3qEZGd4dKKX0okOAACA5PDoo4/KmDFjrEJ4jx49rGJ6bm6ujB07tsrrH3bYYXLyySdL9+7dpVOnTnL11VdLnz59ZOrUqV7X06J58+bNXYcGDRpUux1FRUWyc+dOr0PS2ZOCRfRa9b3PJxMdAADEAYroEaJTUE0hnSI6AAAAkkFxcbHMmjVLhg4d6jovPT3d+v+0adNq/HldK+jbb7+1utYHDx7sddnkyZOt7vR99tlHLrvsMtmypfqi6YMPPij5+fmuQ5s2bSTp7N3mPr0tyTPRC8lEBwAA8YsiehQiXTTOBQAAAEh0mzdvlrKyMmnWrJnX+fr/9evX+/y5HTt2SN26da04l+OPP16eeuopOeqoo7yiXMaNG2cV2B9++GH54Ycf5Nhjj7V+ly+33nqrdbvmsGrVKknqOJeiHd5F9aTtRK8Y5+LsRC+gEx0AAMQOmeiRXly0iE50AAAApLZ69erJ3LlzZffu3VahXDPVO3bsaEW9qLPOOst13d69e1txLxr9ot3pRx55ZJW3qfEvSZ+bXnExze0rRWpXH3OT+EX0Cp3odUwnOkV0AAAQO3SiR6UT3XcHDQAAAJAoGjduLBkZGbJhwwav8/X/mmPui0a+dO7cWfr16yfXX3+9nHbaaVYciy9aYNff9ddff0lKq9h5vi2Jc9F9FdFNnEtJgUjJ3uhvFwAAAEX0KHSik4kOAACAJKFxLP3797e6yY3y8nLr/wceeKDft6M/owuD+rJ69WorE71FixaS0ioW0bUTPVkV+chEz8kTSc+yT9ONDgAAYoQ4lwjKycywjslEBwAAQLLQKJYLLrhABgwYIAcccIA8/vjjUlBQIKNGjbIuHzlypLRq1crVaa7Hel2NZ9HC+eeffy5vvPGGPPfcc9blGvFy9913y6mnnmp1sy9dulRuuukmq3N92LBhktJMnEtGjkhZkcj2JO1EdzjcnehaNPeUlmZ3o+9eL1KwWSS/dUw2EQAApDaK6BFEJzoAAACSzZlnnimbNm2SO+64w1pMVCNavvzyS9dioytXrrTiWwwtsF9++eVWd3nt2rWlW7du8uabb1q3ozQeZt68efL666/L9u3bpWXLlnL00UfLvffem/yZ5/4uLNq8l8iaWckb51JaKFJWXHUnuqrT2C6i79kc9U0DAABQFNGjUESnEx0AAADJ5Morr7QOVdHFQD3dd9991sEXLaxPmjQp7NuYVHEuLfrZRfRkjXMpdEa5pKWLZNetfLnJRa+40CoAAECUkIkeQSwsCgAAACBopmjcsp99rHEuGn2SbFxRLvV0FVrfRXSNcwEAAIgBiugRRJwLAAAAgNDjXPrYxyV7knNxTVNEryrKxcS5qGS87wAAICHEtIiuiwn16dNH8vLyrMOBBx4oX3zxhevyww47TNLS0rwOl156qSReJzpFdAAAAAABKC8T2bvdPl2vhX1QyZiLXlMR3RXnQic6AABIwUz01q1by0MPPSRdunQRh8NhLSY0YsQImTNnjvTs2dO6zpgxY+See+5x/Uxubq4kiuzMDOuYTnQAAAAAgReWndEttRuI1G8nsmudyPblIq37S1IpMkX0+lVfTpwLAABI5SL68OHDvf5///33W93p06dPdxXRtWjevHlzv2+zqKjIOhg7dzoXqYkBOtEBAAAAhLSoqC60mZkt0qCdyKrpybm4qCsTPa+GOBcWFgUAACmeiV5WVibjx4+XgoICK9bFeOutt6Rx48bSq1cvufXWW2XPnj3V3s6DDz4o+fn5rkObNm0kVshEBwAAABAUUzCu3dA+rt/WPibOBQAAILU60dX8+fOtonlhYaHUrVtXPvzwQ+nRo4d12TnnnCPt2rWTli1byrx58+Tmm2+WxYsXywcffODz9rTQft1113l1oseqkJ6d4Syil5XF5PcDAAAASPBFRXMb2Mca56K2J2MRfWcNRXQWFgUAACleRN9nn31k7ty5smPHDnnvvffkggsukB9++MEqpF988cWu6/Xu3VtatGghRx55pCxdulQ6depU5e3l5ORYh3iQk+WMcymhEx0AAABAEHEuphNd41xUMse51MqroRN9q73garq99hQAAEDKxLlkZ2dL586dpX///lYUS9++feWJJ56o8roDBw60jv/66y9JBDmuTnSK6AAAAACCiXNp4B3nokX08vIUi3Nx7kjQhVbNzgUAAIBUKqJXVF5e7rUwqCftWFfakZ4IcrLsDgk60QEAAAAEF+fiLCDntRZJyxApKxbZvUFSqoiekSVSq759mkgXAACQanEuml9+7LHHStu2bWXXrl3y9ttvy+TJk2XSpElWZIv+/7jjjpNGjRpZmejXXnutDB48WPr06SOJwJ2JThEdAAAAQAhxLhmZInmtRHastHPR8xKjscgvRTVkoptIl8LtIgWbRZrsE7VNAwAAiHkRfePGjTJy5EhZt26d5OfnW8VxLaAfddRRsmrVKvnmm2/k8ccfl4KCAmtx0FNPPVX+9a9/Jcwz58pEL2VhUQAAAAAhxLmYXHQtom9bIdJ2kCRdJ3qOj0x0VaexyNaldKIDAIDUK6K/8sorPi/TorkuMJrIXJ3opXSiAwAAAAghzkXV18VFpyTf4qI1xbl4LS66OTrbBCAx6Ofhsikifc9i0WEAyVtET3bZmaYTnSI6AAAAgGA60T2L6GZx0eWSVAr9jHNRdKID8PTR5SLLp9g74w68PNZbAyCJxd3CoskkJ9O5sChFdAAAAACB2Lu9cie6xrmoZOpELysRKSnwv4heQBE9afzxocict2K9FUhkO9eJLJ9qn57xvEg5UboAIociehQ60YlzAQAAABBUnItnJroV5yJ2JnqydaH7k4muiHNJDrs3irx3kcjHl4vsXBvrrUGiWvipiDjs07rg8uIvYr1FAJIYRfQIyiHOBQAAAECgSotFindXUUR3xrnsWC1SVipJodDZcZ9dVySjmrTRXFNEpxM9KWix0+H8nrxxYay3Bolqwcf2cb2W7m50AIgQiuhR6URnShEAAAAAP+3d5jyRJlKrvvv8ei1E0rNEHGUiu5Kke7fIjzx0rzgXOtGTwuLP3ac3LY7lliBR7dogsuIn+/Tpr4qkZdjZ6Ovnx3rLACQpiugRRCc6AAAAgOCjXOqLpHt8ZdPT9dskV6SLLgZYU5SLqmMWFnU+NkhcRbtFln7v/v+mRbHcGiSqRc4ol5b7ibQdJNJjhH3+dLrRK3E4RHatj/VWAAmPInoEkYkOAAAAIGCmUFzbY1HRirnomv+bTEV0fzvRNRNdC0JIXEu/Eykrcv9/85+x3Bokqj8+so97nmQfD7rMPp7/rsjuTbHbrng080WR/+4j8tv4WG8JkNAookehE724jCI6AAAAgAA70XOrKKI3MEX0lZJUC4vWWER3ZqKXFooUF0R+uxA5iz6zjzsMcWeis2MEgdAiuYlyMR3orfcXadXf3kEz69WYbl7cWf2rfbx2bqy3BEhoFNEjKCczwzouKqGIDgAAACDATPQqO9HbJmecS01F9Ow6Ipm17NMsLpq4dEHcP7+0Tx98tZ37r4vLFtA5jACjXHRh2hb9RBq0t89LSxMZ6OxG/+Vle4Fm2Mz7y+ygBRAUiujRiHOhEx0AAABAwHEuDaqJc1mZZEX0GjLRtUDmGemCxLTyZ7tors+ldqKbAii56AglysXQrnRdgHn3BpEFzuvAXURnByQQEoroUYhzKSt3SCmFdAAAAAAhx7m0T81MdOUqotNNmbAWfW4fdz1GJCNTpEk3+/+bFsd0s5BACjaLLJ/qHeViZGaL7D/aPj39WWKCKhXR+ewEQkERPQqd6IpudAAAAABhi3PZuVak1GNxxkRV5GcmumcRXYtoyaS8XGTGCyI//luSmhY0TR56t+Pt4yb72McU0eGvRRNFHGUizfuINOxY+fL+o0QyckTWzhFZNTMWWxh/ny/mM5M4FyAkFNEjKDvD/fCSiw4AAADAL6ZbMLeKOJc6TUQya2tFUmTHakmaTvScGuJcVJ3GyRfnos/1/84U+eImke/uE9m2XJLWht9Fdqy0X78dD7fPc3WiE+cCPy34uOoudM/PiT5nuLvRU53ulNWdDopOdCAkFNEjKDMjXTLS06zTdKIDAAAACKwTvUHV2eCmGz0ZIl0CinNpnFy5vut+E3lxiMiSr9zn7dogSct0oXc6QiQ71z7dpKt9TCc6/KFF4L9/sE/3PNn39QY5Fxhd+Gly7GwMRcFG75k/ZSWx3BogoVFEj1I3enEpRXQAAAAAgSwsWkWci2rgXFx0W6oV0ZMozmXOmyKvHG0vEKuLxZodIya7OBm5olyOc5/XuKu70EeXLPx5DWlXdbPeIo06+b5es54iHQbb1535kqS0ip8pvM+AoFFEj7CcLPshLip1Tp8BAAAAAH860ataWFRp0VVpATbRFZpM9Po1X7dOEiwsqjn2n14t8vEVIqWFIl2GiVzyg0izXsldRNfX6vp5Imnp9qKiRk49kfw29unNf8Zs85AgFnxUfZSLp4HObvRZr4kUF0jK2u3Ria7IRQeCRhE9Sp3oRXSiAwAAAPBn8UVT5KgqzkUlZZxLnv+d6Imaib59lcjYY+yinqSJHH6byNnj7ec50e9bTRZ/YR+3GeTOtjdci4uSi44adi7+Pdk+3fOkmq/fdZhIg/YihdtF5k2QlEUnOhA2FNGj1olOER0AAABADbRjsqzYvziXRO9ELy+3M3oDzURPxDiXpd+JvDBYZO1su2h+7nsiQ24SSU93LxibqPfNH4sm2sfdjq98WWNTRCcXHdVY9LlIealI0x4ijbvUfP30DJGBl9qnpz9v76BMRZWK6EmypgQQAxTRI4xMdAAAAAB+M13oGdki2XWqj3NJ9Ez04l3aem+fzvGjE910MCdSEUh3FPz4H5E3TrGf2xZ9RS7+QaTLUO/ruYrom5Kzg3j5T5Xz0A060RFQlIsfXehGv3NFsuuJbF5s78hKRcS5AGFDET3CcjIzrGM60QEAAAD4nYeuXehpaVVfx7UI5UaR4j2S8FEuGTkiWbVqvr6JPNF4hrISiXt7t4uMP0fku3vtnQX7ni9y0VfumQSekrkT/c+v7AUetYO4YcfKlzfpZh9vIhMd1byXln7vf5SLoTFR+55nn57xvKQk85mi6xEo4lyAoFFEj7DsTDrRAQAAAPhpTw156OYy07m9Y5Ukfh66H1EursckzXtnQ7xa/7vIi4eJ/PmFvZNg+JMiI572vbPALJqajEX0xZ/Zx/tU0YWumnS1j3eudi80C1TM1C8vsXe4mJkL/hp4sf25seQrkc1LJOXozlal+fCJNpMHiDMU0SOMIjoAAAAAv5mp9rk+8tCVdqibbvREjnQJtIiuGcdm50I8F5t1R8hrx4lsWyaS31bkoi9F+l9Q/c8ka5xLSaHIkm9856ErfU7rNrNPp2KREzVb8HHgUS6Gzn7oeox9esYLknLMZ4qZ8RHvOyCBOEYRPcJynEX0otKyWG8KAAAAgISJc6mmE90zF317IhfRA1hUNJFy0VdOs3cQaAH9kh9EWu1X88+YIrreL81RTxbLfhQpKRCp11Kk5b6+r0cuOnzR99LSb+3TPUYEdxuDLrOP575tR8OkCl1Mdfcm7/dYPH92AnGOInqUiuh0ogMAAACo0R4/i+gNkqGIbjrR/VhU1Mg1RfQ47kRf/Yt93HFI9TMKqsp71+xwzXxPFosmuhcU9ZXx79klqwtAInizXhd5tIfIunmSNP6cJFJWLNK4q0jT7sHdRofBdia/7tCZ84akjOLdIqV7vd9jZKIDQaOIHmEsLAoAAAAgrHEuKhXjXDwfl3iOc1n9q33cen//fyYjS6RW/eSKdNGOes2yri4PvVInOkX0kPw2XmTnGpG/nBE6yeCPj9xd6NXtiKmO/pzpRp/xokhZqaQE81mSlSuS38b7bwyAgFFEjzAy0QEAAAAEvrBoTUV004m+UhJWUShxLnFaCCovE1kzO/AiejLmoq/51V7UUBfBbX9o9ddtTJxLWJhO/l3rJCkU7XLvEAgmD91T79Ptz9UdK0UWfy4pwUS56Oem2QFJnAsQtMzgfxT+IBMdAAAAQMCZ6DV1oidTnIsWWf1lYk/iNc5l40I7MiK7nru7OpAi+pYl8d1lH4hFn9nHXY4Sycyu/romakJnVhTvEcnOjfz2JZuCLe4CabIU0a0olyKRRp1FmvUM7bayaosMGCUy5b8iM54X6XGid3a4fh5pF/+O1fbBdXqN/Xlz4BUi+42UhGJ2yNVp6v7s1Ex43dmnCzUjtenrXv9mNe5iz4ZCjSiiRxid6AAAAAD8Zqba17iwaFt30V0X6AwkVzxemOzvgOJc4nxhUZOH3mrfwItUdRolVye6KaJ3O77m62qnrHYJ6+tfdyS06BvxzUs6nnnyO5OkiP7Hh6FHuXja/x8iPz0hsuInkQ8utndYmWK55odXR4vvCVdE3+jeQef6m+LcYeDveg1IXl/eKjLjOZHBN4kccVustyYhEOcSrU70MoroAAAAAMIU55JTz32dRI10CSoT3RSaNydPHnqlOJc4vW+B2PSnXQxPzxLpfFTN19cCqelG159F4Dzz5JOhE71od/iiXIy8lu7bmjdBZOm3doSQKaDrZ2rzPnaG//5jRIbeLXLyCyJp6SLbltvF9kRiPkvqNrE7jc2sn3jdCYno+XWsXUBXfzrXrkCN6ESPUid6UQlFdAAAAABhinMx3ejauauRLs17ScLRDnplFtQMpFs7XotAmgMechE9CTrRFzu70DsM9n+WRJOuIit/Jhc9WJs9dj7sWp/4kR1LJomUFoo06CDSvHf4bvfo+0TqNbcLyvmtRPJa2YtuaoHdV4yQxr+snSOy/CeRvmdKwtjt0Ylu/q7oWhTxuqYEomPZjyKf3+j+//rf7Zif2gH8LU5RdKJHWHaG/UermE50AAAAANUpL3dHnNQU5+KVi57oneh5yRHnokUIUwBuNSDwn3fdtyToRF/kXLix23H+/4yrE50ielA8HzdHWeLvjFnwsX3c86TwRLkYeS1Eht0vctjNIvueJ9LpcJHGnavP4W93sH28YqokbCa6MrOXTGwYUs+WpSITzhcpL7UX29WdVBrxs2pmrLcsIVBEj7CcLDrRAQAAAPhBC+iOcv/iXFT9du7FGFMxzkUXRYsna2fbxw3a2/EJgdJc8EjFuehinVqU1MNf34qs+kVk4yJ70UR9HrRrOVx2bXBnw2sshr/MQqyesSTwX8UYnESOdCkuEPnzq/BGuYSi/aH2sXaiJ2IR3XweuRZmjsOdkIjOjt7/nWWPNVr1FznxKZH2ZgdRgr22Y4Q4lwjLznAuLEonOgAAAAB/olyy64pkZtd8fbO4qMa5JCKNFQi2iF5eIlK0K74WVA0lDz3ScS4/Pyky+cHqr6OvOz1o3r4e9Hnpe5ZInzMD6wS28nUdIi33syMyAu1E3/q3SGmxf+8BuPPDd65278TR/G5dXLTlvpKQlnwtUrrX3lEYD4vMth2kwf0iW5faj6t2sydqnIsiziX1lJWKvHeRHfukEUZnvS2SVdueZTHnTZEVP8d6CxMCnehR60QP4559AAAAAMlbRPenC90UyxI1zkW7yIPpRNfIhazc+Iw9Md3XwUS5RLoTfeNC92umWS+7OKmvM13409DFFXevtxcE1a76v78X+fASkfHniuwOoLC/6LPAo1xUvRYi2fXsKBItViLwPHQtljbtaZ/etVYS1oKPIhPlEizNim7RJ/E6dn3FudCJnnq++pe9kG5mbbuArusCqHYH2cf6ma8zllAtOtEjjE50AAAAAH4x3YH+Lu7lGeeiRel4KDb5q2SPncmqdIG/QLPDd6wUKdgi0rCjxAV9/MPVia55xdo1mBHGr+u7N9jHR94p0usU78tKi+yufp0ZoB3N1uld9kKKU/5rLxK6aobI8MdFug+v/vfoz/39g3262wmBbaO+fjXSRRdn1Xzvpt0D+/lUZorojfdxd0lrx3Qi0kKeK8plhMSNdoeIrPtNZPlUkd6nSdzT2RxmjQ1XJ7pzJg+Z6Kll1msiM56zT5/ygkjLft7jiHot7Z1uuiO445CYbWYioBM9wnKy7IVFyUQHAAAAUC1T2DBT7mtSv419XLzL3cWeKEwXelqGSHadwH62Thzm+moEiT5/GTkizXsHdxu6mGxaemTu26719rHpPvSUmWN3wesOCe221YzcfY4ROfxWkYu/tzvXtet/wnkiH15q5+r6onnrZUX2YnUmniUQrsVFyUUPiHm8mnS1O/oTORP9r29ESgpE8tvakUDxwmRHaxE9EZiZOvoZaxaqznUeE+eSOpZNEfnsevv04bdV3jGlOy9NNzqRLjWiiB5hdKIDAAAAiEici+aZ1m2WmLnohR556IF20LsWx4ujOBfTha75zcFmeadneCycuim8XfKmE928XvylOwTGfCdyyLV2gf+3/4k8d5DI0u+rvv7iz+3jbscHNzOCxUVD70Q3RfSdaxM7yqXHifE1u6btgXYuusYd6eK5CZOH3lgkPb1CnAtF9JSgO3ffOd+e9dXrVJHBN1Z9PVNEX0kRvSYU0SMsJ9NZRC+liA4AAADAnzgXZ7egP8ziohrpkkhceehBLAyqcS7x1olu8tCDjXKpdN/CuINAY1o0PsdXJ3pNtFN96F0io760O8x3rhF54ySRz2/yztAtKxH5c5K7iB4Miuihd6KbOBcz+yCRlJeLLPnGPt3jJIkrOkNIZ2UkSi66WVvB5KEr4lxSh+6ofvsse+e8LjA84hnfO6V0cVG16hc7Bgg+UUSPUhG9qJSFRQEAAJAcnnnmGWnfvr3UqlVLBg4cKDNnzvR53Q8++EAGDBgg9evXlzp16ki/fv3kjTfe8LqOw+GQO+64Q1q0aCG1a9eWoUOHypIlSyTlBBrn4pmLnmiLiwazqKjh6tbeHIdF9CAXFY3k4qKma1az5wONzvHUdqDIpVNFBoy2/z/zBZEXDnV34WsUgGYw6/PTZmBoRXTt9tVceNRMi17acerqRG+ZuAuL6vNetMNePFgLf/EmkSJdCpyd6HWdeeief1voRE9u5WUi748W2bzYnply1v/smWvVfe7q53bpXpF1c6O5pQmHInqEZdOJDgAAgCQyYcIEue666+TOO++U2bNnS9++fWXYsGGycaPzC3sFDRs2lNtuu02mTZsm8+bNk1GjRlmHSZOcHasi8sgjj8iTTz4pzz//vMyYMcMqtuttFhYWSmp2ogdQRG9giugJ1omu3dHBFtHjLRNdu7E3/B6mInqT8Me57F4fXJRLVXLqipzwqMh579vFmS1/ibxylMh397ljOLoea0fTBENzsDNri5QVi2xbHvr2poKtS0UcZSLZ9UTyWro70XVHledMgUTgikXqF96FdcOl/SGJ04nuinPxKKKbvy26w1ZjnpCcvr5DZMlX9mfpWW+7PxN80Q51K64oQV7bMUQRPcJyMp0Li1JEBwAAQBJ49NFHZcyYMVYhvEePHlbhOzc3V8aOHVvl9Q877DA5+eSTpXv37tKpUye5+uqrpU+fPjJ16lRXF/rjjz8u//rXv2TEiBHWZePGjZO1a9fKRx85i3JVKCoqkp07d3odkiYTPaBO9ESNc9keQid6nMW5rPvNzpzVInW+c7HXeOxEDybKxZfOQ0UunybS+3QRR7nIj/8W+XVsaFEuSrObNZJEaRclAoty0WKYzjjIqpOYi4uucRbRW/eXuNTWmR29aZHI7jDu6IoEsyPOs4hu/rbo55XZkYnkMnucyLSn7dMnPSvSys/FeVlc1C8U0SOMTnQAAAAki+LiYpk1a5YVt2Kkp6db/9dO85powfzbb7+VxYsXy+DBg63zli1bJuvXr/e6zfz8fCsmprrbfPDBB63rmUObNiEWL+MpziWgTPQEj3PJSYI4F1fhb//QF0KMRCe6KaSGs4huXqenvixy+mvu16x2PnY8LLTbbdLNXahEYIuKKn0N5iXo4qKmE71ViDM6IkVnwTTtkRgdu1UV0TXSQ6Ny4mknJMJn5XSRidfZp4fcItLrFP9/1rW46Aw7DgZVoogetUx0iugAAABIbJs3b5aysjJp1sw7FkL/r4VwX3bs2CF169aV7OxsOf744+Wpp56So446yrrM/Fygt3nrrbdat2sOq1atkoS3Z1sIcS4rE2t6fiiZ6KZbO16KQOHKQ49UJ/ruDeGLc6lKz5NFLp8u0n+UyPH/Fcl2FumC1djZic7iooF3ohsatZNonehWLNIf4VmgN5ISJdLFFNHreiws6rkT0vy9QfKY8l+R8hKRHiNEhtwc2M82621HQumaBOZ9iEriMGQqudCJDgAAgFRXr149mTt3ruzevdvqRNdM9Y4dO1pRL8HKycmxDkklmDiXvNbaemovCKZFk4oFk3hVGEImuqsIFC9FdI9O9FBFpBN9fWQ60T3pbQ9/PDy3RSd6YEzsjelET9Qiui5oqNnuuu35rSRutTtYZOaLIsvjvIi+u4pOdKWzRnascs98QnLQnehmh+7B19jRWIHQNQh08ei/vrEjXVr0ichmJjo60aNVRC8rt6avAgAAAImqcePGkpGRIRs2ODtbnfT/zZv7LtBp5Evnzp2lX79+cv3118tpp51mxbEo83OB3mbSKS0WKd4VeJxLZrZIXqvEy0V3daLnBV9E10xffdxiaccakZ1rRNLSRVruG/rtufLeN0egiF7D4nLxwlVE/1OknGa0aunjs/kv+3QTjyK6K85lXQJGucRpHrpnEV1t/EOkIE525Pkb5+K5kzZedkIiPLb+be+Iz8gRadYruNtw5aLH+Q6iGKKIHqU4F0WkCwAAABKZxrH079/f6iY3ysvLrf8feOCBft+O/owuDKo6dOhgFcs9b1MXCZ0xY0ZAt5k0XejaVR5od7ZZXHR7IhbRg+hEr1VfJC0jPgpBJg+9aU+RbOdijmHpRA9nnMv6yMa5hFuD9iIZ2fbsCu2YhW87VtqPkz5eZn0EVa+lfbwrgTLRXWsLxGkeulG3iXtHz8qf43fnis8iupnJQyd6UjFd6C372TvXQ9lBpJ3oNAFXiSJ6lDrRTTc6AAAAkMg0iuWll16S119/XRYuXCiXXXaZFBQUyKhRo6zLR44caeWVG9px/vXXX8vff/9tXf+///2vvPHGG3LeeedZl6elpck111wj9913n3zyyScyf/586zZatmwpJ510kqQM16Ki9UXSnQXigHPRE6iIrl3kwRbRdZq6q5tyc/LkoXtmoltd9vaOppDt2hD5OJdw0liBRp3t0+SiV888Po262I9bUnSix3kR3bPYGK+RLoXb7WicKuNcnJ+dxLkkl3C8f3Q2lXay69/VzUvCtmnJJKZF9Oeee0769OkjeXl51kE7Tb744gvX5YWFhXLFFVdIo0aNrIWITj311ErTPONddoZHJ3oJRXQAAAAktjPPPFP+85//yB133GHFs2jW+ZdffulaGHTlypWybp27cKMF9ssvv1x69uwpBx98sLz//vvy5ptvyj/+8Q/XdW666Sa56qqr5OKLL5b999/fyk7X26xVq5akXCd6IIuKGqYDNSHjXIIoonvFnsS4E331rPAuhKiPR3pW+LrRi3a7Y4ISpYjuGU1CLnrgi4omYia6FvvDGYsUae1NEX2qxKXdG92zdip2JRPnkpzCsUM3M8f9tyxeZ1mk8sKirVu3loceeki6dOli5YVrN8uIESNkzpw51iD72muvlc8++0zeffddyc/PlyuvvFJOOeUU+emnON3bVwXtrNFudF1YlE50AAAAJAMdl+uhKpMnT/b6v3aY66GmMfM999xjHVKWmVofSB56pTiXlZJwRfScIDLRPSMJwhl7EqiyEpG1c8JbRE9Ls7vRtfipcQyhLrC429mEllVHJKeeJAxXLjqd6AEvKupVRF9vR3sEushgzGKReojk1JW41+4Q+3jD7/YO0GA+tyOpYGPVXeiKOJfkU7LXfi2GY1aU5qKvmGpHuvS/MCybl0xi+kk6fPhwOe6446wieteuXeX++++3Os6nT58uO3bskFdeeUUeffRROeKII6zsxVdffVV+/vln6/JEzEUvKnFOpwEAAAAAT2ZqvekSTPY4l1A70evEQSFowx92HrXeBxM/Es5Il3DsIHAtKpogeegVO9FNkRhV08VXq+xE11kHaSLlJYnRcZwoi4oa+n7SCB1xiKyYJnHH5KHXbVr5MuJcks+630TKS+11L/LbhHZbrsVF6USvStzsjiwrK5Px48db0z011mXWrFlSUlIiQ4cOdV2nW7du0rZtW5k2zfeHlC5QpAsReR7ipYhOJzoAAACA6jvRQ4hz2b5KpDwBGnc067u0MMQ4F1NE3xz76fOaQRvOTl/X4qLOQlhYFhVNoCgXz85q7URngbuq6ePiqxM9I8v9OkqExUXXmFikBMhDT4RIl92bvHfIVRnnQhE9KfPQdTZTKNocIJKeaS/qnEiz21KliK4LB2n3eU5Ojlx66aXy4YcfSo8ePWT9+vWSnZ0t9evX97q+Zi3qZb7owkUa/WIObdqEuBcmjLnoGukCAAAAAD4z0YPpRM9raX/p1a5T03kczwo9Gp2CjRgxmeixjHMxhYtwF/5cee/h7ERPsCJ6o04iaRn2AquJkusdi9xrndGhOeJVzYRIlMVFdcffmtmJs6io0f5Q+1ijL+KN2QFXp4pOdIroySecC1xn1xFp0c8+TTd6/BXR99lnH2sxohkzZshll10mF1xwgSxYsCDo27v11lutKBhzWLVqlcRaTlaGdVxEER0AAABAVczU+mCyddMzRPJbRy/SpaQwtO5gzzx03fZgmA7LWEZVuAoXYcpDj0QneqIW0XWBu4Yd7dMsLlo104WuM1GyqliEuV7LxOhE37hQpKRAJLueO8YnEbRzdqKvny+yd7skTCa6me2kn53M8kgO4Z7J0e5A+3hF4qxHmTJFdO0279y5s5V5rl3kffv2lSeeeEKaN28uxcXFsn2794fRhg0brMt80Y72vLw8r0Os0YkOAAAAIGILi3ouLrotwkX0rX+LPNJB5NOrY5eH7hXnsiV2z9fWpZHJcQ5nJrpZWFSzchONKaia3G94M4uu+io8mx0n8d6JbhYVbbVv8DvVYkE7/Rt2EnGUi6yMs3X7zGdH3WoWFi0rEinZE93tQvjpjlKNXtEZKS33De8OonjM+0/1InpF5eXlVq65FtWzsrLk22+/dV22ePFiWblypZWZnkhyspwLi5YmQD4hAAAAgOgznYzBxLl45aJHOMN06fd24WXJV8HfRlESFNFN55/GaAT7nEV1YdEE60T3KqLTiR5UEV1jnlS8x+F45jknGpOLHm+RLhr14yvOReM6MrJjG+mya4PI13eKTH44Nr8/mZj3T5PuwcejVdR2kL0w8ZYl7tcSLJkSQxq9cuyxx1qLhe7atUvefvttmTx5skyaNMnKMx89erRcd9110rBhQ6uj/KqrrrIK6IMG6ROaOOhEBwAAABCxOBfVwBTRI9yJvuEPd2FOs81r5YUW5xJqET1WmeiRinJRxLnYmnTzLhbDm69FRY16LRKriJ5Ii4oa7Q4RmT1OZPlPcZqJXkUnui48qZEuuuiw7oSsH8V1BLVo/9MTIjNfdHfB73ueSH6r6G1DsglnHrqh45BmPUU2/G7novc8KXy3neBiWkTfuHGjjBw5UtatW2cVzfv06WMV0I866ijr8scee0zS09Pl1FNPtbrThw0bJs8++6wkGncnOkV0AAAAANXFucR5J/pGj/Wrtvwl0mq/2MS5eGail5eLpKcnfuGiUhE9HHEuziJ63UQsotOJXi0Tc+OzEz0BFhbVHXHm+U3kTvR1c4PfqRjJInpVcS5mJ6R+Npidt5Gmn/nTnhWZ9oxI8S7vyzQWiyJ6/OShG+0Ooogeb0X0V155pdrLa9WqJc8884x1SGSmE50iOgAAAIBKdHE3U8wINc4lkpnoup2mEz2kIvrO8MW5OMrseJhgO/iDoUX71bMiV/hzddmH2Ileste9w6JeAmaiN+piRwroe0N3KJgdJ7CfV7ODpLE+TpKYC4uunaNvYpH8ton5GtUFnRu0F9m2XGTVDJEudkNoTBXtdnd5V9WJ7vl3JtJxLsUFdte5dp/v3Waf16y3yBH/EvnlJZG/vrHX2egwOLLbkazKy0TWzI7MrCgtoutzp0V0xG8mejLKziTOBQAAAIAPWvAoKw6tE93EuexcLVJWIhGhi5cVOQvganOQCz6GoxM9M0ck25n/WhDlXHTdeaCF+8za9pT3cDOFr9K9dhEq1EVFM2uJ1KovCSc7171gLt3oVXeha2SLr/eRifDR4qXuUInnRUVbh3lx3mhHuqjlcZKLbna+6edTdt3YFNFLCkWmPyfyRF+Rb+6yX4ONu4qc/prIJT+K7HOMvSir0iI6grNxoUhJgf23UB/fcGp7kH2s3ehmzRZQRI+GnEx7hWk60QEAAABUYgoZ6Vn2om/B0AXkMnJEHOUiO9dIRGzwiHJRm5eEWEQPMfqgTowWFzVRLi33FcnICv/t62tAC2ChdqObPPS6zewc5ETkykWniF51Hno1hTOdnaE7UOI5Fz2SMzqipX2cFtF1Z5yv973ZWRvuOBfdgfvrqyJP7Sfy5S32tmin/skviFw+XaTnye7orYYd7WOK6KH/LdIZYel23TFsdGaItaPDYc+ygIUiehTQiQ4AAADAJ88ol2CLnVqYMF27kYp00Y40z7iRkIvoIXSie27Hns3Jk4eu9DUQjlz0RF5UtFIuOouLejGPh688dPM6ci0u6nwtxBONh4r0eymauegaTaNRKvGeh+7ViR7GHZC/vy/y9ACRidfYO3LzWomc8LjIlb+K9D2rcpHXVURfFr5tSDWumRwRev9opItaEWcL58YQRfQoyHEW0YtKy2K9KQAAAADijcmKDTbKxTBF9O2RKqI789C7D3cvCKf54IEykTAhF9E9FheNptW/RiaDtqou+1CK6CbORTvRExWd6FUzUUrVFdFVnjMXfWcc5qJrPFTBRpH0TJEWfSVh6eeuZrrr+gzx0LG7e6N7dlKNOyDD1ImuO1Tfu8jOhtcdgMc8LHLVbJEBo3zP1vEsousOFcTf36J2zh1E5KK7UESPAjrRAQAAAPhkChmhLo5pctG3r5SIFtG7HiuSkS1SWmgXwoLtRM8JNc6lceiF5kBpp+nGP6JQRG8ShjgXZ4SH6UZO6E70IPP3k5XZqdC4hiK6qxN9XfwWAJv1EslyxhclqniKdDGfh9UtxBvuOBczK0njha7+TWTQpSJZziih6nY+pKXbmd6m8I/A/o6aGSmtItyJrrMsQlmfI4lQRI8CMtEBAAAA+BXnEor67SIX56ILxemCmqpFH/eicMFEuoQtziUCkQQ10WKC5s5rVEFeizgvom9wZ9smKpP5vXu9e8ZGqtNFQs17vMZOdOdrdGccFtHXzEr8KJeKkS7xEHuh3f2qbtPofXaadTj0/ervuh6Z2SL5bezT5KIHbs1sO69c/+5XF90TCt3RkddapLzUHb2U4iiiRwGd6AAAAAB82rMtPJ3orjiXlZFZyFDjCnQbtbu1cWf7/C3BFNETOM4l0hm0FeMWQopzMQuLJnAmui4+qzssFN3oNmtnlkOkVn33zpYaO9HXxvGiiElQRDexF1rYjHXHrivOpYkfcS5h2jFlZiTltw7s51hcNAx/iyI4I0rXVWh3oH16xbTI/Z4EQhE9ipnoFNEBAAAAVGI6bEPtRHfFuayIXJSLRi/oF+tGXeKgEz0MheZ4zEP3LICFsmiqqxM9gYvoXpEu5KJXWlS0poWIw72wqL7Xnj9E5Id/h3Y7ZSUi635Lnk70Bu2dHbslIqtmxkmcSzVFdLPDNlxxLjucnehmh5e/Uq2IrtFt4cqhd/0tivD7h8VFvVBEjwIWFgUAAADgkylkhNyJ3t6df6zxK5Eoojft4R2zYRY49FdZqUjxrvAU0etEuRNdF78z3bPRKqKHJRM90Yvo3YJ7rSUr8ziY92A0FxZd8pXI+vkiPzwcWmF+w+/2mgr6GWCioRKZ7syIl0iXggA60Yt3i5QWhS/OJT/QInqH1Cmi69/k5w62d0KFOlvB+lsUpR26ZpaF/u0rLZZURxE9mnEuZXSiAwAAAPC1sGiInejayZ7lzKPdsVoi04ne0z5u7OxENznp/ipyRrmEY2FRV5xLlDrRNbJg9waR9EyRFn3ju4iuxQ6zcyaR41w8i8V0olfuRK+JZye6Ft5C/t3O50A7rn95JfjbMQVAjXJJT5KylCk2Lo91EX1TzZnouvMizV67Lyyd0ebvjck491cqdaLrehoaq6Q7HBZ8HNptbVtu/93TBb6b95aIf/7qThfd6bV2jqS6JPm0SpBO9BKK6AAAAAAiFOei3ZCuSJflErE4F9Wos7vb2WScB1JEz8q1F5YLhXm8CqLUiW660LVokVU7sr+rTqPQ7psW+1V6Vuivq3jpRDfF41RnOtHN41IdMwuhrCg8xVLP5+DXscHPeEmmRUWN9oe4s6p18ddY0Jgc8/ekTtPq/1aYmU+hzuQpL3PPdAg6zmVZeHbyxLPVHjE/c94M7bbM+6d5H5HMHIkoKxedSBeDInoU0IkOAAAAIOJxLqp+O3dRIlx2b3JGBKSJNHUW7mrXdxdpAulGN3nooXahe8a5lBREp2jl2T0baZ6d6MEUl0wRXYuoNeVmxzvTca0zAYp2S0rTOCSzDoE/cS5aYDPRHeFYXNR0outsDO2E/f294G4nmRYV9SwIa+d/WbH7/sUqD127zGv6e2JeF6HmoutnjS46rb8z0OgozZJXRTvClxUerzyz8rUYvWVp8LflihUbEN1ZFit+llRHET0KcjLtaTJ0ogMAAACIWJyLatHHPg5nEWfjH+782mxnXEywkS7hWlTUFOK10zpauejRyqD1jKrR2AzzmAWTh163mSQ87aQ3O2xSPRddYxz0NaEzOfyNzqhnctGdr4lgFe8R2eZctHjQ5fbx9OcC38mjndLmM6NVf0kaVsdujCNdXHnojWuOyTEzVEItXpsoF83fT3dGxPhLZ/SY7vVkjnTR94gpopuIpblvJcbfItX2QPt41Qx75kEKo4geBdkZzjgXOtEBAAAAeCovFyncbp8OR+xGm4H28cppErE89IpF9EAKmyb6JRxFdC1amW7KSBfRdfG9db9Fr/svq5ZIdj3v7tJAmEUfE31R0Yrd6Kke6bJ5sTtOyd8s8TyTix5iEX2LdsA77PfcodfZhXxdIHT5lOCiKBp0cMcWJVukS6xiL0weenWLihpmp22on52uInqAUS6plIu+fYW9g0N3+g692z5v7tvBFaT1b9H6edHdCaURZvr3qGin/Z5PYRTRoxnnUkoRHQAAAIAHncbuKA9fnIt2pqWli2xf6c6pDXceutHIFNGd8RLR7kRXpogeTKE5EOt/t3OltfBkij6RVieEhVNNnEsydKJ7FdFTfHHRQBYVrbS46Low/e5u9mdVv3Pc3eiBWD0rul20sSiia9dxsHnxoUZv+VtENzttQ41z0YUyVX7r4H5eZzglexHddKHrgtQ9T7L/juj7cel3gd/W+vl2ZJDOVjJxOJGmMwzaDrJPp3ikC0X0aC4sWpra0x4AAAAAVGCm0mfVCc8CYbXy3B3jK6dLWIvoTXt4n28ymYMqoochE13ViVInuiuDdv/oZYx75qIHyhRMTQE10bG4qPesj8YBFNE1ZkOFulPN7MAwBfyBl9rHi78ILN852nnO0aQzBDR6SHe4mY77mMS5BFBE3+NciDTUTvR8OtFrLKK3OcD+O9/nTPv/c94I/LY83z/RXO+CxUUtFNGjgE50AAAAAD7zgcMV5WK0GeTOLw3HQoameFYpzqWzfbx1qR1LE5NO9MbRL6JHi+lED6qIbhYWTbJOdBNnkqpc3eB+LCoayU50E+fU5Wg74mXmi/5nQ5vicjItKmpoUTOWkS7ms6Kucw2BqMa5BNuJngpF9BnuIrra91z7eNHnIgVbgsxDj/L7x3NxUUcQi10nCYro0VxYlCI6AAAAgCoXFQ1DlIthpl2HoxNdCxulhXb+sWYYe6rfTiQj2758xyr/bk8zVRMxzsVVRO8fgyJ6EEWu3c5M9LpJkoluOq91Yc2SvZKStHBlZn2YQnYgRfRQFxat2ImuBl1mH895078FcPXzROND9HOjeYV4qGTR3iwuGmBWfNTjXBrFSZxLkhfRiwvcs7laH+DOGG/Rz14keP47wf0tivZOqJb7imTWsne6pPACzxTRo4BOdAAAAABR60Q3RXTNTi3aHdptbTRRLt0rL2SoOakNOwUW6WIKbTnhinOJQie6FqZ0YThJi95CbiHHuZhO9CQpomtnba369voBqZqLrnEsxbtE0jMDy+UPx8KiupihKXJ6FvA7Hm7/v3i3XUivielCb94nPPFV8ch07Gr2u78zdGKxsKgrzmVrbONczM5ZLeabv4fJZM1sEUeZvfCq52O073n28ew3/O/s9vpbtJ9EVWa2eyZWCueiU0SPAjLRAQAAAFRpbwQ60bUjUKfW6xf3Nc6p3yEvKlohyqVipMuWAIvo4e5ED2bxTX9ogemz69wZ8OHa7kgW0TWCx/xMshTRNSajzUD79OSHUzNOwOw80AJ6Rpb/P1evpfs9osXwYGz5y96BkZPvvVitPi+mG33G8yLlZX5GUSThoqKeuejpWSIlBSK7wrS4c6CZ6P7Eubg+O0PYAamvJ/NZE2ycS05d92tq6zJJ+igXo/dpIhk59o7qdXP9uy3z91xng0Tzb1GlXPSfJVVRRI9iEZ1OdAAAAABVx7mEsRPdK9JlRpiK6D6iFwJdXDRSRfRgIk9qsvR7kecOEln4id39O/hGiSqT9x5oEd0qpDlE0jLct5EMjrrbLk7++YXIwk8j8zviuTjvWlQ0gDx003GsxTq1yxnzE0qUS8XFDHWRRP382r7SXmQ0VRcVNXQHh5kpEO3YCxNrZWboVMf8zQklzsVEuWTWDm02VTJHurhe8xWK6LrjvPtw+7Q/szji4f3TjiI6RfQoxrmUO0RKyyikAwAAAIhgnItXEX1aeIroTXtUfXmjLoEVi1xF9PoSt3EuJYUiX94q8sZJdgSGdpaO/lqkz+kSVcHeN1Mo1W7UihE8iUwjhQ6+2j79xU0ihc58/XDRRf4eaisy4Tw7OiXeuBb29Mgk94cWvc2MhGAjXar73Vm1RfpfaJ+e/lz17yuNmFLRjEWKBV10NZCdi+Gg0TGuOBd/OtEbuj+TdfZKqFEuFXeuBFVET7JOdN0pt2qmfdrMpPFkIl3mv+vfWg9mJkesFuVtNcDeObtztch2P9dBSTJJ9Bc1/hcWVSwuCgAAACCicS6eRXTtXAu2QKJFSit/tbo4ly7uuIeAiuh58RnnokW+Fw8Tmf6s/f8Bo0Uu+TH6+bOhxLnsTrI8dE+Db7ALbloM/u7e8N3uthUiH15qL3yrXe5PHyAy86Wa40li0okeYBHda3HRtSF2ovtY0HT/f9izNVZMFVn3m+/3li6kqO/ZBu0lqblm6ESxE71wu0h5qf+d6NaOTGfhO9gs8h3OTnTN+w5Fww7J2Ym+Zan9N14X5NTFRCvqMEQkv439d3HRZ9Xfln4Wab56LOOQcuqKtOjjHVOTYiiiR7ETXRHpAgAAACDicS7aOa6Ld+qCf2Zx0EBtXOjOVPbVKa9d2kqLmv50Boc9zqWxuwgUSsFTf/anJ0ReOkJk00K7k/Ocd0VOeFQku47EhGcneiALFJpu47pJWETXrufjH7VPa5FbF28MVVmJyPv/ECnaIdJyX7tApQt4fn6DyNhh7tkYsebqBg8wziUci4u6frePIrp2Ivc4yT49/fnq85ytbtYQupYTQSyK6GZnm362+rNoa0am+3M42EgX7UhWWggORbLGuZhCs36u6MKcFelMoX7n2qfnvFH9belrST+XsurYs3Jipe2B4ZnllqAookdBRnqaZKbbfyToRAcAAADgYooX4Y5zSc9w56YGm4u+4ffqu9BV7fru6ICautF1art2+oa1iO583HTRw73bg7sNnZb++okiX98hUlYsss/xIpdPE+l6tMSU6bK37lsAnaK7TCe6xwKQyaTT4XYOt+a+f3p18DMtjO/vF1k901408/TXRS6aJHLcf0Sy69kzOV4YLPLN3f7FLURyZ5uZbRFoJrrn4qLBFNF1J4N5b1cXJTPocvv49/fcr8Eq85yTeFHRYNeKCIfdG71nsATy+RlsHJZnnEs4iujbkizORT9XanrN9zvHPv77B3tGjM/bcr5/dFaU/n2PlbYmKm66pCKK6FHuRqcTHQAAAICLKY6GuxPds2NsVZBfdjcusI+b+chDDzTSRbvitSAcziK6LuJnbiuYQtC8d0WeO9iOodAOvxOfEjnrLf/iECJN75uJ+Qkk0mX3eu8Ij2Q07AH7sdkw3x29E4y/vhWZ+ph9+sQnRRq0swtUB4wRuXKmvfCfRmRMfVTk2QPtxWZjwXSC57cNbmaE6UTfGUQRXbuD9THIriuS39r39Vr3txdP1B1Rv471nees10t2jQOcoRMOgeShV4rD2hpanEt1rwt/NOjgjqIq2i1Jo7o8dEM/czTWRXcKzn3bjzz0GL9/2jiL6DpDx8wsSyEU0aMkx1lELyqNo0w1AAAAALG1Z1tkMtE9v7gH2zFmYiya9fJzEb0aogvMF+70LDsjNlxMpEsguei68+K9i0Q+cMZ4aKfgpVNE9hsZX1ETweSiuxYWTdJOdKU7OY5yZqJPfrD6Dk5ftFv6w0vs0wMuEunpjCMx8lqKnPmmyFlv253c2iWri81+cIlIQZgy+P21OYQoF88dKsF0ops8dO2urum9Megy+/jXV+yFRA19vMz6Ci1jsL5AtOmOPfP+2xKlbnTzGVE3gE50s/M26DiXMGWi64wmU9BPlm50/XtnItHaHFD9dfc93z6e+5bv6C7XTqgYz+So18y500MXTXV2x6eQzFhvQKp1ohPnAgAAAMBSWmxnnEYizkVpnEtahl3o0MiS+gHk1mr0yoYFNce5qEamiL7E/zz0cBaqtfiydanIhPNFsnPtBQ71fuuxdhXrwev/mfa2ase2nj/kZpFDr7czguONtYPgz+CK6Mm4sKinfc8T+e1/Iit+svPLz3nH/9eVFqq0gK6Pa9Oedme7L92OF+kwWOS7+0RmvCAyb7zIkq9Eht0v0vfs6Ox0MZ3owSwqanYIBLuwaE156J66nyiS19rOyv79fZF9z/UuAGohXgumqUDvq3ZW62dNNLqHXZ3oTYLoRA81ziXETnQT6aLboTMfqlqEM9Gs0fUaHCL124nUrWF2QPcT7DipHatElv1gR1Z5Ktplr9WhTExbLLU90N7ZobnoXYZKKqETPUpyMu3MouIyiugAAAAAtKhsMrzTwhdv4kljH1r0Ca4bXYsj2qGtBWdTJA81/9fEGoT7vpoClXaib19pF2G0+1OLDprrvu43kbWz7Xxa/dK/fIpdQG/YSWT0VyKH3RyfBfSKi4v6Swt3yd6JrrR4fcLjIhnZdlF7wUf+/+zPT4j8/b1IVq7I6a/aC5ZWJ6eeyLEPi/zjW5Fmve3O3Y8uExl3osi25RLXi4p67lDRTnTdQRZMJ3p1eeiGvo80CkdNf879uzwXFU0V/s7QCXsmetMgMtGD6ETXnaJmjYtQO9GTcXFRf6JcDP386X2afXrOm5UvXzvHjkLTBVzjYedo29TNRY/TkUISd6KXUEQHAAAA4FG40KJypBYK0/xS/QKuueh9Tg88ykU7XzOz/cv/1W5w7fBNT6+hEz1Pwkq7iDWGpbTQLjRofnN5mX3s0GPnwTrtvEwLr9rtF0y+dDzHueh9M8W0ZM5EN7SofMh1Ij88JPLFzSKdjqh5J40Wt751RsEc+4h/xWFD87wv/l5k2jMikx8SWfajyITzRC6dKhFlCrHBdqKb14K+RzTKKJCZL4F0oit9L/7wsJ1Xr7ME2h/isahoKhXRu0a3iG4ihgJZz8HEiAVTRDd56LXqi+TUlZAlbRG9higXz5k1GoO08FP7PeoZ8RYveegV11vRnWM6o66mMUISoRM9SrIznAuL0okOAAAAwDOHNhJRLpU6xmYE9nPawe1PlIvS6epalNYCnU5H9yfOJZy0aK+Ln7bazy7S6X1uf7BIxyF2UbXLUSL7HGPHcvQYIdLrFHv6fLwX0D0LYv4W0bWQpjsLdHZDILEOieyQa0UadbY78L+5248s/NH2Y9TrNLtwFcyCr4dcI3LZT3a+//r5IpsiWCjVhRbN+yqQgn/FTldTlDNxP/4oK3XPMPH3d+vnmcbcmG503bG2ZnYKFtH9jLkKlwLnzrOaokOqinMJJhPd5KGHI8rFq4ieBJno+po3hW9/i+gt97WjpcqKROa/F5956J6vbc3T17/5OtMrhVBEj5KcLNOJzsKiAAAAADy6/8zibpEsomtR3BSx/bHRzzx0pV30Go1SU8HITP2PRHRNsnJ1ovu5kKXG1Jifi9eImnDLqmXHuqhfx7o7QCvSaJFP/imyY6VIg/YiJzwWWp55o072jhq16FOJGLMwpebjh7LDTRdHVbsCyEXXxUC1qJdZW6R+W/9/buCl9vGiz0T++tp+7+ttaJEw1TrRtyy1d0ZELc6lSXTiXMKZh55snei6ELDGoWXV8f81r59FZqeeZ6SLfm6ZOKR42QmVlubuRteItBRCET1K6EQHAAAAUKkrNtKd6JqfqgVDXeDMRCoEEufiTxHdM9LFFPyqy4CniB5EJ7qfRfRdzjz0ekmeh15Rh0NF+ukilg6RT68WKSupfB0tsC/8xO4eP+3V8MQKdR9uH2sEQ6SYLnd/41R8yXNGuuxcF3geunaeBhI5pTE7nY+yn4+J19nnteyXOjt2lC6wqjsOykvsnRFRi3MJoIhuduAGs7CoKaKHIw/ds4iuHe4leyWhrXLO/NLZUYG85vucYX8+rZtrz3BROgtFZ9no+iQt+krcaJuauegU0aMkJ8v+g0MmOgAAAACvKfSe2aeRykUP5MtuSaG7o9zvIrofi4uaTvicMGeiJ7NAM9FNJ3oq5KFXdPR9djyFzqKY9rT3Zet/F/nyVvv00Lvs4lY47HOcHZ2j6w5srybKKNSu1lAWFa1qcdGAFxUNooA/6DL7eOfq+MpzjhaNmTI7FyMd6VJcIFJSEKM4lzAV0fXvoNnBGo3FeiNp1S+BRbl47jTd51j79Jy3vKNcmvWqeQHkaGrr7ETX9VYCXaw4gVFEjxI60QEAAABEPc5FtR0YWBFdi3aaGa1FDX+LsY1M/u+ffmSi1/fvNmFHeARSRDd513VTrBPdzOg4+n779OSH3dnKWmB8b5QdS9LlaJFBl4fvd2rB0hSTNLokEszCnsEuKloxzmXn2iAWFQ3id+t6BJ7bHC95zsm4uKj5fMisJZIdwCKfZhaUzorSHO+g4lzaSNgiQhp0SI5IF9OJ3sb5tzcQ+55vH8+bIFJaFH956IZ2xevrTWcxbPlLUgVF9CghEx0AAABA1ONclCnyrZlVdcxFRRuceeia5epvZrRZRK+6L9OFZKIH3YmuUTj+PHemiG66jlNN37NEOgwWKd0r8tn1dofkFzfZRcy6zUVOes7uEA4nXaRWLZooEbEpTJ3oJs4lkIVFQ+lE18+OQc5s9HjKc07GIvpuZxG9TtPAcv7NDlxHuTtuK1ZxLsmSi647x02sWTCFb935pDuvdXbA4i888tDjrIieme2eXZJCuegU0aMkh050AAAAALGIc9FuUC1cl+wRWT+v5uvrIqSBRLmoRp3dURGmWO6zE504F7/payMt3f/cYs3OTeUiuhYQdZHRjByRpd+KfDDGuUhfmsipL7kz5sOpm7OIvuIn/7Pr/VVa7C4ohqsT3d+FRbUzOdQ89j5n2YU2jb0JZ7E1UZidi5GOczGd6IG+vrUQml3Pe6euP3TnlJnREK44l2QpopvOcf2bGMwOcs1Q73u2ex2HtXPjdydU29TLRaeIHiXZmc4ieilFdAAAAABaFN0WnSK6dt66ctGd08zDuaioql3f7oKsrhvdVUSnEz2g587kFvsT6WLyrrXrOlU16iQy+Eb79Px37eMhN9kd6pHQoJ1I8z52N692joaTFhM1WkkLnXnOIni0FhbVBQ21oz8j27k4cRCyc0XGfCdy9v8C65BOFlGLc9kYeB66kdvAO17Mr9+32Y5H0p1TZudMOCRDET2UKBdj3/Ps42U/2I+zjhHMYxNP2jpnudGJjnDLcRbRiyiiAwAAAPDsRI90nItnLrouAlYTXZjRLGQWTMHIVxG9iDiX0BYX9aPLeVeKd6IbB1/t7txue5DI4Jsi+/u6D7ePF34amUVFtaM51CK0KXbqzhh/ooFMjIyud6DdsQhcw052oVk/6wv8mEkScpxLEDMtzE46f2a6eO5gMWsvaDd7uCRDEX31zNDjV3RHoH5uGa0GxOdOqNZ6H9Ps58v87UlyFNGjhE50AAAAAF7M9PlILyyqXJ3o0+2p+NUVY6xIkDSRpgFGODTuXH3XJZ3owTGFsZqK6Pq8mjiXVFxY1JMW9s56S+Sgf4qc8Xrki8CmiP739yJFu8J3u6HGqVQslqZn6QvFv1x0Vx56iDEyqUw78eu3iXw3uivOJYhOdPP3x+zU9cfONeGPcvEsomveui6qGW36fvNnB5MvZaUiq2eF3onu2Y0ej3nonjPQzIw1f3bQJwGK6FGSk5lhHdOJDgAAAMAqeJrp85GOc1Gt9rMLaFpk3bbc9/U2OqNcGnYQya4T2O/QjlVf+b96f00RPYdM9IDkNvYvzkVfT+XOAlCqF9FN9/bR9wYXcREoLXJr13FZsciSr8LfiR7qoqImGsjMUDCxP34taBqGAn4qi0akS0hxLg2D6EQ3RfTWEla6/Vl17Gik7Sslqqb8V+SZ/UUmXhv8begsrpIC+29cqO+bHiNEsuvGbx56iuaiU0SPcic6RXQAAAAkumeeeUbat28vtWrVkoEDB8rMmc7py1V46aWX5NBDD5UGDRpYh6FDh1a6/oUXXihpaWleh2OOOUaSmi7yaWXKRinOJau2SMt+NX/Z3bAg8Dz0SsWiKoropYV2gVHRiR5knEsNRXRTGNWO43BGLKBmGrXginSZGJ7b1B1P638Pz6KiRr0WARTR6UQPC9fOxUgW0Td7f1YEFeeyNfA4l7zW4X8fxSLS5a9vRL691z6tCxGv82MB7ury0LXorTutQpFTV+SUl+woqo6HS9xqa3LRKaIjIpnoZbHeFAAAACBoEyZMkOuuu07uvPNOmT17tvTt21eGDRsmGzc6O+EqmDx5spx99tny/fffy7Rp06RNmzZy9NFHy5o1zk42Jy2ar1u3znX43//+J0nNFCy0O9x0m0WrY6y6addmUdGmwRTRnXEuW5eKlFdoHip05qGnpUfv/iYLUxjbU0Ocy25nREcqLyoaS6aIrp3oJYWh354u1rdpoUhGjkibAyQs/F1cVAv4dKKHb0aEr52L4bJ7Y/BF9JDiXMJcRDezoKJZRN+2QuT9f9gxRzm6g9ch8s2dwd3W6l/s49Zher92O07kiNtCL8hHUhtnbM2630SKCyTZxfEzkVzIRAcAAEAyePTRR2XMmDEyatQo6dGjhzz//POSm5srY8eOrfL6b731llx++eXSr18/6datm7z88stSXl4u3377rdf1cnJypHnz5q6Ddq2nRh56g+gtGObKRXd2y1Vlw+/Bd6LXbyeSkW13nZtORcMV5VIvvgsCiZyJzqKisdVyP3vxzuLdIst+CP32pj5mH/c7J7gFI6tbXHTX2uqvt3OtSPEukfRMd2cw4jjOZVMIneihxLmEORNdRbMTvWSvyITz7L/H+v79x9f2ju2l39mHQK1yzrIL106vRFC/jT0jwVEmsvpXSXaMXqKEIjoAAAASXXFxscyaNcuKZDHS09Ot/2uXuT/27NkjJSUl0rBhw0od602bNpV99tlHLrvsMtmypfov9EVFRbJz506vQ0IxXX/RiHKp2DGm3a1VTd0vL3NHOARTRE/PsHOhq+q6ZFHRMBTR/YxzoYgeG7pzqNvx9umFn4Z2Wxrjoh3tOnPjoKskbPztRDefA/p+JhooPEX07SvCM0OhIl0I0/w9CSkT3blj1x+68Gck4lyiWUTX2RafXS+yfp697sSZb9jRRftrV7qIfH1H5RlV1dFFubctsxfljucM80homzq56BTRo4SFRQEAAJDoNm/eLGVlZdKsmfeihfr/9eudURI1uPnmm6Vly5ZehXiNchk3bpzVnf7www/LDz/8IMcee6z1u3x58MEHJT8/33XQmJiE4lpUNIpF9LpNRBp19p527kmLFtpFnpUr0sA5pT7YSJctFYroRRTRQ89ErynOxdmJzqKisY90Wfy5SFlp8Lfz0xP2cY+TRBo5d0xFMxPdFeVCHnrItLCtMSG6WGYkCsOmg1x3uASzSHWgcS76ujbRUfkJXET/dazI3Lfsx+20se77MvhGe2HQ9fNF5r/r/+2tnumOP0q1v3NtTRHdv2aKREYRPUroRAcAAECqe+ihh2T8+PHy4YcfWouSGmeddZaceOKJ0rt3bznppJNk4sSJ8ssvv1jd6b7ceuutsmPHDtdh1aoK8SGJEucSzU50r0iXab6jXJp2Dz5yxdfioq5O9PrB3W4q87eIvstZ2KITPXbaHWwXMrWwWd3aA9XZtlzk9/ft04dcE9bNcxXRNa7Fr0VFyUMPmcZ1uXLR/4xcHrp2U+tsoKAXFvUzzkV3wOgOAY09CSY+xt8i+vaVoe2Iqs6qX0S+uNk+PfQukY5D3JfVaSRyyLX26e/u9X/2gFlUNJWiXCouLqo75yP1nMUJiuhRwsKiAAAASHSNGzeWjIwM2bDB2fHqpP/XHPPq/Oc//7GK6F999ZX06dOn2ut27NjR+l1//fWXz+tohnpeXp7XIaGYrr/a9WPUMTbD96KiwUS5GI18FItcmegJ9jzFA1Pk0oxqzfCtqROdInrsZGSKdD02tEiXn5+284U7HSHSom9YN0/yTCb6ejvOwhc60cPL187FcCjYGHyUi1ecy9bqXxOVolxaRmZ9C93Rk1lLpLy08toa4drp8M5IkfISkR4jRA76Z+XrDLrMXj9Af/8vL/lfmE/VInrT7vZsC10PwuyMT1IxLaLrFMz9999f6tWrZ+UfatfJ4sXOD2unww47TNLS0rwOl156qSRsJ3oZnegAAABITNnZ2dK/f3+vRUHNIqEHHujsRKrCI488Ivfee698+eWXMmBAzVmhq1evtjLRW7Rwdk0mI5M/G804F88i+trZIqVF3pdtWGAfN+sV/O2bjsstFXaAkIkePH3MtOuzpm50E9FRlyJ6XES6LJzoX1GyYq7ynDfs06YbNhKd6CUFIkU+1pHQbaYTPbwi2YluPhOCXXzW/A3SonLRrpqvv9MsKhqhCDUtzJs4sXBHumiX9Luj7IV1G+8jMuKZqhf2zqotcsRt9ukf/131GiKeSovtv6mea4+kkvQM986DJM9Fj2kRXbMOr7jiCpk+fbp8/fXX1gJDRx99tBQUFHhdb8yYMbJu3TrXQQfhCduJXkIRHQAAAInruuuuk5deeklef/11WbhwobUIqI7fR40aZV0+cuRIK2rF0Izz22+/XcaOHSvt27e3stP1sHv3butyPb7xxhut7wTLly+3CvIjRoyQzp07y7BhwyRpxWJhUaWZ6NrZrNnn637zEefSI7TbNwXdQo8inTlNET1wWuQxsQl7NvsufO4ynehkosdUp8NFsuqI7FwtsnZOYD878wX7vdmqv0j7Q8O/bdm57vegr8VFtVO3cLudFW3ezwhPJ3rFtSLCGedSp2nwr4nM2v7noptO9PxWEjGRykX/5k6RFVNFsuuJnPmmSE4939fte7b9t1B3AE99tPrb3TDfft9qlFOqvmfaOnfQBxtjlSBiWkTXTpQLL7xQevbsKX379pXXXntNVq5cKbNmzfK6Xm5urjU91BwSbqqmRxGdTnQAAAAksjPPPNOKZrnjjjukX79+MnfuXGtcbxYb1fG8Nr4Yzz33nBQXF8tpp51mdZabg96G0niYefPmWZnoXbt2ldGjR1vd7lOmTLEiW5LW3hh1omtB1pWL7vFlVzsQt68IPc5F42lMMcezG51O9NCYLlNfneha9CxzziygEz22tIu1i3Ph5EUT/f85fQ/OfNE+ffA1VXfIhnVxUR+56KYLXbuBs9xrVyBMcS6Bzk6oScEm+ziUfHLPSBe/41wiWUQ3nejLwnebus7AtKft0yc9K9LE+ZxU11099G779IwX7Yx2X1Y5FxVtfUDk3rfxrq3HuCLcr/E4EleZ6LogkGrY0Hsg+dZbb1mZiL169bK6Wvbs2ePzNoqKimTnzp1eh3iQnWEv8MDCogAAAEh0V155paxYscIae8+YMUMGDnRPX9bFQLU5xtDucofDUelw1113WZfXrl1bJk2aJBs3brSK7Xr9F1980VWUT1qmWKGda9HWdmDlIvrGhfax5sCG2h3v6rqsqoieeA1R8VVEdxbMKjJd6LpwK4XP2OvmEenir1mv2e8TXVeg2wkR2zT34qLrashDJ8olrEXh9Ew7M9rELoWL+UyoG0IRvXYARXRXnEtrSZhOdP379vFV7h1UPU707+e6HGXPCNEdlN/dX3MRvc3+krJa7mfHjunr2+yQT0JxU0TXLMVrrrlGDj74YKtYbpxzzjny5ptvyvfff28V0N944w0577zzqs1Zz8/Pdx3atIlQTlOAcrLMwqIU0QEAAICUF6s4F2U60VfNcHeMmSiXZiFEuRiNO1fO/zX5y3SiB8d0mfrqRDeFORYVjQ9dj7YLSpsXi2zyIwdb1yeY9ox9+uCrI7NgY6XFRX0V0U0eOouKhk1GljvnO9y56OHsRPcrzmVVYhXRdcfU+HPtdQA6DBE54nb/f1a7yo+6xz49b4LIunlVX2/1L6mbh+4ZC9SyX9LnosdNEV2z0X///XcZP3681/kXX3yxlYXYu3dvOffcc2XcuHHy4YcfytKlS6u8HS20a0e7OaxaFYHVfIOQneGMc6GIDgAAACBWcS5Kv+hm5Nj52luc36s2/BF6lIuhnbQmusAgziVMRXQfnei7nZ3odZN8Bkei0Nd5xyH26UWf1nz9ee/YRW3tEu9zRmS3zRXnQid6zCJd4ikTPeA4lzVRiHNxFtG3LRMpLwv+dsrLRT68TGTrUpG81iKnjRXJyAzsNlrtJ9LrVF14ws5Ur2jnWnvHgq4hoN3YqaytiXSZJskqPV6mg06cONHqNm/duvq9WWaq6F9/VVjt3UlzEzUz3fMQX53oIXwAAAAAAEh8+sXeVUSPQZxLZo5dGPD8susqortnBYe1WGSK6Dnx8f0s4ehisNV2oq/3LpAi9rqbSJcaiuhaJPzpCfv0gVfY789IyqspzoVO9Iho3CXCnejOyKdQPl/2bKn+esV73N3qkexE19vWmRxlxXaROli6IOjiz+ydxme+EfxjpN3ruj1LvxP569uqo1x0B3ROXUlpbQ+0j+lEjwzNQtQCunaWf/fdd9Khg3N6SzV04SKlixElEtOJXlLmkPLy5A3ZBwAAAFCDoh0ijvLYxbl4doytci4CtmFB+DrRTZyLdv/pDgNFJ3pkO9FdRXQ60ePGPsdpHoTI2jki26uZIb/oM5EtS+z3Rv8LI79duu6Br4VFdSeNzlDR7TY7wxAerp2LYSyi62e3KxM9hE702n7GuZg89Oy6kf0s10U9G7QPLdLl78ki391nnz7+P+4dx8Fm2u//D/u0dqObv2teeegpHOVimMdAd8T5M6shAaXHOsJF887ffvttqVevnqxfv9467N2717pcI1vuvfdemTVrlrXA0CeffCIjR46UwYMHS58+fSSR5GTZC4uq4jIiXQAAAICUZb5cZtWJfNdpTbnoK2eI7FhtF/Z14TsTxRKK+u1EMrJFSgvd+bmFZKKHpYhuFTirsNtZRK9LJnrc0KKm6czUQrmvIujUx+zT+48RyakX+e0yuflVdaKbKJf6be2MY0SgEz2McS46o6m8NHyZ6DV1ouvfCtMprnnhkRRKLrq+r76+w45g2W+kfQjV4BvtmVTr54vMf9d9/mpnEb31AaH/jkRXp7F7Z5GuuZKEYlpEf+6556zc8sMOO8zqLDeHCRMmWJdnZ2fLN998I0cffbR069ZNrr/+ejn11FPl00/9yBSL0050xeKiAAAAQArbuz22XeiqjfMLv3bALvvRPt14H5HM7PB0ETbs5C4YlZXYi7opiujBMTEEPuNcnJnodKLHl+4n2MeLJlZ9+fIpImtni2TWEhl4aXS2ySwsWrBRpMxZgK0U5UIeetg16uzu5i7aFZ7bNJ8HOfmh7ZB1xbn42YkeyTz0cBTRNaZs3W/2++rIu8KzPXUaiRxyrX36u3tFSgrtw9q53n9TU13bQUkd6RJgon7441yq06ZNG/nhhx8kGWRluPfSsbgoAAAAkMLMlPna9WO3DVrA10KZFs1+HRu+KBfPSJdNC+0ifct93eeTiR5iEX2T3WVZsQvUdKKTiR5fup0gMun/RFb8ZBc8K2Yymy70fc8XqRtCJ3EgtGM5LUPEUWYX0k1R3WtRUfLQI/KZq4+9voe3/OX9uRgsff5UqK8df+NcXJ3ocV5En/6sfdznTLv4HS6DLhOZ+ZI9w2rmi3Z8SXmJ/bya+JlU12aQyOxxSVtEj4uFRVNBWlqa5GSyuCgAAACQ8ky3nylcxDq/dM2v9nGzHuG7bc/FRTUqxuToZsS0jytxmagGjcgpdnb1G1pUN5nodelEjysN2ok072OvgbD4C+/LtINVFyrUgvZBV0Zvm3SmiK9IFzrRI6uqRZfDsqhoiEX03Ab+daK7iuhtJOI0h1xtXRbYz21b7o5PGnR5eLcpq7bIEbfZp6f8R2TJJPff0kjH2yRaJ/ra2XanfpIJuYi+c+dO+eijj2ThwoXh2aIklu0sotOJDgAAAKQwzbGNdZyLMnnNRrNe4bttk62ui+ixqGjosuuIZOVWvbioRkOU7LFPm+Io4kf34VVHuvz0uH3c65Tod7GaGQsVFxd1daJTRI9sLnqYFhfdHa4iepzHudSQYuFFO8V1p1WnI0SaRuB13PdskaY97L9rU53v4db7h//3JKqGHUXqNBUpK7YXVU71IvoZZ5whTz/9tHVaFwAdMGCAdZ4u9Pn+++9HYhuTRk6mvbgomegAAABACnPFuTi7/2KlrbMT3QhrnIuz41JjC0wRnSiX0OT6yEXfvcH9+GqxHfFZRNeuc5OFvWWpyIKP7dMHXxP9baqqE1137plYoCbO9y8i1In+Z3x1optZUaV7RYqdO+RiHeei3e46S0O3ycy0qYm+vzRKJBJd6J4zOYbebZ/WSCTPWV0QqyPflYs+TSTVi+g//vijHHroodbpDz/80Mo13759uzz55JNy3333RWIbk4aJc6ETHQAAAEhh8RLn0qCD3TFmbUuD8OZpaya62rVOZIeze5FO9PDlonsiyiW+aVe3LrSrnZlLvrbP+/kpu1u2y9EizcM4A8RfJgfdsxN9k7Owm9daJKde9LcpFYQ9zsVkojs/x4Olz3d6VvW56NoNbj7LoxHnootc128TWC76nLdEinbaM6E6HRm5betylEh7uy4q6ZkiLftF7nclorbOWW5JmIsecBF9x44d0rChPdj78ssv5dRTT5Xc3Fw5/vjjZcmSMH0QJCl3JjpFdAAAACBlmSJFrONcPDvGmvYMb6arFsxNgX7NLPd5CJ7pNt2zueoiOlEu8UnfV6YbfeGn9vM1963YdaErs8PMsxPdlYfOoqIRj3PRmQjlZfET56KvUfP3yFekS+F2kRLnegyei9HGy+Ki+njOeN4+PehSkfQILgGpj9fR99kRW7ojTLPS4WbGFaumi5QnV/0z4FdVmzZtZNq0aVJQUGAV0Y8++mjr/G3btkmtWrUisY1Jg0x0AAAAAK5M9Fh3oqvuJ9rHnY+IXNelWbiUInpkOtFNBAed6PHLFNGXfGXnKGtXeusDRNodFJvtcXWiexbRyUOPOO3gzqwlUlYksn1l/MS5eP498tWJbqJcND89WkXjQIrof04S2bZMpFZ9O7c80rT7/No/RE5/PfK/K9E07yOSVceOcjM751K1iH7NNdfIueeeK61bt5aWLVvKYYcd5op56d27dyS2MfmK6GVh2OMIAAAAIMHjXGKcia56nyZy1WyRg64O/22bSJf1v9vHtchED08RnU70hNNyP5F6LUWKd4vMeM4+75Brwzv7I6iFRelEjyrN027UOXyRLuGKc/FaXHRL1Ze7olxaS9QEUkSf/qx93P/C6K0Nod37GjsDbxmZIq0HuLvRU7mIfvnll1ud6GPHjpWpU6dKunOKRMeOHclE9zfOpYROdAAAACBlxUuci9IiXqNO9pfecNNcWs/F1+hED43pNqWInni0btLtePf/tdu76zGx2x7Tie4V50InelQjXcKxuKj5LAhHJ3pug+rjXHaudmfmx1sRfd08keVT7IVIDxgTlU1DDVyLiyZXET2okdKAAQOsgyorK5P58+fLQQcdJA0axEEnRUJ0olNEBwAAAFLWnjiKc4kkE+diUEQPUxG9YpzLBvu4LkX0uI90+eUl+/TBV0c2s7kmZodL8S6Rol32opGmSNqkwvsWEVpcNMQievEee2ZDuONc9tQQ55LfSqJeRN+23H6N+pq5YbLQe4yIbqc8/CiiTxNJ9TiXV155xVVAHzJkiOy3335WVvrkyZMjsY1JIyczwzqmEx0AAABIUWUlduEqXuJcIsnEuRg5xLmEJJc4l4TW7mCRNoNE2gwU6XVabLclp55Idj13N7qJFtFc/WT/XIqbInqIcS5mZ1pGjv18hivOxWcmegziXOq30+lSIkU7fcfM7N4oMv9d+/Sgy6O3bahe6/1F0tLt7H/z2knFIvp7770nffv2tU5/+umnsmzZMlm0aJFce+21ctttt0ViG5NGdoYzzoVOdAAAACC1FxXVwkDt+pLUtACS4ZEXSyd6hBYWdXaiU0SPbxqZNHqSyOiv4iNHOc8jF5089MSLczGfA5qHHo5sfRMv5jPOxVkIzYtiJ3pWLXfR3leky69jnQv17i/SZv/obRuqpzt2mvdOulz0gIvomzdvlubN7T/On3/+uZx++unStWtXueiii6xYF/iWk2Uy0VlYFAAAAEhJpkChBWVdZC6Z6f1r2Mn9f4rooTGRDXs229EGqrjA7tI0XcRAMIuLuoro5KFHnFlYVN/HvgrW/tAO7HBFufi1sOgq+zi/jURVww6+i+ilRSK/vGyfHnRZdLcLNWt7YNLlogdcRG/WrJksWLDAinL58ssv5aijjrLO37Nnj2RkJPkgMEyd6GSiAwAAACneiR4Pi4pGO9KlVpJ33kerE728VKRwu3eUS1ZueCIdkDpci4uu9VhUlE70iMuu416cM5RIF9OJHq4iuslEryrOpbzcvQhtNDPRa1pcdP579uOg3fHdT4zudiElc9EDLqKPGjVKzjjjDOnVq5ekpaXJ0KFDrfNnzJgh3bqx19KvhUVLKaIDAAAAKckUKFIld9hzcdFaZKKHJFOzj/O8c9E9o1zCEemA1GHif+hET8xIlwJnJ3rdcHWiVxPnor+rvMTOuI72Asa+iug6G2f6c/bpA8aIZGRFd7vgfyf6+t/tbPRULKLfdddd8vLLL8vFF18sP/30k+Tk5Fjnaxf6LbfcEoltTL6FRSmiAwAAAKnJFChM11+ya+QsFiniXMKYi77ZuxM92oUtJL56zk70LUvdBS6K6FFeXDSUIvrmCMW5VFFE37Ha/ZrRbP94KKIvnyqyYb49C2e/C6K7TfB/R137Q3WPh8jc/0kyCOrVf9pplVeSvuACXrQ1oRMdAAAASHGmEz1l4lw8OtFNFzWCl9vYLiaZKAdTRK9HHjqCXFjUilpw2EVUs5MGUepEXxKGTPSm4dkmMzuqeJdIabH34remiB7tKJfqiuimC73v2anz9zQR7TdSZPkUkblvigy+USQ94F7uuBLU1v/www8yfPhw6dy5s3U48cQTZcqUKeHfuiST4yyiF5WysCgAAACQ0pnoqRLn0rSb3XWvi+ll1Yr11iQ+03Vqiui76URHiJ3oJXvsY7rQE6wTPcyZ6Lpmhca1VJWLboromj0ebQ3aO7dpm7tLXgvqiz+3Tw+8NPrbBP91Hy6Sk2/Pdln2gyS6gIvob775ppWDnpubK//85z+tQ+3ateXII4+Ut99+OzJbmSToRAcAAABSXKrFuegielfNErl4cqy3JDmYTuE9W+zjXR6Z6EAwnegGi4pGv4i+bblIaVFoRfRwZaJrh7DZuVsx0mXnGvs437kgarT/htRzvla3LbOPZ7xoz57ofJRIE4/ZTog/WbVFejvTTOa8KSlXRL///vvlkUcekQkTJriK6Hr6oYceknvvvTcyW5l0negU0QEAAICUlGpxLua+5tSL9VYkZye6LgqpKKIjUBoDYjqPFZ3o0aPv1+x6Io4yka3OwnDQnehhinPx3LnrqxM9FkV0r0iXZSKFO0TmvGH/f9BlsdkeBGa/8+3jhZ+6Z+OlShH977//tqJcKtJIl2XLgnzzp1gRnU50AAAAIEXt3Z5acS6I0MKiJs7F2Ylel0x0BEgXiPQswNKJHj1paR656EFEupSVurvFwxXn4rlz18x0iYc4F9WwgzvGRbuZi3fbO306HRGb7UFgWvQTadZLpKxIZN67klJF9DZt2si3335b6fxvvvnGugy+EecCAAAApDhXnAtFdITSib65wsKiFaI5gEAjXehEj02ky5YgFhfdo+9/hz2TIJyzmnRx2XiLc/HsRNeFWGc87+5C150RiH9paSL7OrvRzSyCBJUZ6A9cf/31VoTL3Llz5aCDDrLO++mnn+S1116TJ554IhLbmDRyMjOsY+JcAAAAEG1TpkyRF154QZYuXSrvvfeetGrVSt544w3p0KGDHHLIIbHevNSRinEuiEAn+maRkkKRQufMhnp0oiPYxUXniNTKZzZDtLk60YMoopuZKFr0TrfrTBGLc9HMdjPjJdZF9IWfiJQW2tvZ58zYbAuC0+cMka9vF1k/T2TdbyIt+kpKdKJfdtllMn78eJk/f75cc8011uH333+3ctEvueSSyGxlkqATHQAAALHw/vvvy7Bhw6R27doyZ84cKSqyFzLbsWOHPPDAA7HevNThcKTewqIIr1yPOJfdzi70jByRWvVjullI8E507UKnqzc2nejBxLns3hj+PHSvOBePIvrOtfZxZi13p3qsiuhaQFcDRtkLViJx5DYU6Xa8fXp24najB1xEVyeffLJMnTpVtmzZYh309JAhQ+Ttt98O/xYm5cKiZbHeFAAAAKSQ++67T55//nl56aWXJCsry3X+wQcfLLNnz47ptqWU4gI7E1TRiY5Q4lw0s3inWVS0GQVQBKdRF3dmMWJURF9i72ANhIlzMjNTIlpEX+POQ4/V50wDZya6Ss8U2f8fsdkOhMZEusx/x55JlSpF9KqsWLFCzj/f+YCg2k504lwAAAAQTYsXL5bBgwdXOj8/P1+2b3fGQSDytq+0jzU6IaderLcGicjVCeoQ2bjAPkkeOoK130iRU14SOeyWWG9J6tHFMtMyRIp2uuNS/FXg7ESvG+ZO9KriXHaYPPQYLSqqauW5dyD2PEUkT2OIkHA6HiaS30akcIfIoomS0kV0+J+JTpwLAAAAoql58+by119/VTpfZ5R27OicJo3I2/q399R0IFAZme5C14bf7WOyrBGs7Fw7q5iZMdGXmSPSoH1wkS4mEz3scS5mYdEt7vN2rLKPtfgZSx0PF8nKFTn4n7HdDgRP8/v7nWOfnj1OEhFF9CiiEx0AAACxMGbMGLn66qtlxowZkpaWJmvXrpW33npLbrjhBmvNI0QJRXSEg4lwWD/fPq7XPKabAyDKuei7N8UmziWWTn5e5PpFIs17x3Y7EJp+54pImsiyH0S2rZBEkxnrDUjFTPTiMoroAAAAiJ5bbrlFysvL5cgjj5Q9e/ZY0S45OTlWEf2qq66K9ealjm3LKue7AoHSWAMtum0wcS4U0YGE1LizyJ/OXHR/lRaLbFoU2TgXr070OIhzMV3MGoWGxNagnUjHISJ/TxaZ+5bI4f8nSVlEf/LJJ6u9fM0a5xsLNXeil7CwKAAAAKJHu89vu+02ufHGG61Yl927d0uPHj2kbt26sd601EInOsLBdJ+WFNjHdSmiAynRia4F7XcvEFmrC4KnibTqH5k4F82sLi+zC9c7Vtvn5bcO7+9Cai8w+vdkkTlviQy52X6dJVsR/bHHHqvxOm3btg11e5Jadgad6AAAAIi+cePGyf777y/du3e3iudGYWGhvPPOOzJy5MiYbl/qFdHpREcIcitEONQjEx1I7CK6H53oWnR87yK7S1w7snVB2Kbdw7s9tRs4TzhE9m4XqdNIZKeziJ5HER1h0u0E+zWsry19XXc+UpKuiL5smXPqIYKWk+XORHc4HFZHEAAAABBpF154odSpU0dee+01OfXUU13n79ixQ0aNGkURPRp0Cr7p6KMTHaHGuXiiEx1I7CK6Lt5ZXCCSXafydcrLRaY+KvL9/SKOcpHmfUTOfMO9KGm4Fy7W4qZ2omuxPjPbPh0PcS5IHlm1RHqfIfLLSyJz3kioIjoLi0ZRToY9RcHhECktd8R6cwAAAJBC7r77bjn//PPlrrvuivWmpKbtK+0CSFauSF06hxGCiosJ1msRqy0BEOpCniZCZctflS/fu01k/Nki391r//3QGIzRX0WmgF4xF33vVnceuhbWc+pF7nci9ex3vn286DPvhWzjHEX0GHSim250AAAAIFrOO+88+e677+SFF16Q0047Tfbu3RvrTUrdRUWZkYpwdaKnZ9mFOADJFemy7jeRF4aI/PmlSEaOyIlPiYx4WiSrdmS3x3yeaGGTKBdESou+9qyKsmKRee9IoqCIHoNMdFVMER0AAABRYmIEBw0aJDNmzLAWFz3ooINk+fLlsd601EEeOiLRia6zGtgpAySuxl0qLy46+w2Rl48S2b5CpH47kX98LbJflGLXTGe8xrmYTnSiXBAJ+zq70TXSRSM7EgBF9ChKT0+TrAx7gFNUWhbrzQEAAECK0PV4jLZt28rPP/8s7du3l6OOOiqm25WaRXTy0BHGTnQWFQWSpBP9T5GSvSIfXynyyZUiZUUiXYaJXPKD3bUbLV5xLs5O9Hw60REBfU63Z1ls+F1k7RxJBBTRY9SNTic6AAAAouXOO++UunXruv6fm5srH374oVx77bUyePDgmG5bytjqjHOhEx1hLaKThw4kRRF9zSyRV462u3LT0kWOuF3k7PEitRtEd3tccS5bRHY6O9Hz6ERHBOhru/tw+/ScNyWpiujvvPOOFBcXu/6/evVqKddVgp327NkjjzzySPi3MMlkZ1JEBwAAQPSL6Fo4r2qx0e+//z4m25Ry6ERHuNSqL5KWYZ9mkVogOeJcdPHp9fPsOJXzPhAZfIPGGUR/ezwz0elER6Tte559PP89eyZGnMv094pnn322rFu3Tpo2bWr9v0ePHjJ37lzp2NEeBO7atUtuvfVWuemmmyK3tUkgJ1MHOyUsLAoAAICI+uSTT+TYY4+VrKws63R1eenDhzs7gRAZ5WUi25z58xTRESotrGmhrWCjSL3msd4aAKHQzPOsXJGSPSKtBoic8Xpsi9auOJdtFNEReR2GiNRva+9EWvCJSN8zY71F4Smie+YoVvV/BNaJThEdAAAAkXTSSSfJ+vXrrSYYPV1dEb2sjPV6IkqnxJeXiKRnMS0e4Yt0oYgOJL70DJGTn7cjvwZdLpKZHdvtMQuLFmwmzgXR2Snc7zyRyQ/YUUZxXkQnEz3KclxFdL6oAAAAIHI0etHMItXTvg4U0KMY5dKgvV0wAULVeoCdm9yqf6y3BECoeowQOeSa2BfQPeNctiwRKS3UXe0ieS1jvVVIZv3OsV9ny6e4x0txiiJ6lJGJDgAAAKQY8tARbic8LnLjUpFmPWO9JQCSSW2PhUVV3aYimTkx3SQkufptRDodbp+e+7YkRZyLmjRpkuTn51untWvl22+/ld9//936//bt2yOzhUnbiU4RHQAAAJE1bdo02bJli5xwwgmu88aNG2ctNFpQUGDFvDz11FOSk8MX5IjSafqqYYdYbwmSKhfdWewCgHDHuRhEuSAa9j1fZOl3dhH9sFvjdtZeQEX0Cy64wOv/l1xySaU8RVSPTnQAAABEyz333COHHXaYq4g+f/58GT16tFx44YXSvXt3+fe//y0tW7aUu+66K9abmtzoRAcAJIKKO+dYVBTR0O14kdoN7Bx+LaZ3OUoSOs6luhxF8hT9l51p702hiA4AAIBImzt3rhx55JGu/48fP14GDhwoL730klx33XXy5JNPyjvvvBPTbUwJ25bbxw3oRAcAxDGNbsmq4/4/RXRE63XXx7moqC4wGqfIRI8y4lwAAAAQLdu2bZNmzZq5/v/DDz/Iscce6/r//vvvL6tWrQr4dp955hlp37691KpVyyrKz5w50+d1tWB/6KGHSoMGDazD0KFDK13f4XDIHXfcIS1atJDatWtb11myZIkkBYeDTnQAQGJGuhDngmjZ9zz7eNHnIgWbJaGL6H/++Welwa5moh9++OFywAEHyAMPPBCJ7UviOBe69gEAABBZWkBftszO4y4uLpbZs2fLoEGDXJfv2rVLsrKyArrNCRMmWF3smquut9e3b18ZNmyYbNy4scrrT548Wc4++2z5/vvvrYz2Nm3ayNFHHy1r1qxxXeeRRx6xuuKff/55mTFjhtSpU8e6zcLCQkl4uzeIlOwRSUsXqd821lsDAED1chu4T9OJjmhp3lukRT+RrNoi6+dLQhfRb775Zpk4caLr/zoYHz58uGRnZ8uBBx4oDz74oDz++OOR2s6kQSc6AAAAouW4446TW265RaZMmSK33nqr5ObmWl3hxrx586RTp04B3eajjz4qY8aMkVGjRkmPHj2swrfe7tixY6u8/ltvvSWXX3659OvXT7p16yYvv/yyFQWpDTmmC12/R/zrX/+SESNGSJ8+fazFT9euXSsfffSRJM2iolqIyMyO9dYAAFC92h656BTREU2nviJy/WKRTodLQhfRf/31V6+pnzoY7tq1q0yaNEmeeOIJa+D72muvRWo7k66ITiY6AAAAIu3ee++VzMxMGTJkiBWrogdtgjG08K1d4f7SbvZZs2ZZcStGenq69X/tMvfHnj17pKSkRBo2bOhqzlm/fr3Xbebn51sxMdXdZlFRkezcudPrEJeIcgEAJBLiXBArjTuLZOdKvPK7iL5582Zp3dq9B0qnY2onunHYYYfJ8uXOBXP8pN3rmsNYr149adq0qZx00kmyePFir+voFM4rrrhCGjVqJHXr1pVTTz1VNmzYIIkqx7mwKJ3oAAAAiLTGjRvLjz/+aGWj6+Hkk0/2uvzdd9+1YlkC+U5QVlbmlbOu9P9aCPd3hmvLli1dRXPzc4Hepn6X0GK7OWhMTFyiiA4ASCS5zk709CyRut5/m4FU5ncRXTtF1q1bZ53W6Zfame6Zp6hdKToVMxC6sJEWyKdPny5ff/211ZGinTAFBQWu61x77bXy6aefWgN8vb5O6zzllFMk4TPRyyiiAwAAIDq0yJyRYTdzVBzje3amR9pDDz0k48ePlw8//NBalDQUGk+zY8cO1yGYBVKjYpszzqVBh1hvCQAA/nei57XQ6Wax3hogbmT6e0XtNNfpoM8++6xV0NZCup5nLFiwQNq3bx/QL//yyy+9/q9xMNqRrlNEBw8ebA2GX3nlFXn77bfliCOOsK7z6quvSvfu3a3Cu2cR33Napx6MeJvWmZ1BnAsAAAASt7Ndi/EVZ4bq/5s3b17tz/7nP/+xiujffPONlXtumJ/T22jRooXXbWqOui85OTnWIe7RiQ4ASMRM9Dzy0AFPfu9Suv/++2XRokXSrl07awrmI488InXq1HFd/sYbb7gK3cHSorky+YhaTNfudM98RF2MqG3btj7zEeN9Wqd7YdGyWG8KAAAAEBDtWu/fv79rUVBlFgk98MADff6cfnfQhhxtohkwYIDXZR06dLAK6Z63qY0wM2bMqPY2EwZFdABAIuk4xM5C75W4KRBATDvRtct84cKF8scff0iTJk2sHENPd999t1dmeqB08H3NNdfIwQcfLL169bLO0wxEHajXr1/f73xEndZ53XXXeQ3A46mQbuJcyEQHAABAItKx9gUXXGAVww844AB5/PHHrTjGUaNGWZePHDlSWrVqZTW3qIcffljuuOMOa3apfqcw43hd70gPaWlp1veA++67T7p06WIV1W+//Xbr+4aumZTQ9mwVKbQbhaRBYLN2AQCIiSb7iFy3INZbASRuEd26cmam9O3bt8rLfJ3vL81G//3332Xq1Kkh3U68T+t0d6JTRAcAAEDiOfPMM2XTpk1WYVwL4hq5oh3mZmHQlStXSrpHhupzzz1nrZ902mmned2OLmh61113WadvuukmqxB/8cUXy/bt2+WQQw6xbjPU3PSY2+rMQ6/XQiQ7N9ZbAwAAgEgX0e+55x6/rqeD6UBdeeWVMnHiRPnxxx+9utl1WqcOuHUg7dmN7k/mYrzKzrQXdCITHQAAANG0ePFieeqpp6zZpUrXGbrqqqtkn332CWr8roeqTJ482ev/y5cvr/H2tBtdv2/4+50jYbCoKAAAQGoV0bVLRKdU6sKfDofD5+A3kCK63o4O3D/88ENrsK1TNz1p3mJWVpaVj3jqqae6Bv/a3ZKo+Yh0ogMAACDa3n//fTnrrLOsCBYzjp4+fboVozh+/HjXWBthRh46AABAahXRjz32WPnuu++sgfdFF10kJ5xwgtc0zWAjXDQb8eOPP5Z69eq58hF1QdDatWtbx6NHj7ZyF3Wx0by8PKvorgP/QYMGSSIymejFLCwKAACAKNG4FF07qGKnt0aq6GUU0SNdRKcTHQAAIJH5XQX/7LPPZOnSpTJw4EC58cYbrcWCbr75ZqszPFiaj7hjxw457LDDpEWLFq7DhAkTXNd57LHHrIK9DuwHDx5sxbh88MEHkqjcRXQ60QEAABAd69atsxb8rOi8886zLkOEM9EpogMAACS0gFrJNc5FO1i0cK6F7o0bN8r+++8vBx98sOzduzfgX65xLlUdLrzwQtd1dDGhZ555RrZu3WotNqQF9LjPQy/eI7L5L/tQAXEuAAAAiDZtWpkyZUql86dOnSqHHnpoTLYpJRDnAgAAkFpxLhVp8VwXCVqwYIHMmTNHSkpKrAgWiMiyH0X+d6ZIy/1ELv7e6yI60QEAABBtJ554ojWLdNasWa5YRM1Ef/fdd+Xuu++WTz75xOu6CIOiXSIFG+3TLCwKAACQ0AIuok+bNk3Gjh0r77zzjnTt2lVGjRol55xzjpVXDqeceu6Bc8WLMjPsiyiiAwAAIEouv/xy6/jZZ5+1DlVdptLS0qSsjLV7wmLbcvu4dkOR2vVjvTUAAACIRhH9kUcekddee002b94s5557rjUdtE+fPqH87hQtotOJDgAAgOgqL2fsGXVEuQAAAKReEf2WW26Rtm3byhlnnGF1qGhBvSqPPvpoOLcvaYvoRaV0+AAAACD6CgsLrXWHEGEU0QEAAFKviD548GCreP7HH3/4vI5eDl0NNd8+LikQKS8TSbcjXBSZ6AAAAIg2jWh54IEH5Pnnn5cNGzbIn3/+KR07dpTbb79d2rdvL6NHj471Jiafrcvs44bkoQMAAKRMEX3y5MmR3ZJkkl3Xfbpop0jtBpWL6GUU0QEAABAd999/v7z++utWROOYMWNc5/fq1Usef/xxiuiRQCc6AABA0rArugivzGyRzFpVRrqYhUVLyhxSXu6IxdYBAAAgxYwbN05efPFFa22jjAz3LMm+ffvKokWLYrptSb+wKEV0AACAhEcRPcq56KYTXdGNDgAAgGhYs2aNdO7cucoFR0tKSmKyTUmtpFBkx2r7dAPiXAAAABIdRfQoF9HNwqLWRSUU0QEAABB5PXr0kClTplQ6/7333pN99903JtuU1LavEBGHSHY9kTqNY701AAAAiFYmOsJTRM9MTxNdf9XhECkqKxORrNhsHwAAAFLGHXfcIRdccIHVka7d5x988IEsXrzYinmZOHFirDcviRcVbS/W4B8AAAAJjU70SMnJcy8s6iEtLc3VjU4nOgAAAKJhxIgR8umnn8o333wjderUsYrqCxcutM476qijYr15yYdFRQEAAJJKUJ3o27dvl5kzZ8rGjRutThZPI0eODNe2JUcRvdC7iK6yM9KlsKScTHQAAABEzaGHHipff/11rDcjNVBEBwAASCoBF9G1W+Xcc8+V3bt3S15entVZbehpiujVx7mo7MwMESmV4lKK6AAAAIi8jh07yi+//CKNGjWq1Byz3377yd9/O4u+CI9tzjgXFhUFAABIzTiX66+/Xi666CKriK6D7m3btrkOW7dujcxWJlkR3RXnQhEdAAAAUbB8+XIps9bj8VZUVGTlpCPM6EQHAABI7U50HWT/85//lNzc3MhsUQoV0elEBwAAQCR98sknrtOTJk2S/Px81/+1qP7tt99K+/btY7R1SaqsVGT7Svs0RXQAAIDULKIPGzZMfv31V2tKKIKNczGd6JW7gQAAAIBwOemkk1yxixdccIHXZVlZWVYB/b///W+Mti5J7VglUl4qkpEjUq9FrLcGAAAAsSiiH3/88XLjjTfKggULpHfv3tbg29OJJ54Yju1KfLWcC4sWVV5YlE50AAAAREN5uT3e7NChg5WJ3rhx41hvUgpFuXQQSQ84PRMAAADJUEQfM2aMdXzPPfdUukw7XKrKWkxJOdUV0XVhUTLRAQAAEB3LljkXukT0FhUlygUAACBppAfTzeLrQAE9sDgXOtEBAAAQSdOmTZOJEyd6nTdu3DirM71p06Zy8cUXW4uLIoy2OovoDTrEeksAAAAQJswvjBSK6AAAAIgxnT36xx9/uP4/f/58GT16tAwdOlRuueUW+fTTT+XBBx+M6TYmdZwLAAAAUifO5cknn7S6VGrVqmWdrs4///nPcG1b0hbRTSY6C4sCAAAgkubOnSv33nuv6//jx4+XgQMHyksvvWT9v02bNnLnnXfKXXfdFcOtTNJOdOJcAAAAUquI/thjj8m5555rFdH1tC+aiU4R3f9OdDLRAQAAEEnbtm2TZs2auf7/ww8/yLHHHuv6//777y+rVq2K0dYlIV3I1ZWJTic6AABAShXRPRciYlEiP+Xk28fFu0XKy0TSM9wXUUQHAABAFGgBXcfv2nFeXFwss2fPlrvvvtt1+a5duyQrKyum25hUdq0TKS0USc8UyW8b660BAABAmJCJHik5dd2nK3Sjk4kOAACAaDjuuOOs7PMpU6bIrbfeKrm5uXLooYe6Lp83b5506tQpptuYVEwXen4bkQy/+pUAAACQAIIa2a1evVo++eQTWblypdXR4unRRx8N17YltswckYwckbIiu4heu77ropxMuyudTnQAAABEkuahn3LKKTJkyBCpW7euvP7665Kdne26fOzYsXL00UfHdBuTc1FR8tABAABSuoj+7bffyoknnigdO3aURYsWSa9evWT58uXicDhkv/32i8xWJnIu+h5nEd0DnegAAACIhsaNG8uPP/4oO3bssIroGRnuiEH17rvvWucjTCiiAwAAJKWA41x0GugNN9wg8+fPtxYaff/9963FiLS75fTTT4/MVibZ4qImE724rCwWWwUAAIAUk5+fX6mArho2bOjVmY4QbWVRUQAAgGQUcBF94cKFMnLkSOt0Zmam7N271+peueeee+Thhx+OxDYmXRHddKIXldCJDgAAACQNOtEBAACSUsBF9Dp16rhy0Fu0aCFLly51XbZ58+bwbl2iq5VvHxft9Do7O8N0olNEBwAAAJKCwyGybbl9miI6AABAameiDxo0SKZOnSrdu3eX4447Tq6//nor2uWDDz6wLkNVnejeRfScLOfConSiAwAAAMlhzxbnuD9NpH67WG8NAAAAYllEf/TRR2X37t3W6bvvvts6PWHCBOnSpYt1GfzIRKcTHQAAAEjOKJe8ViJZtWK9NQAAAIhVEb2srExWr14tffr0cUW7PP/88+HcntQoomc5M9FLWVgUAAAASAosKgoAAJC0AspEz8jIkKOPPlq2bdsWuS1KhYVFTSd6KZ3oAAAAQHItKkoRHQAAQFJ9YdFevXrJ3387B4gIMhOdIjoAAACQnEV0FhUFAACQVC+i33fffXLDDTfIxIkTZd26dbJz506vAzzk5PnoRHcuLEoRHQAAAEgO20ycC0V0AACAlM1Ev+eee+T666+X4447zvr/iSeeKGlpaa7LHQ6H9X/NTUeFInqh986F7Ew60QEAAICk7ERvQJwLAABAyhbR7777brn00kvl+++/j+wWpcLCos4iOp3oAAAAQBIo3CGyZ4t9mkx0AACA1C2ia6e5GjJkSCS3JzUWFqWIDgAAACSPrc4olzpN3N8BAAAAkJqZ6J7xLQhHJzrRNwAAAEDCY1FRAACApOZ3J7rq2rVrjYX0rVu3hrpNKdOJTiY6AAAAkARYVBQAACCpBVRE11z0/Pz8yG1Nsi4sWrxLpLxcJN0unudkZrjiXMyCrAAAAAASFIuKAgAAJLWAiuhnnXWWNG3aNHJbk2xqOYvoppBeK9+rE12VlDkkO5MiOgAAAJDwmeh0ogMAAKR2Jjrd0kHIzBHJyK4U6WIy0VVxGZEuAAAAQEKjiA4AAJDU/C6ia+wIwpOLnp3hftiLSlhcFAAAAEhYxXtEdq21TzckzgUAACCl41zKNdMbwRXR92zxKqKnp6dJVkaaFeVCJzoAAACQwLYtt481urF2g1hvDQAAAGLZiY5QO9F3ep9tFhctoYgOAAAAJKxtHlEuRGACAAAkJYrokZbjXFzUoxPdc3FROtEBAACABLb1b/u4AVEuAAAAySqmRfQff/xRhg8fLi1btrQWLv3oo4+8Lr/wwgut8z0PxxxzjCRkEb2wYie6/dDTiY7/Z+9O4Jyo7/+Pf/Y+2eW+BATkBgXLoaj1pCK2Kor1+NuKaLW1SkW0Vvurt5bWE+uBrQdqW49q61GrqMVbUQSrIgICgiDXcu0uu7BXNv/H55tMNskmu8nuJJkkr+fjMeTcySSThMl7PvP5AgAAJJv7779f+vfvL/n5+XLIIYfI4sWLw953+fLlMm3aNHN/3Z6fO3dus/vccMMNzbb7hw0bJkkVojOoKAAAQMpKaIheXV0to0ePNhvh4WhovmXLFt/01FNPSbIPLBpYic7AogAAAEgezzzzjMyePVuuv/56+fTTT832/OTJk6WsrCzk/ffu3SsDBw6UP/zhD9KzZ8+w8x05cmTAdv/7778vSWGX1c6FSnQAAABJ94FFY2HKlClmakleXl6LG9tJG6JneSvRG6hEBwAAQPK466675MILL5QZM2aYyw8++KD85z//kUcffVSuvvrqZvcfP368mVSo2y3Z2dnJud1PJToAAEDKc3xP9Lffflu6d+8uQ4cOlYsvvlh27tzZ4v1ra2ulsrIyYHJiiJ6XQ4gOAACA5FJXVydLly6VSZMm+a7LzMw0lxctWtSuea9evdq0edSq9XPOOUc2bNjg/O3+hjqRio2e84ToAAAAKcvRIbq2cnniiSdk4cKF8sc//lHeeecdU7nuaqEFypw5c6S0tNQ39e3bV5wRoleGrESvI0QHAABAktixY4fZFu/Ro0fA9Xp569atbZ6v9lV/7LHHZMGCBTJv3jxZt26dfP/735c9ewILURy33a8BurtRJKdQpDjwNQEAAEDqSGg7l9acddZZvvMHHnigHHTQQXLAAQeY6vTjjjsu5N9cc801pkejRStSEhqkWwOLBleiZ2d5riZEBwAAQJrzb/Go2/waqu+///7yj3/8Qy644ALnbvdbrVw6DRDJyIjvYwMAACBuHB2iB9NDO7t27Spr1qwJG6JrD3WdHCO/JHQlujWwKCE6AAAAkoRui2dlZcm2bdsCrtfLdvYz79ixowwZMsRs94fjiO3+PuNFfvqCSGP4I2UBAACQ/BzdziXYd999Z3qi9+rVS5JGuJ7o3hC9toENbgAAACSH3NxcGTt2rGm3aGlsbDSXJ06caNvjVFVVydq1a52/3V/QUeSAY0QGN/WIBwAAQOpJaCW6bhz7V5do78PPPvtMOnfubKYbb7xRpk2bZqpadCP6qquukkGDBsnkyZMl2UN0KtEBAACQjLSFyvTp02XcuHEyYcIEmTt3rlRXV8uMGTPM7eeee67st99+pme5NRjpV1995Tu/adMms81fXFxstu3VlVdeKSeddJJp4bJ582a5/vrrTcX72WefncBnCgAAADggRF+yZIkcc8wxvstWT0PdKNcBhb744gt5/PHHpby8XHr37i3HH3+83HzzzYk/bDMahOgAAABIIWeeeaZs375drrvuOjOY6JgxY8yAoNZgoxs2bJDMzKYDXjUUP/jgg32X77jjDjMdddRRZqwj64hTDcz1qNNu3brJEUccIR999JE5DwAAAKR1iH700UeL2+0Oe/trr70mSY+BRQEAAJBiLr30UjOFYgXjlv79+7e4za+efvppW5cPAAAASNue6EnJvxK9sbFZT3Qq0QEAAAAAAADAuQjR41WJLm6RuqqmqxlYFAAAAAAAAAAcjxA91rLzRDJzmrV0oSc6AAAAAAAAADgfIXqsZWSEHFy0qRKdEB0AAAAAAAAAnIoQPR5ChOhUogMAAAAAAACA8xGix7Mvem2l76rcLG8luosQHQAAAAAAAACcihA9HkK1c8nJ8lxVT4gOAAAAAAAAAE5FiB4P+eEr0euoRAcAAAAAAAAAxyJET1gluredS70rUUsFAAAAAAAAAGgFIXqiBhalEh0AAAAAAAAAHI8QPR7oiQ4AAAAAAAAASYkQPa4hOj3RAQAAAAAAACCZEKLHQ15J83Yu2d4QvYEQHQAAAAAAAACcihA9niF6TVMlep43RK9tYGBRAAAAAAAAAHAqQvRE9USnEh0AAAAAAAAAHI8QPWEhundgUUJ0AAAAAAAAAHAsQvQEhej0RAcAAAAAAAAA5yNET9DAolY7l4ZGt7ga3YlaMgAAAAAAAABACwjR41qJXinidgdUoiuq0QEAAAAAAADAmQjR4yHfW4kubpG6qoBKdEWIDgAAAAAAAADORIgeD9n5IpnZAS1dsrMyJTPDe1WDK4ELBwAAAAAAAAAIhxA9HjIyWhxctJZKdAAAAAAAAABwJEL0eAkRoudlZ3muIkQHAAAAAAAAAEciRI+XvJKmwUWDKtHpiQ4AAAAAAAAAzkSIntBKdKudCz3RAQAAAAAAAMCJCNHjHaLXUIkOAAAAAAAAAMmCED3u7Vya90SvcxGiAwAAAAAAAIATEaInsJ2LVYleW0+IDgAAAAAAAABORIge9xC9qZ1LXpa3nQuV6AAAAAAAAADgSIToiWznksPAogAAAAAAAADgZIToiWznYlWiM7AoAAAAAAAAADgSIXoi27n4KtEJ0QEAAAAAAADAiQjR4yW/eTsXKtEBAAAAAAAAwNkI0RPYziUvO8tzFSE6AAAAAAAAADgSIXoie6Jn084FAAAAAAAAAJyMED1e8kqa9US3QnTauQAAAAAAAACAMxGiJ6IS3e32XOWrRHclcskAAAAAAAAAAGEQosc7RHc3itRVm7NUogMAAAAAAACAsxGix0tOoUhGVkBfdAYWBQAAAAAAAABnI0SPl4yMZoOLUokOAAAAAAAAAM5GiJ6QwUWtSnR6ogMAAAAAAACAkxGix5OvEr3SnFCJDgAAAAAAAADORogeT0HtXKxK9DoXIToAAAAAAAAAOBEhegIr0X3tXOoJ0QEAAAAAAADAiQjR4yk/sCe6r50LlegAAAAAAAAA4EiE6Alt55LluUglOgAAAAAAAAA4EiG6EwYWpRIdAAAAAAAAAByJED2e8gLbuTT1RHclcqkAAAAAAAAAAE4M0d9991056aSTpHfv3pKRkSEvvPBCwO1ut1uuu+466dWrlxQUFMikSZNk9erVkirtXKhEBwAAAAAAAABnS2iIXl1dLaNHj5b7778/5O233Xab/OlPf5IHH3xQPv74YykqKpLJkydLTU2NJHWIXuNt55LlrURvIEQHAAAAAAAAACfKTuSDT5kyxUyhaBX63Llz5Xe/+52ccsop5ronnnhCevToYSrWzzrrrJB/V1tbayZLZaUnsHZkO5cc78CihOgAAAAAAAAA4EiO7Ym+bt062bp1q2nhYiktLZVDDjlEFi1aFPbv5syZY+5nTX379hXHtnPxVqLXNTSanQYAAABAMtAjSfv37y/5+flm+3zx4sVh77t8+XKZNm2aub+2cNRCmfbOEwAAAIgnx4boGqArrTz3p5et20K55pprpKKiwjdt3LhRnFuJ3vTy0xcdAAAAyeCZZ56R2bNny/XXXy+ffvqpac+oLRfLyspC3n/v3r0ycOBA+cMf/iA9e/a0ZZ4AAABAPDk2RG+rvLw8KSkpCZicV4ke2BPdqkYHAAAAnO6uu+6SCy+8UGbMmCEjRoww4xcVFhbKo48+GvL+48ePl9tvv920Y9RtdTvmCQAAAMSTY0N0q0pl27ZtAdfr5XAVLI7n387F7Za87KaXn77oAAAAcLq6ujpZunRpQMvFzMxMc7mllouxmKeOg6TjH/lPAAAAQFqF6AMGDDBh+cKFC33X6Ybxxx9/LBMnTpSkDtHdLpH6vaYnpH9fdAAAAMDJduzYIS6XK+qWi7GYp6PHQgIAAEBKSWiIXlVVJZ999pmZrMFE9fyGDRtMwDxr1iy55ZZb5KWXXpJly5bJueeeK71795apU6dKUsotEsnIDBxc1FuNTogOAAAARM7RYyEBAAAgpWQn8sGXLFkixxxzjO+yDiakpk+fLo899phcddVVUl1dLRdddJGUl5fLEUccIQsWLJD8/HxJShkZnmr0mgpPiN6hp2npUlVLOxcAAAA4X9euXSUrK8vWlottnaf2Vw/XYx0AAABImUr0o48+Wtxud7NJA3Sl1eg33XSTOYyzpqZG/vvf/8qQIUMkqeWVBA4uSiU6AAAAkkRubq6MHTs2oOViY2OjudzWlouxmCcAAACQMpXoacl/cFG96A3RaxtciVwqAAAAICJ69KgeOTpu3DiZMGGCzJ071xw9OmPGDHO7tmDcb7/9TM9ya+DQr776ynd+06ZNpoVjcXGxDBo0KKJ5AgAAAIlEiJ7gEJ1KdAAAACSTM888U7Zv3y7XXXedOWJ0zJgxpuWiNTCojm+Umdl0wOvmzZvl4IMP9l2+4447zHTUUUfJ22+/HdE8AQAAgEQiRE9UiF7jaeeSl51lTumJDgAAgGRx6aWXmikUKxi39O/f37RsbM88AQAAgLTtiZ6WfD3RAyvRCdEBAAAAAAAAwHkI0RPdziXL287FRYgOAAAAAAAAAE5DiJ6wEN3bziXHW4lez8CiAAAAAAAAAOA0hOiJbudCJToAAAAAAAAAOBYheoLbueTleAcWrSdEBwAAAAAAAACnIURPcDsXKtEBAAAAAAAAwLkI0eMtP7CdS1NPdEJ0AAAAAAAAAHAaQvQEt3NpqkRnYFEAAAAAAAAAcBpC9IQNLOpp55KX7Q3RG6hEBwAAAAAAAACnIURP9MCi3hC9lhAdAAAAAAAAAByHED2RIbrbLblUogMAAAAAAACAYxGiJypEb2wQqd8nedlZ5iKV6AAAAAAAAADgPITo8ZZTJCIZnvO1e6hEBwAAAAAAAAAHI0SPt8xMv8FF9/j1RHcldrkAAAAAAAAAAM0Qoie0L3qlrxKddi4AAAAAAAAA4DyE6AkeXNTqiU47FwAAAAAAAABwHkL0BIfoVKIDAAAAAAAAgHMRojuknQuV6AAAAAAAAADgPIToCW/nwsCiAAAAAAAAAOBUhOiJkF/SvBLdRSU6AAAAAAAAADgNIXoi5JU0r0SvJ0QHAAAAAAAAAKchRHdIOxcq0QEAAAAAAADAeQjREx6iZ5mzDCwKAAAAAAAAAM5DiJ7IEL2mqSd6LSE6AAAAAAAAADgOIXqCK9FzszyrwNXolgZaugAAAAAAAACAoxCiJ3pg0ZymVUBfdAAAAAAAAABwFkL0hIbolb5KdEVfdAAAAAAAAABwFkL0BLdzyc7KlKzMDM9FQnQAAAAAAAAAcBRC9ISG6JUibrevGp1KdAAAAAAAAABwFkL0RIbojQ0iDTW+vuhUogMAAAAAAACAsxCiJ0JusYhk+Fq6WJXotQ2uxC4XAAAAAAAAACAAIXoiZGYG9EXPzaadCwAAAAAAAAA4ESG6A/qi53lDdNq5AAAAAAAAAICzEKInSkAlepY5SyU6AAAAAAAAADgLIXqiQ/QaKtEBAAAAAAAAwKkI0ROFnugAAAAAAAAA4HiE6A4I0Zsq0V2JXSYAAAAAAAAAQABC9ETJK2k2sCiV6AAAAAAAAADgLIToCQ/R/dq5uAjRAQAAAAAAAMBJCNEd0c4ly3O2nhAdAAAAAAAAAJyEED3hIXql5GZRiQ4AAAAAAAAATkSI7oRK9BzvwKL1DCwKAAAAAAAAAE5CiO6AEN2qRK+lEh0AAAAAAAAAHIUQPVHyrYFFK/0q0QnRAQAAAAAAAMBJCNETJa/ErxLdM7AoPdEBAAAAAAAAwFkcHaLfcMMNkpGRETANGzZMUq6dS7Z3YNEGQnQAAAAAAAAAcJLsRC9Aa0aOHCn//e9/fZezsx2/yNGF6DWVkpeVYc7WEqIDAAAAAAAAgKM4PpHW0Lxnz54R37+2ttZMlsrKSnF0iN5YL/mZDeZsXYMrscsEAAAAAAAAAEiedi5q9erV0rt3bxk4cKCcc845smHDhhbvP2fOHCktLfVNffv2FUfKLfadLZa95pRKdAAAAAAAAABwFkeH6Icccog89thjsmDBApk3b56sW7dOvv/978uePXvC/s0111wjFRUVvmnjxo3iSJlZIrmeavRCtydEpyc6AAAAAAAAADiLo9u5TJkyxXf+oIMOMqH6/vvvL//4xz/kggsuCPk3eXl5ZkoK2tKlbo8UuveZi1SiAwAAAAAAAICzOLoSPVjHjh1lyJAhsmbNGkkJ3r7oBY1UogMAACB53H///dK/f3/Jz883hS6LFy9u8f7PPvusDBs2zNz/wAMPlFdeeSXg9vPOO08yMjICphNOOCHGzwIAAABIwRC9qqpK1q5dK7169ZJUCtHzG6vMKSE6AAAAnO6ZZ56R2bNny/XXXy+ffvqpjB49WiZPnixlZWUh7//hhx/K2WefbY4k/d///idTp04105dffhlwPw3Nt2zZ4pueeuqpOD0jAAAAIIlD9CuvvFLeeecdWb9+vdn4PvXUUyUrK8tshKdSiJ7XWG1OaxtcCV4gAAAAoGV33XWXXHjhhTJjxgwZMWKEPPjgg1JYWCiPPvpoyPvfc889JiD/9a9/LcOHD5ebb75Zvve978l9990XcD9tydizZ0/f1KlTpxaXo7a2ViorKwMmAAAAIO1C9O+++84E5kOHDpUzzjhDunTpIh999JF069ZNUipEd3lCdCrRAQAA4GR1dXWydOlSmTRpku+6zMxMc3nRokUh/0av97+/0sr14Pu//fbb0r17d7Ptf/HFF8vOnTtbXJY5c+ZIaWmpb+rbt2+7nhsAAACQlAOLPv3005LS8kvMSW6Dpyc6A4sCAADAyXbs2CEul0t69OgRcL1eXrlyZci/2bp1a8j76/UWrVQ/7bTTZMCAAaZ9429/+1uZMmWKCdr1SNRQrrnmGtNWxqKV6ATpAAAASLsQPeXleUL0nAZ6ogMAACB9nXXWWb7zOvDoQQcdJAcccICpTj/uuONC/o22f9EJAAAASOt2LinP287FCtGpRAcAAICTde3a1VSGb9u2LeB6vax9zEPR66O5vxo4cKB5rDVr1ti05AAAAEDbEaI7KESvczXKii0MiAQAAABnys3NlbFjx8rChQt91zU2NprLEydODPk3er3//dUbb7wR9v7W2EjaE71Xr142Lj0AAADQNoToDgjRcxuq5cQDPZU4t/znK3G73QleMAAAACA07UP+0EMPyeOPPy4rVqwwg4BWV1fLjBkzzO3nnnuu6Vduueyyy2TBggVy5513mr7pN9xwgyxZskQuvfRSc3tVVZX8+te/lo8++kjWr19vAvdTTjlFBg0aZAYgBQAAABKNEN0BIbrU7pFrpgyX3OxM+WDNTnnjq8DDXQEAAACnOPPMM+WOO+6Q6667TsaMGSOfffaZCcmtwUM3bNggW7Zs8d3/sMMOkyeffFL+8pe/yOjRo+W5556TF154QUaNGmVu1/YwX3zxhZx88skyZMgQueCCC0y1+3vvvUfPcwAAADhChjvFy54rKyultLRUKioqpKTEM5CnY6x+Q+Tvp4v0PEjkF+/J7a+tlPvfWiv7dymU1y8/UvKysxK9hAAAAGnJ0duQCIl1BgAAgFhtQ1KJnkh53hVTu8ecXHz0IOnWIU++3blXHvtgfWKXDQAAAAAAAABAiO6Udi6qOC9brpo81Jy/9801sn1PbSKXDgAAAAAAAADSHiG6I0L0St9V077XRw7qUypVtQ1y5+urErdsAAAAAAAAAABCdEeE6K46kQZP1XlmZoZc96MR5vwzSzbKl5sqErmEAAAAAAAAAJDWCNGdEKL7tXRR4/p3lpNG9xYd8vWml7+SFB/7FQAAAAAAAAAcixA9kTKzRHKKmrV0UVdPGSb5OZmyeN0uWfDl1sQsHwAAAAAAAACkOUL0RMsvaVaJrvbrWCAXHXmAOX/rKyukpt6ViKUDAAAAAAAAgLRGiO6YwUUDQ3T1i6MGSs+SfPlu9z555P118V82AAAAAAAAAEhzhOhOCdFrAtu5qMLcbPnNlKHm/P1vrZGyypp4Lx0AAAAAAAAApDVCdAdXoqtTRu8nY/p2lL11LrnttVXxXTYAAAAAAAAASHOE6I4J0ZtXoqvMzAy5/qQR5vxzS7+TL74rj+fSAQAAAAAAAEBaI0RPtLzQA4v6O7hfJzn14P3M+Zv+/ZW43e54LR0AAAAAAAAApDVC9CQI0dVvThgmBTlZsuTb3fLvL7bEZ9kAAAAAAAAAIM0Roju8J7qlZ2m+XHz0Aeb8H15ZIfvqXPFYOgAAAAAAAABIa4ToDu+J7u+iIwdK79J82VxRIw+9903slw0AAAAAAAAA0hwhepJUoqv8nCy5+sTh5vy8t9fK1oqaWC8dAAAAAAAAAKQ1QvQkCtHVSQf1knH7d5J99S7544KVsV02AAAAAAAAAEhzhOiOGVi09XYuKiMjQ647aYQ5//z/NsnV//xC9tTUx3IJAQAAAAAAACBtEaInWn5JVJXo6qA+HWX2D4aY809/slFOmPuefLhmR6yWEAAAAAAAAADSFiF6krVzsfzquMHy9EWHSt/OBbKpfJ/8v4c/lute/FL21jXEZjkBAAAAAAAAIA0RojslRK+JrJ2Lv0MHdpEFlx0p5xzSz1x+YtG3MuWe92TJ+l12LyUAAAAAAAAApCVCdKeE6K5akYbaqP+8KC9bbj31QPnrBROkV2m+fLtzr/z4z4vk1v98JTX1LvuXFwAAAAAAAADSCCF6ouV6Q3RVW9Xm2Xx/cDd57fIj5cdj+4jbLfLQe+vkh396Tz7bWG7PcgIAAAAAAABAGiJET7SsbJGcQs/52uhbuvgryc+R2388Wh6ZPk66dciTtdur5bQHPpDbX1sptQ1UpQMAAAAAAABAtAjRnSCvpE2Di4Zz3PAe8vqsI+WUMb2l0S1y/1tr5ZT7PpDlmytsmT8AAAAAAAAApAtCdCf1RbcpRFedinLlnrMOlnnnfE+6FOXKyq175OT7PpCZT/1PPt2w27bHAQAAAAAAAIBURojuqBC9fe1cQplyYC/TK33KqJ7ianTLvz/fLKc98KFMvf8DefGzTVLvarT9MQEAAAAAAAAgVRCip2glur+uxXky7ydj5eWZR8i07/WR3KxMM+DoZU9/Jkf88U25783VsrOqNiaPDQAAAAAAAADJjBA9xSvR/Y3ar1TuPGO0fHD1sXL5pCFm8NFtlbVyx+tfy8Q/vCm/ee4LWbk1tssAAAAAAAAAAMmEED0FBxZtjYbnl00aLB/85li5+8zRclCfUqlraJRnlmyUE+a+J2f/5SN5fflW0/4FAAAAAAAAANJZdqIXACKSH98Q3ZKbnSmnHtxHpo7ZT5Z+u1vmf7BeFizfKou+2Wmmfp0L5UcH9ZJjh3WXg/t1kqzMjLguHwAAAAAAAAAkGiF6GvREb01GRoaM69/ZTJvL98kTi76Vpz/ZIBt27ZUH3l5rpk6FOXL00O5yzLDuctTgblJamJOQZQUAAAAAAACAeCJEd1KIXpP4fuS9OxbI1VOGyWXHDZbXv9oqC1eUyTtfb5fde+vl+f9tMpNWpI/bv5OpUD9ueHc5oFuxCeIBAAAAAAAAINUQojtBgivRQynIzZJTxuxnpgZXo3y6oVwWrtwmb64ok9VlVfLxul1mmvPqStP2RQN1ncb17ySFubytAAAAAAAAAKQG0k5HDSya+Er0ULKzMmXCgM5mumbKcNm4a6+8ubJMFq4sk4/W7jRtXx77cL2ZtCB9YNciGdm7VEb2LpFR+3lOOxbmJvppAAAAAAAAAEDUCNGdwIGV6C3p27lQph/W30zVtQ3ywZodJlTXti9bKmpk7fZqM730+Wbf3+zXscCE6f7heo+SPNrAAAAAAAAAAHA0QnRHVaInR4jurygvW44f2dNMavueWlm+uUKWb670nX67c69sKt9npte/2ub72y5FuTKoe7EJ5ft0KpC+nQp953uU5Jve60iwur0ie3eIVO8Q2bvLc35fuUhBR5GS3iIdeouU9BLJLUr0kiYfV4NI5SaRio0i5RtF3I0ixd1FirqKFHlPs/PE8XQsB30ee7aIFHQS6TxQJL800UsFp3DV6/DVIllsbsBPY6NIY4Pne08ncTedN1OYy8U9eC8BAAAASAh+iTixEr2+RqSmXGTfbk9gqafmsv/53SKNLs8PSg3eOvT0nu/hOV/QWSQzs/XH1h+mddUi1ds9Qak59U76OPkdm+Zb3M1zWtRNJCsn5Oy6dciTo4d2N5OlsqZevjKheqUs3+QJ1tdsr5Kd1XWy09tb3W+BpLPskb5Zu2RU8R4ZXLBH+ufsll4Zu6SLa7vkZ9RJZkEnySnuJNmFnSVDw1wN7vRUl9V36r0ut1hMjxk76WumwVBjvedUg87s/PY9jgYK+nqboHpn4KTBQWEX79S56bw+x8ysyB+joU5kn85/V/NTfRwTlOtj6vvAe1q/N7J553tDdROs9xIp2c8TruupXs7M9syrocZzWr/P8z63zjfoZb8pK1ekg/W+839fd2r766zrSgNffZ31MfJLPMutn79YHBHRUCtS8Z1I+QbPZIXl1vnKzSJuV8vz0DBaP2/+kxW063vA954v9ZzX02jeE63R74aKTSKV33lPN/ud1x0Am0TqQuz8K+zqCdODpy4DPevQSfRzYd6H3venvlczczzvQQ3r9FQv6+saqyNn9LOgnzff53BX02X9ng/+/OtraNZ/aXTLpN9d+jx1ver/N3VVnvOuOs9OHes7zZz6X27wu15fr32evzN/752fma93fnV+5/VvVEamSJZ+V+Z6T/M8r22oU/1Mmve7tVPJ732v51v7Xjf/r1V5v8/8vuOsSZdf52e+p3qLlHq/p5Jhp1V7/o/R7xt9PwWceq/X/2cCbvMG175w2zof5jq9rO8pa1vFfxsm4DrvaU2FNzyP0q/+5/kuAQAAAIA4I0R3UoheXSZySw9PkNNeGgRpAOEfsOvj6I9aX1DuDc3b8nga0lsBvnWq4YYVQpgf1+aMaJ39oTplukX6aj8Yt9S7GqVsT41Ul2+XxvLvJKdqixTVbpNOrh2SJ97QpdY7tYNbKyBNKOaZMjQIM5N1nf/5bE/QYwIjb6ikp1agZJ3X24Pp3+UUeqZcPS3ynurlosDrNTD2hTtWsLOr9UC1+YN6AlRfwOYN2TTQ1/XsC8k1yNjlCZXaQoMtDUWLdP5dPY+p8zeB6mbPfE0oUi5S9pXElIaZJlAPCtf1NbYCcg1n/CcT2lSI1FeHnmdGluc5aTDpvwPGnHqv08Bd178JCKtDh4UBt3kDvEhe29I+IqV9Pe8//Q6wPpf6PrOew8410R3Z4h+qWyG7vkb6HDTc18+8hsd6aq6raX6dPrfaisgeU+evIaS+5/Q5mAB4h8h3i5vf16pW18nsFMn0TPp+1mDUuhxw3nu7skI+q4q1WShoBYIaAjd4nocVklvhr9lZ470u1Oc5JK2m9obruq5MyJ7jnawAWK8LmoKv08cM3mEV7r3Z6iJleT7z+n1sff71u8a8D/eI1Hrfi9apTm0JLu2gj6vffTq1V3aBN2Tv6vm/x3znBYXl+h6Ols7TBOt+4bqe1+8Z/e43r6PfZzzkZe+OBf2+t5bRtxNMd375Xdb1FbzTS9+/5nO0vekoIPN+sc7r9Ts9Y6iYHR3W5Gr6/8m6bO0AMe9x6//kZBP0veDbtgAAAACA+Mpwu1P7F0llZaWUlpZKRUWFlJR426Y4jVYh3j0iMHTTH4tWlWmocE8v6w/LqjKRqm2eaY/3VH9sR0uDl+Af+/oYGkBWW4+hp2VtCHuj5yrsLvsKekpFTncpy+gi37k6y9q6Utm2L1Myaiokr6FSSjOqpVSqm512zKiSEqmWvIxIwzEHye0QWG2uk74X9gVVUmqw2hbW+8oXvHlPrYBcH88X9Hiva61S27Tz2Cyyxxuq+0/Wdfo1Y3YyFIjk5HvOa/BlrrNOC7zXFXiCXH2v7dnqfd9t9QT3dtAqVn0cE0K1IWiLhj4vDcg79hPp2NfvfD/PeQ3nQh0xYh2ZYEIzDda3i1RZO7+8Qbv1PmhtJ4Ed78lS/2Cxj+e8uU7P9xbJK266v1aj7lonsusbkV1rvafey9ryxak0kNb3ngkf6+LyPddsB5H1mTefPe/nL+Dz73cESVt3ivl/DsxU1FR176vAz/HuKMjxXmfd5r1e/8a3c1Dn4d1ZqOetHYf+91G688alO3Dqgk5rm3buWKf62TTv+bLmn4FoQnj9nAcfwaOTrmf9/1KPpjDTZnt2Xkcto+k7V78j9Tma77kEbJaZkFqPtsj0HnXhtzPLCrL9A+2A67z30/UffDRYs+0Yv+vMEVyZLUzWYyROUmxDIgDrDAAAALHahiREdwoNRsq/bap+1eAqknYsoWj1mf4YNwGkX8CuwYT5we7fHkLD0q6R97TWcE9/5FvzNSGHFbJ7q2ctAT9+M0Jfr1WzVhhnBXXmsPrcFhejpt4lO6pqZWdVneysrpUdVXVNl81prVTuqZS6vRWyr7Ze6uvqJCujUbLFJVniOc2UwMt6u55vkCypd2dJvWR7zku21EuWNEi21LkDr9O/zpV6Kc6sk9KsOinJqpWSrHrpkFkrHTLrpCizTooz9LRWCqVWijJqpSEzT6qzO8q+7I6yN6dUavR8TkepzSk1oUJ2ZobpB29Os7ynGRmS6T0117ldUti4RwobyqWwoUIK6suloKFc8hsqJMdVI/W5pVKf20nq8zp6ptyO0pDXSVy5JZKRmWkGdNW1kKmnGU2nen2mdVmaLjedes5bt3lOde16/l6CLgffFuLd0PytEnQv33xdtZK1d4fk7C2TrL1lkq2n1dska+92yayvlsa8Ur+pxHOaWyJuPZ/f0ZzX600IaObuloyGfZJZWyGZNeWSWVvuPa3wO6+nerlC3Nn54s4pEndOoWfK1fPeyf+89zbdEeQu0J0g+kiBX7ORfus2f91aCJRcdZJRWykZusy1FWZnk+e5VXiub9hrAlK3mbRyOs88J6uFhl7n9rXUyDe3NRb3MK+fbeqrJav8W8kqXy/Z5eskQ6ulA3ofe9pDZDTrk+w9NS9CpveokSxxa+iXaYV/WeL2hX963nOkiVkf2QVN683swCkIur7A87w9L7KHPp6rXjK8LU08p3WSYR2p0lgvGd4jVjIa67zn9VTvVysZ3us1LDanOg9XrXm8xoIu5r3RWNBZGgs95936nR/igxD83vFpqJXMmt2SsW+nZO7bJZn7dptTrbL3vR9ziwOnHOu0wBuE2izM27PF92206qokc+8O77TdnGY01Hhey4LOTa+r7hy0AvzWuN2SUbNbMvdskqw9WySzarNk7tksWXs2S2aVXt4m7qwc72tnva5+r7F5Xa3znusz6qslw7ecOyRzn3W607PM+3aZ76CQi6O35Hcy743Gwq7m/eJ5n3T1XKfvl/xS73s8W9wZusNDPxPZ5jOQkZnr+SxYt3s/L55wXD8bTYG5OyAsD1iIMMuWWDqWSnZWDN67yb4NiYSuM/0pta8+zjteAQAAUlhBTpbJnOIlpUL0+++/X26//XbZunWrjB49Wu69916ZMGFCRH/LDyCoBlejVNe6TH/2PTUNUlXbIHu8582pudwgVTUN5oeQTjV1nlMz1blMcG+ur280p3UNCWqLAABIaroLtpNUSZeMCumSUWmC853uEjOVS7HZQYvm3r7yaOnfNX4DWbMNmXwSsc721jXIiOtei8tjAQAApIOvbposhbnZjtuGdHxP9GeeeUZmz54tDz74oBxyyCEyd+5cmTx5sqxatUq6d28avBJoiVaulRbqFHpA1LZwNbp9Ybr2eNfTOuvU77paV6PU+93W0Og2f2tOXUGXfafe611N17vcbmn0O29OG93S6Duvy9QoLjPOm2ffmN5mxo0TvZ/ner3JXK+XJcR1fpf1dnPqvV5vty4r63E88/E8jufUqrZuuq91P995vxuC9+SFmn/wncPNKxK+jv0By+Z9rFDLGzT71irL3RFU27dWnRvLx/CfdyTrJxLx2hsb7lWLZi91uOcW6jVti3CvhT5sPHamR/sQduzhb+k1jX5esX2d2jJ/uxfHLVmyS0rNtNrd/IEy4/jes+Mz1Zb3SzwrSwAAAACgvRxfia7B+fjx4+W+++4zlxsbG6Vv374yc+ZMufrqq1v9e6qIAAAAEC22IZMP7VwAAACSX4FD27k4uhK9rq5Oli5dKtdcc43vuszMTJk0aZIsWrQo5N/U1taayf+FAAAAAAC76Q+8eB5uDAAAgMSI3+hMbbBjxw5xuVzSo0ePgOv1svZHD2XOnDlm74E1adU6AAAAAAAAAAApF6K3hVata/m9NW3cuDHRiwQAAAAAAAAASFKOPvawa9eukpWVJdu2bQu4Xi/37Nkz5N/k5eWZCQAAAAAAAACAlK5Ez83NlbFjx8rChQt91+nAonp54sSJCV02AAAAAAAAAEDqc3Qlupo9e7ZMnz5dxo0bJxMmTJC5c+dKdXW1zJgxI9GLBgAAAAAAAABIcY4P0c8880zZvn27XHfddWYw0TFjxsiCBQuaDTYKAAAAAAAAAEBatXOxXHrppfLtt99KbW2tfPzxx3LIIYckepEAAACAtHX//fdL//79JT8/32ybL168uMX7P/vsszJs2DBz/wMPPFBeeeWVgNvdbrcpmunVq5cUFBTIpEmTZPXq1TF+FgAAAEAKhegAAAAAnOGZZ54xLRevv/56+fTTT2X06NEyefJkKSsrC3n/Dz/8UM4++2y54IIL5H//+59MnTrVTF9++aXvPrfddpv86U9/kgcffNAUzRQVFZl51tTUxPGZAQAAAKFluLXsI4VVVlZKaWmpVFRUSElJSaIXBwAAAEmAbcjwtPJ8/Pjxct9995nLjY2N0rdvX5k5c6ZcffXVIdsz6phGL7/8su+6Qw891LRp1NBcf4707t1brrjiCrnyyivN7fq6a/vGxx57TM4666yQy6FHqerkv850OVhnAAAAsHu7n0p0AAAAABGpq6uTpUuXmnYrlszMTHN50aJFIf9Gr/e/v9Iqc+v+69atM2Mf+d9Hf8hoWB9unmrOnDnmftakAToAAAAQC4ToAAAAACKyY8cOcblcpkrcn17WIDwUvb6l+1un0cxTXXPNNaZiyJo2btzY5ucFAAAAtCS7xVsBAAAAwIHy8vLMBAAAAMQalegAAAAAItK1a1fJysqSbdu2BVyvl3v27Bnyb/T6lu5vnUYzTwAAACCeCNEBAAAARCQ3N1fGjh0rCxcu9F2nA4vq5YkTJ4b8G73e//7qjTfe8N1/wIABJiz3v48O8PTxxx+HnScAAAAQT7RzAQAAABCx2bNny/Tp02XcuHEyYcIEmTt3rlRXV8uMGTPM7eeee67st99+ZuBPddlll8lRRx0ld955p/zwhz+Up59+WpYsWSJ/+ctfzO0ZGRkya9YsueWWW2Tw4MEmVL/22muld+/eMnXq1IQ+VwAAAEARogMAAACI2Jlnninbt2+X6667zgz8OWbMGFmwYIFvYNANGzZIZmbTAa+HHXaYPPnkk/K73/1Ofvvb35qg/IUXXpBRo0b57nPVVVeZIP6iiy6S8vJyOeKII8w88/PzE/IcAQAAAH8ZbrfbLSlMDwUtLS2ViooKKSkpSfTiAAAAIAmwDZl8WGcAAACI1TYkPdEBAAAAAAAAAAiDEB0AAAAAAAAAgHTtiW51q9HSfAAAACAS1rZjinc+TCls9wMAACBW2/0pH6Lv2bPHnPbt2zfRiwIAAIAk3JbUHolwPrb7AQAAEKvt/pQfWLSxsVE2b94sHTp0kIyMjLjuxdAN+I0bNzKwUQpjPacH1nPqYx2nB9ZzerBrPesmsm5I9+7dWzIz6YCYDNjuRyyxntMD6zk9sJ5TH+s4PVTGebs/5SvR9cn36dMnYY+vK5EPbOpjPacH1nPqYx2nB9ZzerBjPVOBnlzY7kc8sJ7TA+s5PbCeUx/rOD2UxGm7n7IaAAAAAAAAAADCIEQHAAAAAAAAACAMQvQYycvLk+uvv96cInWxntMD6zn1sY7TA+s5PbCeEW+859ID6zk9sJ7TA+s59bGO00NenNdzyg8sCgAAAAAAAABAW1GJDgAAAAAAAABAGIToAAAAAAAAAACEQYgOAAAAAAAAAEAYhOgAAAAAAAAAAIRBiA4AAAAAAAAAQBiE6DFw//33S//+/SU/P18OOeQQWbx4caIXCe3w7rvvykknnSS9e/eWjIwMeeGFFwJud7vdct1110mvXr2koKBAJk2aJKtXr07Y8qJt5syZI+PHj5cOHTpI9+7dZerUqbJq1aqA+9TU1Mgll1wiXbp0keLiYpk2bZps27YtYcuM6M2bN08OOuggKSkpMdPEiRPl1Vdf9d3OOk49f/jDH8x396xZs3zXsZ5Tww033GDWrf80bNgw3+2sZ8QD2/2phe3+9MB2f3pguz/9sN2fum5wyHY/IbrNnnnmGZk9e7Zcf/318umnn8ro0aNl8uTJUlZWluhFQxtVV1eb9ag/kkK57bbb5E9/+pM8+OCD8vHHH0tRUZFZ5/ohRvJ45513zJfuRx99JG+88YbU19fL8ccfb9a/5fLLL5d///vf8uyzz5r7b968WU477bSELjei06dPH7NxtXTpUlmyZIkce+yxcsopp8jy5cvN7azj1PLJJ5/In//8Z/MDyh/rOXWMHDlStmzZ4pvef/99322sZ8Qa2/2ph+3+9MB2f3pguz+9sN2f+kY6YbvfDVtNmDDBfckll/guu1wud+/evd1z5sxJ6HLBHvqRef75532XGxsb3T179nTffvvtvuvKy8vdeXl57qeeeipBSwk7lJWVmfX9zjvv+NZrTk6O+9lnn/XdZ8WKFeY+ixYtSuCSor06derkfvjhh1nHKWbPnj3uwYMHu9944w33UUcd5b7sssvM9azn1HH99de7R48eHfI21jPige3+1MZ2f/pguz99sN2fmtjuT33XO2S7n0p0G9XV1Zm9nHpYnyUzM9NcXrRoUUKXDbGxbt062bp1a8A6Ly0tNYfzss6TW0VFhTnt3LmzOdXPtlap+K9rPXyoX79+rOsk5XK55OmnnzZVR3p4J+s4tWiF2Q9/+MOA9alYz6lF2yho24WBAwfKOeecIxs2bDDXs54Ra2z3px+2+1MX2/2pj+3+1MZ2f3pY7YDt/mxb55bmduzYYb6ce/ToEXC9Xl65cmXClguxoxvSKtQ6t25D8mlsbDR91A4//HAZNWqUuU7XZ25urnTs2DHgvqzr5LNs2TKz8ayHXmu/tOeff15GjBghn332Ges4ReiPJG2toId1BuOznDo0uHrsscdk6NCh5pDOG2+8Ub7//e/Ll19+yXpGzLHdn37Y7k9NbPenNrb7Ux/b/enhEIds9xOiA0CIPdn6ZezfYwupQ//j1Q1nrTp67rnnZPr06aZvGlLDxo0b5bLLLjM9TnWgP6SuKVOm+M5r/0vduN5///3lH//4hxnwDwCA1rDdn9rY7k9tbPenjykO2e6nnYuNunbtKllZWc1GgNXLPXv2TNhyIXas9co6Tx2XXnqpvPzyy/LWW2+ZwWgsuj710O3y8vKA+7Ouk4/upR40aJCMHTtW5syZYwYQu+eee1jHKUIP59NB/b73ve9Jdna2mfTHkg4Ep+e1IoH1nJq0+mTIkCGyZs0aPs+IObb70w/b/amH7f7Ux3Z/amO7P311TNB2PyG6zV/Q+uW8cOHCgMPD9LIeQoTUM2DAAPOh9F/nlZWV8vHHH7POk4yOH6Ub0nqI35tvvmnWrT/9bOfk5ASs61WrVpk+XKzr5Kbf07W1tazjFHHccceZQ3e16siaxo0bZ/rmWedZz6mpqqpK1q5dK7169eLzjJhjuz/9sN2fOtjuT19s96cWtvvTV1WCtvtp52Kz2bNnm0OE9MM6YcIEmTt3rhm8YsaMGYleNLTjw6l7t/wHFdIvZB14Rgcq0B56t9xyiwwePNhsgF177bVmsIOpU6cmdLkR/aGcTz75pLz44ovSoUMHX+8sHTBKDw/S0wsuuMB8xnXdl5SUyMyZM82X8qGHHproxUeErrnmGnMomH529+zZY9b522+/La+99hrrOEXo59fqaWopKiqSLl26+K5nPaeGK6+8Uk466SRzKOfmzZvl+uuvN5XBZ599Np9nxAXb/amH7f70wHZ/emC7P/Wx3Z8+rnTKdr8btrv33nvd/fr1c+fm5ronTJjg/uijjxK9SGiHt956y60fleBp+vTp5vbGxkb3tdde6+7Ro4c7Ly/Pfdxxx7lXrVqV6MVGlEKtY53mz5/vu8++ffvcv/zlL92dOnVyFxYWuk899VT3li1bErrciM7555/v3n///c33c7du3czn9fXXX/fdzjpOTUcddZT7sssu811mPaeGM888092rVy/zed5vv/3M5TVr1vhuZz0jHtjuTy1s96cHtvvTA9v96Ynt/tR0pkO2+zP0H3tjeQAAAAAAAAAAUgM90QEAAAAAAAAACIMQHQAAAAAAAACAMAjRAQAAAAAAAAAIgxAdAAAAAAAAAIAwCNEBAAAAAAAAAAiDEB0AAAAAAAAAgDAI0QEAAAAAAAAACIMQHQAAAAAAAACAMAjRAQBRy8jIkBdeeCHRiwEAAAAghtjuBwAPQnQASDLnnXee2ZgNnk444YRELxoAAAAAm7DdDwDOkZ3oBQAARE83nOfPnx9wXV5eXsKWBwAAAID92O4HAGegEh0AkpBuOPfs2TNg6tSpk7lNq1PmzZsnU6ZMkYKCAhk4cKA899xzAX+/bNkyOfbYY83tXbp0kYsuukiqqqoC7vPoo4/KyJEjzWP16tVLLr300oDbd+zYIaeeeqoUFhbK4MGD5aWXXorDMwcAAADSB9v9AOAMhOgAkIKuvfZamTZtmnz++edyzjnnyFlnnSUrVqwwt1VXV8vkyZPNxvcnn3wizz77rPz3v/8N2FjWjfFLLrnEbGTrhrduKA8aNCjgMW688UY544wz5IsvvpATTzzRPM6uXbvi/lwBAACAdMV2PwDER4bb7XbH6bEAADb1Rvzb3/4m+fn5Adf/9re/NZNWpPziF78wG8SWQw89VL73ve/JAw88IA899JD85je/kY0bN0pRUZG5/ZVXXpGTTjpJNm/eLD169JD99ttPZsyYIbfcckvIZdDH+N3vfic333yzbwO9uLhYXn31VXo0AgAAADZgux8AnIOe6ACQhI455piAjWXVuXNn3/mJEycG3KaXP/vsM3NeK1NGjx7t25BWhx9+uDQ2NsqqVavMhrJuVB933HEtLsNBBx3kO6/zKikpkbKysnY/NwAAAAAebPcDgDMQogNAEtKN1+DDLO2i/RIjkZOTE3BZN8J1gxwAAACAPdjuBwBnoCc6AKSgjz76qNnl4cOHm/N6qj0T9VBMywcffCCZmZkydOhQ6dChg/Tv318WLlwY9+UGAAAAEDm2+wEgPqhEB4AkVFtbK1u3bg24Ljs7W7p27WrO66BB48aNkyOOOEL+/ve/y+LFi+WRRx4xt+lAQNdff71Mnz5dbrjhBtm+fbvMnDlTfvrTn5q+iEqv1/6K3bt3lylTpsiePXvMBrfeDwAAAEB8sN0PAM5AiA4ASWjBggXSq1evgOu0mmTlypXm/I033ihPP/20/PKXvzT3e+qpp2TEiBHmtsLCQnnttdfksssuk/Hjx5vL06ZNk7vuuss3L93QrqmpkbvvvluuvPJKs5F++umnx/lZAgAAAOmN7X4AcIYMt9vtTvRCAADsoz0Kn3/+eZk6dWqiFwUAAABAjLDdDwDxQ090AAAAAAAAAADCIEQHAAAAAAAAACAM2rkAAAAAAAAAABAGlegAAAAAAAAAAIRBiA4AAAAAAAAAQBiE6AAAAAAAAAAAhEGIDgAAAAAAAABAGIToAAAAAAAAAACEQYgOAAAAAAAAAEAYhOgAAAAAAAAAAIRBiA4AAAAAAAAAQBiE6AAAAAAAAAAAhEGIDgAAAAAAAABAGIToAAAAAAAAAACEQYgOAAAAAAAAAEAYhOgAAAAAAAAAAIRBiA4g5fTv31/OO++8RC8G0uB99qMf/SjRiwEAAFIA26+Il8cee0wyMjJk/fr1iV4UAEgqhOgAksbatWvl5z//uQwcOFDy8/OlpKREDj/8cLnnnntk3759kux0Q1Y3aK0pMzNTOnfuLFOmTJFFixa1eb4PPPCA2VhOxh+T/q+H/3TCCSckevEAAABaxfarM7dfn3/+ebOMXbt2ldzcXOndu7ecccYZ8uabb8bsMQEAyS070QsAAJH4z3/+Iz/+8Y8lLy9Pzj33XBk1apTU1dXJ+++/L7/+9a9l+fLl8pe//EVSwdlnny0nnniiuFwu+frrr82PiGOOOUY++eQTOfDAA6Oen/69/kBIxuqmMWPGyBVXXNHsev2hAwAA4GRsvzpv+9Xtdsv5559vAvqDDz5YZs+eLT179pQtW7aYYP24446TDz74QA477DBJVT/96U/lrLPOMu9LAEDkCNEBON66devMht7+++9vqkN69erlu+2SSy6RNWvWmB8pqeJ73/ue/OQnP/Fd/v73v28qZebNm2d+UKST/fbbL+C1AAAASAZsvzpz+/XOO+80AfqsWbPkrrvuMtXzlv/7v/+Tv/71r5KdnZoxSXV1tRQVFUlWVpaZAADRoZ0LAMe77bbbpKqqSh555JGAHyCWQYMGyWWXXRb273ft2iVXXnmlqYIpLi42h9HqRv3nn38ecL+3337bbEj/4x//kBtvvNEEuB06dJDTTz9dKioqpLa21mxwd+/e3cxnxowZ5jp/+veXXnqp/P3vf5ehQ4eaw3bHjh0r7777bpufv/4IsQ4H9jd//nw59thjzfJoJcmIESPMD5Xgliha5fTOO+/4DrM9+uijfbeXl5eb59S3b18zD30t//jHP0pjY2OLy6S9wPWw5FAmTpwo48aN811+44035IgjjpCOHTua101fl9/+9rdiF61Q0vl+8803MnnyZPPjQCvVb7rpJlNtFPzjQSvbreery3LHHXc0u5/629/+JhMmTJDCwkLp1KmTHHnkkfL66683u59Wk+n9dF3ra/LEE0/Y9twAAEByYvvVeduv2j5nzpw5MmzYMLP95x+g+1dp63adRbcv9WgCbVGj24SHHnpos50f8VwH3377rfzyl7809ykoKJAuXbqY5Qvub271PdfXUO+vj92nT5+A2/z/ZsmSJWY7Wqv/db4DBgwwFftt2Y62nssLL7xgjr7Q+44cOVIWLFjQ4voBAKdLzV2sAFLKv//9bxNOtvWwSt341Y043cDUDcJt27bJn//8ZznqqKPkq6++atYaRDeudePx6quvNlVC9957r+Tk5Jgej7t375YbbrhBPvroI7MBqvO77rrrAv5eN1afeeYZ+dWvfmU2GrX6Rnt4L1682GxIRsvawNUg15/+4NAN0pNPPtlUzOjrpBvJ+gNCK5zU3LlzZebMmWaDXatrVI8ePczp3r17zWuwadMm06uzX79+8uGHH8o111xjDmnVvw3nzDPPNIcl6yG648ePD9iw19fm9ttvN5f1B5AG7gcddJAJtfX10NdUD5ONRH19vezYsaPZ9RqU6zqy6KHD+hrrDxv90aob6ddff700NDSYx1W6ga+v1VtvvSUXXHCBaRXz2muvmcOp9TW4++67ffPTH0C6nvU9p3+vvTI//vhjU0l2/PHH++6nz0V/IOn8pk+fLo8++qgJ9fVHj64bAACQnth+dd72qxY+6M4JDbQjqcTW11zXnz6mvi4aWD/++ONm2Z977jk59dRT474OdNtbn68e5aChuL7O+prqTgZ9X2jQ709f227dupnH0hA8lLKyMrN9q/fTZdfCF53vv/71L999otmOtl5r/Xt9fN2h8Kc//UmmTZsmGzZsMK8jACQlNwA4WEVFhZY2uE855ZSI/2b//fd3T58+3Xe5pqbG7XK5Au6zbt06d15envumm27yXffWW2+Zxxo1apS7rq7Od/3ZZ5/tzsjIcE+ZMiVgHhMnTjSP5U//XqclS5b4rvv222/d+fn57lNPPbXF5dZl0r+98cYb3du3b3dv3brV/d5777nHjx9vrn/22WcD7r93795m85g8ebJ74MCBAdeNHDnSfdRRRzW778033+wuKipyf/311wHXX3311e6srCz3hg0bWlwv+vpdccUVAdffdttt5rXS56zuvvtus+z6fKKlr631egZPc+bM8d1P17VeN3PmTN91jY2N7h/+8Ifu3Nxc32O/8MIL5n633HJLwOOcfvrpZpnXrFljLq9evdqdmZlp1lfw+0bnG7x87777ru+6srKykK8LAABIH2y/OnP79Z577jHL9Pzzz7sjMWvWLHN/fT6WPXv2uAcMGODu37+/b/3Ecx2Eev0WLVpk/vaJJ57wXTd//nxz3RFHHOFuaGgIuL91m647pa+HXv7kk0/CvhaRbkdbz0W3wf2v+/zzz8319957b9jHAACno50LAEerrKw0p1rB0FZayaEVIFbF8s6dO31tRT799NNm99cKa60asRxyyCG+QYj86fUbN2401c7B7Uy0EtmiFTKnnHKKqdbQx2+NVlBrJYgOcqSHwq5YscL0b9SKZ3/+ldh6qKhWbGtljlYu6eXWPPvss2b+WiGkf2tNkyZNMsvZ0iG81iHFetiq/yGcWj2j1eD6nJVWsqgXX3yx1UNsQ9HXWNvBBE86eFUwPWw0+DBSHbzrv//9r7nulVdeMVVHWt3jTw9L1efw6quvmsta9aXLqhU71vvGf77+9BBk63BlpetN31e6DgAAQHpi+9WZ26/RrhfddtTWLtqW0KLr4KKLLjKV2lr5He914P/66RGb+r7Qdja6zR3qfXHhhRe2WnVvba+//PLLZp7hXotItqMtuj4OOOAA32U9KlV/P7CNDCCZEaIDcDTd2FJ79uxp8zw0ENVDDAcPHmx+kGivP93I/+KLL0JurFsBsKW0tNScav+/4Ot13sHz0McJNmTIEHMo6Pbt21tdXt0w16BYD2+9/PLLTf/GUD9etCWKbqBqaxPd+NXnZPUaj+RHyOrVq03bE/07/0nnaR3a2RJt6aI/ABYtWuTrebl06VJzvf99Dj/8cPnZz35mDsPVQ081eI80UNd1pcsTPOkgXf70R2Zwj3Z9zf0PJ9ZWM3roc/APp+HDh/tut56Hzk8D8tYEv1eU/qjTQ3YBAEB6YvvVmduv0a4X3TbUnRbBgrcd47kO9HXVQg+rL7n1vtA+8aFeP20b0xrdiaGtVrSdoc5Pg3vtXe/ftz3S7ehwr4ViGxlAsqMnOgBH041d3WD78ssv2zyP3//+93LttdeaKpCbb77ZDAykIan2QwwV5oar1gh3fahBKdtDN6CtHwLaT1wfV/sTHnPMMb4BOzXoPe6448zASHfddZfZkNa+3Voloj+4Igmp9T4/+MEP5Kqrrgp5uxVCh3PSSSeZvosaimu/SD3V11V7d1q0WkYrgrR/og7CpD96tFpdB5TSQToj6UfpZPF6TwAAgOTB9qszt1/1cdWyZctk6tSpYrd4rAPtFa8Bt74PtHJdA3k9UlILVUK9fv6V6+Ho32uPd+3XrjtBtPJd33d6JIFep9X30WIbGUAqIkQH4Hi6If6Xv/zFVDzrxmK0dKNQN+AfeeSRgOu1YkOrLeymFTLBvv76axM4a6VItHRApYceekh+97vf+Ua11w1crQ556aWXAio9NKwOFtyCxKKHWFZVVfl+8ERLK4h03ehhtfpDSMNxPbw2eKAr/cGnP5h00vvpj0J9TrqsbX3sYPqjQQ8P9f/hpK+56t+/vznV6nVt7aLVR/5VNCtXrvTdbr0uOj89RFcHTQIAAIgW26/O237VtixaDf3UU0+Z6vfWijl023DVqlXNrg/edoznOtD3hQ5mrwG3paamxrwv2ktbMup06623ypNPPinnnHOOPP300+aI0ki3owEgldHOBYDjaaWJBra6Abdt27Zmt2tVyz333BP273UDObjqQYNfHUk+FvTHkn9PQm15oj3BddT7tlRe66GuP//5z01VyGeffWaus+bj/7z0EE6tTAmmr12oDeszzjjDLKvON5jeP7hPYyjarmXz5s3y8MMPy+effx7QykXt2rWr2d9YwbT/IaJ2uO+++3zn9XXRy9qXUsN7deKJJ5rDiv3vp7TySX+oaY93pZVJGvzfdNNNzSp6qJ4BAACRYPvVeduvGkb/5je/Mf3a9TTUdt3f/vY3Wbx4sW/bUc9brQtVdXW12TmiRRqRtP6zex2Eel/ce++9EfWtD0dbrATPM3h7PdLtaABIZVSiA3A8rTjRaggNaLXvng7aM2rUKDNo5Icffmh+UJx33nktVgJpIDpjxgzTdkQP4fz73//erIe2XXTZJk+ebAbe0V6FDzzwgLle+wy21WWXXSZz586VP/zhD6YiRDem9fBXbamiP1C0Ikerfbp37y5btmwJ+FsdoGjevHlyyy23mIGH9D7aTuXXv/61qQTS10dfP72f/jDQ10erXLSXeGuVTrpBrdUoV155pdmo136K/vR113YuP/zhD02Fivap1NejT58+AYM0haM/FPXHTDA9rNT/MNz8/HxT5aSVOTpYkw5upO1jtMrIqtzR10orurQySp/b6NGjTUsZ/XGih8Rag7HY5cAAAOb1SURBVB/pa6T30UOntbL+tNNOM+vxk08+MVX2c+bMaXW5AQBAemP71Znbr/r3y5cvN5XcWgGvA5/qYKhbt241g8traK7rR2k7Gq1a14BYXxdtqfP444/LunXr5J///GezAejjsQ70ef/1r381bVw0xNfgXSvEu3Tp0ubH1eekj3Xqqaea961Wm+t60bZEuq0fzXY0AKQ0NwAkia+//tp94YUXuvv37+/Ozc11d+jQwX344Ye77733XndNTY3vfvvvv797+vTpvst62xVXXOHu1auXu6CgwPzNokWL3EcddZSZLG+99ZaWYLifffbZgMedP3++uf6TTz4JuP76668312/fvt13nV6+5JJL3H/729/cgwcPdufl5bkPPvhgM+/WrFu3zvz97bffHvL28847z52VleVes2aNufzSSy+5DzroIHd+fr55Tf74xz+6H330UTMPnZdl69at7h/+8Ifm9dLb/J/znj173Ndcc4170KBB5jXt2rWr+7DDDnPfcccd7rq6OnckzjnnHDPfSZMmNbtt4cKF7lNOOcXdu3dvM389Pfvss826bI2uR51vqElvs+i6Lioqcq9du9Z9/PHHuwsLC909evQw68flcgXMU5/v5ZdfbpYjJyfHrCN9vRsbG5s9vr6Wuu50HXbq1Mm8bm+88UbA8unrGiz4fQUAANIX26/O3H597rnnzHZj586d3dnZ2eZ1PvPMM91vv/12wP10+/L00093d+zY0SzzhAkT3C+//HLAfeK5Dnbv3u2eMWOGec7FxcXuyZMnu1euXNns/RPusf1vs17vTz/91Gyf9+vXzzxu9+7d3T/60Y/cS5YsadN2tPVcggUvIwAkmwz9J9FBPgCkCj2c8ZJLLml2qCNiR6uQtPJIq5kAAAAQHbZfE491AADOR090AAAAAAAAAADCIEQHAAAAAAAAACAMQnQAAAAAAAAAAMKgJzoAAAAAAAAAAGFQiQ4AAAAAAAAAQBiE6AAAAAAAAAAAhJEtKa6xsVE2b94sHTp0kIyMjEQvDgAAAJKAdjzcs2eP9O7dWzIzqTtJBmz3AwCAZNrWq6mpkbq6upg/Tm5uruTn58f8cVJdyofouiHdt2/fRC8GAAAAktDGjRulT58+iV4MRIDtfgAAkCzbehqgDxgwQLZu3Rrzx+rZs6esW7eOIL2dUj5E10oU60NRUlKS6MUBAABAEqisrDSBrLUtCeez1pX+PKQOHQAAtMStQbbf9kO8aQW6BuixziutbVp9PEL09kn5EN06lFPfkIToAAAAiAZtQZJvXem/rDUAAJAM23olJYVmip2GGM47vdDgEQAAAAAAAACAdK1EBwAAAAAAAADnaYhxtTiV6HahEh0AAAAAAAAAgDCoRAcAAGnF5XJJfX19ohcDCZaTkyNZWVmJXgwAAACkNSrRkwUhOgAASAtut1u2bt0q5eXliV4UOETHjh2lZ8+eCR9QCgAAAICzEaIDAIC0YAXo3bt3l8LCQoLTNN+hsnfvXikrKzOXe/XqlehFAgAAQFqiEj1ZEKIDAIC0aOFiBehdunRJ9OLAAQoKCsypBun6vqC1CwAAAIBwCNEBAEDKs3qgawU6YLHeD/r+IEQHAABA/LliXC2u84cdMm2ZCwAAQBKghQv88X4AAAAAEAkq0QEAAAAAAAAg7uiJniyoRAcAAAAAAAAAIAxCdAAAAAfbunWrzJw5UwYOHCh5eXnSt29fOemkk2ThwoVxW4bzzjtPpk6dGpN5H3300TJr1qx2309bs1hTSUmJjB8/Xl588UWblxYAAACIRSV6LCfYgRAdAADAodavXy9jx46VN998U26//XZZtmyZLFiwQI455hi55JJLEr14jjN//nzZsmWLLFmyRA4//HA5/fTTzWsGAAAAAO1BiA4AANKO2+2WvXUNCZn0sSP1y1/+0lRWL168WKZNmyZDhgyRkSNHyuzZs+Wjjz7y3W/Dhg1yyimnSHFxsanCPuOMM2Tbtm2+22+44QYZM2aM/PWvf5X+/ftLaWmpnHXWWbJnzx7ffZ577jk58MADpaCgQLp06SKTJk2S6upq87ePP/64qeq2Kr3ffvtt8ze/+c1vzDIVFhaaSvlrr71W6uvrI35crXB/55135J577vHNW3cctFXHjh2lZ8+eZpluvvlmaWhokLfeeqvN8wMAAABii0r0ZMHAogAAIO3sq3fJiOteS8hjf3XTZCnMbX0TbNeuXabq/NZbb5WioqKQgbFqbGz0BegaSGtwrFXqZ555pi/sVmvXrpUXXnhBXn75Zdm9e7cJ2v/whz+Y+Wv19tlnny233XabnHrqqSbkfu+990zgf+WVV8qKFSuksrLSVHqrzp07m9MOHTrIY489Jr179zYV3xdeeKG57qqrrorocTU8//rrr2XUqFFy0003mft369at3a+xvgaPPPKIOZ+bm9vu+QEAAABIb4ToAAAADrRmzRoTYg8bNqzF+2lvdA2w161bZ/qlqyeeeMJUrH/yySemN7gVtmvgrSG3+ulPf2r+1grRNXg+7bTTZP/99ze3a1W6RavTa2trTZW3v9/97ne+81pproH7008/HRCit/S4WpmuIbdWsgfPuy10R0BWVpbs27fPPK4uk4b2AAAAgDO5vFMs5w87EKIDAIC0U5CTZSrCE/XYkYi07YtWiWt4bgXoasSIEaZSXW+zQnQNlK0gW/Xq1UvKysrM+dGjR8txxx1ngvPJkyfL8ccfb/qJd+rUqcXHfuaZZ+RPf/qTqTavqqoyQby2k/HX0uPa7e677zZtaL755hu5/PLLzbJZVfMAAAAA0Fb0RAcAAGlHe29rS5VETPrYkRg8eLC578qVK215zjk5Oc1eA63WVlq9/cYbb8irr75qAvh7771Xhg4daqrbw1m0aJGcc845cuKJJ5pWLf/73//k//7v/6Suri7ix7WbVrMPGjTI7ATQ1jPa0iZWgT0AAABgT6V4LPuhU4luF0J0AAAAB9IKaq0Kv//++80An8HKy8vN6fDhw2Xjxo1msnz11Vfmdg3EI6Xh9uGHHy433nijCcS1zcrzzz9vbtPzLlfgBviHH35oWr9ocD5u3DgT+n/77bdRP89Q87bDhAkTZOzYsaZtDAAAAAC0ByE64GTPXyzy19O0oWyilwQAkAAaoGvArIHwP//5T1m9erVp0aJtSiZOnGjuo+1LtA2LVoV/+umnsnjxYjn33HPlqKOOMuF2JD7++GP5/e9/L0uWLJENGzbIv/71L9m+fbsJ6K2WLF988YWsWrVKduzYIfX19SY01/tqD3Rt56LLZIXu0dB56+OvX7/ezLulKnVdps8++yxg2rZtW9j7z5o1S/785z/Lpk2bol4uAAAAIPYa4jDBDoTogFPV7xP5/EmRtQtFqrYmemkAAAkwcOBAE4wfc8wxcsUVV8ioUaPkBz/4gRmYc968eb4K8hdffNH0Lz/yyCNNqK5/p/3KI6V9zN99913TmmXIkCFmwNA777xTpkyZYm6/8MILTXsXDeW7desmH3zwgZx88smm7/ill14qY8aMMZXp1157bdTPUQcj1XYyWjWv89ZgPpwnn3xSDj744IDpoYceCnv/E044QQYMGEA1OgAAAIB2yXBHOmpVkqqsrJTS0lKpqKhoNtAV4Gi7vxW55yDP+V/9T6TzwEQvEQAkrZqaGtPfWwPV/Pz8RC8OkuB9wTZk8rHWWYH+yEn0wgAAAEfTMHSfSMK29Zq2Nd+XkpLiGD5OlZSWHsE2rQ2oRAecqnp70/mG2kQuCQAAAAAAAJC2shO9AADCqPLr8UqIDgAAAAAAkGJi3becnuh2oRIdcKqqsqbzhOgAAAAAAABAQlCJDiRFiF6TyCUBAAAAAACA7VwxrhbX+cMOVKIDTlVNJToAAAAAAACQaFSiA0nRE51KdAAAAAAAgNRCT/RkQSU64FRV25vOU4kOAAAAAAAAJASV6EAyVKK7CNEBAAAAAABSC5XoyYJKdMCpqv0r0WnnAgAAAAAAACQCITrgRHXVInVVTZdp5wIAaIPHHntMOnbsaOs8zzvvPJk6daqt8wQAAADSuxI9lhPsQIgOOFFVWeBlKtEBIC1pYJ2RkWGm3NxcGTRokNx0003S0JBcG8PvvPOOHHvssdK5c2cpLCyUwYMHy/Tp06Wuri5mYT8AAAAA2IUQHXB6KxdFJToApK0TTjhBtmzZIqtXr5YrrrhCbrjhBrn99tslWXz11VfmOYwbN07effddWbZsmdx7771mp4DL5Ur04gEAAAAJRCV6siBEB5w+qKiiEh0A7OV2e1tnJWDSx45CXl6e9OzZU/bff3+5+OKLZdKkSfLSSy+Z23bv3i3nnnuudOrUyVR4T5kyxYTtoaxfv14yMzNlyZIlAdfPnTvXzLuxsdGE2hdccIEMGDBACgoKZOjQoXLPPfe0uHyffPKJdOvWTf74xz+GvP311183y3/bbbfJqFGj5IADDjCh+kMPPWQe4+2335YZM2ZIRUWFr+pedxSo2tpaufLKK2W//faToqIiOeSQQ8z9LVYF+wsvvGCq2/Pz82Xy5MmycePGqF5jAAAAAGhJdou3AnBIOxfP4e4AAJvU7xX5fe/EPPZvN4vkFrX5zzV43rlzp6/di4bmGqqXlJTIb37zGznxxBNN9XdOTk7A3/Xv398E8PPnzzdV4Ra9rPPRgL2+vl769Okjzz77rHTp0kU+/PBDueiii6RXr15yxhlnNFuWN998U0477TQTkOv9QtEAXSvptQr9yCOPbHb7YYcdZoL86667TlatWmWuKy4uNqeXXnqpeS5PP/209O7dW55//nkTwGs1u4bmau/evXLrrbfKE088Yarbf/nLX8pZZ50lH3zwQZtfYwAAACA+XDGuFufIT7tQiQ44ET3RAQBB3G63/Pe//5XXXnvN9Be3wvOHH35Yvv/978vo0aPl73//u2zatMlUZofys5/9TJ566ilT4a0+/fRTE0hrJbjS4P3GG280IbtWo59zzjnmtn/84x/N5qWB9imnnCJ//vOfwwbo6sc//rGcffbZctRRR5kw/tRTT5X77rtPKisrze0afJeWlpoKdA3cddIQfcOGDSbg10Bfn59WsGtV+hFHHGGut2jwr/ObOHGijB07Vh5//HET/i9evLjdrzkAAAAAKCrRASeq9obo2QUiDfvoiQ4Adssp9FSEJ+qxo/Dyyy+bUFnDYm258v/+3/8z7U4WLlwo2dnZpsWJRavHtQXLihUrQs5r6tSpcskll5gAXKu1tR3KMcccY6rULffff788+uijJsTet2+fGfxzzJgxAfP5+OOPzXI999xzZp4tycrKMqH3LbfcYirX9W9///vfm/YvGnRrsB6KhvvaXmbIkCEB1+sOAH2eFn0Nxo8f77s8bNgw0+JFX4MJEya0uGwAAABAYsW6bzk90e1CiA44uRK9Y1+RHV9TiQ4AdsvIaFdLlXjSkHvevHmmYltbmmho3FY6D+2hrqG2tmF58sknA3qea9sUrfa+8847TWV3hw4dzCCmGnz706pwDbI1bP/hD3/YrHVMKNrX/Kc//amZbr75ZhOOP/jgg6byPZSqqioTwC9dutSc+rPavQAAAABAPBCiA44O0ft5Q3Qq0QEgXemAmoMGDWp2/fDhw6WhocEE3NpXXGmvdO0rPmLEiLDz05YuOsDnAw88YP5ew3SL9hHXeWlfccvatWubzaNr167yr3/9S44++mjTK13bvUQSpFt0IFStQK+urvaF+1p17u/ggw8215WVlZl2LuHoc9DBUq2qc33+5eXl5vUBAAAAnI1K9GRBT3TAiaq2eU5L+3pOXYToAIBAOrCm9iS/8MIL5f3335fPP/9cfvKTn5iKb70+HA2XDz30UDMIqfYq14FK/eepgbT2Xf/666/l2muvlU8++STkfLp3727as6xcudLMR8PsULRn+sUXXyyvv/66CeSXL19uHltPTzrpJHMfbSejlefaombHjh1msFCtVNee7Fo5r4H9unXrTPuXOXPmyH/+8x/f/DW8nzlzptmZoFXrOkiqPj9auQAAAACwCyE64DRut0j19qZ2LopKdABACNqWRQfT/NGPfmTar+jgo6+88kqrVeEXXHCB6XV+/vnnB1z/85//3FSmn3nmmabXula2+1elB9NBQDVI1/7lGngHV5MrDbM1IP/FL34hI0eONAOMfvTRR2bwUz2vtPpdb9fH7datm9x2222+56ch+hVXXGF6vWv/dQ31+/Xr55t/YWGhCeW1V/zhhx9uWr0888wzUb+WAAAAQOIq0WM5wQ4Zbv21lcIqKyultLRUKioqpKSkJNGLA7Sudo/InD6e86c9LPKvn4n0GS/ys/8meskAIGnV1NSYSuYBAwZIfn6+pDvtSf7ss8/KF198IclMB0adNWuWad9i9/uCbcjkY60zPbYiI9ELAwAAHE3D0H0iCdvWa9rWfFRKSgpj+Dh7pbT0fLZpbUBPdMCp/dBzCkUKO3vOM7AoAMAGWhG+fv16ue++++SWW25J9OIAAAAAaY6e6MmCdi6AU0P04u4i2d6qONq5AABscOmll5r2LzogaHArFwAAAABAaITogNNUe0P0Ig3R8zznqUQHANjU/qS2ttb0DM/KypJkp4OItrWVCwAAAJB4rhj3Q28+ZhHahhAdcHQluhWi1yV0kQAAAAAAAIB0RU90ICnauVCJDgB2SPHx1BEl3g8AAABILFeMq8WpRLcLleiAU9u5FPfwq0SnJzoAtEdOTo453bt3b6IXBQ5ivR+s9wcAAAAAhEIlOuDUSvSiboGV6Fotl5GR0EUDgGSl/b87duwoZWWe79jCwkLJ4Ds1rSvQNUDX94O+L1KhPzwAAACSkdW7PJbzhx0I0QHHtnPxq0QXt4irXiQ7N5FLBgBJrWfPnubUCtIBDdCt9wUAAAAAhEOIDji5J3qWFaJ7q9EJ0QGgzbTyvFevXtK9e3epr69P9OIgwbSFCxXoAAAASCwq0ZMFITrgJNqypdp/YFG/EN1Vl7DFAoBUosEp4SkAAAAAIFKE6ICT1FZ6Ks5VUXdPD3StRnfVNl0PAAAAAACAFOCKcbW4zh92yLRlLgDsUbXdc5rbQSS30HPeN7hobeKWCwAAAAAAAEhTVKIDTlK1zXNa3K3pOm3povk5legAAAAAAAAphJ7oyYJKdMBJfP3QezRd56tEJ0QHAAAAAAAA4o1KdMBJqrwhepF/JXqu57SBgUUBAAAAAABSB5XoyYJKdMCJITqV6AAAAAAAAIAjUIkOOLInevfAnuiKgUUBAAAAAABSCJXoyYJKdMBJqreHCNGpRAcAAAAAAAAShUp0wJE90alEBwAAAAAASG1UoicLKtEBR/ZE9wvRs6wQnUp0AAAAAAAAIN6oRAecwu0WqQ4RoluV6K66xCwXAAAAAAAAYsAV42pxnT/sQCU64BQ15U1BeUA7F3qiAwAAAAAAAIlCJTrgFFXeQUXzSkVyvMF5QE90QnQAAAAAAIDUoVXoWTGeP+xAJTrgFFXbPKfF3QKv91WiM7AoAAAAAAAAYmPOnDkyfvx46dChg3Tv3l2mTp0qq1atCrhPTU2NXHLJJdKlSxcpLi6WadOmybZt3kwrhRGiA07h64feI/B6KtEBAAAAAABSUEMcpsi98847JiD/6KOP5I033pD6+no5/vjjpbq62nefyy+/XP7973/Ls88+a+6/efNmOe200yTV0c4FcIoqb4he1C1MiE4lOgAAAAAAAGJjwYIFAZcfe+wxU5G+dOlSOfLII6WiokIeeeQRefLJJ+XYY48195k/f74MHz7cBO+HHnqopCpCdMBpIXrYSnRCdAAAAAAAgNQRn57olZWVAdfm5eWZqTUamqvOnTubUw3TtTp90qRJvvsMGzZM+vXrJ4sWLUrpEJ12LoDjQnR6ogMAAAAAAMAeffv2ldLSUt+kvc9b09jYKLNmzZLDDz9cRo0aZa7bunWr5ObmSseOHQPu26NHD3NbKqMSHXB8T3QrRKcnOgAAAAAAQOpwRd23PPr5i2zcuFFKSkp810ZSha690b/88kt5//33Y7h8yYMQHXCKKu9IxkXdA6+nnQsAAAAAAADaSAN0/xC9NZdeeqm8/PLL8u6770qfPn181/fs2VPq6uqkvLw8oBp927Zt5rZURjsXwCmqtntOi4NDdCrRAQAAAAAAUk9DHKbIud1uE6A///zz8uabb8qAAQMCbh87dqzk5OTIwoULfdetWrVKNmzYIBMnTpRURiU64ASNjX7tXIJC9KxczymV6AAAAAAAAIgRbeHy5JNPyosvvigdOnTw9TkvLS2VgoICc3rBBRfI7NmzzWCjWt0+c+ZME6Cn8qCiihAdcIKacpFG797BojADi7oI0QEAAAAAAFJHQ4wbhURXiT5v3jxzevTRRwdcP3/+fDnvvPPM+bvvvlsyMzNl2rRpUltbK5MnT5YHHnhAUh0hOuAEVd4q9PyOTT3QLfREBwAAAAAAQIxpO5fW5Ofny/3332+mdEKIDjhpUNHiHs1voyc6AAAAAABACnJWJTrCY2BRwAmqwwwqqqhEBwAAAAAAABKGSnTASZXowf3QA0J0KtEBAAAAAABSh8s7xXL+sAOV6ICTeqK32M6lLr7LBAAAAAAAAIBKdMBZITqV6AAAAAAAAOnBFeO+5VSi24VKdMAJqiOoRHfV6jDJ8V0uAAAAAAAAIM0lNESfM2eOjB8/Xjp06CDdu3eXqVOnyqpVqwLuU1NTI5dccol06dJFiouLZdq0abJtm7d/NJByPdFbGFhUMbgoAAAAAABAimiIw4SkD9HfeecdE5B/9NFH8sYbb0h9fb0cf/zxUl1d7bvP5ZdfLv/+97/l2WefNfffvHmznHbaaYlcbMB+Vds9p8Xdw1eiK1q6AAAAAAAAAOnTE33BggUBlx977DFTkb506VI58sgjpaKiQh555BF58skn5dhjjzX3mT9/vgwfPtwE74ceeqikrW3LRco3iAydImlj82ci1TtEBk+SlNLYKFLdQoieqR/TDBFx21+JvneXyNeviYw4RSS30N55AwCST+0ekRUve7YvCjomemkAAACAFNfgzXxiOX+kXE90Dc1V586dzamG6VqdPmlSU2g6bNgw6devnyxatCjkPGpra6WysjJgSknPzhB56iyRshWSNp4+R+Tvp4vs2SopZd8uEbd3oIeiEAOLZmQE9kW303t3irzwC5Elj9g7XwBAcvrkYc//C4vuT/SSAAAAAIBjOCZEb2xslFmzZsnhhx8uo0aNMtdt3bpVcnNzpWPHwEqoHj16mNvC9VkvLS31TX379pWU7qG9aamkBR1Qc88WTzW2OU3BdVnQWSQrJ/R9rL7odleiW4/93RJ75wsASE67vvGcptr/tQAAAIAj0RM9WTgmRNfe6F9++aU8/fTT7ZrPNddcYyrarWnjxo2Skqze2Fu/lLSg4bFVrV3jOWIhZVSVeU6Le4S/j1WJbndP9Pp9ntNtafI+AgBENkZHXdP4NAAAAACQ7hLaE91y6aWXyssvvyzvvvuu9OnTx3d9z549pa6uTsrLywOq0bdt22ZuCyUvL89MKU17aFtharqEn/V7m87XVKZoiB6ilUusK9GtEH3nWk9gkltk7/wBAMnFOkLJ//9dAAAAADFCT/RkkdBKdLfbbQL0559/Xt58800ZMGBAwO1jx46VnJwcWbhwoe+6VatWyYYNG2TixImStvyrkbcu87Q6SXV1VU3nU60SvTqBlei++bnTq78+ACA0a6BrKtEBAAAAwBmV6NrC5cknn5QXX3xROnTo4Otzrr3MCwoKzOkFF1wgs2fPNoONlpSUyMyZM02Afuihh0ra8g9Sa8pFKjeJlDZV8Kck/x/zqRaiW5XoRd3D3yc7N7btXKwdMn3G2Tt/AEDy0J3y1v9J/juvAQAAAMSIK8aV6N7WyEjuEH3evHnm9Oijjw64fv78+XLeeeeZ83fffbdkZmbKtGnTpLa2ViZPniwPPPCApLXgQ6y1L3o6hei1qdrOpXsEleh1sQvR06U1EAAgNN1J7fK2DaMSHQAAAACcEaJrO5fW5Ofny/33328meNUHVSNvWyYy9ARJaalciV4dTYhudzsX/0p0QnQASGtWKxdVR090AAAAIPZi3bOcnugp0RMdNlaip7pUDtEjqkSP8cCiattyz6C1AID0HlRU0c4FAAAAAJxRiY42Cq5G3pZuIXplGvZEj1Eluv9RDXV7RCo2iHTqb+9jAACS6/8jRTsXAAAAIA6oRE8WVKInI6t6uLCL53Tn2tQ/7Nq/Ii6VKtEbXSJ7d3jOF/cIf7+s3NhUojcEvZfS4agGAEDrIXpjvf3jcAAAAABAkiJET+YQvWM/kaJu2l1epGyFpE0Lm1QK0ffuFHFrC5WMpiA7XpXoGuC7vAFJn/Hpc1QDAKDlMTos9VSjAwAAALGvFI/1BDsQoicjq3o4u0Ckx6imwUVTmf9h5bUVqdd/VgP0rOzWe6Jbobfd/dD7jPOcbk3x9xEAILKe6IqWLgAAAABgEKInI6uPdU6BSM9R6dGGI1XbufgGFW2hlUusKtH957WfN0SnEh0A0lfV9sDLhOgAAABAjLliXIWu84cdCNGTubWJhug9DkyP8DN4YNFGbYGSSiG6tuWR1ivR7eyJbr2PsvJEeo32nN+9PvUGbgUAtLES3W8HNgAAAACkMUL0ZNQQohJ923IRt1tSVsDAqe7U+WFfncBKdP8jGgo7i3To7blc9pV9jwEASB7VwZXoKT5oOQAAAJBw9ERPFoToyciqINZgtesQkaxckdpKkfJvJWUFh+ap0tLFqkQ3A8S2IDs3BiG63xENytcaiL7oAJB2dEd8cIsx2rkAAAAAgEGInoz8K4izckS6DU39vujBP+R1p0Fa9kS3cWBRK5C35u0bpDaF30cAgND27RZprPec79Tfc5oqR30BAAAAjkUlerIgRE/2di4qHfqiB4foKVOJ7u0/W9w9wp7odlai7/Oc5hR6TtNlkFoAQPiduvkdRQo6ec5TiQ4AAAAARrbnBMnZzsWvDcfnKd6Goz5FQ3Sr/2yrIXp+DAYWtUL0/MCdMdoTvdElkpll32MBAJJnjI7cosDtDQAAAAAxopXisRzj0BXDeacXKtGTvZ1LurThsKrhrKrpmhRr51IUaYhuYyV6Q1AlepcDPDtmNDTZtc6+xwEAJFF7se5NITrtXAAAAADAIERPRs0GhPRWEO9enzrhcrgQvUOv1KlEdzWI7N0ZWU90HTw2VpXoVkCvlefdh3vOb0vhoxoAAK2E6MWe87RzAQAAAGKMnujJghA9GQUPCFnYWaRD76ZWHKnI+iFf0jt1QvS9OzyH7GRketZhvCvRg9u5KPqiA0B6ssbo0COjrCOUCNEBAAAAwCBET0bBA0IGhJ8pWEHsdjcP0WsrUiiw6NZ6/3FrYFFXXQwGqPV7H6XDILUAgJbH6PC1c6EnOgAAABBbrhhXodMT3S6E6MkoVAVxKvdFN8/XnXrtXKq2R9YPPdaV6Na8/XfGbFtu3+MAAJK0nQs90QEAAABAZfMyJCHfgJDenuiqx8jUbcPhfzh5SoXo25oCi9ZYleix6IkeUInufR9VbBTZt1ukoJN9jwcASIL/k3o0Beq0cwEAAABizNVUOBoTjTGcd3qhEj0Z1Vs90f1CdGtwUe2J3phiH5B674/4nKKmUDcVBlCt9qv6a028eqLnl4p07Oc5TzU6AKRfOxdtMZZbGDiQOQAAAACkOUL0ZBQq/Ox8gCdo1R+8u9dJSrEq4fRHfX5JClWiRxOi59pfiW4d0eC/M8a/NVAqHtUAAGhOd777/k/qQTsXAAAAIG4a4jDBDoToSd3Oxa8NR1a2SPfhqTm4qC9EL/JUSqdaiJ6wnujWwKJhQvRtKfY+AgCEpu273N4Bh4q6+g0sSjsXAAAAAFCE6Mko1ICQqTy4qFUJp5VxVoheW5lmPdG967qxQaTRppGVrcP0g0N0a3BRKtEBID1Y/x8VdBbJyiFEBwAAAOKGSvRkQYiebNxuv3YuweHngakZftZ5w179UZ/n185FX4tU6D8bzcCidrZ0saraw+2MKVsh4uLLFjDfNcn+fQNENEZHj6YxSPz//wUAAACANEeInmxcdU2j9oZtw5FqIbo1sGhhUyW6vg52tjZJZOVfJO1csvxDdJued7idMZ0GeKr+XbUiO9fY81hAMsvISPQSALHl64fezXPqq0SvYgcSAAAAEFNUoicLQvRkY7XgCDkg5EjPacVGT3/TlGvnUuQJdzO8b9uaJG7p0lDXtI6syr+WaM/7zGx7K9HDheiZmSLdR6TmDhmgrQjSkcr8BxX1D9G1T7qdA1oDAAAAQJIiRE821mCQGVmevqX+CjqKlPbznN+2XFJvYNFiT8Cb1yH5Bxe1WrnoeizoFNnfWNXodlWiW/MJDtED+qIzuCgApLzgI6OsEF3RFx0AAACIIVeMq9BtGlcPhOhJx38wyFCVkak4KKT1nK0f9akwuKiv/2x3z46BSFh90U1LHxtf1+AjGlK5NRAAoPUxOjKzmsbLqCdEBwAAAABC9GQTbjDIZuHnshRs51IYGKLXlEvSqvIGFkXe/rORsNa5bT3RrUr0EO+lVB2kFgAQvhLdf6BrX190QvT22rp1q8ycOVMGDhwoeXl50rdvXznppJNk4cKFcVuG8847T6ZOnRqTeR999NEya9asdt8vIyPDN5WUlMj48ePlxRdftHlpAQAAnIae6MmCED3Z+IJPb6CcDpXo/u1cVF5p8rdz8QUWEfRDD65Et6s/bcO+8O8l0xM9Q6Rqq0j1DnseDwDg7B27hOi2W79+vYwdO1befPNNuf3222XZsmWyYMECOeaYY+SSSy5J9OI5zvz582XLli2yZMkSOfzww+X00083rxkAAACQaIToSdvOpZVK9LIVIq6GFAvRg9q51KRIO5eEVaLvC39UQ16xSOcBnvP0RQeA9OqJrnKKAo8GQ5v88pe/NJXVixcvlmnTpsmQIUNk5MiRMnv2bPnoo49899uwYYOccsopUlxcbKqwzzjjDNm2zbteROSGG26QMWPGyF//+lfp37+/lJaWyllnnSV79uzx3ee5556TAw88UAoKCqRLly4yadIkqa6uNn/7+OOPm6puq9L77bffNn/zm9/8xixTYWGhqZS/9tprpb6+PuLH1Qr3d955R+655x7fvHXHQVt17NhRevbsaZbp5ptvloaGBnnrrbfaPD8AAADnoxI9WRCiJ5uWBoNUnQZ4fvi6akV2rpGUCtGtiun8VKhEb0uInmtfJXpjo997qbCV1kApNEgtACBQo0tk747mR0f5KtG9O+8RtV27dpmqc604LyryG6zVLzBWjY2NJkDX+2sg/cYbb8g333wjZ555ZsD9165dKy+88IK8/PLLZtL7/uEPfzC3afX22WefLeeff76sWLHChOSnnXaauN1uufLKK00of8IJJ5j76XTYYYeZv+vQoYM89thj8tVXX5kg/KGHHpK777474sfVv5k4caJceOGFvnlru5r20vD8kUceMedzc73bPyHU1tZKZWVlwAQAAADEQnZM5orYaWkwSKWDVPYYIfLdJ55BIbsPk5Rr55Jfkjohun/VXzwr0f3nEe6oBu2LvuIlBhcFgFS2d6eIu9HTwquwS9P1tHNptzVr1pgQe9iwlrfFtDe6tixZt26dL4B+4oknTMX6J598YnqDW2G7Bt4afKuf/vSn5m9vvfVWE15r8KzB+f77729u16p0i1ana+CsVd7+fve73/nOa6W5Bu5PP/20XHXVVb7rW3pcrUzXkFsr2YPn3Ra6IyArK0v27dtnHleXSXcAhDNnzhy58cYb2/24AAAAiaOV4hkxnL87hvNOL1SiJ5uWBoMMriBOlTYc4dq51FamWSW6jT3R/UP0cDtkfO8jQnQASFm+nbpdRbL8aiusHde0c2kzDdAjoZXjGp77V3CPGDHCVKrrbRYNlK0gW/Xq1UvKyjzrb/To0XLccceZ4PzHP/6xqSjfvXt3q4/9zDPPmN7jGoBrKxkN1bW1jL+WHtduWgX/2Wefyauvvmpeg4cfflg6d+4c9v7XXHONVFRU+KaNGzfGZLkAAAAAQvRk09JgkMGDi6ZKBXFwiJ6XApXo7eqJXmtfP/TMnMDQJNT7aPtKkYa69j8mACA5+qGrXO92BpXobTZ48GDTI3zlypW2zC8nJyfgss5bq7WVVm9rGxgrfL733ntl6NChpro9nEWLFsk555wjJ554omnT8r///U/+7//+T+rq6iJ+XLtpmD9o0CA5/vjjzSCj2tKmpcA+Ly/P9JD3nwAAAJKKHhXqdsVwis12WzoiRE82LQ0GaelxYGr1sq4PN7BoRfKHFv79ZyOuRK+x730Urre+Ku3rea0b60V2fN3+xwQAOE/19tA7da3/c602coiaVlBPnjxZ7r//fjPAZ7Dy8nJzOnz4cFNB7V9FrT3K9XYNxCOl4bZWlWt7Ew3Etc3K888/b27T8y6XK+D+H374oWn9osH5uHHjTOj/7bffRv08Q83bDhMmTJCxY8eatjEAAABAohGiJ5v6CCrRtSe62rNFpHqnpGw7l5okbeeileTWDoCibompRG+IIETPyPAbXDRFjmoAAITZqRscotPOxQ4aoGvArIHwP//5T1m9erVp0fKnP/3JDMipJk2aZNqwaFX4p59+KosXL5Zzzz1XjjrqKBNuR+Ljjz+W3//+97JkyRLTjuVf//qXbN++3QT0VkuWL774QlatWiU7duyQ+vp6E5rrfbUHug4eqstkhe7R0Hnr469fv97Mu6UqdV0mbdfiP23b5n0PhjBr1iz585//LJs2bYp6uQAAAJJCYxwm2IIQPdk0RNATPa+DSKcBnvPblqVgiJ7k7Vys/rPaSqWgU+R/lxWDSvSWjmhQPUamVn99AECYnuhBO3UZWNQWAwcONMH4McccI1dccYWMGjVKfvCDH5iBOefNm+erIH/xxRelU6dOcuSRR5pQXf9O+5VHStuYvPvuu6Y1y5AhQ0xv8zvvvFOmTJlibr/wwgtNexcN5bt16yYffPCBnHzyyXL55ZfLpZdeKmPGjDGV6ddee23Uz1EHI9V2Mlo1r/MO7qnu78knn5SDDz44YNL+7eGccMIJMmDAAKrRAQAAkHAZ7khHPUpSlZWVUlpaagYbSok+ia//TuTDe0UmXioyuYUfFM/8RGTFv0WOv1XksEslaWk1003eoPnXaz0Dn23+n8hfjhbp0FvkiqYBt5LGpqUiDx0rUrKfyOyvIv+7l2eLLHlE5KirRY65pn3L8M07Ik+cLNJtmMglH4e/39LHRf79K5GBR4uc+2L7HhMA4Dz/ukjki2dEfnCzyOG/arr+/bki/71eZPTZIqc+KOko5bYh02id6XF2GYleGAAA4Ggahmp5YaK29Xzbmtu1ICKWjyNS2i1xzzOVUImebOprWm/nEtAXPcnbcPj3YrWes9XOpbYytar+WmNVjbvsaOdS03o7F//BRbd+KZLa+9sAID2FbedCJToAAAAAWLJ955BkPdHzIw8/k5nvx3tGU+CbV9rUp9XVIJKVnZwhejSDigYMLFpr386J7FZC9O4jRDIyRfbu8AQtHXq2/7EBAM5Rtb2VnuiE6AAAAEDM6PjsrhjPH7agEj3Z+AaEbK0S3Ruib18p0lAnScsa0Ex/zOtAl/490ZO1Gt0XorexEt2WnugRVqLr7V0GpcYOGQBA+Er0ouAQ3budQYgOAAAAAIToSccKP1sbELJjP0/FdmO9yI6vJWlZFdPWYeUqK0ckpyh5BxetdkAlum9nTCshuv8OmVQYpBawA62NkCr0aK69O0P/n2T9v1tPiA4AAADETGMcJtiCED1ZQ+XWwk+t2u4xMvn7olsVcFZFnMWqRk/GEN3XEz2o6i/iEN2OSvR9ke2MSaXWQACAQNqqS4dU0rZdhZ0Db6OdCwAAAAD4EKInm0gHhAwIP5elQDsXv0p0/8FFkzlED+4/G9ee6NFUoqfIILWAXazWUkCy8x/oOjMr8DYGFgUAAADi1xM9lhNsQYiebCIdEDKgDUcqVKJ7K+KCQ/Rk7Ile3dYQPT8xIbq1M2bH6qZ2QgCA5NfSkVHW2CuE6AAAAAAg2dHcuby8XJ5//nl577335Ntvv5W9e/dKt27d5OCDD5bJkyfLYYcdFrslRdCAkFG24dAevslYPVlnta8JaueSlwLtXNrcE92GINuaRyTtXDr0EinoLLJvl2eg2t5j2v/4AABn79T1b+eSrNsQAAAAgNPFum85PdHjW4m+efNm+dnPfia9evWSW265Rfbt2ydjxoyR4447Tvr06SNvvfWW/OAHP5ARI0bIM888Y9/SoYUBIYNC5VC6Dff0OdWep1XbJDXbuSRZJbpWgFvV83r4fMIr0SN4H2lw0jMFjmoAAASytg1ChujW/7vupv8zAAAAACBNRVSJrpXm06dPl6VLl5qgPBQN1l944QWZO3eubNy4Ua688kq7lxXRDgipg3F2PkBk52pPNXqHnpJy7VySrRLdqkLPymt6DpHSv7F7YNFIjmiw+qKve5fBRQEglVRtDx+i++9k1f+Lgwf4BgAAAGBPpXgs+5ZTiR7fEP2rr76SLl26tHifgoICOfvss820c+dOu5YPYdu5RNDLWmkFsYbo25aJDJ4kyRuiB1eiJ2k7l2q/wCLaQ+PtHFg0miMaFJXoAJC6leiheqJnZnr+j9CxWMxRYVEePQUAAAAA6dbOpbUAvb33RxsGFo00RLcGF03WCmLr+QZXwCXrwKItHTrfGuvoA1dtfI9oCHgfLfP0xgUApFBP9DBjdFg7sK3/iwEAAADYyxWHCfGrRH/ppZcinuHJJ5/cnuVBS1z1Im5XdOFnzwM9p9uWS3L3RE+xdi6hqv7iWYnua+cS4c6YbkNFMrNFaspFKjeJlPZp/zIAABwy0HW38CG6HkFlHRUGAAAAAGkqohB96tSpAZczMjLE7VeNqpctLhe7OGLGf2CvSNtwWBXEO772tIKJtAe209u55CVpOxdfYNGOSnQ7eqJb84g0RNcAv+tQkbLlnqMaCNEBIPm1tmPX2oFt7dAGAAAAYH/P8lj2LacnenzbuTQ2Nvqm119/XcaMGSOvvvqqlJeXm+mVV16R733ve7JgwQL7lgwthOgZTVXJrSnpLVLQyVPBvn2lpE5P9I7JGaL7Dp1PdCW699D87AhD9IC+6Mva//gAgMQf3bZvV8vtXKwd9lSiAwAAAEhzEVWi+5s1a5Y8+OCDcsQRR/iumzx5shQWFspFF10kK1assHsZ0WwwyILIB6XU+2k1+vr3PINC9h4jScX64Z6TIgOL+nqihwksIgrRazx9yaMdmLQ9A9SqHiOTu78+AKD5QNcZWZ6d7aFYO7Dr6IkOAAAAxESs+5bTMCS+lej+1q5dKx07equA/ZSWlsr69evtWi60FHxG2g89uC96MoafYSvRk7Unuje0KArTfzaSEN3dKNLYYFM7lyjeS1ZrIN0ZAwBIbv4DXWdmthKi084FAAAAQHqLOkQfP368zJ49W7Zt8/740kxt2zb59a9/LRMmTLB7+RCqBUc01cPJHn62FqLXVnqqstOiEt0v8G5vS5c2tXPx7ozZuZZD+wEg2UWyU9fXE53vfAAAACCmleixnJCYEP3RRx+VLVu2SL9+/WTQoEFm0vObNm2SRx55xJ6lgj2DQQb3st66LLkC54AQ3ftDPnhgUa3KTqYKOevw+bb0RM/KszFEb8N7SZfZDD7nFimjbRMAJDXfGB0t7NTNpSc6AAAAALSpJ7qG5l988YW88cYbsnKlZ6DK4cOHy6RJkySjPT2aEZvqYdVtmEhmtkhNuUjlJpHSPpI06qsDf8hbNPzNzBFprPe0dMnrII6nIYQV+LelnYsebp+VK+Kqa9qh0ha6I6WtRzXoDpm1b3p2yPQZ1/ZlAAA4p51LONZRYNb/xQAAAADs1eidYjl/JKYSXWlYfvzxx5uBRGfOnCk/+MEPCNDjoS3Vw1Yv7a5DYtsXvXyDyNwDRd6/Oz7tXPT95uuLXilJoaqsaSdIW0N/q6VLe0J0DeG1mjzdWgMhdiq+837+54pjfPAnkbsPFNkdo7E6dq0TuXecyJJHJWktf0HkrhEiGxcnekmQyHYuLYbotHMBAAAAgDaF6I2NjXLzzTfLfvvtJ8XFxbJu3Tpz/bXXXks7l1hry2CQzcLPZRITqxZ4gvSvXrRvno2upucc3M5F5Zck1+Ciu9Z6Tjv29ewEaAutRG9vO5f6fU3noz2qwdoZs/vbtj8+Us+69zyf/y//KY7x1QsiFRs8302xsPxfIjtXi3ySxCH61ws8Ryfp0SVI30p006YrjBzauQAAAAAxRU/01A3Rb7nlFnnsscfktttuk9xcb6AnIqNGjZKHH37Y7uWDHe1cAvqix6iC2Arn7awK9//Rbv2Q9+erRE+SEN167a0dGomqRLdC9AxtD5MT3d9avXOt8AXwfz9YR1s4gfVdFKsdh9uWe063rxRp0KM7kpDVXoqAND1FMkaHdRQY7xEAAAAAaS7qEP2JJ56Qv/zlL3LOOedIVlaW7/rRo0f7eqTDYe1c4tGGwwqI7Qy0rR/tGVmeljThQvTaJGnnYr321g6NtrBeB9OSpY0a9jXtmIi2Ir64W2D4Avi/H/S00SEN16zvoljtOLTmq+MyaEV6MrK+YwlI01NEPdFp5wIAAADElNuvL3osJm83XyQgRN+0aZMZXDRUm5f6+nobFglhtXUwSNXzQM/pzrX2/xjWtitlK5oCbR240tZ+6MWhw968kiStRPeui0RXolvzalMleplzwlI4J4xzu0T27RJHsL4X9LvJ1WDvvPUz5B+cxyqojzVC9PRmHTlifa+HQiU6AAAAALQtRB8xYoS89957za5/7rnn5OCDD452doiGFZy2Kfzs7u176m4KvO2iwbxV3awV0u0JeEO1GggeVLRZO5dySYqjCHZ8bV8lert6orfjiIaibn5h6e62LwNSi38bFye0dNH3uMv7GdHTnWvsnb9+h7r9diLFqmVMrNV5d8zWE5CmHf0/xPq/0/peDyWXnugAAABATNETPWlkR/sH1113nUyfPt1UpGv1+b/+9S9ZtWqVafPy8ssvx2YpEVhBHKo/eCR6jBT5pkxk6zKRPuPsW67gAEkrQNsS0IarvLd+xIcN0ZOgnYv2TdbguaCzSIdeNoToNTa0c2nDOtIe6voctNpYq4+LurR9OZCiIfo2kR4jErk0zVs8aSul7sPsm39wW6ykrUSnJ7qkewumTP1O79R6Oxd2tAAAAABIc1FXop9yyiny73//W/773/9KUVGRCdVXrFhhrvvBD34Qm6VEUIjehkp0/wpou/uiBwdIdoXavnYurVWiVyRXP/Ro+5DbXonejnYu/v1zqx1QcQxn8H8vOKFffvB3kO44jMV3Xr/DYjvWRKzRziV9+fdDb+n/JNq5AAAAALFFJXrqVqKr73//+/LGG2/YvzSIXQWxfy9uu6smgwMku0JtXzsXbyVcMg8s6uuH3o5WLnb3RG/rEQ0aumhlvRPadiDxXPUie3c2D+cSKfg7yO6Q25rfgaeLbPzIs+NgzzaRDi30lnYiQvT0VbW99UFFFSE6AAAAALQ9RFdLliwxFehWn/SxY8e2dVaItpd1dkE7K9GXewaFzIz6QISWA+KMTE+fYNtC9BSsRG93iG5jJXpbj2gwvfUd0vsaiVe9I/CyE94XVq9n6zvJzh2HOnCyNb++E0Q6H+AZZFTbWiVTiK7/B1gtswhI04+1s8v6Pg8nx/v/r75X7NxuAAAAAOChw201xnj+sEXUv4a+++47U4k+YcIEueyyy8w0fvx4OeKII8xtiCEr8Ghr+Nl1iEhWrkjdHpGKDfYs095dIns2BwbEtXaF6HtbrpjOK0mOEN2EbsvaP6hoQCV6bfuPaGjrzhirctEJFcdIvOD3gRNCdOvoFOs7qWpr87C/rSo2er7jtJd016FNn+lk64tuvgfcnvOE6Onbgqm4hUFFg3diW9sgAAAAAJCGog7Rf/azn0l9fb2pQt+1a5eZ9LwOMqq3IYasFh5tbcOhg0J2G2pv4GOFwx33Fyntm5h2Lk4fWLRyk6cyNjNbpFs7Bze0Y2BR64iGtrYF8vVEd0DvayRe8PvACb3yre+gkv1EOg2wty+69d2p36XZuU1BfbL1RfcPzgnR04+1s6u4laMnzP8T3p7pvE8AAAAA+9ETPXVD9HfeeUfmzZsnQ4d6w1gRc/7ee++Vd9991+7lg50DQvr3Rbcr8PENmHmg/e1VUqWdixW66ZEAVgjeVll2tHPZ274QnXYuCFWJrpXZ5rIDdq5Y3wn6HWH3gMrBrZn0uy8ZK9H9A1GrVQfSh/X93Vo7Fx101NqRbe3YBgAAAIA0FHWI3rdvX1OJHszlcknv3r3tWi7EYkBI5Ws9YHNVpgZK+SX2Voa3GqInSTsX7ZVsRz90uyrRG9pbie6tXCREh//7wDrKwgltfqzvIP2OsHtA5eDWTNbnesfXTUd5JIOAqmJ3U5snpFkleishusr1bnNQiQ4AAADYj0r01A3Rb7/9dpk5c6YZWNSi57U3+h133GH38iFk+NmeSnS7qzL9AiW7K8PrrRA9zE4D6/Fctc4Or6zwrr390P2PQnDVJe6IBquHrhPadsA5YZz1/t67Q6TRlT6V6CW9RQo6ibhdIttXStIIDkQJSNO0J3okIbrf4KIAAAAAkKaiDtHPO+88+eyzz+SQQw6RvLw8M+n5Tz/9VM4//3zp3Lmzb4LNrB+wbR0Q0r/1wO717a8Yd9WLbF/lV4keq3YuYXqi53Zo6tVqDSToRMGhmy0Di9bYcERDOyvRtRd2osNSOCeM6z7c83l0N4rs3emcEN363Ol3VUM7dj6p2iqRXesCv0u13UUy9kUPbs1Bq470EmlPdP8QnfcIAAAAYL/GOEywRXa0fzB37lx7HhnxHxBSFXYW6dBbZM9mkbKvRPod2vZ5afsCrYjWMFsHFs0rsTfQbq2dS2am5zFrKzyhWSQVdfGmz2Hn2sDQzZZ2Lu3oiW61bWjr+6iwq19YuqupMh3pHcbpIJ6FXTyV6HpdIj+P1neQfj907CeSV+r5ntixqn2fQ/3O1NYnxT1FivRz4KXzXP9ecvVFD64qrqPKOK22JazPSFEE3985VojO0QoAAAAA0lfUIfr06dNjsySIfQWxRdsbaIiuvX3bE6JvW+457THSE2jHqhK9pR7w+pgmRHdoJXqZtndwewZvsyNUtLMSva1HNGRl+4Wl2wjR051vgMJunve49b4QG468sKMS3VSKjxTZ8KHnO6s9IXpwP3RLUlai085F0v3oER2o2vp/O6JKdN4jAAAAgO20UjyWB/lTiZ64EN1SVlZmpsbGwLVx0EEH2bFciEUFsX/gs/r19gc+wYFSvNu5mMcsEdGHqykXR/LvGW+H7Nz2V6LbcUSDFZbSFx2+3so9PO8LrdbWVj+J5B+iW58/DdH1O2v0Wfa3ZvIfsNnt9gT3Tkc7l/TlP6hoJO9VQnQAAAAAiD5EX7p0qalGX7Fihbg1LPCTkZEhLhc9kmNCe09bg0m2pyd6QODTzhA9OFDSQFvVxKmdSyyCe7tttbEful2V6HbsjLHCUiuMQXrSHuP7dje9J/SIC2Uq0RPI+g6yvpPsqhT3DRIcVM3ebZhIRpZnZ17lJpHSPuJ4VKKnL/8QPRLWjmzeIwAAAID9Yt23nEr0xIXoOnjokCFD5JFHHpEePXqY4BxxYLXgUDneILWtengDIA1BNZzPzLInUIpZJXoEIbpTBxa1Qjs7+qEH9ESvs6GdSzveR76wlBA9rVkV55k5Ivkdm0K5RL8vfJXoHZvvOGxrpbgedeVrYTWq+eey6xCR7Ss8j5EUIfrelnukI3VZO7ms7/HW5HpbqhGiAwAAAEhjUYfo33zzjfzzn/+UQYMGxWaJEJp/5XF7K9G7HOCZh4Ymu9aJdG3DutSQzLRxyBDpPjwwsKqvFnE1eHpnt4fOp7UQ3RrM1ImV6BrWhQvdnNATvaVe860pdkjFMRwSxnXzjIvghBBddwzW7QncydZ9hEhGZlO/9g49o5/v7nWe7yTtI90lxHemBvUaomsLp6EniOPRziV9WTu/Iq5Ep50LAAAAEDPa0COWTT1oGGKbzGj/4LjjjpPPP//cviVAZKwqQQ1wNKxqD608t4Jvq2d3W/uhayBv/cDO69B0ux2V4cnezqX8W8/rkJUr0nWwzZXotTaE6O2oRLfCl0T3voazwjjti26uT2CI7v/dY+1k09ZFVvDd1jZW1lEl3YeF3kHoaxnj3XHmdLRzSV/Wzq9o27lYO7YBAAAAIA1FXSr88MMPm57oX375pYwaNUpycnICbj/55JPtXD7YORhkcNXk5k89gdLIU+0ZYC8rRySnyPNDW3sDF3Zu+/JpuxKrB3xEIboD27lYYV23oZ7Xxg66E8W2nujtqUT3hqVUoqe34DBOK9LN9QkM0a0danq0jTUQr/VdteNrz47DwZOin6/vqJIDYzvWRLwQoqcvX0907/d4a6hEBwAAAGKHSvTUDdEXLVokH3zwgbz66qvNbmNg0RiyYzBIf1YQ1NaB9nz90Ec1D7VNiN7OUNu/4k2D+XB8g5k6sBLdt6PBpn7oAe1c2lOJXmNDT3QrLKUSPa0FD1Do27lS5oBBRb072Cz6XbX8X20PucN951msz/mutZ5+41Yfaacf3aSvk35/EpCmD+vzaX2Pt8ba4cp7BAAAAEh57777rtx+++2ydOlS2bJlizz//PMydepU3+3nnXeePP744wF/M3nyZFmwYIGkuqj7gsycOVN+8pOfmBeysbExYCJAjyE7BoO0s2oyXEBsV6htDXqnAxb6V5Mm08CiVsubcKFbu9q52NETvR07ZOiJjoAwzgrRvad7d3rGRUjooKLe7yK7dhxara/CjW/QoYcnlHQ3ipStEMezeqBb646ANH1Y7ZaibedC33wAAADAfo1xmKJQXV0to0ePlvvvvz/sfU444QSTC1vTU089Jekg6kr0nTt3yuWXXy49ekR4GDDsYcdgkP50oD1V+Z3I3l3RtV7RKmhtixCuEt2WED2Cfuh2Pl4shGp5017WThRXbWKParAqjq2wtL2DyCLJwzjv+6Gwi2cATw2SdRDPtgzgaVuIHqISXe1Y7TkaI5oxAXSe5RsC5xOKfta/ecsTuPcZK45mfcdqkLpzNSF6OmlzOxfvzm0AAAAAKWvKlClmakleXp707JmA3/vJVol+2mmnyVtvvRWbpUF4VuVxewaD9FfQUaS0X9sGwtu+UqSxQSS/o0jJfoG3WQP5tbcy3Kp4syrgwrEez2khuraU2L3ec76nne1c2jmwqKves+7MvNrxXrLCUnF7gnSkeRjXrWnQ4sKugbfFm/XdY303WDr0EinoLOJ2iWyPslLc+o4s6SNS0Cn8/ZKpL7p/iO5/GalN17PvKIQI27nQEx0AAACIfU/0WE5aQ1tZGTDV1ra9OPPtt9+W7t27y9ChQ+Xiiy82BdfpIOry0SFDhsg111wj77//vhx44IHNBhb91a9+ZefyIbh/rQ6WZxcNfCo2eAKiAd+P/O+sgEirLjMyHFKJ7rB2LmVfeU479G7fAKthe6LXiLjdzV//SI9oaG8luhWWaiWytnTRVhZIP8HtXKxQ1rwvEhSih6tE189Kj5Ei69/zfIf1Pti+fuh2tYyJJ+s71lp3/uNQIHVZn0vdlsjrENnfEKIDAAAASa9v374Bl6+//nq54YYbop7PCSecYAqsBwwYIGvXrpXf/va3pnJdx9DMysqSVBZ1iP7www9LcXGxvPPOO2YKHliUED1GrMEg7RpY1ArBV73S1Os3UttaCJTsCtGtnQatDc7n1HYuseiHrvz7w7vqmirTI+XfS729/fWtsNRq6YH0E6othL4vtFV+tcNCdOuoEA3Row25W+uH7pu/93bdMdmWnVwJqUT3ViMTkKaH6u1N6z3S96cVorOjBQAAALCfX7V4zOYvIhs3bpSSkpKAlixtcdZZZ/nOa3H1QQcdJAcccICpTj/uuOMklUUdoq9bty42S4LIQmW72rm0p/XA1hYCJd/AonFq52IFZXV7RBpdngrpVO2HHhx8ayAebYjuf0RDewM+KyxNVMUxEr9jr7YiMIj1r2xO1KCz1ndP8MCi/p/HqL/zIqxE7zpEJCvX01Km/FuRTv3F+SG6dwcIIXp6sD6XkfZDV1SiAwAAAElPA3T/EN0uAwcOlK5du8qaNWtSPkSPuie6nd5991056aSTpHfv3qaK/YUXXgi4/bzzzjPX+0962EB690S3aWBR/0CpbIVncMhIaHVlPCrRI23n4t/3uL192O0UaegWLQ3oLA11iT2iwReWEqKnJavSXN+TOj6CxeqxXeWteHVUJbpVKb7M810WCd05p9+R/u1awsnKEek21Pl90fU5WQMM+0J0745LpF8LptZYO7N1OyTSbQUAAAAAkdGfpo0xnCL86dtW3333nemJ3qtXL0l1UVeiWy/QSy+9JBs2bJC6usAg76677op4PtXV1TJ69Gg5//zzTT+dUDQ0nz9/frsPN0h6VvjZ3hYc/joN8Pw41vBk5xqR7sNa/5vKzSL7dotkZIl0G574EF3bm+iOBa2w1sdsadC/eDGh21eRhW7R0upxfQ9omOHfmiVSVnBmR4juC0sJ0dOSFZJrGOd/VENxoivRy8OH6N2GiWRme74rKr4T6RjYEy6knWs9nxv9nuk8oPX762dej9bRnY3DfySOZB2R4j+4ZJ3fdUiDFkxRhOj+O++1pUtWiM8WAAAAgJRQVVVlqsr9O5J89tln0rlzZzPdeOONMm3aNOnZs6fpiX7VVVfJoEGDZPLkyZLqog7RFy5cKCeffLIp11+5cqWMGjVK1q9fL263W773ve9FNS9tPK9TSzQ01xUTKR1d1n+EWR1xNrXaudjYEz0zU6T7CJHvFnsCn0hCdO31q7oODt1axqoMb29VuBWiR1J5r49phehOsGudZ3m0ZUqXA+yfv7ZwMSF6G0ZStgYWtWNnjBXC0BM9zdtCBIVxVmVzot4X1ndPXmnoz462XNGdXPpdFkmIbvVD1+/KSNpFabX7535tr5zI+n7NyGwa+JhWHemhug0hun5udMe52+XZ2RJqBxUAAAAAR/dEj9SSJUvkmGOO8V2ePXu2OZ0+fbrMmzdPvvjiC3n88celvLzcdBY5/vjj5eabb06Loueo27lcc801cuWVV8qyZcskPz9f/vnPf5rm9EcddZT8+Mc/tn0BtTF99+7dZejQoXLxxRebQwRaMmfOHCktLfVNwaPPJn87FxtD9IC+6MvsGWDPautgVYO2uxK9lZ7oAdXvDtlh4gvdhsemR7sVgLelEt0K0e1oC2SFpYmqOIYzwzirsjlRRyi01M7F/7sr0gGVo23N5Ju/g9u5+H+/5nbwnNdqez2KBqmtLZXoeqSJ9X8xO1sAAACAlHb00UebQung6bHHHpOCggJ57bXXpKyszHQm0aLqv/zlL9KjRxRjLqVTiL5ixQo599xzzfns7GzZt2+fFBcXy0033SR//OMfbV04beXyxBNPmOp3nfc777xjKtddLleLIX9FRYVv0oA/JfgPCGmnaAOf1gIluwLtSNu5BAxmWpHa/dAtWd69e+2pRLdjgFpfWJqg3tdwZhjn27ni0BA92gGVox0kuKe3hdPu9c7ZsRd24OaiwO9Y/zYvSE1t6Ymucr07XumdDwAAANirMQ4TEtPOpaioyNcHXZvGa/+bkSNHmss7duwQO5111lm+8wceeKAcdNBBcsABB5jq9HAjvurhAyl5CIGdA0KGCnyiDpQOjG2g7QvRI6iYtqsPu11ae43aSw+tV23qiW7j+4hK9PQWLoyzQvV9u0Rc9Z7BNuPJCq6t7yLbdhxG+HnW9igdeovs2expG9PvUHEcq/+5HpFiWnVkirgbPd+7ed7KdKT4ESRRVopYO1uoRAcAAACQpqKuRD/00EPl/fffN+dPPPFEueKKK+TWW281g4PqbbGkfdi7du0a0OA+bdg5IKQ/7fMrGSJVW0Wqd7RexawDkEZSia59id3tGAJYBy+Ltp1Le/uw28XqGx+rSnSrnYurPT3RbRxY1ApLkaY90YPCuILOnv7JqjrORynod06rlegHNg0Y2loguHeXJwxXPTw7i2PSJive/I/0oVVHmh5B4j2SKNoQnaMVAAAAgNj0RI/lhMSE6HfddZcccsgh5ryOyKoV4c8884z0799fHnnkEYml7777zvRE1wr4tGPngJD+8opFOg+ILPDRqkqtVizsGr6KzQquTFVjVXzauViDmTqhEn3fbpGKjdGHbm2qRE9wO5dEhqVIPGudB4dxOmCxr9VPnI9S0O8NHfywpRBdd/6Y6nm3SNmKludnfSd26h9dhbb12XdqX3RfO5fioCpjWnWktNqqphA86nYu1o4W3iMAAAAA0lN2W6rB/Vu7PPjgg21+8KqqqoCq8nXr1slnn30mnTt3NpOG9NOmTZOePXuatjFXXXWVDBo0SCZPnixppz5GlehWe4Nd33gCnwOaRuBtsde3Vi+GoiF/Zo5IY70n1G5ra4CoeqI7qJ2LVYVe2i98iJfIgUUbbBxY1ApL9SgGrW4s6d3+eSIJK1p7hA6qzfsizjtXrKNRdOdOS+9x/Q5b+6YnJO8zzr5+6Bbr/pG2yYo3K0i1vl99ITpVxinN2qmVU+TZgR4N6/PE0QoAAACAvWJdLU4leuIq0e20ZMkSOfjgg82kZs+ebc5fd911kpWVJV988YWcfPLJMmTIELngggtk7Nix8t5776Vmz/NEhuiR9kWPJFDScN2OUNuqdtMf+62xazDTZBhUVGXntr8S3a4jGqwq5EQNIglnDlBotfqJdyW6fyuXcDv6oumLHm0/dIt1fz16p9GBo7gEjzlBQJreR49Egp7oAAAAANJcRJXonTp1koyWAgk/u3btivjBjz76aHG30Df7tddei3heKc/OASFjHShpgLV3R/tCbasiMqJKdKudS7kk3LZlbatcjVcler2Nlei+KuRlTYPVIT3o57NuT2Bg7s+qTo/3+8I3qGipTTsO2/h57nyA53OqFd+714l0OUCc3c6FVh1pPY5BJOibDwAAAMSG1l3FsvbKgXVdKR2iz507N/ZLgvgNCBnMqprevkqkoa6p0tmf7uywWpW0Fij5Qu32VKJH086lY/sfL6kq0dvRE923M8amSnSrCplK9PRiheMaFIdq2eTrib49QZXo3u+gVnccLvdUimtromA6WK5+J7bl85yVLdJ9uMjm/3laxjguRA/6fqXKOM2OHqESHQAAAABiEqJPnz496hnDZnYOCBmstK+nclMDqB2rQleal28Qqa3w9DvvOqTl+dnSzsUKeSLo22o9ntUPOVFcDU0DFcalEr0t7Vz22rszxte2gxA9rVjhuK7/UEcpWZWuiWzn0pKug0Wycj3V9OXfNg2u7G/H1yKuOpHcDiId949+WfQ7QEN0PcJn5FRxFOtIH+uIFKuti/X9gPQbx6A11nuEEB0AAACwFz3RU7cn+iuvvBKyzcrrr78ur776ql3LhVgOCBlMQ7DWBsKzWr10Gxa6Ut1fXkn7Qm2teq8P6tkbyeMluhJ95xoRV60n+O8UIpRzQiV6vc1tgawQnXYu6cUKx0P1Qw94X8R7YNGKwO+EcLJyPN9lLbWx8h15M7Ll/urtbRmTCLRzSU/W93SoFkwRV6LzHgEAAACQnqIO0a+++mpxuZrvxmhsbDS3IUas8NOuASGj7YseTZsSXyV6G3uUa/VnY0MU7VwcMrCo9dp1HxG6PYQjeqJbFah2DSxqVRwToqeV1sI4XzuXRFWie1s8tSfk1jYs5n6jYjvWRCLQziXNK9HbEqJ7d7RwtAIAAABgf89yVwwneqLbJuqkb/Xq1TJixIhm1w8bNkzWrFlj13LBn1Zmx7IS3aq29A+O2jPAXnvbufgHOTnRhOgVntcqUdobukVK21C0NUT39US36X3kC0sJ0dNKa2FconauRNrOJZKQ27q+ra2ZrO/Uio0i+3aLoxCip3lP9PZUovMeAQAAAJCeog7RS0tL5Ztvvml2vQboRUURBJ6Inn9YGoue6P7BrwZHoYJoq1rTCoZiWRluHS6uFdc6QF+rj+dt3eB2JfYHfntDt7j0RLf5iIZE9b6Gs8M4K1zXo1Ha8j5tK+s7p7WBRSPZceg7+ibEGBGRKOgoUtovsDWMU/jaZRUF7qwkIE1t7emJbu145T0CAAAA2KsxDhMSE6KfcsopMmvWLFm7dm1AgH7FFVfIySefbM9SIfSgonYOCBlMW5BkZIrs3dk8EK3dI7J7XeSBUrsr0YMGvWuN3i8zO/GDi7Y3dIu6J3p72rnY3BM93mEpEsv6jghXia7tVHQQ4nj3RY+mEt36nOrAosE7/DRsNC1rMkS6D2//zkmn9UWnEj396M5xXxsm7xFE0aBvPgAAAACb1NfXy8aNG2XVqlWya9cuSdkQ/bbbbjMV59q+ZcCAAWYaPny4dOnSRe64447YLGW6s0J0DaUiqcxuCw1VuwwKHfiUrfCcFvcUKeoav3Yu1o/21uigf+19zPaq3iFStdUbujVvd2Qrq4pce8e3uZ2LTSF6osJSJJa1rsOF6DomQCL6okcTohd2FunQ23O+7KvA26zq9C4HRDYuQ6stY8JUuycKIXr60R3M1vd/u9q50BMdAAAAsJUrDpMD7NmzR+bNmydHHXWUlJSUSP/+/U2e3K1bN9l///3lwgsvlE8++UScLLst7Vw+/PBDeeONN+Tzzz+XgoICOeigg+TII4+MzRLC/uCzpcBnx9eewGfwpLb3+s4raV9VuFXpFk14pY+pVfSJCtGt16jzAJG8CMP/hFSi77P3iAYrLN2z2VO9W9rHnvkiSSrRW2gLoQG7eV/EceeK9Z1jfQe1Rr/TdBn189vvUPtbMyVdJTpVxinL+hzmdhDJbcOYGOxoAQAAANBGd911l9x6661ywAEHyEknnSS//e1vpXfv3iZT1kr0L7/8Ut577z05/vjj5ZBDDpF7771XBg8eLE7TprLmjIwM88R0QhxYLTjs6mPdUuCz/F/NA59oAyXbKtGj+KHf3j7sydIPvd090a0Bam18L/nCUgYXTbtAzqo2D8WqUndqJbr1eV39evPBRX2tmdr5eba+D/RoHldD7I4kaut3rNUL3QpIrf9rkH4tmFrDjhYAAAAgNmLdt9wBPdE/+eQTeffdd2XkyNDjLE6YMEHOP/98efDBB2X+/PkmUE+ZEB1xVh+vSnRvj+CwgdKB8Qm0rSAnmkr0RLdziVc/9PZWojdYIXobKhHDsUIZq98uUlttVdPAlK1Vosf7fRFtiB6uUty3U6ydn+dOAzxtqTR43LlGpPswcQTauaQfXz/09obovEcAAAAAROepp56K6H55eXnyi1/8QlKmJzoSwO7BIFsLlHasbgruGxtFti2PshK9pJ2V6FY7lyjaovges1xSvxI9r/2V6HYe1ZCIimMkPozTHTEttS6y+i7H8wgFa8ed9X3QGisk157oja6mz5W2tbKjEl3bHVljJATvnEwUV72Iy/vdQYiePqpsCtEb60Ua2jAeBwAAAIC07okeboDR5cuXyxdffCG1tW3IuOKMED0ZxKsneodeIgWdRdwuke3ewUR3r/NUnWblNQ082hqrClSDGiuMb0+VZDSP2dY+7O2hgcL2VZ7zPUIfmuKIdi4aElqDkdpZie4LSxlYNC1EGsZZVerxCtH182AdaRFpJboOHKrjA+iOyl3rPNdtXynS2OAZNLdkv/Yvl6/a3SGDi/oH5daOSuuUED11WZ/Dtgwq6t/6R1lHogAAAABAG2nLFh1c9JhjjpGjjz5a+vbtKwsWLBAnI0RPBnYPBhlORkbz9gZW9WT34ZH389WByySj7aG2r19vFEFvXgLbuexY5anO02Xo2M+5lej+7V/s7omuqERPD5GGccXd4hui+7ePinRg0cwsz3eb0gGVg1sz6Xdie1lHpzilEt06sikzWyQ7N/C7lhA9Ddq5tNCCqSX6XsnM8ZznfQIAAADYJ00q0Ru104WfWbNmyd///ncpKyszg4vecsstcvHFF0tKheiffvqpLFvWVFH34osvytSpU83IqnV1HOIbE7EYDDLSvuhtGWBPWxhYIVZbQm1fJXo07VwSGKJbr5FWodsRukVciV7TtveR3TtkfL2vqURPC5EOUGiFdfHqiW7tsNOdeBqORyrcjkO7WjP1CNN3PVFCHeljndejh7TdC1L4CJIWBgNuja/tDwPQAgAAAIjOIYccYjJli2bI/fo1FaLq+ZqaNnSzcHKI/vOf/1y+/trTL/abb76Rs846SwoLC+XZZ5+Vq666KhbLiFgMBhl1oBTlAHvtCbXb086lrYOZtof1GrW3f3KksnLbVoluhejamkd3dNjezoVK9LRg7SxpLUSPd5sfazyESFu5hN1xuMzez3OP/9/encBHVZ/7H3+ykEAIe0hYREAFFFSs+966tO7W5V+Xtldcalut1qtytVQRbb3a6r0udau1V6vdrFq13tq6W71Vwa1VREClKC7syBYgCZn5v54z+Z2chEwyZ50z53zer9eQkGXmJGfmzOQ5z+/7TMy9Xb9EpHGFFF1XMyec79NlnEx+41w6xP603YcAAAAA+JeJ4BIDt956q3zrW9+SCy+8UBobG2XGjBmy2267yd577229PfHEE+U///M/Jc5cV9K0gL7LLrtY72vh/MADD5Tf/e538qtf/Ur++Mc/hrGNMLniQQ6D7DF6YLZINuutE70oRXSfw0xLZahoEJ3oQa9osLOv6URPVyd6D7EQpsjetMbbbATPQ0UHeD9xqMe8oB/P1f1EBo2NTy56V3FZVlRHZce4FyS0E91jnIuqIvYHAAAAgPdO9Ndee03q6+utonlVVZXMnz9fLrvsMpk+fbpVbz7zzDMlUUX0bDZr59g888wzcuSRR1rvawD8ihUx6LJLIlPUCHuwqBo6IVdM0WL0sndF1izyNjDTT1Hb7pT00okecRHdz4mGyDPRQ1rRYOIBoiqWorjMyZK+Q3t+TJpVE1FEupjHvjn2FMoc29Z+kjvmbfxcpKxCZOj2wW2bOTYsnSNFZ6I4Oh9f7agOCqSJo89TjUHGuXAfAQAAAAKTCTkPPSad6KqiokKmTZsmjz/+uNxyyy1WBroW1DUmfOTIkRJ3rovou+++uxX2/utf/1peeOEFOeqoo6yPL1y4UBoafHQ4IT/TcRxFEV0LtHUTcu+/dX/u7YBRIn0GubseP0Vtc9LASxHdyyBTv125G1aIlJWL1LfFNkTVia75xXFY0dB7YLTFUpTGgEKdD2CvUoiyiO6yE723YyCwOebVjQ92xUbnyJi4xbk4/09UR/Jo1FFrc3BxLi0U0QEAAAC4N2fOHCvFpLW1VZ5++mk59thj5YADDpDbb79dSoHrIvpNN91kBcGfd955Vsv9dtttZ338oYcekn333TeMbYSJ4YgizsXZNTn7Qe+xBmawqJeitpc4Fz+DTP0wXehDtovmJEeHInqzjjcu/ooGLZZGnX+N+A8WdXarR1FEN8cacyzwUuQ2x7ygV5V0njVRTPmOr3QZJ5c5LlcP8HdyiPsIAAAAELyUZKLfcMMNsscee8j1118v++yzj9x1110yZcoUmTVrlsycOdP62OzZMYhA7UZbCGrhdt555y5/KP0laFs+QtAS4WBRu2j+B5F1i70XlHxlovuJc4m4E12z46PMQ3fGuZhu9PI+xV/RoAVVjcNguGjyYyEKjXNxFtqjuF947UQ3x7j5j7cf84J+PJvrWz5PZHNzLoO8WOyVPp2eT8zzi4l7QTpPfHXHvo9QRAcAAADgznXXXWfFuBx00EHy0UcfyeGHHy5nn3221NXVyX333Wd1pp900kkyd+5cSUwR3Xj99dftH2yHHXawYl4QErv4GXEnutEQdRHddEx7KKJr7rdmhTsLzWGKOg9dOX82vW8UWhQ3xbPKkIroijiXZGta156tX0hBzr5fLI93Eb3zMS7ox7PGxWgXsM4NWPFetMeLQk9SEueSgggmn0V0uxOd+wgAAAAQGJNdHub1x0A2m5Xy8lwgijZh6/+dvvzlL8s//vEPiTPXRfRPPvlETj31VHnppZdk4MCB1sdWr15tRbncf//9stVWW4Wxnelmx3BE1YneFm1gDOv0f1eDRaOKc+nX/r7epp/haW6YjOPOv7Mw6eBXzWDPZtwNF20JuRM9qtgOFI8phmvBtZDHZ98oO9HXehss2uWJw52CjzzSAaaLXs4dM4paRG/Mk4lOVEdirQ+qiG5OtLBaAQAAAIA7//Ef/yFHHnmkTJ48Wd577z255pprtvia3r0jah6OKhP9W9/6lrS0tFhd6KtWrbIu+n4mk7E+hxCENRAyHy1Am+KXFu4HjYm4E91DEb28wl8Ou9f9suL93PtRFsW0IGfuC2aVQiFMB3EYKxrsYilF9ERzGwtRCoNF1cAx7QXCmjr/xcZuc9GLnPFGJnr6mMefn6GiivsIAAAAEF4nepiXGJg6daqVfX7hhRfK3//+d/n2t78tpcZ1J/oLL7wgL7/8skyYMMH+mL5/yy23WBNVEYKwBkL2VPBZ8JxI/cRcgdotU8hyW9DW5RxeMtHNbertbVotkVg+VyTbKtJnsEi/4RIpjXTR+4XmK7seUBtGJ7oplhYhE33Z3NzJjInHSmy883BuBUfduHCu/9M3RN570t331AwR2e0Mf3ncbotxZkVI3IvouqRMj3WfvJo79umJqqCZyBizeqVYTAG088omk5HeEmCBVI/nb/8h97sdvnNw15s085/I/f7HHhjzTnQy0QEAAAB4t9NOO1mXUuW6iD5q1CirE72z1tZWGTFiRFDbhagGQuYzYtdcEX3EF7x9v+kKd9uJbv2sWW9FdK+36dXy+bm3GtMQRtGtO1460UONcxkaXfZ1Zw+dKbLsXZGznhEZtYcU3cIXRR46Q2TkbiJnPxfObTwwRWTNx+6/Tx8ju5waXTHOnFyJIivfnLAzxwG3Ru6aK6J7PeYV3In+Tq64HPUxo8c4l9rgC6Tadf/Id0SGjBM5//XgrjdJNqwSuf/ruePyDxZ5O2kdWSY6ufkAAABA4DJtlzCvv8h+8pOfyPe//32pqek5pnrWrFmyYsUKOeqoo6Tki+jXX3+9nH/++XLbbbfZw0R1yOgFF1wg//Vf/xXGNiLMDuJ89vt+roi962nevt9rnIuzgOM2A96+zYjiXNYtzr3tP1IiV9HWTewmE92Oc0lQJ7oOutQCuvr09XgU0T95Lfd28VvhDLnVQrZVQC8T2eOs3NuefPZmrntd3/oporstxkUZ82N3oudmdbh24CW5+7H1Ow2BdmPrLIMNK3KPk37DJPFxLuYYufL93HHZS1590q39NLeiSQvTG1aGEyVkxzC1Hae9MvcRszoOAAAAAArw7rvvyujRo+VrX/uaHHPMMVY9eejQXDPm5s2brc9rxMtvfvMb+eyzz+S+++6TOHJdRD/99NNlw4YNstdee0llZaX9A+v7Z555pnUxNC8dARbRw8iy7q4gfcBF/r7fS0HbdLhpAd1tR56fHHYv1rd1XYdR9AilE92cjAkzEz3iTvSlbQV00+EbB2Y7MptzqxWCjrEwmdpDthU56r8L+5637s91BPv9HbktxpnHhj6utTjrdnVJVHEuqu8Qf8e8nujJqyHbiax4L7cfYltED7DL2Hn8XzpHZPQ+wV13UjhPMOn7oRTR247LfX0O3DYntolzAQAAAIITdm55DDLR77vvPnnrrbfk1ltvla9//euydu1aqaiokOrqaqvGrL7whS9Ysza17hzXAaOui+g33XRTOFuCeMW5+GUKWc3rRFo3i1RU+svrLeg2I45zcTtkMUimu9lNJ7p9MsbD77bQOBfd380b2rNzw7Z0dtfvF5Mz81rfD7qIbq7fZGy7yuOe4y9KxG0xrrpf7qSNHsO0QDh4rIQik8mtSlBx7nbW/aBFdL2vjju0ONvQkqeI3ssU0QPsMnbOp9D7LUX0Horo+pyyY/CPDXsFid9OdOJcAAAAAHgzefJkueuuu+TOO++Ut99+Wz766CPZuHGj1NXVyS677GK9jTvXRfQpU6aEsyXIzyydjjLOxS9nLrFmFdcMLuz7TAHHS8eq12GmXgVVmIi6Ez2MFQ26v02xVH8vVWMkEs7O6mXz3J2wCYP+jld+EG53vLlOk7FdiLrxIuW9RJrWiKxeJDJodDSd6Fqs15NMepualx9WEd16zGf9ZaJHQffZnIeLu2oiyjgX5wlNs4ICHTnnBYQxU0JPZOiqmCA60cO4jwAAAABpl4JOdKfy8nKraK6XUlPu5ZsWLFggl19+uZx66qmybFnuD8C//vWvMmfOnKC3D2EPhAxLZVV7x7OborbpcOs89C6WcS7LgilM+OlEb/WSiR5Cl7gWS6PMv+6q61t/F5q9XEyaz57NhNsdb3ei7+Tu8Th0+47f74Up8rlZfRHF/cI85vVETpSxV26ZfeZnH5RSEd157C/mz1xSneghXb/OCtDjgB/2fYRMdAAAAADp47qI/sILL8hOO+1kTUt9+OGHZf36XNFTs21mzJgRxjamm0YvhDkQMkymI9RNUTtfgSes2wuiOFHUTvQm9ydjwshEdxZWoyqia0yByUSvqYtHLrq5fef26GM4KLq/NQ7EbSe68+u9/o705/ASYRTF0FlTrI1zF7pzH6x4v/3xGLV8JyrDiOpwHov1sZqJWQtEHDPR4zpUVNGJDgAAAARPSwaZEC8BliTSznUR/Qc/+IFcffXV8vTTT0tVVXtX08EHHywzZ84MevvQ2tze2RpW8TMsXjrD7SJ6TXTDTL1obRHZuCoGmehu4lw2hHsyxvwenPEEYfp8YS7fWR8X2x8Vj25XzRxXk44TKavI3UfWLQnu+pfPy0UzaFdp/5HuvtfORffYHa+PYz0eObvL3eTlhxFVEdRQ0aj0Gy7SZ5BItlVk+dzibIPpIu68IsUcc81xIgjOY7+eDF61MLjrToqw41y8rB7Jxzl8NsiTgwAAAACQxCL67Nmz5fjjj9/i4/X19bJixYqgtgudc6zDiuEIk5eitj30zkucS4Sd6KYwoYXSPgXmvRd7sGjYA2qj7kQ3Gcv1O4gMnxyTInrb7Y/cXaRuXMePBZqHvpP74aCmC9oU+t0y+7V6gLvIlCg60c0xJu5FdN1n5mRGMVZNbG4WybREmIne6dgfl+G/qYpzCXAAtrmP6EkgN889AAAAAHrORA/zguIU0QcOHCiLFy/e4uP/+Mc/ZORIl52RKLzwWVYuUtFLSkrviONcosxEd+ahl3saLVC8waJhrWiIOhPdFIO1KKlF5WLHuWhnpnPop10snR1CHrrLKBfre9p+R9oN3LTexyBdlzMAzMyAKDLRzTEnzsx9tRgnfJxRLVsU0UOMczH3gWLHLcW+iL48xOeqAIrovRz3mSBXLAAAAABInQ8++ECefPJJ2bgxV6vKlsBqV9fVv1NOOUUuvfRSWbJkiZSVlUkmk5GXXnpJpk6dKqeddlo4W5lmdgRHjfvO05KOc/FRRHczyNR3HnoRolw6dKK3xWu4KaKHtaLB7kQPsePYyRQhtShZP1HPNImsXyLSWKQVMWs+FmlaI1LeS6RugqPz+50QfmYPRfS+Q3JxIhqIpgNQo8pWtjvRoyiix7wTXRWzE90cXyuqtjwpa44LQQ6NNMfi0fvGY6VI3LRuFtmwMprBokE8V1VUilRUB3+yBQAAAEizTASXGFm5cqUceuihMn78eDnyyCPtRu2zzjpLLr74YklUEf2aa66R7bffXkaNGmUNFZ04caIceOCBsu+++8rll18ezlamWdjDIMNkhvy5KWqbP8y9FHo1ZiKyOJdiF9E9dKLbcS4hDxYNM/vayRQhtShZXSsyeGzbx4sUGWG2Z+gEkcqq9s7voIqlzk53L53ozu/z8jsyXbKmqzhOWfmlMlhUOU+uRH2m3ZyU7eokpfmYxr24OTnXHXMsHr1/7i2d6B1t0BN+jvuAFtS1sB7n5yqGiwIAAADw4cILL5TKykpZtGiR1NS01/5OPvlkeeKJJyRRRXQdJnrXXXfJv/71L/nzn/8sv/nNb2TevHny61//WioqKsLZyjSzu4dDyrGOXSf6Bh+Z6I5O9EzIoU9eu3KLmYluCmiVYWWiR5B9bWxcLbJmUe79homdBmcWqVDXOWqlYVLu7cr3O8428Grd4tygUs3hH7q9t+vw0x3vNVs50jiXEuhE131XXimyabXI2k+jvW1zkrKr46uzsB5Ul7FdRN8n93btJyIb2gYyo/0xVVOXi2zTgrpVWI/xc5Ud+0OcCwAAABCIlGWiP/XUU/LTn/5Uttpqqw4fHzdunHz00UeSqCL6j370I9mwYYPVia5t9yeddJL1g2qGjX4OAduchCL62ojiXBxdqE3rJFReu3KDYpbUu8pED3mwqF0sXR5dHvqAUSJ9BhU/JsPZ3W2K5/2GidQMEclmRJbNDeD6236uuvHeVxP4+R157Wg1xTs9ieMli70QWpAulSK6ngDTfViM+2p3x1eNd7GjOgLoMtYTfOb4pI/TgVv7G2ybROZYqTFLYZ1sCvq5yu5EJ84FAAAAgHuNjY0dOtCNVatWSXV129+kSSmiX3XVVVaMS2daWNfPIWClHOdiDxZtK3C56pSs8VacMr+nsCNdSq0TXWMjwj4hYxdLG8MrlnY3YDOMDHIv22S2Q2cYBNkdv7RTkd4Le3vmiGQyHotxLovoGrVj4pnCWqVgTtSVQhG9w36IOHrIFMfzxWWZ424QQyOdJ0+r+7XHG5GL3sXzyNBwBjPrY9zEawXWiW6y84lzAQAAAAKRsk70Aw44QO677z77/2be5nXXXScHHXSQJKqIrtNS9Qfs7K233pLBgwcHtV3oarBoqek90Mdg0VqPtxnRcFG7MFEimejOrwuriO4sloaZf+3s+nYO2DSFyeXzg8t0LpSeNFi1sG072oqF1vYFmIu+xMdQUWPIdrluYz3R8Xnb9kZx4ijsvPxSinNx7sM4daJ3iOpYH9w+0Zz68ori/cxxZq/uaAhndoDGP2XbXjH3rQvmOslEBwAAAOCDFst/8YtfyBFHHCHNzc1yySWXyI477igvvviiFfOSiCL6oEGDrCK5FtB1gqq+by4DBgyQL3/5y1a0CwIW9jDIMHkpaHc3+M7NbUbWiV5f3E701gI70Z2Z3GFlojt/H2HmX+frRNe4CB0uq4MRV8yXSC17N5dnbBXDHLEJgXaiv7Nlkd6tikqR+h28bZN94shDLITdZRtWJ3qJFdGLld/f00nKIAukTZ32SbG67+PMHCc1aqU2hMeInbk+JBfXEwRz39ETcQAAAAD8y0RwiZEdd9xR3nvvPdl///3lq1/9qhXvcsIJJ8g//vEP2XbbbSXOKgv9wptuusnqQj/zzDOt2BYtnDuHjY4ZM0b22adteBiCE/YwyDBpB6LnTvS+0d2mr+JHsTvRXRbRdaChFlLDor+Pzz8Mt4jeurk9Y9x0etvxKZNEFr2c63Z1fi6yPPROXeLO7luN1OliFU/B+2/lBx2v0yv9/sX/zG3TxK8W9j267Waf+ulED+t+YU7Umcd/3Jn75soFuWOe1+Nd4J3oARbRnZ3ozvvtsnm5x3CYx6FS4XxMmWNDkDMlwnieohMdAAAAgA+LFi2y5mxedtllXX5u663b5mnFUMF/xU6ZMsV6O3bsWNlvv/2kspI/gCMR9jDIMHnpCjcxAr38dqKHGOeihWuT817sTnS3cS5hxwKF0U3Z2aoFuZ9H7yODxnb8nBbqtIgedYdv5zx0o26CSHmvXFfumo/bhyt66XTXAaU1df6zjb1kU2/8PNfh73VAYdhF9FLrRNffhxY2NbpDTwhttXs0t2u6h/PNnDDHhyCL6GafDByT62LWY/zK99tXRKRZVyuaAu1E9zgMuDtB3kcAAAAAhJ9bHrNM9LFjx8rixYulvr7j3ykrV660PtfaGrMN9pOJ3q9fP5k7t60LVET+9Kc/yXHHHSc//OEPrSwbBCzsYZBhcha0tZO1EM0lEOdiYi20ONpnkJTEYFHTiR72gNqws687dH1PFCnvdAgzneDma6KyJE/USmWVyNAJHb/Gz/Vrkd5rN7vhJZvaFON0zoG577lhCv9hZOXrsaXUiugd9sPsGMW51AZYRDfDXts60fWxaobikou+5WyNMB4jduZ6kJ3oAebmAwAAAEidbJ5Zm+vXr5feveMdZe26nfw73/mO/OAHP5CddtpJ/vWvf8nJJ59sZdc8+OCDsmHDBiv2BQFqKeUielvxRAebaVFGB0+GHedibjPMIrqze9BvQTOqwaJR3Y/Czr7Ol4feuTC51Gd8ihuZjMjSOR1v30m3U7dHL9sfGfzP7JYpZK5ZJLJxtUiftgHAYRbjTPd6kFEVzvt2ZnPHx38p0H254LloV00UI87FeWJD73sfz2rLRf+a/9sodV3FrQS5WsPPMOB87PtI2wlvAAAAAP5oZnmYzdcxyUS/6KKLrLdaQJ8+fbrU1LSvkNbu81mzZskuu+wiiSqia/i7+aG0cP7FL35Rfve738lLL70kp5xyCkX0oEXVQRwGXfatGdxa4NKCSk9FdC1G2nEDBRTcgxpm6pYpBHqJtShWJ3pUKxrs2I4wO9HzRKeo+okiZeUiG1aKrFsi0n+4hO7zhbn7bUW1yJBxW35et/Ntnx3H9s8cQM67rp4YMCoXL6PF/zH79fw9fvLQnd8XxskVU6zV/e71uFEMZl8uSVMR3cMqiKRqbRHZuKqLOJdl8X6uMlFAxLkAAAAAcEEHh5pO9NmzZ1vzNQ19f/LkyTJ16lRJVBFdf9iMFjtF5JlnnpGjjz7ael9D4VesWBH8FqadnWVdgp3o2gWsBRQtaFpF7ZGFDVHtLrO34DiXtszyqHJsS2WwaGRxLiEOFrW7srsoKOvjZMh2IiveyxWIoyiimy70+u27HpZoCodeO461o97cRhCd6OZ6rCL6O+6K6F6LcWHeL5zF2mKtDPHCvl/MyZ1A7BxNFAZT+OzVQxHdnMwMetirOXEQ9cyCOEe5lFWI9Bms7+T+r4V1LbBX9ArgNnye/OoKcS4AAABAsDIhd4vHpBP9+eeft96eccYZcvPNN0v//iW0kryN67/ad999d7n66qvl17/+tbzwwgty1FFHWR9fuHChNDQE+IcaOhaWS7GI7iygFBKvYhfRy0Qq+4R/e3HKmY0qEz30waIhdhyrRu0wX9yeid5tcTKirOnuivrOwuGqhSJNHgpPqxflBpNqBn/deB8b6iOP228shHOwaKHzEfwUa0tB3TiRiiqR5nUiqz9KRye6rhTR47ven8JcrVIKzGNKT0zpCRRdIaIF9SBnStgrSILsRA/wPgIAAAAgde65556SLKB76kTXuJZvfOMb8uijj8pll10m2223nfXxhx56SPbdd98wtjHdWto60b0WlYvNzaBP09mmf6R77crUwYfW7a2NNsc2ahod4iYT3V7REHInujP7OoxMclMYHzRWpLpf/gLxnIeji4zoLl5G9a0TqR0msn6JyLK5IqP28FakH7p9blBp0F3QrgYgeizGmceK3g+b1gWbXV6KQ0WVdhrrPl3ydm4fDx6b/CK6Rnrpz7nqX7mfufYgSS1zEsGcYNLnPH1fTxJqgb3/iHg+V9mrFchEBwAAAALRGnImepjX7dHrr78uDzzwgCxatEiam5s7fO7hhx+WuHJdqdx5552t7Jo1a9bIjBkz7I9ff/31cu+99wa9fbA7iEswE71DEX1t+ENFO9xemINFQ1giH3on+oZoTsaYgpBmsGuxNOqCtbMjPKrICFPY7y5qZZiP7vhCfma3THe8FvUzreF3oms8U1W/4DOfS7mIXoyMcOeJyq6YmJcgojrMMb/zCRO/8UZJ0VUsWJADePVxvWFF8M9VQd5HAAAAAKTO/fffbzVhz507Vx555BFpaWmROXPmyHPPPScDBsT77/rAQlh79+4tvXoFkOGJPAMhQ47hCIspoBSSUW7n9dYEcHtRFNGHxiATfZO7FQ1hxwJpcc5k5gYVSeAmOsVZbF7xfvvPHRa9n2ncivN2gy6WFlKkd0s7+bUYpseXlQui6Wg1j5egc9FLuYg+LOKCsjmZ1mMn+obw9ksxBqrGUVd55UHGYekskqyGH5aJ1AyRwBDnAgAAAISTiR7mJUauueYaufHGG+V///d/rYGimo8+b948Oemkk2TrrbeWOItgkhmCiXPpnYI4F9OJXuv/9kxOchjCGNbmtRM92yrSutlFtn4E9yM7/zqEXHRTeGuYlP9r+g3PDerT383yuRIqE4fSf6tcpnE+frpvw+hE1+gIkylfSHe8feKoPn55+aVcRLdPrkSU3x9lnIs5BnfeL3Si5x/WG+QAXjtzva7rgcdeUUQHAAAA4MOCBQvs+ZpaRG9sbJSysjK58MIL5Re/+IXEGUX0uLOLnyXaiV7toqgddJxL0AMMDbPUvpiZ6M6TKq1NLjLRI7gfmd9L0LEdm5tFls/ruaCsOez24MyQC3WFFrjtjuM5IhkXp4E1EufzhcF3ojtPRPT0O9LttTPRfdzng4yqSMJgUWdXtg4WDXOOQ6EnKs3HA4lzWdPxOaDzY2H5/NxjOq26igVzDuAN6vqDfp4KcrUCAAAAgPZM9DAvMTJo0CBZty4X/zty5Eh5551cTWL16tWyYcOG0i+ir10bwR/3KO5AyFh1ovso9JpCWmZzOIPPdGVA0xr/BcWgOtELzUU32fpRrGgIshDktPJ9kUxLbh8PHN3910aVi15o1MqQcblhsFqcXP2hi+t/N/dWB5NqR2mQCu0I3rgq19XfuWs2LisUSrkTvWawSL8R7oa8eqUnFU1xPN/JNHPs9dtlrHnc+TrRB4zKfUwfyyvmS2p1tbojyBOQQawe6baIvj68E9UAAAAAEuvAAw+Up59+2nr/a1/7mlxwwQVy9tlny6mnniqHHHKIlHwRXc8SLFuW+4Ps4IMPts4OICJRDYSMxWDRHobeFUK/t6yi8Nt0yyyzr6gqbtGuvEKkvLLwXHR7QG0E96MgIwnyRblot3l34taJrnEK9du73yZTpA8yysVtNrUpxmlEToWPuRem45ZM9OLkouvJNisju4A4F78nIJ1DhTsPFtXHbtQDVeOosYsit33sXB7O9QfBvu9k259XAAAAAHiXsk70W2+9VU455RTr/csuu0wuuugiWbp0qZx44onyP//zPxJnBQVl1tbWysqVK6W+vl7+9re/WZNTEZGoBkKGxc2gT3vonY9MdC3Q6G1u/Dx3m/2HS2hL8Hsq5IZNu8r1xEMhRXR7RUMURfSQsq/dDNi0u6xn57olw9hX2m27bG7Pg07tbdpJZPFbuWLpxGMLuw3TnRx0lIszzmXdZyIbVuW6orti9qPfYpwd5xJ0Ed10PJdgnIvZt+8/FX4R3dldnreIXhtMJ7o53usxyrlqxvkzf/RSunPR7czyLoroQRw7w+pEd65i0PuJn5VjAAAAAFJn8OD22kN5ebn84Ac/sP+/cePG0i+iH3rooXLQQQfJDjvsYP3/+OOPt8Lfu/Lcc88Fu4VpF2XxMzZxLn3936YpokcxDK5YtDhlFdGb4rWiIazsazcDNodOyHXq631gzSciA0dJ4FYuENm8MVdUGjy256/30h1v/8wFFOndqu4nMmiMyOcf5gZbbvPFrr8uiDz0DidX6EQvyqoJs9JHjwG6kqWQqA6vJ5962ifDIh6oGjd6zDa/ow6d6A3xz0TX+44e8/Q5pUWfs2PwXAgAAACUMl0wnAn5+mOuqalJbrvtNrnuuutkyZIlUtJF9N/85jdy7733WhNUX3jhBZk0aZLU1NB9FO1g0RIvorsZLOp3+KWb23TL7sp1DIMrFpNvXlARPcpO9JCyr03XaiFd33qCoW68yLJ3c98XRhHddMbX75C/KJmvO74QOtAzzE50c71aRNffUb4ielcds3HKyi/1Irq5P+t9VVc3FHJf8qKQmRPm2KuxL3pc8TqLI18eeld5/GGtFIkz8xgo7yXSZ9CWJyA3rc79/rvq4o/Dc5UpovtdsQCE6Ot6uAvw+m7PXlj4Fy+7sbCvu9fFBkwp7Mtec/GQ36PQn2ligT+P3v7cAm/7uuB/dqkvfE7DuQU+74Sy378kwTvDxdfeU8Tbvyekn0lCuP2/SeD7s+DHh5s/oe4N+HHk5jpd/D7PLfBnv93Fz35ugce7290cb+4Jdl8Wm5tj7WuXBH+d8h+FHUNfKyv8eSZo2mZ0cNFuPX2amprkyiuvtPLQtTH7kksukeOOO07uueceK9aloqJCLrzQxXNvXIvoffr0ke9+97vW+6+//rr89Kc/lYEDB4a9bWhtyQ3IjGogZBjMoM+COtHX+49zcXubbtlduTHovjMFlkKK6NoxHXWcSxC5vsa6pbnrKyvPFa0LoYU6LUxqh++EIyRwdkZ7gQVu0327elHuvtlT0ffzhblOTx1IOmQ7nxubb5t2Epn35+67oJ0RRkFl5QdZODUFW/O4LzVDts11h2tRctVCkbqQ9rUdl9XNSh/n57RA6rWIbo69+faJPob1sbxhpci6JcHHbsWdM6/c+TjQgroW1nXoqh7vBmwVz+cqvZ9sWEERHQAAAAiqU7w1+Z3oV1xxhdx5551W2snLL79sDRU944wzZObMmXLDDTdY/9dCeskPFnV6/vnn7QJ6Npu1LgiJc2hXqXeiF1REL6DIE/RtlnInuhZX3Q4WjeJkjB3nsjRXLA2C6d4evG3hGbz2wMaQIiOWuoxa0QJZ/7aimOkwL+T6teCog0nDUEh3vF1E91mMM53src3BPjZLvRNdO8/NiaGw7quFnqTUbTGRT+brw9gn+nw2ZFzu/TTmoueLBdOCelArecJ8rgoqOx8AAABAajz44INy3333yUMPPSRPPfWUtLa2yubNm+Wtt96yBo3GvYDuqYiu9IfeaaedrA51vey8887y61//OvitSztncbRUO9FNEUV/lp46pgPLRG9bJRFqJnrAObNhd6KbIrrfqJxC1IZQLHWTh965QBxW1rTbTnS3+ddefma3zHUvn59b+dJt16zPYpx2NVcPCDbSRbfZdFiXahE9qlz0Qo+vdi56Y7jDXtOci97d6g67iL7c3+NChwWH9VwVxH0EAAAAQE5rBJcY+OSTT2S33Xaz3t9xxx2lurraim8pK6F4T9dFdG2xP+ecc+TII4+UBx54wLocfvjhVtzLjTcWL8sokezu4T6lmxmrwws7F1Z67JT0W0QPMc7FLn7UxygTfZOLAbURnIzRLlMT4xBUpMtSLwXrtg7xVf8KvtijBap1n7Vt06TCv89NLrqbDHivBo7O7Ss94bHiva6/xhTzgijGmW52U5j3y3lMKdU4F+c+DrMru9CZE2alhzk5EdbqAGcueto0drO6o28AneiNK3StoEhZhUhN++T7wJj7CEV0AAAAAAXSznPNQjcqKyulttZnnHPEXGcE3HLLLXLHHXfIaaedZn/s2GOPtYaNakB83EPgS4rdPVyiUS4mHkCLW5pbrIWV7iIhCsnsLfZgUWeWbUl2okd0X9Lfj/7+tRBU1xbb4Ifdlb2Tu23QgpTus2VzRbbaXQJjCn9ahO6u2zbuneh6ck5PAix6JXd7XZ0QsGMhgiiiN4is/CC4obM6gNHES4QVeZO4TvTaAqM6Qoxz6VBELyDaKGm6W9EUxIkm871968IZVBvEfQQAAABAe2Z5JvmZ6NlsVk4//XSrA11t2rTJasju27djDfDhhx+WxHSiL168WPbdd98tPq4f088hQFEOgwyTXdReE1GcS5iZ6AFFWwTZid7qoohu8o7DZndTBtBx3LKpvUvaTSd6mJERXor6zo5jLepnullTtXG1yJpF7jvdveiuO163UQcIBlVEt/PyA1qhUOp56IbZx2s/aY/hKOU4F3Os7251gHlsrng/9xhPk27jXBr8HzvN4yusk73mPuJntQIAAACAVJkyZYrU19fLgAEDrMs3v/lNGTFihP1/c4kz16172223nRXh8sMf/rDDx//whz/IuHEBdJyinSkslGoeumEKKT0VtQsZfBfk7bmlRSWzjZ0HwhW1E93FYNEo4lw65PoGUERfPk8k29o2mHOE+wLxgueCj4zwEi+jBo/NxWlo8UljZvJ16Zvu3AGjcj93sbqgN6wUyepp6zKRmjr/t2UXCAPqRDerTUo5ysWcBBi4tcjqRbl9P/aAEIvoPcS5mLgXX5noBZzc6DdcpM9gkY2rRJbPFRnxBUmN7ob1BnEC0jy+wprdEcR9BAAAAEBOq9eJlS6uPwbuueceKXWui+hXXXWVnHzyyfLiiy/KfvvtZ33spZdekmeffdYqriNApssrimGQYSq0M7zQzN6Cby/gOBdT1NCTGs6s97jHuWSzjlUNNdEW0YPIvnYWrN3OBjCd4kHHZJjOdrdRKxqtUD9R5NPXc9eRt4jusUjvK497Tv77fM2QYOJSAs9ET0gnutkPVhH9nXCK6C1u41yCGCzaNuS5K/pY1sfPwhdzj880FdG7G9YbxAnIoIYB97hagTgXAAAAAOnh+lzHiSeeKLNmzZK6ujp59NFHrYu+/+qrr8rxxx8fzlamVZTDIMNUaFG7OeBM9KA70c2QTC1yxGHQa6Gd6K0tbd3EEa5qsAtBAXQcL/FRUHbmLmcCCgLT36d2x3veprboju66470W6b2o3yHXaa6Ft86FOzsPPaBiXBBRFV0Wa5NQRJ8Ubi56lHEu9smN/sUfqFpymegBnIDsrtM9CEGcaAEAAADQMRM9zAsC4am1cLfddpPf/OY3wWwBCsixLvUiegHxKpq9bLql/ca5FHJ7cVwi75a5X/TUie7MrY0qX9+OJAgg+9oU2LwUlLXTu6JKpHmdyOqPcnEqfmmGc2uzSFW/3GBRtwrpjrc70UPOQzfxHkO2zQ381OL9dod0ceJoaPyy8t0Ua0vBsG6y6Uu2iD6g+ANV4/i8bmKIusosDyQTvZsifRDs+wiZ6AAAAADSI8zUHfhl51gnJM7FFA664izYBNWJ3t3tlfpQ0Q5F9B460c3ny8pzBeUoBJV9rVE0pivbS9d3RS+RodsH2+3qLHCXl/vojs+zPa2bc4NHra91ObjUq3zbFHgnelhF9CR0orftg2XzcveBoJnojZ6eT4KI6jDH3p72i3OorT7W08Dc9/VY3NXvx8zb0N+heQ3g+jYCftx2ZnL1iXMBAAAAgsksD/uCQFBEj7OkxLkUMujTFNG10GtiSrwyhQntwN7cLIEJe4m85ziX5sI60Sv7RBdDY2df++xEX/upyKbVImUV7cVwt4LORfcbtWK6y/Vn27Bqy8+vWpB77GuxM4jO+ULk6wi2O1oDus/bURXLg4nXScpgUTVobG4VTmuTyMr3g79+Oy6rtrAiunMFixtaDDfH+p72y9AJIuWVua9f84mkgr26o6Hr47E+f1VU+zvZFPQKks6IcwEAAACQQhTR4yxNg0XNz6p/nPst9DoLN0F2o4c9rC2sTvSWTdFGuXSOJPDTYWqKunXjvZ9M6qnz2y2/Qz81esTEwHS1TaZIrwNIdRBpFPJlU9snjgKKhTDF+ExL7uSIX0nqRNdVDbrPw4o3iSrORY/lmc2F7Rc9EVg3IV256HYsWJ4Ctz7/OU82+bmN0AeLUkQHAAAAfEthJ/r8+fPlvPPOk0MOOcS66Pv6sbijiB5npvhZmYLBomZZuN8oF6WFR82rDjoXPeiuXL9MNEtPmegmaz7KIrqzWLrxc+/XszSAAZt2l3VAWdOmwGk63IPujveTAe95e9pua8V7He9PQZ840qJp74HBRbokqYgedi666yK6x6gOc5zX1SOFHM/TloteSCyYn8HMujLJHHPDzkT3uloBAAAAQGr98Y9/lB133FHeeOMNmTx5snV58803rY/p5xJbRG9paZE5c+bI22+/LU1NPRTy4KMTPcLiZxgKGfRpCjxBdd0X0v2elkz0YgyotYqlA/wXS5fO8df17fxeHSza3YmcQujPYhWWy0Tqd/C/TV12ovvsdPei/8hccVs7iJfPC/fEkZ8CYWdmfyZhsKhzn4fSiV7gicpePruMncNeC1lV5MxFT4NCYsH8DOA13esak9NnkITCvo+QiQ4AAAD4pov3MyFeYjZ+6pJLLpFp06bJK6+8IjfccIN1efnll+WHP/yh9blEFtH/7//+T8aMGSMHHXSQfOlLX5JRo0bJE088EezWpZ2diV7qRfQCCtqFdkkGWbh3y+7KDam7z3MmelP84lycJxvM781X17ePgnLN4FyR2FmU97w9bYW+Idv6u6921x1vd6JHNFRUabGzq+74ME4c2fcLn3n5iexE3ymY+2m3kVmFdqJviGafpK0TvZDVHX4G8DY6Tnx5GXxcCOJcAAAAAHi0ePFiOe2007b4+De/+U3rc3FW8F9YmU5D4P793/9dfvvb38qyZctk1apVcvXVV8s555wTxjaml91BnJAienf55HYRvTa623Qr6Hxov0xnuQ4ijOOKBj/dlKaIp0M2nZndXgWVi+43D73z9mjXd2tL+8cbV4qsW9xxAGlUOv+OWjeLbFgZ/H3edLUHGufSFhFT6qxM9DKR9UtEGleUZia6OeYWWkQ3j+1V/0pHUdbORO/mMWVnoi+L5/MURXQAAAAgOCnLRP/Sl75kNWZ39ve//10OOOAAibPKQr9wr732kjvvvFN23XVX6//Nzc2y9dZb25/X9zdt6iFWAt6K6KXeiV5dhE706oA70ZvWtxejw8qZDasTvVgrGvx0U6plc0WymVzRtZ/PTmjtdn3/Sf+56EF0xisdLKq5/c3rRFa8L9IwsWOkxaAxItVtuf5R6dwdv0GLuFmRsnKRmiEhDJ0NIM6lac2Ww4RLWXWtyOCxuYKy7odtDwrmevUkeKEnKs3nPWeiu9wnGmui9wm9Pyx9V2TUHpJo65f3XOT28xixI5jCLKLXOobIZsLreAcAAACQOMcee6xceumlVib63nvvbX1s5syZ8uCDD8pVV10ljz32WIevLcki+q233irf+ta35Itf/KLVdT5jxgzZbbfdZMKECVY2+rx58+SWW24Jd2vTxh4ImZDBotqhmGnNDf7szC7wBJ2JHlAnuilmaBasFrpiVUQvNBO9WEV0j8VSU1AOIhvc7rL2GZNhZ7T77IzXopN2mn88M9f5bYroxchD76oTPZtt3281dV0/Zr0yWdB+41y0eGdnoickzsXsBy2i634IqohuPZdkC5s7YY7BXodGblrtfp/oz2wV0Wcnv4heSCyYvVrDw2PEPG7DnN3hfJ7W+0lcnhMBAACAUqSd4mUhX3+MnHvuudbb22+/3bp09TlVVlYmra3x2vhyN53or732mtTX11vF86qqKpk/f75cdtllMn36dHnvvffkzDPPDHdr0yYxcS6OjsSmdT0MvQs4ziWoTnRT8OtuGFzci+hRn4yxIwmWF7fr21kgXvZu7kSOF9rxv2J+cFErXeWimyJ9lHnoxtDtRcoqRDZ+LrL2M0fHbMDFuKA60bWL3xSGk1RE7yqb3i9nvnmPRXS/g0U9nNhIUy56IXMG/MyTiOK5ynpN0vYqn0gXAAAAAC5oXHghl7gV0JWrNbgVFRXWBNXHH3/c6jrXDHQtqB933HEycmTb4D4Ep1gDIcMo9pr87nxF7UKH3hVrsGgU3X1umd9pj3EupogeUJd/VJnodv54AAVlHQSqhR+9n61a6O06ls8XyWzOFQcHbOV/m7rKaQ+y+94tPclSN759m+z7/NB43S86F2srqkp/tU4Y+f1dnaTUlTQ9RW/YcS6NuW7/KIa9NoQ4UDVO9Hdq9oXpNg86CiuK5yq9D9knWzzG/gAAAADIyURwQbRxLmrOnDlWbMtOO+0kTz/9tNx7771W6PvFF1/coeUeAbGLnyVeRDcFlfWb8he1g85ED3qwqJ0zOzSGRfSeOtE3dfz6qPjpONY4EbsrO4CCssaR1O8g8tmbuUJ13Xb+ivplZcF1HJufUweMaqHe+lwRiujmdpfPzXXHmwiXwDvRgyqieyjWlgKz7/W+sLlZpLLK/3W6icuyT7Zlc89Bbo/Jnorok9ofC0nO2Db3eT0WdzfzwDzPaIFa952bfWBWkIT9XKX3E90+r7E/AAAAAFLrtddek+eff16WLVtmdZ073XDDDRJXBf+lqj/EHnvsIddff73ss88+ctddd8mUKVNk1qxZVgC8fmz2bJ9D+5DMwaKFFLWdnZJB3l5gnegFLMEvWpxLc/dfZ4ockQ8W9ZF9vfqj3H2lvFd7d7RffiMjgoyXUVrU10gEPcmgha8V74m0NucGjurg0WJ3QYd14sgZ8+Ol0znpRfQBo3I/U6alPT7ILzcnKZ0rVpwxMIUyx3g3w17rxuVWFGhEjz72k8qOWqnv/kScFthNjJvbk02NET1X+Y39AQAAAJDTGsElRq655horMvyee+6R119/Xf7xj3/Yl3/+85+SiE706667zopxOeigg+Sjjz6Sww8/XM4++2ypq6uT++67z+pMP+mkk2Tu3LnhbnGaJCUT3VlQiaoTvafbC2MYXFw70c3nIy+im070Ze67S03BWnO6K3oFsz12ZITHInrQUSt6Xx+8jciqBbnrNh2k2pUbRKe7F+Zn09+/5qOHUYwzRflsq8jGVSJ967xdj5dibSnQfa/74aOXcvshiHz8lsbCZ07o41RPZur3WCc3h4Z/ckMf4/pYX/J27vE5eKwkklmVYyKNursP6EnI1YtyhXc3vw87ziXk5yo79oc4FwAAAACFu/nmm+Xuu++W008/XUpNwVWtbDYr5W1FMM1G1/87ffnLX7bOGiBAdvEzAXm/PXWGm47HoONcTG5yYJ3oMSqia+dmIZnophM96jiXDsXSz919ryl0Bxlr4qcTXY93S0LeJlOkL1aUi/O2tbBvOoKDvs9rwbTPYP+RLkntRA8jF93tSUo/XcZeBouGNVA1btysaPISh6XPBeZxEXoR3dxHiHMBAAAAfElZJnp5ebnst99+UooKLqL/x3/8hxx55JGy7777yi677CIXXXTRFl/Tu3cCir1xYsdwRDwQMgw9FbVNN1tgRfSB4cS59NRBWKxO9E4ntboeUBvx/ahDsdRlLrpmcgc9YNPkLq/9RGTDKnffu25xrmtau7OH7hDgNjm6403xsBhDRZ2Fu5o6kWxG5NM32z5WH6+8fL/F2lJgn1wJKCLNFMMLPQaY7HRPRXRzcqN/8QeqxraIPjScAbzma/UEq3kODIuf+wgAAACA1Lrwwgvltttuk1JUcJzL1KlT5bDDDrMHi26//fbhbhmKNxAyDKagEtlg0YDjXOLYiW4y0XUAoA6lzDeAsJgrGvT3pcVnKw5nYnE70bXYOnDrXESCDjAce0Dh32sK3JrdHOTv0dmJbiKDgojv8EpjJHSb/vW33AqCsE4caRFRB5h6ycv3W6wtBfagzXdyJ8j8xvu4PUlpojpMDEwUKwSCPnEQR27yyr0M4DXXr4/ZsCOh7E504lwAAAAAX/RP7zBfvscsE33q1Kly1FFHybbbbisTJ06UXr06Rvg+/PDDElcuQorFKp5/7Wtfo4AehUyrSGtT8jrR8w0WbQkpzkVvz8/wQqVFrDhnoveUi17MFQ1eCkHaYfz5hx07tYPiNRc96Dx0e3varm/Zu20F5bK2gaNF1PlnDOM+b3fZ+ulEX53cTvT6iSJl5SIbVoqsW+L/+uy4rAIy0f3GuZhjvNv9Yu53GiMUVAxX3LgZ1msP4F0WTqd7YJnodKIDAAAAKNz3v/99ef7552X8+PEyZMgQGTBgQIdLyXei/+QnP7F+yJqanotws2bNkhUrVlhnFeCDsyiahEx0e9BnW+ErqsGi2qXdvM5foU2LQmZ/9I1jJ7qeWWyO54oGL5EEWlBW/YaL9B0S7PZot+v8x93nLoeRh64GbJW7b5ru3SHbBvcY8MrZCa/xNSaSJ6yhs17Zg0Xj/STriQ4BHrKdyIr3cid8+g8vjUz0zc3tJ+3cDnytGSzSf6TI2k9zK0VG7yPpzkT3EecSxfOUuY+Y/Q0AAADAm0zI3eIxy0S/99575Y9//GNJ1o0L6kR/9913ZfTo0XLuuefKX//6V1m+vH0J/ubNm+Xtt9+W22+/3cpLP/nkk6Vfv35hbnM6mMKnquwjyR8s2rYkvFdABUQ98VBRHUyky/q2+3tVv/Yc2DjQ5frmZyyoE70I96NaDx3HYeShb5G77DIywnSuB90Zr/vQeZ3FzEPvahu0Y7ZtoHSgTKcsg0V73g9BxJvYcS4FHr/MqhW3UR3OlUZui+hpyEU3x8FCVnf4yUSPYsWUfR+hEx0AAABA4QYPHmxFuZSigqoj9913nzzzzDPS0tIiX//612XYsGFSVVVlFcurq6vlC1/4gtx9991y2mmnWZnpBx54YEE3/uKLL8oxxxwjI0aMkLKyMnn00Uc7fD6bzcoVV1whw4cPlz59+sihhx4q77//vqSCKXxqkTSMIlbU7EGfa6PpRC9kmKnrwkcES+S9dqNvbov+6TYTvYhFdDfZ12HkoRvmOpfNE2ndXNj3tGwUWflB+NsU1vW7VTdepLxXuMU404nrJqoibUX0YQEWlFvcxrmYqI4N3vaJnnCsKHjkSjpy0a1YsOWFP668DN+NMnbMvo+QiQ4AAAD47hQP+xIjV155pcyYMUM2bCi9Va0F/5U7efJkueuuu+TOO++0Os8/+ugj2bhxo9TV1ckuu+xivXWrsbHRut4zzzxTTjjhhC0+f91118nPfvYzq9V/7NixMn36dGu4qXbG9+6dgIiT7hRzGGQYuhv0qUMxTRxJ0EV0LSr47UR3MwyuGEX0pp460Te2fW0Riuimm3Ld4lzUg5volDC6sgeOyRV/tPCjsTFDC5jvoAW9bEakpi6c+4Dz5wy6090LHVCrvxft1g+rGGd32foZLLo2uYNFnfcFt9FDxYxz8TvsNcmd6HrMMSczColbMSdttfBe6HBZ+4RvBM9VfnLzAQAAAKTWz372M1mwYIE0NDTImDFjthgs+uabb0pcuW4VKy8vt4rmevHriCOOsC5d0S70m266SS6//HL56le/anfE6y9ZO9ZPOeWULr+vqanJuhhr15bogDLzx3YSolx6inNx/hFeaKdkQbfZQw57GMPgomZyzgspohfjhIwp5vzrbyJXD/WezR0UXdXRMEnk41kidx7gcnt2LKyQVeqd6GY7rCJ6Q8gxPz6GZqalE33l+7nHsJ+VJJ6L6Ouj3SfmMb/03dxw7fIKSQzzPKKRZdUFPM+ZQru+FtD9UF1ATJ45KRXFc5V9Hym97hEAAAAgVlpL/PpdOu6446RUeVhvHY2FCxfKkiVLrAgXQ6e07rXXXvLKK6/kLaJfe+21ctVVV0liMtGLEcERBlNUcWbmdi7wlFfmumCD0n+EyKdviHz+YXTD4GId51KEPPcRX8gVg9zGdgzbOTdYMQyTjhf5+NXc0NlClZWLTAzpQF8/SWTErrmTPjpYMQ52OFbk3T+JbHtwONc/cOv2LlstvHopuia9iK6DdbXg2tIosvaz3NBZr9zOnPDaZWyO7173yeBtcieON28UWfUvkbpxkhh2lEuBBW4ttJv9r89BBRXRi9GJTpwLAAAAkCQavX399dfLG2+8IYsXL5ZHHnmkQ+E7m81acSyaVrJ69WrZb7/95I477pBx4wr7+02/t1TFtoiuBXSlnedO+n/zua5MmzZNLrroog6d6KNGjZKSo0WEJBXRzZA5LXx1XpoeRh66iQaY+7/+4xDcDIMrWid6UwGrGorQid53iMhFc3OFIDc0UzmsWQB7nyOy62kimQIz0ZVmhIc1VFZPHH37eYmV7Y8UmfZJeJ3AfQaKDBglsuZjkaVzREbv6+779RhiCrZeBliWAj1G6jHn84W5IqqvIrrJRHdZRDfHDrcnNrzuE72/NUzMnfzUGKUkFdG9FLjd7n83met+EecCAAAAJLITnejtEiyie6WDTvVS8loSVkQ3nYlauNTCjLOYYwqsQUa5BJmvG2Vhwq2Kqu6L6Do80xSLi3Vf0gGDFTHrFg76hE0ShR2loY9PLaLrSS63RXRdXWHmKCS1E91ZRPUzgLUomegD/N0vtIiux+0dt3zBVrK8xIK52f/6msGcWKKIDgAAAKCTznHX+eqnYURvO7W2tsqNN94oDzzwgCxatEiamzvOz1u1apXEVUitnv4NGzbMert0aVv3Vhv9v/lcohVzGGRYf3CXVXQcCBh2J7rJFF4+Lze81Hfxo770MtHNioYknZBBMpjHp2avu2WKtRqzE/TJtzixs+P9FtHbIjcK/V2Zr3OdiR7AsFc7F32OJIqXWDA3+998TUV1NKszzH3E7SojAAAAAB1lIriIWCkdGpNtLhqHHXT0diE0gvuGG26Qk08+WdasWWOliWjHu87gvPLKKyXOPBfRP/jgA3nyySdl48aN9tmIIOmSAC2WP/vssx3OmsyaNUv22WcfSTw7x7p3cqIJ7EGfa7ouoged2T1wdC4WRDtWV7yfzkx0czLG+tqE3JeQDGaliJe4JVOsrQ4x9icO+gZVRDcnKgs8xppjcbE60ZXfGK64Md3kbrrE3ex/5/NUGAOQg7qPAAAAACiKjz/+2Cpam4vGYUcVve3029/+1spTv/jii6WyslJOPfVU+eUvfylXXHGFzJw5U+LMdfVh5cqV1hmH8ePHy5FHHmmFzKuzzjrL+gW4sX79evnnP/9pXcwZDX1f2/nLysrk3//93+Xqq6+Wxx57TGbPni2nnXaajBgxoqQnuRbMZNEmqXvYFFa2KKK77JIslBYSGib5i3TRk0N28cPFMvy4dKI7VzREUVgB3BZLl80VybgMaUv6UFHDnLgzedp+n09cx7lsiHawqDLH7LWfiGyI7zK+yOJcCt3/UT9PEecCAAAABKM1gouI9O/fv8OlWFHYS5YskZ12yq1Arq2ttQr66uijj5bHH39cElVEv/DCC60zBVrorqlp72rTNvwnnnjC1XW9/vrr8oUvfMG6KG3h1/f17IO65JJL5Pzzz5dvf/vbsscee1hFd72NpAfVW1o2JSvOxVlYMYUWr0PvvERG6JA6Lzatbs9ejmWcS9tBz2xj0lc0IDkGj811s2rk0MoF7r43NUX0oR3nMnihJyjsInrYcS4+B4sqXbGkq4iSFuniJ86lkP0f9Yopcx/R5xi3J8EAAAAAlKRhAURvb7XVVnZD9rbbbitPPfWU9f5rr70W+xmXrovo+sP99Kc/tX5op3HjxslHH33k6rq+9KUvWTEwnS+/+tWvrM9rN/qPfvQj6yzFpk2b5JlnnrE64FMhaYNFnYWVfHEuYRTR/Q4XXd9WvKgeEM9CdI+d6BvCicoBghhcWj/RWy56kynWJr2IHkAnujkGuDkOVBUxziXIodBxst5PnMvScDrd/XBGA9GNDgAAAMQ+Ez0u0dvHH3+8/f3aOD19+nSrpqzpI2eeeabEWaXbb2hsbOzQge6cnhr3MwYlZfPGFMa5hFDoNUPqvObrmuKFm8JHlCqreiiimxUNMTwBAOhKkU9fzz0+dzyx8O9LSye6XUT10YluFzjLCn8+MSc0nQV4V4NFB/i/X8x/PDm56B1iweo9nEQpYP83RtyJrs8pOtg3m8ndx/wMkwUAAAAQG5oConMwDRO9PXjwYNl6663t6G0tfGtRXYvgbqK3f/KTn3RINdHr1KGken3HHHOMJKqIfsABB8h9990nP/7xj+1u8UwmI9ddd50cdNBBYWxjOiWx+Nl7YA+d6AFnoqv6HXLFIy0waKee22K4l8JHUTrRm9KTrY/k8NpxHFSxtlTiXPRknhZivcw1cB5fC/1+cyzW44dGdeiqgaJ0onuM4YobjTAzJzrdxILZcT7Let7/UZ/w1W3R+4n+bHSiAwAAAN5l4nX9Gr3trO9q9LaaMmWKlRyi0dvaYK3R26tXr5b999/fV/S2drAX2sVeckV0LZYfcsgh1i+1ubnZ+uXNmTPH6kR/6aWXwtnKNEpiDIfpVOtcRHc79M4Nvc4h24qs/CCXi77dIeEvwS9GJnq+TnQ7E50iOmLI60oRu1ib8O5XU3BtbcoVK70Up73EZTm/Vo/P1f2iLaKbWRbL5om0bhapcP1SJV5MJ3lVP3crrsz+1+N4T/vf3EaUz1V6P7GK6C6z8wEAAADElonezqesLXpbL4V67LHHCv7aY489VuLK9V+mO+64o7z33nty6623Sr9+/aw2/xNOOEG+973vyfDhw8PZyjRK4kDIvINF14d7wkC7GrWIrt2uXovocRwqWlAn+sbkrWhAcjRMyr1d95nIhlUiNYML+760xLlowVULr83rckVSX0V0F8dX63ihXc/Z3PcXUkTPZNqP7X73y8AxuS5nfW5Y+X7biqISZneJDw1v/5vbiPK5yjxnu439AQAAANCuNfenV8l2uheg0KgXLdC3tuovJCFF9EWLFsmoUaPksssu6/JzmmWDACSxE73HwaIhxLmYrsZ3H/WWr1syneg9FNGTdD9CcmhxdtAYkc8/zK0U2eaLhX2fKdaaY0qS6bFnlRZRl4rUbRdNJ7qJ6tDibaFRHfq15pWf3/1SXp47wfLxrNxxu9SL6H7yygvd/41F6kRXxLkAAAAA6IbGgCdBudtv0ND45cu3HHK1cuVK63MISCIz0fMNFg0xzkU17OQtdzkJmehJXNGAZPGSi56WTnTnsccci9xq8XiS0m2B1OyTiupgjjdJykW3VzS57EQvdP/rPjIruiItorfdp4hzAQAAALzLRHBBcYromouj7fWdaayL1xB5dGHzxuRlWdtF9LX+OyW95OuueC9/sTlOS+Td0IJVd5noZkVDZYLuR0gWL7noaSyim0KsW16Pr66L6AEPezXHbS8riOJmvc9OdOd1dHf9epwPa0VXt/cR4lwAAAAAdO+5556TiRMnytq1nWqCIrJmzRqZNGmSvPjiixJnBce5mGmsWkCfPn261NS0x0NoXs2sWbNkl112CWcr08iO4eiT/MGipostrCJ6/5EivQeKbFotsnyeyPDJhX9vMYa1BRrnwmBRxJyXjmO7YJuCOJe+ARXR3UY6mQx1t53oQe0TPyuI4sbPiqZC9r8zdqyLJofQuL2PAAAAAEhlJrq66aab5Oyzz5b+/bf8m3HAgAHyne98R2688UY58MADpeSL6P/4xz/sTvTZs2dLVVWV/Tl9f/LkyTJ16tRwtjKNUhXnEnInuhYVtNv1w//LdTUWWkTXzKaSiXPZlJ4VDUgW03G8fL5Ia4tIRa+evydVnegNHVfFuGWfpHQb51LbMQ4m6n3SMDE33FR/bj2Z6XYoZ5z4ma1RyP4v1vMUcS4AAAAACvTWW2/JT3/607yf/8pXviL/9V//JXFWcBH9+eeft96eccYZcvPNN3d55gBhDBZNUPHTFFfMUMDOP2tYRXTT7apF9KVzCv8e7VzPbPaeZRunwaJJOhmDZBk4OjeIUo8LK95vK572IFVF9KEdB0e65XXmhNs4F3NcD2qf6O0P3kZk1YLcKoXag6X0M9Hrw9n/psDuJS7GDwaLAgAAAP6lpBN96dKl0qtX/qa5ysrKLmdwlnQm+j333EMBPQr2QMgEFdG1UGaK5pub/XdKeul2dRMZYQoTGgVjitVxY4rjrT0U0d1GOQBR0ZUiDZMKj+5o3dzeHV2dhiK63070qDLR13Q8zgchKbnovjLRC9j/JnYs6pO95j5iToQDAAAAQB4jR46Ud97J/7fd22+/LcOHD5dEdKI7vf766/LAAw/IokWLpLnZUQwVkYcffjiobUs3u4M4gUV007VYWaf5QN4ze73kLmsxRm+zkNxYP4WPqFRWFdaJ3otOdMSYPj4XvSKyZLbIzid1/7XOlSypykT32onuceZEr77uojrCWB2guejv/qm0c9H1+caOWxkazv4vVie62/sIAAAAgOg7xWPSiX7kkUdaMzYPP/xw6d27Y41q48aNMmPGDDn66KMlUZ3o999/v+y7774yd+5ceeSRR6SlpUXmzJljTVnVIHgEJImDRSsqRar6dSy4tDa3R6aEGecydHuRsgqRjatE1i0OP8c2KmSiIwncdKJrzJIp4BWSn17qzPFHC7FakI28E31DcQaLJqUTXe+v+jznOc6lgP1vol6izo0nzgUAAABAgS6//HJZtWqVjB8/Xq677jr505/+ZF00J33ChAnW5y677DJJVCf6NddcY01L/d73vif9+vWz8tHHjh1rTVGNe9t9ydA/lJNa/NQCS/O69oKL84/vMIvo2oldN05k+bxcQab/iJ6/J+5DRQvKRDcDahN2P0Ky6ODfQoulm9ampwvdGdGhhVgtyPYZ5O77vc6c8BrnEmgnelsRfcX83DEurrFa3TEd5Bo95GVFUCH7n0x0AAAAoHRlQs5ED/O6XWhoaJCXX35ZzjnnHJk2bZpk25qEysrK5LDDDpPbbrvN+ppEdaIvWLBAjjrqKOv9qqoqaWxstH7gCy+8UH7xi1+EsY3p4yyIJm0gpCmwdC6iV1SH31Xa4DIX3RQmvHQPxqUTnTgXlIL6HfSpM3fiyqwAySdNQ0XNY9f8rD39brpiojZM9IbrAul6l4NFB0pgBmyV+9l1tdKK96Qk2QXuod73f3UP+9/P4FI/KKIDAAAAcGH06NHyl7/8RVasWCGzZs2SmTNnWu/rx7RBO+5cF9EHDRok69at2yIUfvXq1bJhA8OlAuEc0pW4TvQBHQsudtRABIMv3UYDmA7C2lIooufpRLdXNDBYFDGmxbgh2+be11z07qStiN4hF3tZugaLWkNnXaxSiCN7RZOPjorabva/lblepOcqiugAAABAMJ3oYV9iZtCgQbLHHnvInnvuab1fKlwX0Q888EB5+umnrfe/9rWvyQUXXCBnn322nHrqqXLIIYeEsY3pY7qKyyuTl/lrCiydO9GrasO/bVOMKXRIXaLiXOhER8zZK0V6eHyaE3BBFmvjzhRgTVezGybT3GsR3XlStxgnN8zJz1IdLmp3iQ8NoIjexf7XlQJmH1FEBwAAAID4ZKLfeuutsmlTrjCnge+9evWyMm1OPPFEKyQeAWhJcPewHefSVghr8dgl6acYs/KD3O+4py7/YuXMuqExOKq1KdeRqJ2bTqa4ksT7EpJFH5/vPtpzx3EaO9FNFIjpOHbD64lK8/WFxrnYWfUDwjm50tMKhbhaH2Anelf731y/7q8onkedKKIDAAAA/rXm0k2TnomeyiL64MGD7ffLy8vlBz/4gf3/jRvbir8IpoiexO7hfJnoURR5tYhRUyeyYYXIsndFRu5WWJyLnw7CsDkH7Wk3eufsc7OqgUx0xF2hK0XCKtaWRCe6j0x0t5FZ5pjserBowCsEGia13y+6OlFYMkX0oQHE+SwNp9PdK5Ozb06GAwAAAECCuY5z6UpTU5PccMMNJRECXxKSXPjsnS/OJYIOOi2+FJqLnsk4cmZj3InuPNHS1XBR04lembBsfSSPeWzqAMl88URhFmvjzBRIi5KJXkCcixa3w1ohoENny8pFNqwUWbdE0p2Jvjyc6/eKTnQAAAAgmE70sC+ItoiuhfJp06bJ7rvvLvvuu688+uij1sfvueceq3h+4403yoUXXhjMVqVdkiM4thgsuj66THQ3ucsbV4lk2440fesktqzM/LbOzK4KjyYTPWkDapE8/UeK9B4oktkssnxe/q9LZZxLQ8eCaaFaN+einnzFuRRQINUTeJmWcPaLHruGjCvdXHTTPW66yYPORA+i091vEb21WWRzc/S3DwAAAABxLKJfccUVcscdd8iYMWPkww8/tIaKfvvb37aK59qFrh+79NJLw93atEjyMMgtBot6HHrn1bCdCutEN8WKmiHxHu6q3fXmfmKKZc5uevMxiuiIO2ulSAGPz1QOFu2miNodZ8yG5070AjLRzfFcO8bDOCE6rIRz0U33uJ+hn92dRAkic90r574m0gUAAADwJhPBBdEW0R988EG577775KGHHpKnnnpKWltbZfPmzfLWW2/JKaecIhUVFcFsEdLRib5FnEtNxJ3oc3IRBPnYObM+Ch9RqazquhN9s2NGAUV0lIJCVoqksRPdjnNxOVjUHF/LKkQq2o4ThTLH5EI60c0+0RMbYWSWF7qCKG46xILVhxPnE0Snu5/nnvLKwmN/AAAAACANRfRPPvlEdtstN4hxxx13lOrqaiu+pazUhnyVgkRnopsiepHiXOrGi5T3EmlaI7J6Uf6vs7v7SqGI3rvrTHSzosH6GoroKAGFdBxvWp17q9EvaeHsRNbCbKHsk5S17ovb5pisq1k0FqaYw14LXUEUN3pfNTE3fgZ/2vt/+Zb7P4givR/kogMAAAD+kImevCK6dp5XVbV3slVWVkptbUSFz7Rp2ZjcwmfeTvS+0XXODd2+567GxlIqold33YluVjRoB2p5IDOEgXA5O47zrRSxC7YpinMxBVjNizcnEQrh5/jq/J6eojrCHvZq7hcr329/fiwF5mSsnvAxx+mg97/pRC9aEb228NgfAAAAAChhbetwe5bNZuX000+3OtDVpk2b5Lvf/a707dvxj/OHH344+K1MG1MkSGIER+ciuin0RlVEN92uS2fnIl22P6rrrynmEvmgOtHtFQ0JvB8hmfQEl0aPbPxcZO1nIgNGbvk1aYxz0ZN/fQblfi96bKoZHH4R3Tr5Vpkr3Or1dPf7Dnt1QL9hufkUG1aKLJsrMnJXKQl2gbshvP1vIn6K9VxFJzoAAADgj3aKhxny0U2SMdwpuD11ypQpUl9fLwMGDLAu3/zmN2XEiBH2/80FATBZ1kmOc9HhgLos3XSv9YqwiN5QQGREEMPgIu9E35SeFQ1IJj3maeRSvpUi2p1uBoumqYjuLJJ2lYudj5+ZExr/0qvAAmnY+0S3pRRz0YOMWulq/+vjodid6GZ2izkhDgAAAABp70S/5557wt0SdNGJnsDBojp4zpIVaV4XfZyLapjUczGm2IUJT53ozelZ0YDk0sfn8rm5k1zjD+v4OT3pls10OpakhB6LVsx3WUT3OXNCj8s6P6KnIrpzsGhYNBd94QullYse5PNIV/tfT15oZn1Qt+EFcS4AAACA/05xusVLAkHJcWR3EPdOZqdpRXV74aW5GHEubUPqVi0Uacrzh3+xh7W5YX6fW8S5UERHCQ8X7eoklynW6nDgtN2vzbHIzGsohN+4rEKjOsIeLKpKsRPdFLyDiFrpav+b69eTF8V6PBDnAgAAACAlKKLHUdKzrO1c9LX+OyW96FsnUjssd6pv2bsJyETPN1h0U3JPxiC5GtpOcunMgu7y0DXiI01MrrY5NhXCFDa9rmoyMTCFdqKHOezVnFxZ0s3Q2bgxRe7atsGggcS5LO2iSB/A9fsuohPnAgAAAHiNRA/7gmBQRI+jpMdwmEKL1YnuI7M3kIJMF7nomdbcALsgBsIVc7Co6UJNYiwQkss8Nld+0H4s3KLjOWVRLs5CqZnXEEmcS4FRHVEMe62bkFuBoPEyaz6WkmC6xoN4HjGd6M79H+T1e2WfaCHOBQAAAECyUUSPo6QPhHQOFy1GJnpP0QCNK9pyl8tEaoZIyXai2ysa6ERHCdGCYE1d7jHYeaVIFMXaJHai+41z6WloZBTDXiurRIZOyL1fKrnoQa5oqu2mEz2ITnev7BMtxLkAAAAAXtCJXjooosdR0oufdpzLGkdmb4RxLs5c9K6KMaa7T2NfKgqevRvfTnTiXFBKNKbFGd3hlOoiuodM9OaoMtEjGCxairnopms8kMGiDfkz0YvaiU4mOgAAAIB08FQhnD9/vtxyyy0yd+5c6/877LCDnH/++TJhQluXGPxJegxHtTPOZX2RO9HniGQyIuXlpZmH7uxEb23uOhM9qfcjJJc+Pv/1ty2LpabjOexibRx1FefRE/v46vEY0KtvfOJclJ5cebtEiuj6vBLkgGo7zmdZvJ6rzPNLC0V0AAAAwItM2yXM60eROtH/+Mc/yo477ihvvPGGTJ482bq8+eab1sf0cwhA0gdCmkKLFgCs2JQiFHqHbCdSUZ37w//zheF1D0Ya59KpE32zydZP6P0IyZVvpcim1entRDeFUi3MaoHWVZxLbcid6BHEuThPfpZCnMvGVSLZ1uAGf9qd6Ctyczus92PwXEWcCwAAAICUcN2Jfskll8i0adPkRz/6UYePz5gxw/rciSeeGOT2pVPSO9FNoWXtZ+0fi7oTXWNa6ncQWfzPXFfjkG237O4ruSJ6U54BtQm9HyG5nCtFstlcxEuUxdo40ngpndOghVkt0Fr/jygT3cTC9NiJ3j+akyur/pX72aJ+3nDDPI/0GSxS0cv/9Zn9rft/w6pcDnocnquIcwEAAAB8CTu3nEz0InaiL168WE477bQtPv7Nb37T+hwCkPhM9P4di+g6QLW8IvrtyJe7HIfuvkAy0RO+ogHJVTdepLyXSNMakdWL2j+e5kx0LcTWDHY3XNSeOeG3iN5NnEtrS3uUR++BEiotJNcO00qyyNJOQ2fjJui8cmv/D+mYix6HVVMU0QEAAACkhOsi+pe+9CX5v//7vy0+/ve//10OOOCAoLYr3eziZx9JJFNoWbfYX16vXw1tXY2d83VN8aPUMtHzDRbtldD7EZKrskpk6PZbPj7TXER3FmSdudjdMcVvk20eRoG0aV37+1Fk1TdMyr1dOltKo4geQJSLYZ6TrCi0rGMINkV0AAAAoNQz0cO8oEhxLscee6xceumlVib63nvvbX1s5syZ8uCDD8pVV10ljz32WIevhQdJL35Wd+pEL9aS/Hyd6PYS+YA6CCPrRG/Os6IhofcjJJs+PrVQqo/P7Y/KfSzNg0XzDZeMJM6lmwKpyanXQr3GZEVxv1jwbPxz0U2BO8jnEe04Xz4314Guv3czTJpOdAAAAAAIneu/eM8991zr7e23325duvqcKisrk9ZWknc8SXycy4COXZJeh94F1dG4ZpHIxtUifQZ2inMJsIMwTBVVeTrRNyZ7RQNSkovu6DimE71jgbYnJsvcbxHdnNjtStT7JN8KorgJY0WTKZbriV5z/fp7N6uRioEiOgAAAOALmegJjnPJZDIFXSige6RLtNMyWNQoVid6n0EiA0bl3l/2bgI60fMNFqWIjhLU1UqR1BfRHUXUQvg9UWm+r7tM9KiHvQ5zDJ3NZEogzqU+nJMoQWeue2XfRyiiAwAAAEg210V0hEyHtGUzyR4IaQaLGsU8WdDQqVCnv/8Nq0osEz3PYNHNFNFRwsxj8/OF7bnbdsE27XEubatlCo5z8XiMNcfmbuNc1kS7T4aME6mozhX2V38o8Y9zqQ8nzsecSCn285S5j+hwWW0CAAAAAOBKxtGNHsYlxq1HJcdTgOlrr70mzz//vCxbtszqOne64YYbgtq2dDKFzyQXP7foRC9SnIuJdHnvr+2REY0rdDmASFmFSM1gKa3Bonk60ZN6MgbJ1rdOpHaYyPolIkvfFdl6LzrR7cGiBXSi64yETEtAmegxinPR3PX67UUWv5U7+Tl4G0ldJ7petx07VuQiurmPZDbnMtqLGS0DAAAAAHEqol9zzTVy+eWXy4QJE6ShocHKPjec78MjU/gsK2/Puk6auMS5dBUZYboHtYBXXiEl3YnewmBRlDh9fH6gRfTZIsMni7Q2pbyI3taJbAqo3dHOYEOHfoYV52KGvUa5TzQXXYvomos+8dgUZaJ30YkelyK6WbFAER0AAABwJRNytzid6EUsot98881y9913y+mnnx7gZqDLYZBJPSmhhRk9SWBia4pZRDdD6pbNFcm0hlP4CJspWmgXoBNxLkhCpMsHz+ROcplirZSJVPWTVHLTiW4iWMp7iVR6PCFb5SLOpbp/cfPy40SfSzasCD6zvEMm+vKOES/FUtErF6+jJ7j0ZEuprOACAAAAgLCL6OXl5bLffvu5/TYUynQTJ7nwqScHtOCyaXXxi+iDx+YyXXWY68oF4SzBjyzOpXMnOkV0lLhhbSe5tOPYWawtT+k4D3Nyb8PKXKG2u9Uydh563wCiOlpy8TBdFeOjHizqzMs3MVxxo/vHOklcJlIzJPj9r7Fj6z6Lx2BRc7JlY1P3sT8AAAAAumSyy8O8fgTDdSXiwgsvlNtuuy2gm8cWtJibhsKns+BSzCK6FqHqJ7YXZOKyRD7QTPSE35eQXHax9F2RjZ+nO8rFxEyZVTzW/IZumAgWPzMnnDEwzniYYg4WdXair17UfvtxYp5HtICuGe5BsQryukItK7JsXnyeq+zYn25WLAAAAABAiXP9193UqVPlqKOOkm233VYmTpwovXr16vD5hx9+OMjtSx+TY530YZDOgksxi+imIPPp67loANPNHYfChN9MdHtVQ8LvS0iuIdvloiK0gKsZ2FEXa+NGT/ppIVUz0TXSo183XcimK9jP8VU7z3U2h0ZFaYG0z6Atv6YYw151O/pvJbL2E5Glc0RG7yuxYq9oCrhLXAvyeiJF97/diR6D5yp7AG032fkAAAAAukQneoI70b///e/L888/L+PHj5chQ4bIgAEDOlzgU1oiOHoPjE8R3e52fae9g7BviXeiZ7OOVQ1tucZAqdGiYf0Oufc//HvubZo70TvkorcVanuMc/H5+DfHj3xdxsUYLBr3XHS7iB5CXnnn56a+cSqi04kOAAAAILlcd6Lfe++98sc//tHqRkcI0jIM0llwcUYGFDN3WYsxQ7aNT86sn050Z0E96asakGxaLF38T5GPXs79P+1FdDNIssciegBxLub7dX5Fvi5jM9uiekD0Jz/feyKeuei6SiCs5xHtPF82p/3/xR4s6jzRYk7cAgAAAChYpu0S5vWjSEX0wYMHW1EuCDnOJelFdB0OGJtO9Em5t7o8PrM5vA7CsJgiuW67GTboLGYk/b6EZGvYqWNh0nnsSCNTmDW/jzAHizq/P9/QyGLEuZRKJ3oYBW5nfIvG2nQ17LVomejEuQAAAABILtdxLldeeaXMmDFDNmyg4ygUpviZ9GGQcRksqqr7iQwaE34HYVg0s7hzB7rpSi+vFKnoOLcAKCmmWGqkvRO9tsBOdPNcElgRPd9g0SLFuZiTK8vm5k4epiETvXMRPS7PU8S5AAAAAL4z0cO8oEid6D/72c9kwYIF0tDQIGPGjNlisOibb74Z0KalVFqGQcapiG6iAT7/MF45s4VyxrXo/UczkE22ftJPxiD5zEoRI/VF9EIz0dcHMxOhu6GRmYwjEz3iFQKDx+Z+Nj1ZsOpfInXjJDbMbI0whn46n5viEOXizN3Pt1oBAAAAANJYRD/uuOPC2RLkpGUYpLPgEociuuaiz/tze/e2LpMvpeGLZRUi2db2TvS0DKhF8uljccAokTUfF6dYGzemiGoKtT3GudSG12WshfVspjgnNzS2qn6iyKeviyyZHa8ieuPy8Irozu7z2HSiE+cCAAAAeEUmeoKL6Brlgggy0ZM+DDKOnejO7r5y10lHxaX3l5bG9pUMaVnRgHTQx6ddRE97J3p9x0JtVJnoXQ2NNF3oGilVjOcsjfrRIvrSd0R2PEHil4keRhHd0X0eRpHeC+JcAAAAAKRAiVUKU2DzxhQOFvXZKRl07nJcChNuVFbn3tqd6CnJ1kc6OB+fqR8sWmgnetCZ6OvzDxXVfVJWJkU7+Rmn4aKtm0U2rAwxE70hfs9VFNEBAAAAX53iYeah04lexE701tZWufHGG+WBBx6QRYsWSXNzc4fPr1q1KsDNS6G0xHA4u0njEF0zcLRIVT+R5nWllYfeuYjeaorom9JxP0I6OFeKpL4Tva2IumGVSGtL/sHBpujtt4jeq5sCqSmiF2ufaAyX0k70uNiwQkSyImXlIjWDQ85Ej8lzlbmP6GooAAAAAEgo10X0q666Sn75y1/KxRdfLJdffrlcdtll8uGHH8qjjz4qV1xxRThbmSb2QMiEx3DErYiuXZQ6wPDjmfHJmfXTiZ6WFQ1IB4ro7foMbp+B0LhCpP/waOJcuiyiry3uPtFMdLX2U5EnpuUK18WmJzfsWLCK4K+/xrH/4/JcZe4jn/1T5MnLwr2t/S8S6Tsk3NsAAAAAImQ6xsO8fhSpiP7b3/5W7rrrLjnqqKPkyiuvlFNPPVW23XZb2XnnnWXmzJny/e9/P6BNS3snegwKy2Eyf/zX1MUnf3zkrrki+sBRUnLMSReThZ6WFQ1Ih8Fjc4Va7XyOS+GwWPR4qQXa9UtEGpdFWETf0E0nepEidvR2h2wnsvIDkZm3S6wM3Dqc69XC/ICtRFZ/FJ/nKhMro9v0yq3h3tbuZ1JEBwAAAFAaRfQlS5bITjvlllDX1tbKmjW5P6KPPvpomT59evBbmDZpGQiphZ8T/yc+ma6mw63/SJFdvi6ln4luVjRQREcCaOHw5N+KrFssMmBksbem+GrbiuhmgGVXTLSGidoIIxPdDBYt5uqAE34h8u6fRLJZiQ3tiN/xxPCu/4S7cicOhk6QWNjmIJHDrs09PsPWe2D4twEAAABEKBNybjmZ6EUsom+11VayePFi2Xrrra0O9Keeekp23XVXee2116S6uq2QB+/SVPzc6f9J7ApT+54nJSlvJ3rCT8YgPcYeUOwtiA+Thd1dET2SOJfVxR/2OnK33CVNtt4rd4mLikqRfc4t9lYAAAAAQKhc52gcf/zx8uyzz1rvn3/++Vb3+bhx4+S0006TM888M4xtTBdiOOAFmehAephIm/VLi1xEL/JgUQAAAABISCZ6mBcUqRP9Jz/5if3+ySefbHWkv/LKK1Yh/Zhjjglos1KM4ie8qOgc57IpPSsagLTRVTOqcXn+rzEZ5lW1/m7LfL+Jh+lysCgRGwAAAACAZHNdRO9sn332sS4IiCl+UkSHp050E+fSVkDjfgQkuBM9T5yL5oObDPMqn0OqzZDrbjvRixjnAgAAAAAlLOxucTrRIy6iP/bYYwVf4bHHHutne2CKn3QQw1Mmuolz4WQMkPxM9DxxLnocyLa9VCLOBQAAAACAaIroxx13XEFXVlZWJq2tnOPwxS5+MhASfgaLmjgX7kdA4tTWdx/n4ix49+obTJyLiYdxajJxLhTRAQAAAMCLTNslzOtHhEX0TIZfefSDRX0uwUe6B4vacS7cj4DEFtHzdaKb/HI9iVZRGVAn+vpcTExZ2Zad6NXEuQAAAAAAks13JjpCKqLTQQw3zP2ltXOcC/cjILGZ6Bs/F9ncLFJZ1XUnehAn0UymusbD6Ek65zGFOBcAAAAA8IVM9NJRXugXPvfcczJx4kRZu7Zt+bbDmjVrZNKkSfLiiy8GvX3p0rpZJNOSe58sa7hhimh0ogPJ13ugSHll/kgXU0Q3USx+OONgOueibyLOBQAAAACQDgUX0W+66SY5++yzpX//LZdtDxgwQL7zne/IjTfeGPT2pcvmti50RREdbpCJDqRHeXn7cNHGZVt+XqNXghgqqjQOxhxHTEyM9f6m9pUvvYlzAQAAAAAvso5c9DAuev2IuIj+1ltvyeGHH57381/5ylfkjTfeCGizUsoUPhXFT/jJRDcnZDgZAyRT7dDc2/VdFdE3BFdEd16PsxPdRLlImUhVv2BuBwAAAACAUs9EX7p0qfTq1Sv/FVVWyvLlXSwrR+FMBEdln47D2wDXnegU0YFU5KJ3WUQ3cS4BxTlZkS4rOxbRm0yUS/9cZzwAAAAAwDUy0UtHwX/5jhw5Ut555528n3/77bdl+PDhQW1XOjEMEkF1ottxLhTRgUQycS7rl3YT5xJAJnqHTvS263V2oleThw4AAAAASL6Ci+hHHnmkTJ8+XTZtckSOtNm4caPMmDFDjj766KC3L11M9zCFT7hVUZ1nsCgnZIBEqq0vYLBo0HEubccVtWl17i1DRQEAAADAdyd6mBdEHOdy+eWXy8MPPyzjx4+X8847TyZMmGB9fN68eXLbbbdJa2urXHbZZQFtVkoRwQHfneibOq1q4L4EJLqI3lUneksUmegmzoUiOgAAAAAg+Qouojc0NMjLL78s55xzjkybNk2y2dx817KyMjnssMOsQrp+DXxgGCR8Z6I3iehjk1UNQEqK6F11oq93ZJmHHOeimegAAAAAAE8ybZcwrx8RF9HV6NGj5S9/+Yt8/vnn8sEHH1iF9HHjxsmgQYMC2pyUoxMdQQwWbW0RybYt2OG+BKQwEz2sOJeuBovSiQ4AAAAASD5XRXRDi+Z77LFH8FuTdnb3MDnW8DFY1KxoUBTRgWSqbVv51bgsuiK6iYnpMFiUTnQAAAAA8Crs3HIy0YswWBQRIMcaQXSim5MxUiZSUVXMrQIQltqh7cXslk3hFtF7dRfnQic6AAAAACD5PHWiIyTEucCryrZieWuz435Uo0MLirpZAELSe2DuJJk+5huXiwwcFW2cC4NFAQAAAMA3OtFLB53occIwSATRiW6vaCAWCEgsPUFmctE7R7pEUkRnsCgAAAAAID3oRI8T4lwQRCa6yS3WTnQAyY50WfuJyPpiFtHpRAcAAAAArzJtlzCvH8GgEz1O7OInRXT4yURvOxnDgFogHcNFOxfRWxo7ZpmHUURvIs4FiNqvfvUrGThwYKDXefrpp8txxx0X6HUCAAAASUQRPU4ofsJvJ3o2I9K0Lvc+cS5AsvVtGy5azE70auJcADcF67KyMutSVVUl2223nfzoRz+SzZs3Syl54YUX5OCDD5bBgwdLTU2NjBs3TqZMmSLNzc2hFfsBAACSKuPIRQ/jQid6cCiixwkxHPDKeeJl0+rcW+5HQDo60aPKRDcd7oo4F8CTww8/XBYvXizvv/++XHzxxXLllVfK9ddfL6Xi3XfftX6G3XffXV588UWZPXu23HLLLdZJgdZWxlYBAAAguSiixwkDIeFVRVsnutrYVkRnRQOQbLVtg0XXL23/WDbrKKLXBnM75nrM9bZuFmlen3u/N92mgBvV1dUybNgwGT16tJxzzjly6KGHymOPPWZ97vPPP5fTTjtNBg0aZHV4H3HEEVaxvSsffvihlJeXy+uvv97h4zfddJN13ZlMxipqn3XWWTJ27Fjp06ePTJgwQW6++eZut++1116ToUOHyk9/+tMuP//UU09Z23/dddfJjjvuKNtuu61VVL/rrrus2/jb3/4mZ5xxhqxZs8buutcTBaqpqUmmTp0qI0eOlL59+8pee+1lfb1hOtgfffRRq7u9d+/ecthhh8nHH3/s+vcMAABQapnoYV4QDIrocdKyMfeWTHS4VV4uUt4r9/7Gz3Nv6UQHUlJEX97peSSbe78qoGOAOZaYIrrJQ1e9iXMB/NDCs4lB0bgXLYprUf2VV16RbDYrRx55pLS0tGzxfWPGjLEK8Pfcc0+Hj+v/9Xq0wK6F9K222koefPBBq4P8iiuukB/+8IfywAMPdLktzz33nHz5y1+W//zP/5RLL720y6/RArp20msXelf23Xdfq5Dfv39/6+v0ooVzdd5551k/1/333y9vv/22fO1rX7MK8M4TBRs2bLBu/7777pOXXnpJVq9eLaecckre358W5teuXdvhAgAAAISBInoci+iVFNHhgek8t+Nc6EQHEq1vF53oztzyoE6kdc5EN1Euev0VbSfvALiiBfJnnnlGnnzySStfXAvJWjz/5S9/KQcccIBMnjxZfvvb38qnn35qdWZ35Vvf+pb8/ve/twrJ6s0337TiVbQTXPXq1UuuuuoqK3pFu9G/8Y1vWJ/rqoj+yCOPyFe/+lW588475dvf/nbe7dbC96mnnipf/OIXZfjw4XL88cfLrbfeahevNdZlwIABVge6Ftz1UltbK4sWLbIK/FrQ159PO9i1uL7//vt3OBGgJwz0+vbZZx/Zbbfd5N5775WXX35ZXn311S6359prr7Vuz1xGjRrlck8AAAAUV2sEF6SgiK7LP81SUHPZfvvtJbGIc0EQw0XtOBdOxgDpyER3dKKbmBUtcJdXBB/nonExphOdPHTAtT//+c9WUVmjSjSu5eSTT7Ze786dO1cqKyutiBNjyJAhVgSLfq4rxx13nFRUVFgFcBOHctBBB1ld6sZtt91mFaM1okVv9xe/+IVV0HaaNWuWVRz/9a9/bW1Pd/T2tOj9ySefWJEuGs1yzTXXyKRJk6yu83y0uK/xMuPHj7e2w1x0SOmCBQvsr9PfwR577GH/X1/3a8RLvt/BtGnTrOgYcyH6BQAAAGGplJjTF+XaqeN8cZ1YDBZFoJ3oFNGBRKsdmnurRW1dyaSPefM8EtRQ0Q7Xlc3djulErybKBXBLi9x33HGH1bE9YsQIX69r9To0Q12L2ieccIL87ne/65B5rrEp2u393//931Znd79+/awhplo0d9KucC3Y33333XLUUUdZHew90eL5v/3bv1mXH//4x1Zx/Oc//7nV+d6V9evXWwX4N954w3rrpMV0PxnzegEAAChVYXeL04kenNhXpPWPC10KmgotbZ3oDISEF5VVHTvRKaIDyaZFbH2+0FVM65eJDBrdHrkS5MlY53Xp9W+iEx3wSgdqbrfddlt8fIcddpDNmzdbBW7NFVcrV66U+fPny8SJE/Nen0a66IDP22+/3fp+LaYbmimu13XuuefaH3N2fRt1dXXy8MMPy5e+9CU56aSTrLiXQgrphg5C1WiXxsZGu7ivXedOX/jCF6yPLVu2zIpzyUd/Bs2F33PPPa3/68+vuej6+wEAAACKKdZxLkozIrVTZ5tttrGyHDsvQU3UgKHNZrAonejwwJx8sQeLUkQHEq2szJGLvqxjnIuJYAlqcLE9XHR9eyc6RXQgMOPGjbMyyc8++2z5+9//Lm+99ZZ885vftDq+9eP5aHF57733tgaBala5Dip1XqcWpDV3/b333pPp06fLa6+91uX11NfXW4NF582bZ12PFrO7opnp55xzjjz11FNWQX7OnDnWbevbY445xvoajZPRzvNnn31WVqxYYQ0L1U51fR2vnfNasF+4cKGVc66Z5o8//rh9/Vq8P//8862TCdq1rkNS9eczRXUAAICkyURwQQqK6JoLqfmOTzzxhLX0VV9wa/fKunXr8n5PSQ8YMoNFyUSHn0x0E+fCigYg+WrbiuiNpojeGHyci/P6NC7GLqIT5wIESWNZNL/86KOPtuJXdPjoX/7ylx67ws866yxpbm6WM888s8PHv/Od71id6Zpzrq+ptbPd2ZXema781EK65pdrwbtzN7nSYrYWyL/73e9akYs6YHTmzJnW8FN9X2n3u35eb1ez2DU73fx8WkS/+OKLrax3zXTXov7WW29tX39NTY1VlP/6178u++23nxX18oc//MH17xIAAABIVZyLDlwydt55Z+sPgNGjR1vLTPUPhnwDhi666CL7/9qJXjKFdFNEZyAkfHWiE+cCpK6Ivn5p7m2zyUQPeEWT3YneyGBRwCNtDOkpFuW+++7L+3ntytZLZ59++qnstNNOHQZyKs0K18K1Xjo3nOTbJo1l0QiVfDSWRQeQ9kSbX/TipCcDNDM9X266oYV/ZywNAABAkpGJXjpiXUTvbODAgdZy0A8++CCZA4Y011ZR/ISfTvTWptxb7kdAioroy8OLc3FenzPOhcGiQFFpR/iHH34ot956q1x99dXF3hwAAAAg0WId59LVHwuav6hdMomTyVBEhz+d41tY0QAkX9+I41yswaJkogNxcN5551nxLzoQtHOUCwAAAEqrEz3MC1JQRJ86daq88MILVpfNyy+/LMcff7xUVFRYA48SxxTQFVnW8KKiquP/ydYHUhjnEnYR3ZmJThEdKCaNYmlqarIyw/X1canTqJrVq9si6QAAAICYiXWcyyeffGIVzHUQkg4m2n///a3hRfp+4pg8dEUnOrzofPLFZBgDSE+cS0tjOI9/u4juiHOhiA4AAAAAvmQ1nCLk60cKiuj333+/pMbmje3dxOWl302EImai2/+nEx1IvNqGPJ3oQWeiE+cCAAAAAEivWBfRU6WFPHT4RCc6kD5921ZmNS6PLhO9aW3ufYroAAAAAOBL2LnlZKKnJBM9VVo25N4yDBJBdaKTiQ6kJ85FY1a0wB12Eb3F0Yle3T/Y2wAAAAAAIKboRI/bYFEKnwiqE50TMkDyaWyLrjrRE7Hrl4VXRO/Vdn1NmolOJzoAAAAABCETciZ6mNedNnSix60TnQgOBNaJThEdSLyysvZIlzCL6Ob6NDYm27YgkCI6AAAAACTKlVdeKWVlZR0u22+/fbE3KxboRI9bJjrDIOEVRXQgvcNFV38k0hhBEX3tp7m35ZUcYwAAAAAggZnokyZNkmeeecb+f2Ul5WPr9xDgfoEfmzfm3tKJjsAGi1LgAlKVi75+aS4b3cS8BMlc39rF7V3o2gUPAAAAAIi9tWvbYjnbVFdXW5euaNF82LBhEW1Z6SDOJS5aTBGdTnQE1InOqgYgZUX05eFFg1W1XZ92uyuiXAAAAAAgsE70MC9q1KhRMmDAAPty7bXX5t2m999/X0aMGCHbbLONfOMb35BFixZF9vuIMzrR41ZEp/AJr5z3HX2fLlEgHfo6O9FDjnPJto2lqe4f7PUDAAAAAELz8ccfS//+7X/H5etC32uvveRXv/qVTJgwQRYvXixXXXWVHHDAAfLOO+9Iv379JM0ooseuE504F3hUUdX+PlEuQDrjXEwnelhxLgad6AAAAADgm7YpZUK+fqUFdGcRPZ8jjjjCfn/nnXe2iuqjR4+WBx54QM466yxJM+JcYpeJTic6guhEp4gOpK6I/vmHW8avBKXzCV6K6AAAAACQeAMHDpTx48fLBx98IGlHET0uWjbl3tKJjiAy0elEB9KjtqFTEb0s+BNpneNhehPnAgAAAAClkonu1fr162XBggUyfPhwSTuK6HFBJjr8ct53KKID6dF3aO6tHeXSV6S8POQ4l4HBXj8AAAAAoOimTp0qL7zwgnz44Yfy8ssvy/HHHy8VFRVy6qmnStqRiR67OBeKnwhosCiAdMW5GEEPFe3qOhksCgAAAACBZJa3hnz9bnzyySdWwXzlypUydOhQ2X///WXmzJnW+2lHET12cS4U0eERcS5AOmmBWzvFm9eHFwtmHVPKRCSb+z+Z6AAAAACQOPfff3+xNyG2iHOJC7MMn4GQ8IoiOpBezm70ztErQSgr69iNThEdAAAAAALpFA/7gmBQRI+LzaYTnRgOBFBEJ84FSJe+9eHGuXS+XgaLAgAAAABShDiXuHWih7EMHykcLMr9CEhvJ3oURXQ60QEAAADAr9aQM9HDvO60oRM9bpnodBAjkDgX7kdAeovoIZ1E60URHQAAAACQTnSix8Xmjbm3ZFnDKzrRgfSqbQg3E926XkcRvZo4FwAAAADwK+zccjLRg0Mnely0UESHTxVV7e+zogFIl75D298nzgUAAAAAgEDRiR4XxLnAr7IykYpqkdYmTsYAaRNpJnoZnegAAAAAEAAy0UsHnehxwWBRBMGchKGIDqQ3zsWZXR5GEb26n0g5Lx8AAAAAAOlBJ3pcbG7rRGcgJPwOF21iRQOQOlHGuRDlAgAAAACBoBO9dNBKFgfZrCMTnU50BNGJzv0ISJUo41woogMAAAAAUoZO9DC0tog0ry/86zc3ayU99z4dxPDbia5Y0QCki0Y4aU5509rwiugmJoY8dAAAAAAIRKbtEub1IxgU0cOw8EWR35zg7XvJskYQRfRK7kdAKrvRwyyi04kOAAAAAEgp4lziZNxXRCp6FXsrUMomHCHSb4TIyN2KvSUAojbxq+E+/sfsn8teH39YONcPAAAAACmTceSih3GhEz04dKKHYZuDRKavdP99FewO+HTw5SIHXSZSVlbsLQEQtUOuEDl4eniP/xG7iEx9n+MLAAAAACB1qNqGoVwb/GnyR5FQ4ALSK+zHP8cXAAAAAAiM6RgP8/oRDCq9AAAAAAAAAADkQSc6AAAAAAAAAEQsE3JuOZnowaETHQAAAAAAAACAPOhEBwAAAAAAAICIkYleOuhEBwAAAAAAAAAgDzrRAQAAAAAAACBiZKKXDjrRAQAAAAAAAADIg050AAAAAAAAAIgYmeilg050AAAAAAAAAADyoBMdAAAAAAAAACJGJ3rpoBMdAAAAAAAAAIA86EQHAAAAAAAAgIhlRSQT8vUjGHSiAwAAAAAAAACQB53oAAAAAAAAABAxMtFLB53oAAAAAAAAAADkQSc6AAAAAAAAAESMTvTSQSc6AAAAAAAAAAB50IkOAAAAAAAAABHLtF3CvH4Eg050AAAAAAAAAADyoBMdAAAAAAAAACJGJnrpoBMdAAAAAAAAAIA86EQHAAAAAAAAgIiRiV466EQHAAAAAAAAACAPOtEBAAAAAAAAIGJkopcOOtEBAAAAAAAAAMiDTnQAAAAAAAAAiFgm5G5xMtGDQyc6AAAAAAAAAAB50IkOAAAAAAAAABHLhNwtTid6cOhEBwAAAAAAAAAgDzrRAQAAAAAAACBirSF3OIeZt542dKIDAAAAAAAAAJAHnegAAAAAAAAAEDE60UsHnegAAAAAAAAAAORBJzoAAAAAAAAARCzTdgnz+hEMOtEBAAAAAAAAAMiDTnQAAAAAAAAAiBiZ6KWDTnQAAAAAAAAAAPKgEx0AAAAAAAAAIkYmeumgEx0AAAAAAAAAgDzoRAcAAAAAAACAiGVCzi2nEz04dKIDAAAAAAAAAJAHnegAAAAAAAAAEDHtQi8L+foRDDrRAQAAAAAAAADIg050AAAAAAAAAIhYJuTccjLRg0MnOgAAAAAAAAAAedCJDgAAAAAAAAARIxO9dNCJDgAAAAAAAABAHnSiAwAAAAAAAEDE6EQvHXSiAwAAAAAAAACQB53oAAAAAAAAABCxTNslzOtHMOhEBwAAAAAAAAAgDzrRAQAAAAAAACBiZKKXDjrRAQAAAAAAAADIg050AAAAAAAAAIhYNuTccr1+BINOdAAAAAAAAAAA8qATHQAAAAAAAAAiFnZmOZnowaETHQAAAAAAAACAPOhEBwAAAAAAAICI0YleOuhEBwAAAAAAAAAgDzrRAQAAAAAAACBiGREpC/n6kaJO9Ntuu03GjBkjvXv3lr322kteffXVYm8SAAAAAAAAACAFYt+J/oc//EEuuugi+fnPf24V0G+66SY57LDDZP78+VJfXy9xlM1mZWMLqUMAAABB6dOrQsrKwuzTAQAAAKJFJnrpiH0R/YYbbpCzzz5bzjjjDOv/Wkx//PHH5e6775Yf/OAHW3x9U1OTdTHWrl0rUdMC+sQrnoz8dgEAAJLq3R8dJjVVsX/pCgAAACCBYh3n0tzcLG+88YYceuih9sfKy8ut/7/yyitdfs+1114rAwYMsC+jRo2KcIsBAAAAAAAAoLBO8bAvCEas23lWrFghra2t0tDQ0OHj+v958+Z1+T3Tpk2z4l+cnehRF9J1ubF2SwEAACC411cAAAAAUAyxLqJ7UV1dbV2KSfM6WW4MAAAAAAAAIJ+M1hFDvn6kIM6lrq5OKioqZOnSpR0+rv8fNmxY0bYLAAAAAAAAAJAOsS6iV1VVyW677SbPPvus/bFMJmP9f5999inqtgEAAAAAAACAn07xMPPQ6UQPTuwzRzTffMqUKbL77rvLnnvuKTfddJM0NjbKGWecUexNAwAAAAAAAAAkXOyL6CeffLIsX75crrjiClmyZInssssu8sQTT2wxbBQAAAAAAAAASkXYneJ0oqeoiK7OO+886wIAAAAAAAAAQJRKoogOAAAAAAAAAEmiueXZEK+fTvSUDBYFAAAAAAAAAKCY6EQHAAAAAAAAgIjRiV466EQHAAAAAAAAACAPOtEBAAAAAAAAIGJhd4rTiR4cOtEBAAAAAAAAAMiDTnQAAAAAAAAAiBiZ6KWDTnQAAAAAAAAAAPKgEx0AAAAAAAAAIpYJuRM9zOtOGzrRAQAAAAAAAADIg050AAAAAAAAAChCJ3pZiNdPJ3pw6EQHAAAAAAAAACAPiugAAAAAAAAAELHWCC5e3HbbbTJmzBjp3bu37LXXXvLqq69K2lFEBwAAAAAAAADIH/7wB7noootkxowZ8uabb8rkyZPlsMMOk2XLlkmaUUQHAAAAAAAAgCJkood9ceuGG26Qs88+W8444wyZOHGi/PznP5eamhq5++67Jc0SP1g0m81F6K9du7bYmwIAAIASYV47mteSiD+zr5oDvt61a5sK/+J1BX7dJgn8OteH8TO5WANe6O2vDeFnl96F/63XXMz97nVNfXfc/D6LefutIf1MEsLth7A/C358FHrbbn5PYVyni5+94MfcuhCuM4THh5tjbTG5+dlDOX4XeAwt5u+zse1tsV/rZSO6/s510erqauvSWXNzs7zxxhsybdo0+2Pl5eVy6KGHyiuvvCJpVpYt9r0lZJ988omMGjWq2JsBAACAEvTxxx/LVlttVezNQAF43Q8AAErltd6mTZtk7NixsmTJktBvq7a2Vtav73jKQqNarrzyyi2+9rPPPpORI0fKyy+/LPvss4/98UsuuUReeOEFmTVrlqRV4jvRR4wYYT0g+vXrJ2VlZZHdrp7h0Rfxetv9+/eP7HYRLfZzOrCfk499nA7s53QIaj9rn8m6deus15Io3df9PO7jj30Uf+yj0sB+ij/2UbwU+7WeDuxcuHCh1fkdxc/auSbaVRc6Ul5E1yUHxewe0gMjB8fkYz+nA/s5+djH6cB+Tocg9vOAAQMC2x4U93U/j/v4Yx/FH/uoNLCf4o99FB/Ffq2nhXS9xEldXZ1UVFTI0qVLO3x86dKlMmzYMEkzBosCAAAAAAAAQMpVVVXJbrvtJs8++6z9sUwmY/3fGe+SRonvRAcAAAAAAAAA9Oyiiy6SKVOmyO677y577rmn3HTTTdLY2ChnnHGGpBlF9JBotpCG9JMxlGzs53RgPycf+zgd2M/pwH6GE/eH+GMfxR/7qDSwn+KPfYRScfLJJ8vy5cvliiuusAaf7rLLLvLEE09IQ0ODpFlZVtPlAQAAAAAAAADAFshEBwAAAAAAAAAgD4roAAAAAAAAAADkQREdAAAAAAAAAIA8KKIDAAAAAAAAAJAHRfQQ3HbbbTJmzBjp3bu37LXXXvLqq68We5Pgw4svvijHHHOMjBgxQsrKyuTRRx/t8HmdzasTi4cPHy59+vSRQw89VN5///2ibS+8ufbaa2WPPfaQfv36SX19vRx33HEyf/78Dl+zadMm+d73vidDhgyR2tpaOfHEE2Xp0qVF22a4d8cdd8jOO+8s/fv3ty777LOP/PWvf7U/zz5Onp/85CfWsfvf//3f7Y+xn5PhyiuvtPat87L99tvbn2c/Q/G6PD54TR1/vB6OP17Llh5eiwLJQRE9YH/4wx/koosukhkzZsibb74pkydPlsMOO0yWLVtW7E2DR42NjdZ+1D/CunLdddfJz372M/n5z38us2bNkr59+1r7XJ8YUTpeeOEF64XMzJkz5emnn5aWlhb5yle+Yu1/48ILL5T//d//lQcffND6+s8++0xOOOGEom433Nlqq62sF7JvvPGGvP7663LwwQfLV7/6VZkzZ471efZxsrz22mty5513Wn9sOrGfk2PSpEmyePFi+/L3v//d/hz7GbwujxdeU8cfr4fjj9eypYXXokDCZBGoPffcM/u9733P/n9ra2t2xIgR2Wuvvbao24Vg6EPmkUcesf+fyWSyw4YNy15//fX2x1avXp2trq7O/v73vy/SViIIy5Yts/b3Cy+8YO/XXr16ZR988EH7a+bOnWt9zSuvvFLELYVfgwYNyv7yl79kHyfMunXrsuPGjcs+/fTT2S9+8YvZCy64wPo4+zk5ZsyYkZ08eXKXn2M/Q/G6PL54TV0aeD1cGngtG0+8FgWSh070ADU3N1tnhHXpoVFeXm79/5VXXinqtiEcCxculCVLlnTY5wMGDLCWC7PPS9uaNWust4MHD7be6mNbu3Gc+1pjA7beemv2dYlqbW2V+++/3+qu0qWw7ONk0U66o446qsP+VOznZNGoB42G2GabbeQb3/iGLFq0yPo4+xm8Li8tvKaOJ14PxxuvZeON16JA8lQWewOSZMWKFdYTWUNDQ4eP6//nzZtXtO1CePTFvupqn5vPofRkMhkrs26//faTHXfc0fqY7s+qqioZOHBgh69lX5ee2bNnW39o6PJwzSB85JFHZOLEifLPf/6TfZwQ+gelRjfoEtrOeCwnhxbXfvWrX8mECROsKJerrrpKDjjgAHnnnXfYz+B1eYnhNXX88Ho4vngtG3+8FgWSiSI6AHTRNaBFGGe2LpJDC276R4Z2Vz300EMyZcoUK4sQyfDxxx/LBRdcYGW56iBBJNcRRxxhv69Zo1pUHz16tDzwwAPWUEIAgHe8Ho4vXsvGG69FgeQiziVAdXV1UlFRscVUZf3/sGHDirZdCI/Zr+zz5DjvvPPkz3/+szz//PPW4B5D96cuDV+9enWHr2dflx7t/Nhuu+1kt912k2uvvdYacnbzzTezjxNCl8jq0MBdd91VKisrrYv+YanD6vR97fJhPyeTdnSNHz9ePvjgAx7P4HV5ieE1dbzwejjeeC0bb7wWBZKLInrAT2b6RPbss892WAan/9flVkiesWPHWk90zn2+du1amTVrFvu8xOiMK/2DQZdDPvfcc9a+ddLHdq9evTrs6/nz51v5u+zr0qbH6aamJvZxQhxyyCHWMmft0DKX3Xff3crLNu+zn5Np/fr1smDBAhk+fDiPZ/C6vMTwmjoeeD1cmngtGy+8FgWSiziXgF100UXWcio9MO65555y0003WYM+zjjjjGJvGnz8Ua5dbc7BR/rkpwN2dPiHZgVeffXVMm7cOOuF5vTp060hZ8cdd1xRtxvul6z+7ne/kz/96U/Sr18/O49Oh1ppLIC+Peuss6zHuO77/v37y/nnn2+90Nl7772Lvfko0LRp06wICH3srlu3ztrnf/vb3+TJJ59kHyeEPn5NdqvRt29fGTJkiP1x9nMyTJ06VY455hgrwuWzzz6TGTNmWJ3Hp556Ko9nWHhdHi+8po4/Xg/HH69l44/XokCCZRG4W265Jbv11ltnq6qqsnvuuWd25syZxd4k+PD8889n9aHS+TJlyhTr85lMJjt9+vRsQ0NDtrq6OnvIIYdk58+fX+zNhktd7WO93HPPPfbXbNy4MXvuuedmBw0alK2pqckef/zx2cWLFxd1u+HOmWeemR09erR1fB46dKj1eH3qqafsz7OPk+mLX/xi9oILLrD/z35OhpNPPjk7fPhw6/E8cuRI6/8ffPCB/Xn2MxSvy+OD19Txx+vh+OO1bGnitSiQDGX6T7EL+QAAAAAAAAAAxBGZ6AAAAAAAAAAA5EERHQAAAAAAAACAPCiiAwAAAAAAAACQB0V0AAAAAAAAAADyoIgOAAAAAAAAAEAeFNEBAAAAAAAAAMiDIjoAAAAAAAAAAHlQRAcAAAAAAAAAIA+K6AAA18rKyuTRRx8t9mYAAAAAoeD1LgDAiSI6AJSY008/3XpR3/ly+OGHF3vTAAAAAN94vQsAiJvKYm8AAMA9/QPinnvu6fCx6urqom0PAAAAECRe7wIA4oROdAAoQfoHxLBhwzpcBg0aZH1Ou3TuuOMOOeKII6RPnz6yzTbbyEMPPdTh+2fPni0HH3yw9fkhQ4bIt7/9bVm/fn2Hr7n77rtl0qRJ1m0NHz5czjvvvA6fX7FihRx//PFSU1Mj48aNk8ceeyyCnxwAAABpwOtdAECcUEQHgASaPn26nHjiifLWW2/JN77xDTnllFNk7ty51ucaGxvlsMMOs/4Iee211+TBBx+UZ555psMfDfpHyfe+9z3rjw39A0T/YNhuu+063MZVV10lJ510krz99tty5JFHWrezatWqyH9WAAAApA+vdwEAUSrLZrPZSG8RAOA7I/I3v/mN9O7du8PHf/jDH1oX7cz57ne/a/1hYOy9996y6667yu233y533XWXXHrppfLxxx9L3759rc//5S9/kWOOOUY+++wzaWhokJEjR8oZZ5whV199dZfboLdx+eWXy49//GP7D5Xa2lr561//SlYlAAAAfOH1LgAgbshEB4ASdNBBB3X4o0ENHjzYfn+fffbp8Dn9/z//+U/rfe3QmTx5sv0Hhdpvv/0kk8nI/PnzrT8Y9I+LQw45pNtt2Hnnne339br69+8vy5Yt8/2zAQAAALzeBQDECUV0AChB+iK+83LToGhuZCF69erV4f/6x4j+YQIAAAD4xetdAECckIkOAAk0c+bMLf6/ww47WO/rW82O1CWpxksvvSTl5eUyYcIE6devn4wZM0aeffbZyLcbAAAAKASvdwEAUaITHQBKUFNTkyxZsqTDxyorK6Wurs56X4cn7b777rL//vvLb3/7W3n11Vflf/7nf6zP6UCkGTNmyJQpU+TKK6+U5cuXy/nnny//9m//ZuVDKv245kzW19fLEUccIevWrbP+8NCvAwAAAMLG610AQJxQRAeAEvTEE0/I8OHDO3xMu2rmzZtnvX/VVVfJ/fffL+eee671db///e9l4sSJ1udqamrkySeflAsuuED22GMP6/8nnnii3HDDDfZ16R8cmzZtkhtvvFGmTp1q/bHy//7f/4v4pwQAAEBa8XoXABAnZdlsNlvsjQAABEezGh955BE57rjjir0pAAAAQOB4vQsAiBqZ6AAAAAAAAAAA5EERHQAAAAAAAACAPIhzAQAAAAAAAAAgDzrRAQAAAAAAAADIgyI6AAAAAAAAAAB5UEQHAAAAAAAAACAPiugAAAAAAAAAAORBER0AAAAAAAAAgDwoogMAAAAAAAAAkAdFdAAAAAAAAAAA8qCIDgAAAAAAAACAdO3/A1aizrWK7exMAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 261\u001B[39m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# Create tables for clamp rate analysis\u001B[39;00m\n\u001B[32m    260\u001B[39m epochs_to_show = [\u001B[32m0\u001B[39m, \u001B[32m5\u001B[39m, \u001B[32m10\u001B[39m, \u001B[32m20\u001B[39m, \u001B[32m30\u001B[39m, \u001B[32m49\u001B[39m]  \u001B[38;5;66;03m# Select specific epochs to display\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m261\u001B[39m clamp_data_table = \u001B[43mpd\u001B[49m.DataFrame({\n\u001B[32m    262\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mEpoch\u001B[39m\u001B[33m'\u001B[39m: epochs_to_show,\n\u001B[32m    263\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mConstant LR Step Size\u001B[39m\u001B[33m'\u001B[39m: [avg_step_sizes_const[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m epochs_to_show],\n\u001B[32m    264\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mConstant LR Clamp Rate (\u001B[39m\u001B[33m%\u001B[39m\u001B[33m)\u001B[39m\u001B[33m'\u001B[39m: [avg_clamp_rates_const[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m epochs_to_show],\n\u001B[32m    265\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mPolyak Step Size\u001B[39m\u001B[33m'\u001B[39m: [avg_step_sizes_poly[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m epochs_to_show],\n\u001B[32m    266\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mPolyak Clamp Rate (\u001B[39m\u001B[33m%\u001B[39m\u001B[33m)\u001B[39m\u001B[33m'\u001B[39m: [avg_clamp_rates_poly[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m epochs_to_show]\n\u001B[32m    267\u001B[39m })\n\u001B[32m    269\u001B[39m \u001B[38;5;66;03m# Display the table\u001B[39;00m\n\u001B[32m    270\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mStep Size and Clamp Rate Analysis Table:\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "56b43b613da16243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T23:43:34.795791Z",
     "start_time": "2025-04-28T23:42:43.669727Z"
    }
   },
   "source": [
    "# Cell 13: Linear Regression for 1(b)\n",
    "'''\n",
    "1b: Investigate Batch Sizes and Noise Levels\n",
    "'''\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_sizes = [5, 20, 100]\n",
    "noise_levels = [0.1, 0.5, 1.0]\n",
    "results = {}\n",
    "num_trials = 3\n",
    "\n",
    "# Hyperparameter tuning for constant LR\n",
    "alpha_candidates = [1e-3, 1e-2, 5e-2]\n",
    "best_alpha = 1e-2\n",
    "best_mse = float('inf')\n",
    "alpha_final_mses = {}  # Store final MSE for each alpha\n",
    "\n",
    "# For Appendix 1b table: final MSE for each alpha at σ=0.5, batch_size=20\n",
    "for alpha in alpha_candidates:\n",
    "    mse_sum = 0\n",
    "    alpha_mses = []  # Store individual trial MSEs\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        set_seed(42 + trial)\n",
    "        X = torch.linspace(-5, 5, 200).unsqueeze(1)\n",
    "        y = 4 * X - 2 + 0.5 * torch.randn_like(X)  # Using σ=0.5 for tuning\n",
    "        dataset = TensorDataset(X, y)\n",
    "        loader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "        model = LinearModel().to(device)\n",
    "        hist, _ = train_sgd(model, loader, mse, num_epochs=50, step_method='constant', alpha=alpha)\n",
    "        final_mse = hist[-1]\n",
    "        mse_sum += final_mse\n",
    "        alpha_mses.append(final_mse)\n",
    "    \n",
    "    avg_mse = mse_sum / num_trials\n",
    "    alpha_final_mses[alpha] = {\n",
    "        'trial_mses': alpha_mses,\n",
    "        'avg_mse': avg_mse\n",
    "    }\n",
    "    \n",
    "    if avg_mse < best_mse:\n",
    "        best_mse = avg_mse\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(f\"Best Constant LR Alpha: {best_alpha}\")\n",
    "\n",
    "# Print table for Appendix 1b - Final MSE for different constant α values\n",
    "print(\"\\nAppendix 1b: Final MSE for Different Constant α Values (σ=0.5, Batch Size=20)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'α (Constant Step Size)':^25} | {'Final MSE':^15}\")\n",
    "print(\"-\" * 80)\n",
    "for alpha in alpha_candidates:\n",
    "    avg_mse = alpha_final_mses[alpha]['avg_mse']\n",
    "    print(f\"{alpha:^25.0e} | {avg_mse:^15.4f}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Based on these results, α={best_alpha:.0e} was selected as it achieved the lowest final MSE.\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dictionary to store convergence epochs\n",
    "convergence_data = {\n",
    "    'noise_level': [],\n",
    "    'const_lr_epochs_to_converge': [],\n",
    "    'polyak_lr_epochs_to_converge': [],\n",
    "    'const_lr_walltime_per_epoch': [],\n",
    "    'polyak_lr_walltime_per_epoch': []\n",
    "}\n",
    "\n",
    "for noise_std in noise_levels:\n",
    "    for batch_size in batch_sizes:\n",
    "        hist_const_all = []\n",
    "        hist_poly_all = []\n",
    "        const_epochs_to_converge = []\n",
    "        polyak_epochs_to_converge = []\n",
    "        const_walltime_per_epoch = []\n",
    "        polyak_walltime_per_epoch = []\n",
    "        \n",
    "        for trial in range(num_trials):\n",
    "            set_seed(42 + trial)\n",
    "            X = torch.linspace(-5, 5, 200).unsqueeze(1)\n",
    "            y = 4 * X - 2 + noise_std * torch.randn_like(X)\n",
    "            dataset = TensorDataset(X, y)\n",
    "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            # Target MSE threshold is σ²\n",
    "            target_mse = noise_std**2\n",
    "            \n",
    "            # Constant step size\n",
    "            model_const = LinearModel().to(device)\n",
    "            start_time = time.time()\n",
    "            hist_const, metrics_const = train_sgd(\n",
    "                model_const, loader, mse,\n",
    "                num_epochs=50,\n",
    "                step_method='constant',\n",
    "                alpha=best_alpha,\n",
    "                alpha_max=1.0\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            const_time_per_epoch = (end_time - start_time) / 50\n",
    "            const_walltime_per_epoch.append(const_time_per_epoch)\n",
    "            \n",
    "            # Find first epoch where MSE ≤ σ²\n",
    "            const_converged_epoch = 51  # Default if never converges\n",
    "            for epoch, loss in enumerate(hist_const):\n",
    "                if loss <= target_mse:\n",
    "                    const_converged_epoch = epoch + 1  # +1 because epochs are 1-indexed\n",
    "                    break\n",
    "            const_epochs_to_converge.append(const_converged_epoch)\n",
    "            \n",
    "            # Polyak step size\n",
    "            model_poly = LinearModel().to(device)\n",
    "            start_time = time.time()\n",
    "            hist_poly, metrics_poly = train_sgd(\n",
    "                model_poly, loader, mse,\n",
    "                num_epochs=50,\n",
    "                step_method='polyak',\n",
    "                alpha=1e-4,\n",
    "                f_star=0.0,\n",
    "                alpha_max=1.0\n",
    "            )\n",
    "            end_time = time.time()\n",
    "            polyak_time_per_epoch = (end_time - start_time) / 50\n",
    "            polyak_walltime_per_epoch.append(polyak_time_per_epoch)\n",
    "            \n",
    "            # Find first epoch where MSE ≤ σ²\n",
    "            polyak_converged_epoch = 51  # Default if never converges\n",
    "            for epoch, loss in enumerate(hist_poly):\n",
    "                if loss <= target_mse:\n",
    "                    polyak_converged_epoch = epoch + 1  # +1 because epochs are 1-indexed\n",
    "                    break\n",
    "            polyak_epochs_to_converge.append(polyak_converged_epoch)\n",
    "            \n",
    "            hist_const_all.append(hist_const)\n",
    "            hist_poly_all.append(hist_poly)\n",
    "            \n",
    "            # Log trial metrics\n",
    "            logging.info(f\"Noise={noise_std}, Batch={batch_size}, Trial {trial+1}: \"\n",
    "                         f\"Constant - {metrics_const}\")\n",
    "            logging.info(f\"Noise={noise_std}, Batch={batch_size}, Trial {trial+1}: \"\n",
    "                         f\"Polyak - {metrics_poly}\")\n",
    "        \n",
    "        # Average results\n",
    "        results[f\"noise_{noise_std}_batch_{batch_size}\"] = {\n",
    "            'constant': np.mean(hist_const_all, axis=0),\n",
    "            'polyak': np.mean(hist_poly_all, axis=0),\n",
    "            'final_const_mse': np.mean([h[-1] for h in hist_const_all]),\n",
    "            'final_poly_mse': np.mean([h[-1] for h in hist_poly_all]),\n",
    "            'const_epochs_to_converge': const_epochs_to_converge,\n",
    "            'polyak_epochs_to_converge': polyak_epochs_to_converge,\n",
    "            'avg_const_epochs': np.mean(const_epochs_to_converge),\n",
    "            'avg_polyak_epochs': np.mean(polyak_epochs_to_converge),\n",
    "            'avg_const_walltime': np.mean(const_walltime_per_epoch),\n",
    "            'avg_polyak_walltime': np.mean(polyak_walltime_per_epoch)\n",
    "        }\n",
    "        \n",
    "        # Log final metrics\n",
    "        logging.info(f\"Noise={noise_std}, Batch={batch_size}: \"\n",
    "                     f\"Final Constant MSE={results[f'noise_{noise_std}_batch_{batch_size}']['final_const_mse']:.4f}\")\n",
    "        logging.info(f\"Noise={noise_std}, Batch={batch_size}: \"\n",
    "                     f\"Final Polyak MSE={results[f'noise_{noise_std}_batch_{batch_size}']['final_poly_mse']:.4f}\")\n",
    "        \n",
    "        # Log convergence data\n",
    "        logging.info(f\"Noise={noise_std}, Batch={batch_size}: \"\n",
    "                     f\"Const LR epochs to MSE ≤ σ²: {results[f'noise_{noise_std}_batch_{batch_size}']['avg_const_epochs']:.1f}\")\n",
    "        logging.info(f\"Noise={noise_std}, Batch={batch_size}: \"\n",
    "                     f\"Polyak LR epochs to MSE ≤ σ²: {results[f'noise_{noise_std}_batch_{batch_size}']['avg_polyak_epochs']:.1f}\")\n",
    "        \n",
    "        # Save convergence data for summary table\n",
    "        if batch_size == 20:  # Only collect for the middle batch size for summary table\n",
    "            convergence_data['noise_level'].append(noise_std)\n",
    "            convergence_data['const_lr_epochs_to_converge'].append(results[f'noise_{noise_std}_batch_{batch_size}']['avg_const_epochs'])\n",
    "            convergence_data['polyak_lr_epochs_to_converge'].append(results[f'noise_{noise_std}_batch_{batch_size}']['avg_polyak_epochs'])\n",
    "            convergence_data['const_lr_walltime_per_epoch'].append(results[f'noise_{noise_std}_batch_{batch_size}']['avg_const_walltime'])\n",
    "            convergence_data['polyak_lr_walltime_per_epoch'].append(results[f'noise_{noise_std}_batch_{batch_size}']['avg_polyak_walltime'])\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(len(noise_levels), len(batch_sizes), figsize=(18, 12), sharey=True)\n",
    "for i, noise_std in enumerate(noise_levels):\n",
    "    for j, batch_size in enumerate(batch_sizes):\n",
    "        ax = axes[i, j]\n",
    "        key = f\"noise_{noise_std}_batch_{batch_size}\"\n",
    "        ax.plot(results[key]['constant'], label='Constant LR')\n",
    "        ax.plot(results[key]['polyak'], label='Polyak LR')\n",
    "        ax.axhline(y=noise_std**2, color='r', linestyle='--', label='σ² threshold')\n",
    "        ax.set_title(f\"Noise: {noise_std}, Batch: {batch_size}\")\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('MSE Loss')\n",
    "        ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print convergence rate summary table for batch_size=20\n",
    "print(\"\\nConvergence Rate Summary (Batch Size = 20)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Noise Level':^15} | {'Const-LR Epochs to MSE ≤ σ²':^30} | {'Polyak-LR Epochs to MSE ≤ σ²':^30}\")\n",
    "print(\"-\" * 80)\n",
    "for i, noise in enumerate(convergence_data['noise_level']):\n",
    "    const_epochs = convergence_data['const_lr_epochs_to_converge'][i]\n",
    "    polyak_epochs = convergence_data['polyak_lr_epochs_to_converge'][i]\n",
    "    \n",
    "    # Format epoch counts\n",
    "    const_str = f\"{const_epochs:.1f}\" if const_epochs <= 50 else \">50 (Not achieved)\"\n",
    "    polyak_str = f\"{polyak_epochs:.1f}\" if polyak_epochs <= 50 else \">50 (Not achieved)\"\n",
    "    \n",
    "    print(f\"{noise:^15.1f} | {const_str:^30} | {polyak_str:^30}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print wall-clock time summary\n",
    "print(\"\\nWall-clock Time Summary (seconds/epoch)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Noise Level':^15} | {'Const-LR Time/Epoch (s)':^30} | {'Polyak-LR Time/Epoch (s)':^30}\")\n",
    "print(\"-\" * 80)\n",
    "for i, noise in enumerate(convergence_data['noise_level']):\n",
    "    const_time = convergence_data['const_lr_walltime_per_epoch'][i]\n",
    "    polyak_time = convergence_data['polyak_lr_walltime_per_epoch'][i]\n",
    "    print(f\"{noise:^15.1f} | {const_time:^30.6f} | {polyak_time:^30.6f}\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Constant LR Alpha: 0.01\n",
      "\n",
      "Appendix 1b: Final MSE for Different Constant α Values (σ=0.5, Batch Size=20)\n",
      "================================================================================\n",
      " α (Constant Step Size)   |    Final MSE   \n",
      "--------------------------------------------------------------------------------\n",
      "          1e-03           |     0.6936     \n",
      "          1e-02           |     0.2516     \n",
      "          5e-02           |     0.2614     \n",
      "================================================================================\n",
      "Based on these results, α=1e-02 was selected as it achieved the lowest final MSE.\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1200 with 9 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Convergence Rate Summary (Batch Size = 20)\n",
      "================================================================================\n",
      "  Noise Level   |  Const-LR Epochs to MSE ≤ σ²   |  Polyak-LR Epochs to MSE ≤ σ² \n",
      "--------------------------------------------------------------------------------\n",
      "      0.1       |              42.0              |       >50 (Not achieved)      \n",
      "      0.5       |              39.0              |       >50 (Not achieved)      \n",
      "      1.0       |              38.0              |       >50 (Not achieved)      \n",
      "================================================================================\n",
      "\n",
      "Wall-clock Time Summary (seconds/epoch)\n",
      "================================================================================\n",
      "  Noise Level   |    Const-LR Time/Epoch (s)     |    Polyak-LR Time/Epoch (s)   \n",
      "--------------------------------------------------------------------------------\n",
      "      0.1       |            0.007873            |            0.012015           \n",
      "      0.5       |            0.007422            |            0.010940           \n",
      "      1.0       |            0.007568            |            0.011224           \n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "id": "85ddb47dee21589f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T23:44:47.228013Z",
     "start_time": "2025-04-28T23:43:36.500695Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Cell 10: MLP Training for 1(c)\n",
    "# Function to create dataloaders with different batch sizes\n",
    "def create_dataloaders(dataset, batch_size=32):\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_ds, test_ds = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Create default dataloaders\n",
    "train_loader, test_loader = create_dataloaders(ds, 32)\n",
    "\n",
    "# Loss function\n",
    "bce = nn.BCELoss()\n",
    "\n",
    "# Hyperparameter tuning for constant LR\n",
    "logging.info(\"1(c) Starting hyperparameter tuning for constant learning rate\")\n",
    "alpha_candidates = [0.01, 0.05, 0.1, 0.5]\n",
    "best_alpha = None\n",
    "best_loss = float('inf')\n",
    "num_trials_tune = 3\n",
    "tuning_results = {}\n",
    "\n",
    "for alpha in alpha_candidates:\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    for trial in range(num_trials_tune):\n",
    "        set_seed(42 + trial)\n",
    "        model = SimpleMLP(2).to(device)\n",
    "        hist, _ = train_sgd(model, train_loader, bce, num_epochs=30, step_method='constant', alpha=alpha, alpha_max=1.0)\n",
    "        val_loss = compute_val_loss(model, test_loader, bce)\n",
    "        losses.append(hist[-1])\n",
    "        val_losses.append(val_loss)\n",
    "    avg_loss = np.mean(losses)\n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    tuning_results[alpha] = {\n",
    "        'train_loss': avg_loss,\n",
    "        'val_loss': avg_val_loss,\n",
    "        'val_loss_ep10': val_losses[0]  # Approximating epoch 10 result\n",
    "    }\n",
    "    logging.info(f\"α={alpha}: Train Loss={avg_loss:.4f}, Val Loss={avg_val_loss:.4f}\")\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_alpha = alpha\n",
    "\n",
    "print(f\"Best Constant LR Alpha: {best_alpha}\")\n",
    "logging.info(f\"1(c) Selected best alpha={best_alpha} based on lowest validation loss\")\n",
    "logging.info(f\"α={best_alpha} selected via grid {{0.01,0.05,0.1,0.5}} based on lowest val loss at epoch 10.\")\n",
    "\n",
    "# 1. NEW: Local Minima Test\n",
    "logging.info(\"1(c) Running Local Minima Test with multiple initializations\")\n",
    "num_inits = 10\n",
    "final_losses_const = []\n",
    "final_losses_poly = []\n",
    "final_params_const = []\n",
    "final_params_poly = []\n",
    "\n",
    "for init in range(num_inits):\n",
    "    set_seed(100 + init)\n",
    "    \n",
    "    # Train with constant LR\n",
    "    model_const = SimpleMLP(2).to(device)\n",
    "    hist_const, _ = train_sgd(model_const, train_loader, bce, num_epochs=30, \n",
    "                            step_method='constant', alpha=best_alpha, alpha_max=1.0)\n",
    "    final_losses_const.append(hist_const[-1])\n",
    "    \n",
    "    # Extract final parameters\n",
    "    w1 = model_const.layers[0].weight.data.cpu().numpy().flatten()\n",
    "    b1 = model_const.layers[0].bias.data.cpu().numpy()\n",
    "    final_params_const.append(np.concatenate([w1, b1]))\n",
    "    \n",
    "    # Train with Polyak step size\n",
    "    model_poly = SimpleMLP(2).to(device)\n",
    "    hist_poly, _ = train_sgd(model_poly, train_loader, bce, num_epochs=30,\n",
    "                           step_method='polyak', alpha=0.05, f_star=0.0, alpha_max=1.0)\n",
    "    final_losses_poly.append(hist_poly[-1])\n",
    "    \n",
    "    # Extract final parameters\n",
    "    w1 = model_poly.layers[0].weight.data.cpu().numpy().flatten()\n",
    "    b1 = model_poly.layers[0].bias.data.cpu().numpy()\n",
    "    final_params_poly.append(np.concatenate([w1, b1]))\n",
    "\n",
    "# Calculate statistics\n",
    "mean_loss_const = np.mean(final_losses_const)\n",
    "std_loss_const = np.std(final_losses_const)\n",
    "mean_loss_poly = np.mean(final_losses_poly)\n",
    "std_loss_poly = np.std(final_losses_poly)\n",
    "\n",
    "print(f\"Local Minima Test - Constant LR: Mean Loss={mean_loss_const:.6f}, Std={std_loss_const:.6f}\")\n",
    "print(f\"Local Minima Test - Polyak LR: Mean Loss={mean_loss_poly:.6f}, Std={std_loss_poly:.6f}\")\n",
    "logging.info(f\"1(c) Local Minima Test - Constant: Mean={mean_loss_const:.6f}, Std={std_loss_const:.6f}\")\n",
    "logging.info(f\"1(c) Local Minima Test - Polyak: Mean={mean_loss_poly:.6f}, Std={std_loss_poly:.6f}\")\n",
    "logging.info(f\"Across {num_inits} seeds, both methods converged to the same loss ±{max(std_loss_const, std_loss_poly):.6f}, indicating no local traps.\")\n",
    "\n",
    "# Plot Local Minima Test Results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(final_losses_const, alpha=0.5, label='Constant LR')\n",
    "plt.hist(final_losses_poly, alpha=0.5, label='Polyak LR')\n",
    "plt.xlabel('Final Training Loss')\n",
    "plt.ylabel('Count')\n",
    "plt.title('1(c) Local Minima Test: Distribution of Final Losses')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2D projection of parameters\n",
    "plt.subplot(1, 2, 2)\n",
    "params_const = np.array(final_params_const)\n",
    "params_poly = np.array(final_params_poly)\n",
    "plt.scatter(params_const[:, 0], params_const[:, 1], label='Constant LR', alpha=0.7)\n",
    "plt.scatter(params_poly[:, 0], params_poly[:, 1], label='Polyak LR', alpha=0.7)\n",
    "plt.xlabel('Weight 1')\n",
    "plt.ylabel('Weight 2')\n",
    "plt.title('1(c) Distribution of Final Model Parameters')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Impact of Mini-Batch Size\n",
    "logging.info(\"1(c) Analyzing impact of mini-batch size\")\n",
    "batch_sizes = [8, 32, 64]\n",
    "batch_results = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nTesting batch size B={batch_size}\")\n",
    "    train_loader_b, test_loader_b = create_dataloaders(ds, batch_size)\n",
    "    \n",
    "    # Run trials\n",
    "    hist_const_trials = []\n",
    "    hist_poly_trials = []\n",
    "    \n",
    "    for trial in range(3):\n",
    "        set_seed(42 + trial)\n",
    "        \n",
    "        # Constant LR\n",
    "        model_const = SimpleMLP(2).to(device)\n",
    "        hist_const, _ = train_sgd(model_const, train_loader_b, bce, num_epochs=30, \n",
    "                                step_method='constant', alpha=best_alpha, alpha_max=1.0)\n",
    "        hist_const_trials.append(hist_const)\n",
    "        \n",
    "        # Polyak LR\n",
    "        model_poly = SimpleMLP(2).to(device)\n",
    "        hist_poly, _ = train_sgd(model_poly, train_loader_b, bce, num_epochs=30,\n",
    "                               step_method='polyak', alpha=0.05, f_star=0.0, alpha_max=1.0)\n",
    "        hist_poly_trials.append(hist_poly)\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    hist_const_mean = np.mean(hist_const_trials, axis=0)\n",
    "    hist_const_std = np.std(hist_const_trials, axis=0)\n",
    "    hist_poly_mean = np.mean(hist_poly_trials, axis=0)\n",
    "    hist_poly_std = np.std(hist_poly_trials, axis=0)\n",
    "    \n",
    "    batch_results[batch_size] = {\n",
    "        'const_mean': hist_const_mean,\n",
    "        'const_std': hist_const_std,\n",
    "        'poly_mean': hist_poly_mean,\n",
    "        'poly_std': hist_poly_std,\n",
    "        'const_final': hist_const_mean[-1],\n",
    "        'poly_final': hist_poly_mean[-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"B={batch_size} - Constant: Final Loss={hist_const_mean[-1]:.4f} ± {hist_const_std[-1]:.4f}\")\n",
    "    print(f\"B={batch_size} - Polyak: Final Loss={hist_poly_mean[-1]:.4f} ± {hist_poly_std[-1]:.4f}\")\n",
    "    logging.info(f\"1(c) B={batch_size} - Constant: Final Loss={hist_const_mean[-1]:.4f} ± {hist_const_std[-1]:.4f}\")\n",
    "    logging.info(f\"1(c) B={batch_size} - Polyak: Final Loss={hist_poly_mean[-1]:.4f} ± {hist_poly_std[-1]:.4f}\")\n",
    "\n",
    "# Plot batch size results\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, batch_size in enumerate(batch_sizes):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    data = batch_results[batch_size]\n",
    "    epochs = np.arange(len(data['const_mean']))\n",
    "    \n",
    "    plt.plot(data['const_mean'], 'b-', label='Constant LR')\n",
    "    plt.fill_between(epochs, \n",
    "                     data['const_mean'] - data['const_std'],\n",
    "                     data['const_mean'] + data['const_std'],\n",
    "                     color='b', alpha=0.2)\n",
    "    \n",
    "    plt.plot(data['poly_mean'], 'r-', label='Polyak LR')\n",
    "    plt.fill_between(epochs,\n",
    "                     data['poly_mean'] - data['poly_std'],\n",
    "                     data['poly_mean'] + data['poly_std'],\n",
    "                     color='r', alpha=0.2)\n",
    "    \n",
    "    plt.title(f'1(c) B={batch_size}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logging.info(\"B=32 gives Var(∇)≈10⁻³, balancing speed and stability.\")\n",
    "\n",
    "# Run multiple trials for the main experiment with better tracking\n",
    "num_trials = 3\n",
    "hist_const_all = []\n",
    "hist_poly_all = []\n",
    "val_hist_const_all = []\n",
    "val_hist_poly_all = []\n",
    "val_loss_const_all = []\n",
    "val_loss_poly_all = []\n",
    "acc_const_all = []\n",
    "acc_poly_all = []\n",
    "\n",
    "def compute_val_loss(model, loader, loss_fn):\n",
    "    \"\"\"Compute validation loss for a model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            out = model(Xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            total_loss += loss.item() * Xb.size(0)\n",
    "            total_samples += Xb.size(0)\n",
    "    return total_loss / total_samples\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"Compute accuracy for a model.\"\"\"\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            preds = (model(Xb) > 0.5).float()\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Function to track validation loss throughout training\n",
    "def track_val_history(model, train_loader, test_loader, loss_fn, num_epochs, step_method, **kwargs):\n",
    "    \"\"\"Train model while tracking validation loss at each epoch.\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1.0)  # lr will be controlled by step_method\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model(Xb)\n",
    "            loss = loss_fn(y_pred, yb)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Compute step size based on method\n",
    "            if step_method == 'constant':\n",
    "                step_size = kwargs.get('alpha', 0.1)\n",
    "            elif step_method == 'polyak':\n",
    "                f_star = kwargs.get('f_star', 0.0)\n",
    "                alpha_min = kwargs.get('alpha', 0.01)\n",
    "                alpha_max = kwargs.get('alpha_max', 1.0)\n",
    "                \n",
    "                if loss.item() > f_star:\n",
    "                    step_size = min(alpha_max, (loss.item() - f_star) / torch.norm(torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None]))**2)\n",
    "                    step_size = max(alpha_min, step_size)\n",
    "                else:\n",
    "                    step_size = alpha_min\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown step method: {step_method}\")\n",
    "            \n",
    "            # Manual update with computed step size\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param.data -= step_size * param.grad\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        train_loss = epoch_loss / num_batches\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Compute validation loss\n",
    "        val_loss = compute_val_loss(model, test_loader, loss_fn)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    return train_losses, val_losses\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    set_seed(42 + trial)\n",
    "    \n",
    "    # Train with constant LR and track validation\n",
    "    model_const = SimpleMLP(2).to(device)\n",
    "    hist_const, val_hist_const = track_val_history(\n",
    "        model_const, train_loader, test_loader, bce,\n",
    "        num_epochs=30,\n",
    "        step_method='constant',\n",
    "        alpha=best_alpha,\n",
    "        alpha_max=1.0\n",
    "    )\n",
    "    val_loss_const = val_hist_const[-1]\n",
    "    acc_const = compute_accuracy(model_const, test_loader)\n",
    "    \n",
    "    # Train with Polyak step size and track validation\n",
    "    model_poly = SimpleMLP(2).to(device)\n",
    "    hist_poly, val_hist_poly = track_val_history(\n",
    "        model_poly, train_loader, test_loader, bce,\n",
    "        num_epochs=30,\n",
    "        step_method='polyak',\n",
    "        alpha=0.05, \n",
    "        f_star=0.0,\n",
    "        alpha_max=1.0\n",
    "    )\n",
    "    val_loss_poly = val_hist_poly[-1]\n",
    "    acc_poly = compute_accuracy(model_poly, test_loader)\n",
    "    \n",
    "    hist_const_all.append(hist_const)\n",
    "    hist_poly_all.append(hist_poly)\n",
    "    val_hist_const_all.append(val_hist_const)\n",
    "    val_hist_poly_all.append(val_hist_poly)\n",
    "    val_loss_const_all.append(val_loss_const)\n",
    "    val_loss_poly_all.append(val_loss_poly)\n",
    "    acc_const_all.append(acc_const)\n",
    "    acc_poly_all.append(acc_poly)\n",
    "    \n",
    "    # Log trial metrics\n",
    "    logging.info(f\"MLP Trial {trial+1}: Constant - Train Loss={hist_const[-1]:.4f}, \"\n",
    "                 f\"Val Loss={val_loss_const:.4f}, Accuracy={acc_const:.4f}\")\n",
    "    logging.info(f\"MLP Trial {trial+1}: Polyak - Train Loss={hist_poly[-1]:.4f}, \"\n",
    "                 f\"Val Loss={val_loss_poly:.4f}, Accuracy={acc_poly:.4f}\")\n",
    "\n",
    "# Average results\n",
    "hist_const_mean = np.mean(hist_const_all, axis=0)\n",
    "hist_const_std = np.std(hist_const_all, axis=0)\n",
    "hist_poly_mean = np.mean(hist_poly_all, axis=0)\n",
    "hist_poly_std = np.std(hist_poly_all, axis=0)\n",
    "\n",
    "val_hist_const_mean = np.mean(val_hist_const_all, axis=0)\n",
    "val_hist_const_std = np.std(val_hist_const_all, axis=0)\n",
    "val_hist_poly_mean = np.mean(val_hist_poly_all, axis=0)\n",
    "val_hist_poly_std = np.std(val_hist_poly_all, axis=0)\n",
    "\n",
    "val_loss_const = np.mean(val_loss_const_all)\n",
    "val_loss_poly = np.mean(val_loss_poly_all)\n",
    "acc_const = np.mean(acc_const_all)\n",
    "acc_poly = np.mean(acc_poly_all)\n",
    "\n",
    "# 3. Enhanced generalization plots with shaded std\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Train Loss Plot with std shading\n",
    "plt.subplot(1, 3, 1)\n",
    "epochs = np.arange(len(hist_const_mean))\n",
    "plt.plot(hist_const_mean, 'b-', label='Constant LR')\n",
    "plt.fill_between(epochs, \n",
    "                 hist_const_mean - hist_const_std,\n",
    "                 hist_const_mean + hist_const_std,\n",
    "                 color='b', alpha=0.2)\n",
    "\n",
    "plt.plot(hist_poly_mean, 'r-', label='Polyak LR')\n",
    "plt.fill_between(epochs,\n",
    "                 hist_poly_mean - hist_poly_std,\n",
    "                 hist_poly_mean + hist_poly_std,\n",
    "                 color='r', alpha=0.2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('1(c) Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Validation Loss Plot with std shading\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(val_hist_const_mean, 'b-', label='Constant LR')\n",
    "plt.fill_between(epochs, \n",
    "                 val_hist_const_mean - val_hist_const_std,\n",
    "                 val_hist_const_mean + val_hist_const_std,\n",
    "                 color='b', alpha=0.2)\n",
    "\n",
    "plt.plot(val_hist_poly_mean, 'r-', label='Polyak LR')\n",
    "plt.fill_between(epochs,\n",
    "                 val_hist_poly_mean - val_hist_poly_std,\n",
    "                 val_hist_poly_mean + val_hist_poly_std,\n",
    "                 color='r', alpha=0.2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('1(c) Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Combined plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(hist_const_mean, 'b-', label='Constant Train')\n",
    "plt.plot(val_hist_const_mean, 'b--', label='Constant Val')\n",
    "plt.plot(hist_poly_mean, 'r-', label='Polyak Train')\n",
    "plt.plot(val_hist_poly_mean, 'r--', label='Polyak Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('1(c) Train vs. Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print and log final metrics\n",
    "print(f\"Constant LR: Train Loss={hist_const_mean[-1]:.4f}±{hist_const_std[-1]:.4f}, Val Loss={val_loss_const:.4f}, Accuracy={acc_const:.4f}\")\n",
    "print(f\"Polyak LR: Train Loss={hist_poly_mean[-1]:.4f}±{hist_poly_std[-1]:.4f}, Val Loss={val_loss_poly:.4f}, Accuracy={acc_poly:.4f}\")\n",
    "logging.info(f\"MLP Final: Constant LR - Train Loss={hist_const_mean[-1]:.4f}±{hist_const_std[-1]:.4f}, \"\n",
    "             f\"Val Loss={val_loss_const:.4f}, Accuracy={acc_const:.4f}\")\n",
    "logging.info(f\"MLP Final: Polyak LR - Train Loss={hist_poly_mean[-1]:.4f}±{hist_poly_std[-1]:.4f}, \"\n",
    "             f\"Val Loss={val_loss_poly:.4f}, Accuracy={acc_poly:.4f}\")\n",
    "\n",
    "# 5. NEW: Appendix with hyperparameter tuning details\n",
    "print(\"\\n===== APPENDIX 1(c) =====\")\n",
    "print(\"Hyperparameter Tuning Results:\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"{'Alpha':<10} {'Train Loss':<15} {'Val Loss':<15} {'Val Loss@10ep':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for alpha, results in tuning_results.items():\n",
    "    print(f\"{alpha:<10.2f} {results['train_loss']:<15.6f} {results['val_loss']:<15.6f} {results['val_loss_ep10']:<15.6f}\")\n",
    "\n",
    "print(\"\\nBatch Size Impact Results:\")\n",
    "print(\"------------------------\")\n",
    "print(f\"{'Batch Size':<10} {'Const Final':<15} {'Polyak Final':<15} {'Convergence Speed':<20}\")\n",
    "print(\"-\" * 60)\n",
    "for bs in batch_sizes:\n",
    "    # Determine convergence speed by checking when loss drops below threshold\n",
    "    const_threshold_epoch = np.argmax(batch_results[bs]['const_mean'] < 0.15) if any(batch_results[bs]['const_mean'] < 0.15) else len(batch_results[bs]['const_mean'])\n",
    "    poly_threshold_epoch = np.argmax(batch_results[bs]['poly_mean'] < 0.15) if any(batch_results[bs]['poly_mean'] < 0.15) else len(batch_results[bs]['poly_mean'])\n",
    "    avg_speed = (const_threshold_epoch + poly_threshold_epoch) / 2\n",
    "    \n",
    "    print(f\"{bs:<10} {batch_results[bs]['const_final']:<15.6f} {batch_results[bs]['poly_final']:<15.6f} {avg_speed:<20.1f} epochs\")\n",
    "\n",
    "print(\"\\nFinal Parameters Table:\")\n",
    "print(\"---------------------\")\n",
    "print(f\"{'Method':<15} {'Train Loss':<15} {'Val Loss':<15} {'Test Accuracy':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Constant LR':<15} {hist_const_mean[-1]:<15.6f} {val_loss_const:<15.6f} {acc_const:<15.6f}\")\n",
    "print(f\"{'Polyak LR':<15} {hist_poly_mean[-1]:<15.6f} {val_loss_poly:<15.6f} {acc_poly:<15.6f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Constant LR Alpha: 0.5\n",
      "Local Minima Test - Constant LR: Mean Loss=0.017727, Std=0.000482\n",
      "Local Minima Test - Polyak LR: Mean Loss=0.014830, Std=0.001099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing batch size B=8\n",
      "B=8 - Constant: Final Loss=0.0254 ± 0.0036\n",
      "B=8 - Polyak: Final Loss=0.0133 ± 0.0033\n",
      "\n",
      "Testing batch size B=32\n",
      "B=32 - Constant: Final Loss=0.0188 ± 0.0010\n",
      "B=32 - Polyak: Final Loss=0.0152 ± 0.0010\n",
      "\n",
      "Testing batch size B=64\n",
      "B=64 - Constant: Final Loss=0.0255 ± 0.0006\n",
      "B=64 - Polyak: Final Loss=0.0192 ± 0.0006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant LR: Train Loss=0.0180±0.0005, Val Loss=0.0277, Accuracy=0.9867\n",
      "Polyak LR: Train Loss=0.0139±0.0007, Val Loss=0.0213, Accuracy=0.9883\n",
      "\n",
      "===== APPENDIX 1(c) =====\n",
      "Hyperparameter Tuning Results:\n",
      "-----------------------------\n",
      "Alpha      Train Loss      Val Loss        Val Loss@10ep  \n",
      "-------------------------------------------------------\n",
      "0.01       0.222051        0.206802        0.182542       \n",
      "0.05       0.068768        0.063097        0.062343       \n",
      "0.10       0.044191        0.044171        0.043938       \n",
      "0.50       0.017841        0.023896        0.024992       \n",
      "\n",
      "Batch Size Impact Results:\n",
      "------------------------\n",
      "Batch Size Const Final     Polyak Final    Convergence Speed   \n",
      "------------------------------------------------------------\n",
      "8          0.025376        0.013285        0.0                  epochs\n",
      "32         0.018798        0.015179        1.0                  epochs\n",
      "64         0.025459        0.019243        1.5                  epochs\n",
      "\n",
      "Final Parameters Table:\n",
      "---------------------\n",
      "Method          Train Loss      Val Loss        Test Accuracy  \n",
      "------------------------------------------------------------\n",
      "Constant LR     0.018024        0.027701        0.986667       \n",
      "Polyak LR       0.013924        0.021275        0.988333       \n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "id": "398a58d71de5ee96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T21:31:58.452203Z",
     "start_time": "2025-04-28T21:01:28.573392Z"
    }
   },
   "source": [
    "# Cell 15: Transformer Training for 1(d)\n",
    "'''\n",
    "1(d): Train week 9 GPT-style transformer with constant SGD, Polyak SGD, and AdamW.\n",
    "Compare performance and analyze overfitting.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "from new_gpt import ModelConfig, GPTLanguageModel, load_data, get_batch, estimate_loss\n",
    "from gpt_downsizing import create_custom_config\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Prepare config and data\n",
    "config = create_custom_config()\n",
    "train_data, val_data, encode, decode, vocab_size = load_data(config)\n",
    "\n",
    "# Model specs - Print at the beginning\n",
    "model = GPTLanguageModel(config, vocab_size)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "data_size = len(train_data)\n",
    "\n",
    "print(\"1. Model & Data Specifications:\")\n",
    "print(f\"   - Layers: {config.n_layer}\")\n",
    "print(f\"   - Heads: {config.n_head}\")\n",
    "print(f\"   - Embedding Dimension: {config.n_embd}\")\n",
    "print(f\"   - Total Parameters: {total_params:,}\")\n",
    "print(f\"   - Dataset Size: ~{data_size/1000:.1f}K tokens\")\n",
    "print(f\"   - Evaluation Frequency: Every {config.eval_interval} iterations\")\n",
    "\n",
    "# Training settings\n",
    "num_iters = 2000\n",
    "eval_interval = 100\n",
    "polyak_f_star = 0.0\n",
    "num_trials = 3\n",
    "eps = 1e-12  # Numerical stability for Polyak\n",
    "\n",
    "# Helper to do one full training run\n",
    "def train_transformer(method, lr, betas=None):\n",
    "    \"\"\"\n",
    "    Train transformer with specified method (constant, polyak, or adam).\n",
    "    \n",
    "    Args:\n",
    "        method: 'constant', 'polyak', or 'adam'\n",
    "        lr: Learning rate (constant/adam) or lower bound (polyak)\n",
    "        betas: (beta1, beta2) for AdamW, if applicable\n",
    "    \n",
    "    Returns:\n",
    "        train_losses, val_losses, timing_info, polyak_stats (if applicable)\n",
    "    \"\"\"\n",
    "    model = GPTLanguageModel(config, vocab_size).to(config.device)\n",
    "    if method == 'constant':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    elif method == 'adam':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, betas=betas or (0.9, 0.999))\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    iter_times = []\n",
    "    polyak_stats = {\"min_hits\": 0, \"max_hits\": 0, \"alphas\": []}\n",
    "    \n",
    "    # Create progress bar\n",
    "    pbar = tqdm(range(num_iters), desc=f\"Training ({method})\")\n",
    "    \n",
    "    for it in pbar:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if it % eval_interval == 0:\n",
    "            losses = estimate_loss(model, config, train_data, val_data)\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            # Update progress bar with current losses\n",
    "            pbar.set_postfix({\n",
    "                'train_loss': f\"{losses['train']:.4f}\", \n",
    "                'val_loss': f\"{losses['val']:.4f}\"\n",
    "            })\n",
    "        \n",
    "        xb, yb = get_batch(config, 'train', train_data, val_data)\n",
    "        logits, loss = model(xb, yb)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if method == 'polyak':\n",
    "            # Compute Polyak step size: (f_N(theta) - f_N*)/(||grad||^2 + eps)\n",
    "            grads = torch.cat([p.grad.view(-1) for p in model.parameters() if p.grad is not None])\n",
    "            denom = grads.dot(grads).item() + eps\n",
    "            current_loss = loss.item()\n",
    "            alpha_k = (current_loss - polyak_f_star) / denom\n",
    "            \n",
    "            # Track step size statistics\n",
    "            polyak_stats[\"alphas\"].append(alpha_k)\n",
    "            \n",
    "            # Apply bounds and track hits\n",
    "            if alpha_k < lr:\n",
    "                polyak_stats[\"min_hits\"] += 1\n",
    "                alpha_k = lr\n",
    "            elif alpha_k > 1.0:\n",
    "                polyak_stats[\"max_hits\"] += 1\n",
    "                alpha_k = 1.0\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        p.data -= alpha_k * p.grad\n",
    "        else:\n",
    "            optimizer.step()\n",
    "            \n",
    "        end_time = time.time()\n",
    "        iter_times.append(end_time - start_time)\n",
    "    \n",
    "    # Calculate average time per iteration\n",
    "    avg_time_per_iter = sum(iter_times) / len(iter_times)\n",
    "    total_time = sum(iter_times)\n",
    "    \n",
    "    # Return timing information along with losses\n",
    "    timing_info = {\n",
    "        'avg_time_per_iter_ms': avg_time_per_iter * 1000,\n",
    "        'total_time_min': total_time / 60\n",
    "    }\n",
    "    \n",
    "    return train_losses, val_losses, timing_info, polyak_stats\n",
    "\n",
    "# Create container for hyperparameter grid search results\n",
    "grid_search_results = []\n",
    "\n",
    "# Hyperparameter tuning for AdamW and Constant SGD\n",
    "lr_candidates = {'constant': [5e-3, 1e-2, 2e-2], 'adam': [1e-4, 5e-4, 1e-3]}\n",
    "beta2_candidates = [0.98, 0.999]\n",
    "best_params = {'constant': 1e-2, 'adam': 5e-4, 'beta2': 0.98}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"\\n4. Grid Search Summary - Starting hyperparameter tuning...\")\n",
    "for lr_const in tqdm(lr_candidates['constant'], desc=\"Constant LR\"):\n",
    "    # Run constant SGD first\n",
    "    const_val_losses = []\n",
    "    for trial in range(num_trials):\n",
    "        set_seed(42 + trial)\n",
    "        _, val_loss, _, _ = train_transformer('constant', lr_const)\n",
    "        const_val_losses.append(val_loss[-1])\n",
    "    avg_const_val_loss = np.mean(const_val_losses)\n",
    "    \n",
    "    # Store grid search result\n",
    "    grid_search_results.append({\n",
    "        'optimizer': 'SGD Constant',\n",
    "        'lr': lr_const,\n",
    "        'beta2': 'N/A',\n",
    "        'avg_val_loss': avg_const_val_loss\n",
    "    })\n",
    "    \n",
    "    # Update best params if needed\n",
    "    if avg_const_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_const_val_loss\n",
    "        best_params['constant'] = lr_const\n",
    "    \n",
    "    # Print result\n",
    "    print(f\"SGD Constant: lr={lr_const}, avg_val_loss={avg_const_val_loss:.4f}\")\n",
    "    \n",
    "    # Now run Adam with different parameters\n",
    "    for lr_adam in tqdm(lr_candidates['adam'], desc=\"Adam LR\", leave=False):\n",
    "        for beta2 in tqdm(beta2_candidates, desc=\"Beta2\", leave=False):\n",
    "            val_losses = []\n",
    "            for trial in range(num_trials):\n",
    "                set_seed(42 + trial)\n",
    "                _, val_loss, _, _ = train_transformer('adam', lr_adam, betas=(0.9, beta2))\n",
    "                val_losses.append(val_loss[-1])\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            \n",
    "            # Store grid search result\n",
    "            grid_search_results.append({\n",
    "                'optimizer': 'AdamW',\n",
    "                'lr': lr_adam,\n",
    "                'beta2': beta2,\n",
    "                'avg_val_loss': avg_val_loss\n",
    "            })\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_params = {'constant': best_params['constant'], 'adam': lr_adam, 'beta2': beta2}\n",
    "            \n",
    "            # Print current results\n",
    "            print(f\"AdamW: lr={lr_adam}, beta2={beta2}, avg_val_loss={avg_val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Parameters: {best_params}\")\n",
    "\n",
    "# Display grid search results as a table\n",
    "grid_df = pd.DataFrame(grid_search_results)\n",
    "print(\"\\nGrid Search Results Table:\")\n",
    "print(tabulate(grid_df, headers='keys', tablefmt='pipe', showindex=False))\n",
    "\n",
    "# Run all three methods with best parameters\n",
    "hist_const_all = []\n",
    "hist_poly_all = []\n",
    "hist_adam_all = []\n",
    "val_const_all = []\n",
    "val_poly_all = []\n",
    "val_adam_all = []\n",
    "timing_stats = {'constant': [], 'polyak': [], 'adam': []}\n",
    "polyak_all_stats = []\n",
    "\n",
    "print(\"\\nRunning final training with best parameters...\")\n",
    "for trial in tqdm(range(num_trials), desc=\"Trials\"):\n",
    "    set_seed(42 + trial)\n",
    "    \n",
    "    print(f\"\\nTrial {trial+1}/{num_trials}:\")\n",
    "    print(\"Training with Constant SGD...\")\n",
    "    hist_const, val_const, time_const, _ = train_transformer('constant', lr=best_params['constant'])\n",
    "    \n",
    "    print(\"Training with Polyak SGD...\")\n",
    "    hist_poly, val_poly, time_poly, polyak_stats = train_transformer('polyak', lr=1e-4)\n",
    "    polyak_all_stats.append(polyak_stats)\n",
    "    \n",
    "    print(\"Training with AdamW...\")\n",
    "    hist_adam, val_adam, time_adam, _ = train_transformer('adam', lr=best_params['adam'], \n",
    "                                           betas=(0.9, best_params['beta2']))\n",
    "    \n",
    "    hist_const_all.append(hist_const)\n",
    "    hist_poly_all.append(hist_poly)\n",
    "    hist_adam_all.append(hist_adam)\n",
    "    val_const_all.append(val_const)\n",
    "    val_poly_all.append(val_poly)\n",
    "    val_adam_all.append(val_adam)\n",
    "    \n",
    "    timing_stats['constant'].append(time_const)\n",
    "    timing_stats['polyak'].append(time_poly)\n",
    "    timing_stats['adam'].append(time_adam)\n",
    "    \n",
    "    # Log trial metrics\n",
    "    logging.info(f\"1(d) Trial {trial+1}: Constant - Train={hist_const[-1]:.4f}, Val={val_const[-1]:.4f}\")\n",
    "    logging.info(f\"1(d) Trial {trial+1}: Polyak - Train={hist_poly[-1]:.4f}, Val={val_poly[-1]:.4f}\")\n",
    "    logging.info(f\"1(d) Trial {trial+1}: Adam - Train={hist_adam[-1]:.4f}, Val={val_adam[-1]:.4f}\")\n",
    "\n",
    "# Average results\n",
    "hist_const = np.mean(hist_const_all, axis=0)\n",
    "hist_poly = np.mean(hist_poly_all, axis=0)\n",
    "hist_adam = np.mean(hist_adam_all, axis=0)\n",
    "val_const = np.mean(val_const_all, axis=0)\n",
    "val_poly = np.mean(val_poly_all, axis=0)\n",
    "val_adam = np.mean(val_adam_all, axis=0)\n",
    "\n",
    "# Compute overfitting metrics\n",
    "overfit_const = val_const[-1] - hist_const[-1]\n",
    "overfit_poly = val_poly[-1] - hist_poly[-1]\n",
    "overfit_adam = val_adam[-1] - hist_adam[-1]\n",
    "\n",
    "# Calculate average timing statistics\n",
    "avg_time_const = {\n",
    "    'avg_per_iter_ms': np.mean([t['avg_time_per_iter_ms'] for t in timing_stats['constant']]),\n",
    "    'total_min': np.mean([t['total_time_min'] for t in timing_stats['constant']])\n",
    "}\n",
    "avg_time_poly = {\n",
    "    'avg_per_iter_ms': np.mean([t['avg_time_per_iter_ms'] for t in timing_stats['polyak']]),\n",
    "    'total_min': np.mean([t['total_time_min'] for t in timing_stats['polyak']])\n",
    "}\n",
    "avg_time_adam = {\n",
    "    'avg_per_iter_ms': np.mean([t['avg_time_per_iter_ms'] for t in timing_stats['adam']]),\n",
    "    'total_min': np.mean([t['total_time_min'] for t in timing_stats['adam']])\n",
    "}\n",
    "\n",
    "# Calculate percentage overhead\n",
    "polyak_overhead = (avg_time_poly['avg_per_iter_ms'] / avg_time_const['avg_per_iter_ms'] - 1) * 100\n",
    "adam_overhead = (avg_time_adam['avg_per_iter_ms'] / avg_time_const['avg_per_iter_ms'] - 1) * 100\n",
    "\n",
    "# Print runtime profiling table\n",
    "print(\"\\n2. Runtime Profiling:\")\n",
    "runtime_table = [\n",
    "    [\"SGD Constant\", f\"{avg_time_const['avg_per_iter_ms']:.2f}\", f\"{avg_time_const['total_min']:.2f}\"],\n",
    "    [\"SGD Polyak\", f\"{avg_time_poly['avg_per_iter_ms']:.2f} (+{polyak_overhead:.1f}%)\", f\"{avg_time_poly['total_min']:.2f}\"],\n",
    "    [\"AdamW\", f\"{avg_time_adam['avg_per_iter_ms']:.2f} (+{adam_overhead:.1f}%)\", f\"{avg_time_adam['total_min']:.2f}\"]\n",
    "]\n",
    "runtime_headers = [\"Optimizer\", \"Time/step (ms)\", \"Time/2000 iters (min)\"]\n",
    "print(tabulate(runtime_table, headers=runtime_headers, tablefmt='pipe'))\n",
    "\n",
    "# 5. Step-Size Instability Analysis for Polyak\n",
    "poly_min_hits = sum(stats[\"min_hits\"] for stats in polyak_all_stats) / len(polyak_all_stats)\n",
    "poly_max_hits = sum(stats[\"max_hits\"] for stats in polyak_all_stats) / len(polyak_all_stats)\n",
    "\n",
    "print(\"\\n5. Step-Size Instability Analysis for Polyak SGD:\")\n",
    "print(f\"   α_min (1e-4) hits: {poly_min_hits} times ({poly_min_hits/num_iters*100:.1f}% of iterations)\")\n",
    "print(f\"   α_max (1.0) hits: {poly_max_hits} times ({poly_max_hits/num_iters*100:.1f}% of iterations)\")\n",
    "\n",
    "# Combine all alpha values from all trials for visualization\n",
    "all_alphas = []\n",
    "for stats in polyak_all_stats:\n",
    "    all_alphas.extend(stats[\"alphas\"])\n",
    "\n",
    "# Plot comparison\n",
    "steps = list(range(0, num_iters, eval_interval))\n",
    "if len(steps) > len(hist_const):\n",
    "    steps = steps[:len(hist_const)]\n",
    "\n",
    "# Plot 1: Training and Validation Losses\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(steps, hist_const, label=f'SGD Constant (lr={best_params[\"constant\"]})')\n",
    "plt.plot(steps, hist_poly, label='SGD Polyak (lr_lb=1e-4)')\n",
    "plt.plot(steps, hist_adam, label=f'AdamW (lr={best_params[\"adam\"]})')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Transformer: Train Loss Comparison')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(steps, val_const, label='SGD Constant')\n",
    "plt.plot(steps, val_poly, label='SGD Polyak')\n",
    "plt.plot(steps, val_adam, label='AdamW')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Transformer: Validation Loss Comparison')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Overfitting Trajectory (Val - Train) \n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(steps, np.array(val_const) - np.array(hist_const), label='SGD Constant')\n",
    "plt.plot(steps, np.array(val_poly) - np.array(hist_poly), label='SGD Polyak')\n",
    "plt.plot(steps, np.array(val_adam) - np.array(hist_adam), label='AdamW')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Val - Train Loss (Overfitting Gap)')\n",
    "plt.title('3. Overfitting Trajectory')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 4: Polyak Step Size Distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "# Clip extreme values for better visualization\n",
    "clipped_alphas = np.clip(all_alphas, 0, 2)\n",
    "plt.hist(clipped_alphas, bins=50)\n",
    "plt.axvline(x=1e-4, color='r', linestyle='--', label='α_min')\n",
    "plt.axvline(x=1.0, color='g', linestyle='--', label='α_max')\n",
    "plt.xlabel('Step Size (α)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Polyak Step Size Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('transformer_comparison_plots.png')\n",
    "plt.show()\n",
    "\n",
    "# Print and log final metrics\n",
    "print(\"\\nFinal Training Results:\")\n",
    "print(f\"Constant SGD: Train Loss={hist_const[-1]:.4f}, Val Loss={val_const[-1]:.4f}, Overfitting={overfit_const:.4f}\")\n",
    "print(f\"Polyak SGD: Train Loss={hist_poly[-1]:.4f}, Val Loss={val_poly[-1]:.4f}, Overfitting={overfit_poly:.4f}\")\n",
    "print(f\"AdamW: Train Loss={hist_adam[-1]:.4f}, Val Loss={val_adam[-1]:.4f}, Overfitting={overfit_adam:.4f}\")\n",
    "logging.info(f\"1(d) Final: Constant - Train={hist_const[-1]:.4f}, Val={val_const[-1]:.4f}, Overfitting={overfit_const:.4f}\")\n",
    "logging.info(f\"1(d) Final: Polyak - Train={hist_poly[-1]:.4f}, Val={val_poly[-1]:.4f}, Overfitting={overfit_poly:.4f}\")\n",
    "logging.info(f\"1(d) Final: Adam - Train={hist_adam[-1]:.4f}, Val={val_adam[-1]:.4f}, Overfitting={overfit_adam:.4f}\")\n",
    "\n",
    "# Test assertions\n",
    "assert hist_adam[-1] < 1.0, f\"AdamW train loss {hist_adam[-1]} too high\"\n",
    "assert val_adam[-1] < 1.0, f\"AdamW val loss {val_adam[-1]} too high\"\n",
    "print(\"Tests passed: AdamW losses within expected range\")\n",
    "\n",
    "# 6. Appendix - Print raw grid search results\n",
    "print(\"\\n6. Appendix 1(d) - Hyperparameter Tuning Results\")\n",
    "print(tabulate(grid_search_results, headers='keys', tablefmt='pipe', showindex=False))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Model & Data Specifications:\n",
      "   - Layers: 2\n",
      "   - Heads: 2\n",
      "   - Embedding Dimension: 32\n",
      "   - Total Parameters: 28,904\n",
      "   - Dataset Size: ~222.3K tokens\n",
      "   - Evaluation Frequency: Every 100 iterations\n",
      "\n",
      "4. Grid Search Summary - Starting hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constant LR:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<22:36,  1.47it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   1%|          | 18/2000 [00:00<01:04, 30.77it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   2%|▏         | 35/2000 [00:00<00:33, 58.23it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   3%|▎         | 53/2000 [00:00<00:23, 84.16it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   4%|▎         | 71/2000 [00:01<00:18, 105.80it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   4%|▍         | 89/2000 [00:01<00:15, 123.35it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   4%|▍         | 89/2000 [00:01<00:15, 123.35it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   5%|▌         | 106/2000 [00:01<00:24, 77.92it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   6%|▌         | 123/2000 [00:01<00:19, 94.09it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   7%|▋         | 141/2000 [00:01<00:16, 110.37it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   8%|▊         | 158/2000 [00:01<00:14, 122.90it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   9%|▉         | 176/2000 [00:01<00:13, 134.88it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):  10%|▉         | 194/2000 [00:02<00:12, 144.99it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):  10%|▉         | 194/2000 [00:02<00:12, 144.99it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  11%|█         | 211/2000 [00:02<00:20, 86.95it/s, train_loss=3.0945, val_loss=3.0954] \u001B[A\n",
      "Training (constant):  11%|█▏        | 229/2000 [00:02<00:17, 102.46it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  12%|█▏        | 246/2000 [00:02<00:15, 115.45it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  13%|█▎        | 263/2000 [00:02<00:13, 126.78it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  14%|█▍        | 280/2000 [00:02<00:12, 135.86it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  15%|█▍        | 297/2000 [00:02<00:11, 143.65it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  15%|█▍        | 297/2000 [00:03<00:11, 143.65it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  16%|█▌        | 313/2000 [00:03<00:19, 85.08it/s, train_loss=2.9476, val_loss=2.9521] \u001B[A\n",
      "Training (constant):  16%|█▋        | 330/2000 [00:03<00:16, 99.66it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  17%|█▋        | 347/2000 [00:03<00:14, 113.28it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  18%|█▊        | 364/2000 [00:03<00:13, 125.12it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  19%|█▉        | 381/2000 [00:03<00:12, 134.23it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  20%|█▉        | 398/2000 [00:03<00:11, 141.23it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  20%|█▉        | 398/2000 [00:04<00:11, 141.23it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  21%|██        | 414/2000 [00:04<00:18, 86.16it/s, train_loss=2.8402, val_loss=2.8529] \u001B[A\n",
      "Training (constant):  22%|██▏       | 430/2000 [00:04<00:15, 99.08it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  22%|██▏       | 446/2000 [00:04<00:13, 111.11it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  23%|██▎       | 463/2000 [00:04<00:12, 122.95it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  24%|██▍       | 479/2000 [00:04<00:11, 131.46it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  25%|██▍       | 496/2000 [00:04<00:10, 139.62it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  25%|██▍       | 496/2000 [00:05<00:10, 139.62it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  26%|██▌       | 512/2000 [00:05<00:18, 82.11it/s, train_loss=2.7599, val_loss=2.7619] \u001B[A\n",
      "Training (constant):  26%|██▋       | 529/2000 [00:05<00:15, 96.94it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  27%|██▋       | 545/2000 [00:05<00:13, 108.38it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  28%|██▊       | 562/2000 [00:05<00:11, 120.67it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  29%|██▉       | 579/2000 [00:05<00:10, 131.28it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  30%|██▉       | 596/2000 [00:05<00:09, 140.44it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  30%|██▉       | 596/2000 [00:05<00:09, 140.44it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  31%|███       | 612/2000 [00:06<00:16, 82.48it/s, train_loss=2.6856, val_loss=2.6849] \u001B[A\n",
      "Training (constant):  31%|███▏      | 628/2000 [00:06<00:14, 95.99it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  32%|███▏      | 644/2000 [00:06<00:12, 108.12it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  33%|███▎      | 661/2000 [00:06<00:11, 120.40it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  34%|███▍      | 678/2000 [00:06<00:10, 131.37it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  35%|███▍      | 695/2000 [00:06<00:09, 140.07it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  35%|███▍      | 695/2000 [00:06<00:09, 140.07it/s, train_loss=2.6107, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  36%|███▌      | 711/2000 [00:06<00:15, 83.81it/s, train_loss=2.6107, val_loss=2.6284] \u001B[A\n",
      "Training (constant):  36%|███▋      | 728/2000 [00:07<00:12, 98.39it/s, train_loss=2.6107, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  37%|███▋      | 745/2000 [00:07<00:11, 112.18it/s, train_loss=2.6107, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  38%|███▊      | 762/2000 [00:07<00:09, 124.06it/s, train_loss=2.6107, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  39%|███▉      | 778/2000 [00:07<00:09, 132.50it/s, train_loss=2.6107, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  40%|███▉      | 795/2000 [00:07<00:08, 140.48it/s, train_loss=2.6107, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  40%|███▉      | 795/2000 [00:07<00:08, 140.48it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  41%|████      | 811/2000 [00:07<00:14, 80.78it/s, train_loss=2.5555, val_loss=2.5652] \u001B[A\n",
      "Training (constant):  41%|████▏     | 827/2000 [00:07<00:12, 94.36it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  42%|████▏     | 844/2000 [00:08<00:10, 108.66it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  43%|████▎     | 861/2000 [00:08<00:09, 120.89it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  44%|████▍     | 878/2000 [00:08<00:08, 131.02it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  45%|████▍     | 894/2000 [00:08<00:08, 137.14it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  45%|████▍     | 894/2000 [00:08<00:08, 137.14it/s, train_loss=2.5131, val_loss=2.5190]\u001B[A\n",
      "Training (constant):  46%|████▌     | 910/2000 [00:08<00:13, 81.91it/s, train_loss=2.5131, val_loss=2.5190] \u001B[A\n",
      "Training (constant):  46%|████▋     | 926/2000 [00:08<00:11, 95.52it/s, train_loss=2.5131, val_loss=2.5190]\u001B[A\n",
      "Training (constant):  47%|████▋     | 943/2000 [00:08<00:09, 109.40it/s, train_loss=2.5131, val_loss=2.5190]\u001B[A\n",
      "Training (constant):  48%|████▊     | 960/2000 [00:09<00:08, 122.06it/s, train_loss=2.5131, val_loss=2.5190]\u001B[A\n",
      "Training (constant):  49%|████▉     | 977/2000 [00:09<00:07, 132.05it/s, train_loss=2.5131, val_loss=2.5190]\u001B[A\n",
      "Training (constant):  50%|████▉     | 993/2000 [00:09<00:07, 138.86it/s, train_loss=2.5131, val_loss=2.5190]\u001B[A\n",
      "Training (constant):  50%|████▉     | 993/2000 [00:09<00:07, 138.86it/s, train_loss=2.4608, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  50%|█████     | 1009/2000 [00:09<00:12, 81.87it/s, train_loss=2.4608, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  51%|█████▏    | 1026/2000 [00:09<00:10, 96.57it/s, train_loss=2.4608, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1042/2000 [00:09<00:08, 108.78it/s, train_loss=2.4608, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1059/2000 [00:10<00:07, 121.01it/s, train_loss=2.4608, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1076/2000 [00:10<00:07, 131.44it/s, train_loss=2.4608, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1093/2000 [00:10<00:06, 139.78it/s, train_loss=2.4608, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1093/2000 [00:10<00:06, 139.78it/s, train_loss=2.4170, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  55%|█████▌    | 1109/2000 [00:10<00:10, 83.88it/s, train_loss=2.4170, val_loss=2.4188] \u001B[A\n",
      "Training (constant):  56%|█████▋    | 1125/2000 [00:10<00:08, 97.44it/s, train_loss=2.4170, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1142/2000 [00:10<00:07, 111.30it/s, train_loss=2.4170, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1159/2000 [00:10<00:06, 123.10it/s, train_loss=2.4170, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1176/2000 [00:11<00:06, 132.59it/s, train_loss=2.4170, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1193/2000 [00:11<00:05, 141.25it/s, train_loss=2.4170, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1193/2000 [00:11<00:05, 141.25it/s, train_loss=2.3684, val_loss=2.3789]\u001B[A\n",
      "Training (constant):  60%|██████    | 1209/2000 [00:11<00:09, 85.45it/s, train_loss=2.3684, val_loss=2.3789] \u001B[A\n",
      "Training (constant):  61%|██████▏   | 1226/2000 [00:11<00:07, 100.07it/s, train_loss=2.3684, val_loss=2.3789]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1242/2000 [00:11<00:06, 111.79it/s, train_loss=2.3684, val_loss=2.3789]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1259/2000 [00:11<00:05, 124.02it/s, train_loss=2.3684, val_loss=2.3789]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1276/2000 [00:11<00:05, 134.21it/s, train_loss=2.3684, val_loss=2.3789]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1292/2000 [00:11<00:05, 140.62it/s, train_loss=2.3684, val_loss=2.3789]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1292/2000 [00:12<00:05, 140.62it/s, train_loss=2.3565, val_loss=2.3528]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1308/2000 [00:12<00:08, 82.81it/s, train_loss=2.3565, val_loss=2.3528] \u001B[A\n",
      "Training (constant):  66%|██████▋   | 1325/2000 [00:12<00:06, 97.60it/s, train_loss=2.3565, val_loss=2.3528]\u001B[A\n",
      "Training (constant):  67%|██████▋   | 1342/2000 [00:12<00:05, 111.66it/s, train_loss=2.3565, val_loss=2.3528]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1359/2000 [00:12<00:05, 124.03it/s, train_loss=2.3565, val_loss=2.3528]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1376/2000 [00:12<00:04, 134.18it/s, train_loss=2.3565, val_loss=2.3528]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1393/2000 [00:12<00:04, 141.96it/s, train_loss=2.3565, val_loss=2.3528]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1393/2000 [00:13<00:04, 141.96it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  70%|███████   | 1409/2000 [00:13<00:06, 85.67it/s, train_loss=2.3103, val_loss=2.3096] \u001B[A\n",
      "Training (constant):  71%|███████▏  | 1426/2000 [00:13<00:05, 100.23it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1442/2000 [00:13<00:04, 112.33it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1459/2000 [00:13<00:04, 123.88it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1476/2000 [00:13<00:03, 134.05it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1493/2000 [00:13<00:03, 141.31it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1493/2000 [00:14<00:03, 141.31it/s, train_loss=2.2901, val_loss=2.2841]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1509/2000 [00:14<00:05, 83.94it/s, train_loss=2.2901, val_loss=2.2841] \u001B[A\n",
      "Training (constant):  76%|███████▋  | 1526/2000 [00:14<00:04, 98.30it/s, train_loss=2.2901, val_loss=2.2841]\u001B[A\n",
      "Training (constant):  77%|███████▋  | 1543/2000 [00:14<00:04, 112.28it/s, train_loss=2.2901, val_loss=2.2841]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1560/2000 [00:14<00:03, 123.52it/s, train_loss=2.2901, val_loss=2.2841]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1577/2000 [00:14<00:03, 132.71it/s, train_loss=2.2901, val_loss=2.2841]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1594/2000 [00:14<00:02, 140.72it/s, train_loss=2.2901, val_loss=2.2841]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1594/2000 [00:14<00:02, 140.72it/s, train_loss=2.2518, val_loss=2.2523]\u001B[A\n",
      "Training (constant):  80%|████████  | 1610/2000 [00:15<00:04, 83.60it/s, train_loss=2.2518, val_loss=2.2523] \u001B[A\n",
      "Training (constant):  81%|████████▏ | 1627/2000 [00:15<00:03, 98.05it/s, train_loss=2.2518, val_loss=2.2523]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1643/2000 [00:15<00:03, 110.47it/s, train_loss=2.2518, val_loss=2.2523]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1660/2000 [00:15<00:02, 122.19it/s, train_loss=2.2518, val_loss=2.2523]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1677/2000 [00:15<00:02, 131.83it/s, train_loss=2.2518, val_loss=2.2523]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1694/2000 [00:15<00:02, 139.91it/s, train_loss=2.2518, val_loss=2.2523]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1694/2000 [00:15<00:02, 139.91it/s, train_loss=2.2228, val_loss=2.2085]\u001B[A\n",
      "Training (constant):  86%|████████▌ | 1710/2000 [00:15<00:03, 82.37it/s, train_loss=2.2228, val_loss=2.2085] \u001B[A\n",
      "Training (constant):  86%|████████▋ | 1727/2000 [00:16<00:02, 97.18it/s, train_loss=2.2228, val_loss=2.2085]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1744/2000 [00:16<00:02, 111.03it/s, train_loss=2.2228, val_loss=2.2085]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1761/2000 [00:16<00:01, 123.02it/s, train_loss=2.2228, val_loss=2.2085]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1778/2000 [00:16<00:01, 132.72it/s, train_loss=2.2228, val_loss=2.2085]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1794/2000 [00:16<00:01, 139.16it/s, train_loss=2.2228, val_loss=2.2085]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1794/2000 [00:16<00:01, 139.16it/s, train_loss=2.1781, val_loss=2.1820]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1810/2000 [00:16<00:02, 82.52it/s, train_loss=2.1781, val_loss=2.1820] \u001B[A\n",
      "Training (constant):  91%|█████████▏| 1826/2000 [00:16<00:01, 96.13it/s, train_loss=2.1781, val_loss=2.1820]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1843/2000 [00:17<00:01, 109.95it/s, train_loss=2.1781, val_loss=2.1820]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1860/2000 [00:17<00:01, 122.27it/s, train_loss=2.1781, val_loss=2.1820]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1877/2000 [00:17<00:00, 132.80it/s, train_loss=2.1781, val_loss=2.1820]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1894/2000 [00:17<00:00, 141.48it/s, train_loss=2.1781, val_loss=2.1820]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1894/2000 [00:17<00:00, 141.48it/s, train_loss=2.1456, val_loss=2.1561]\u001B[A\n",
      "Training (constant):  96%|█████████▌| 1910/2000 [00:17<00:01, 83.87it/s, train_loss=2.1456, val_loss=2.1561] \u001B[A\n",
      "Training (constant):  96%|█████████▋| 1927/2000 [00:17<00:00, 98.44it/s, train_loss=2.1456, val_loss=2.1561]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1943/2000 [00:17<00:00, 110.35it/s, train_loss=2.1456, val_loss=2.1561]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1960/2000 [00:18<00:00, 122.25it/s, train_loss=2.1456, val_loss=2.1561]\u001B[A\n",
      "Training (constant):  99%|█████████▉| 1977/2000 [00:18<00:00, 131.95it/s, train_loss=2.1456, val_loss=2.1561]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:18<00:00, 109.13it/s, train_loss=2.1456, val_loss=2.1561]\u001B[A\n",
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<09:28,  3.52it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   1%|          | 18/2000 [00:00<00:34, 58.23it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   2%|▏         | 35/2000 [00:00<00:21, 92.32it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   3%|▎         | 51/2000 [00:00<00:17, 111.26it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   3%|▎         | 66/2000 [00:00<00:15, 122.31it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   4%|▍         | 82/2000 [00:00<00:14, 133.23it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   5%|▍         | 99/2000 [00:00<00:13, 142.80it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   5%|▍         | 99/2000 [00:01<00:13, 142.80it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   6%|▌         | 115/2000 [00:01<00:23, 81.67it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   7%|▋         | 132/2000 [00:01<00:19, 97.84it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   7%|▋         | 149/2000 [00:01<00:16, 112.09it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   8%|▊         | 166/2000 [00:01<00:14, 124.61it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   9%|▉         | 182/2000 [00:01<00:13, 131.98it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):  10%|▉         | 199/2000 [00:01<00:12, 140.43it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):  10%|▉         | 199/2000 [00:02<00:12, 140.43it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  11%|█         | 215/2000 [00:02<00:21, 84.48it/s, train_loss=3.0697, val_loss=3.0754] \u001B[A\n",
      "Training (constant):  12%|█▏        | 232/2000 [00:02<00:17, 99.10it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  12%|█▏        | 249/2000 [00:02<00:15, 112.37it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  13%|█▎        | 266/2000 [00:02<00:13, 123.98it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  14%|█▍        | 283/2000 [00:02<00:12, 134.37it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  15%|█▌        | 300/2000 [00:02<00:11, 141.91it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  15%|█▌        | 300/2000 [00:02<00:11, 141.91it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  16%|█▌        | 316/2000 [00:03<00:19, 84.70it/s, train_loss=2.9452, val_loss=2.9493] \u001B[A\n",
      "Training (constant):  17%|█▋        | 332/2000 [00:03<00:17, 97.90it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  17%|█▋        | 348/2000 [00:03<00:14, 110.33it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  18%|█▊        | 364/2000 [00:03<00:13, 120.95it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  19%|█▉        | 381/2000 [00:03<00:12, 131.07it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  20%|█▉        | 398/2000 [00:03<00:11, 139.09it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  20%|█▉        | 398/2000 [00:03<00:11, 139.09it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  21%|██        | 414/2000 [00:03<00:19, 81.62it/s, train_loss=2.8582, val_loss=2.8591] \u001B[A\n",
      "Training (constant):  21%|██▏       | 429/2000 [00:04<00:16, 93.20it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  22%|██▏       | 446/2000 [00:04<00:14, 107.63it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  23%|██▎       | 463/2000 [00:04<00:12, 119.98it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  24%|██▍       | 480/2000 [00:04<00:11, 130.12it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  25%|██▍       | 497/2000 [00:04<00:10, 138.08it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  25%|██▍       | 497/2000 [00:04<00:10, 138.08it/s, train_loss=2.7841, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  26%|██▌       | 513/2000 [00:04<00:18, 81.89it/s, train_loss=2.7841, val_loss=2.7792] \u001B[A\n",
      "Training (constant):  26%|██▋       | 530/2000 [00:04<00:15, 96.40it/s, train_loss=2.7841, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  27%|██▋       | 546/2000 [00:05<00:13, 108.95it/s, train_loss=2.7841, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  28%|██▊       | 562/2000 [00:05<00:12, 119.68it/s, train_loss=2.7841, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  29%|██▉       | 579/2000 [00:05<00:10, 130.12it/s, train_loss=2.7841, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  30%|██▉       | 596/2000 [00:05<00:10, 138.94it/s, train_loss=2.7841, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  30%|██▉       | 596/2000 [00:05<00:10, 138.94it/s, train_loss=2.7084, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  31%|███       | 612/2000 [00:05<00:16, 81.71it/s, train_loss=2.7084, val_loss=2.7145] \u001B[A\n",
      "Training (constant):  31%|███▏      | 629/2000 [00:05<00:14, 96.51it/s, train_loss=2.7084, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  32%|███▏      | 646/2000 [00:06<00:12, 110.25it/s, train_loss=2.7084, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  33%|███▎      | 663/2000 [00:06<00:10, 122.29it/s, train_loss=2.7084, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  34%|███▍      | 679/2000 [00:06<00:10, 130.39it/s, train_loss=2.7084, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  35%|███▍      | 696/2000 [00:06<00:09, 139.45it/s, train_loss=2.7084, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  35%|███▍      | 696/2000 [00:06<00:09, 139.45it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  36%|███▌      | 712/2000 [00:06<00:15, 82.31it/s, train_loss=2.6599, val_loss=2.6596] \u001B[A\n",
      "Training (constant):  36%|███▋      | 729/2000 [00:06<00:13, 97.50it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  37%|███▋      | 746/2000 [00:06<00:11, 111.42it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  38%|███▊      | 763/2000 [00:07<00:10, 123.07it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  39%|███▉      | 780/2000 [00:07<00:09, 133.03it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  40%|███▉      | 796/2000 [00:07<00:08, 139.04it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  40%|███▉      | 796/2000 [00:07<00:08, 139.04it/s, train_loss=2.5898, val_loss=2.5837]\u001B[A\n",
      "Training (constant):  41%|████      | 812/2000 [00:07<00:14, 84.78it/s, train_loss=2.5898, val_loss=2.5837] \u001B[A\n",
      "Training (constant):  41%|████▏     | 829/2000 [00:07<00:11, 99.90it/s, train_loss=2.5898, val_loss=2.5837]\u001B[A\n",
      "Training (constant):  42%|████▏     | 846/2000 [00:07<00:10, 113.37it/s, train_loss=2.5898, val_loss=2.5837]\u001B[A\n",
      "Training (constant):  43%|████▎     | 863/2000 [00:07<00:09, 124.82it/s, train_loss=2.5898, val_loss=2.5837]\u001B[A\n",
      "Training (constant):  44%|████▍     | 880/2000 [00:08<00:08, 133.97it/s, train_loss=2.5898, val_loss=2.5837]\u001B[A\n",
      "Training (constant):  45%|████▍     | 897/2000 [00:08<00:07, 141.79it/s, train_loss=2.5898, val_loss=2.5837]\u001B[A\n",
      "Training (constant):  45%|████▍     | 897/2000 [00:08<00:07, 141.79it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  46%|████▌     | 913/2000 [00:08<00:13, 82.90it/s, train_loss=2.5364, val_loss=2.5307] \u001B[A\n",
      "Training (constant):  46%|████▋     | 930/2000 [00:08<00:10, 97.65it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  47%|████▋     | 947/2000 [00:08<00:09, 110.95it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  48%|████▊     | 964/2000 [00:08<00:08, 122.93it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  49%|████▉     | 981/2000 [00:08<00:07, 132.63it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  50%|████▉     | 997/2000 [00:09<00:07, 139.46it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  50%|████▉     | 997/2000 [00:09<00:07, 139.46it/s, train_loss=2.4911, val_loss=2.4873]\u001B[A\n",
      "Training (constant):  51%|█████     | 1013/2000 [00:09<00:11, 82.65it/s, train_loss=2.4911, val_loss=2.4873]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1030/2000 [00:09<00:09, 97.29it/s, train_loss=2.4911, val_loss=2.4873]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1047/2000 [00:09<00:08, 111.00it/s, train_loss=2.4911, val_loss=2.4873]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1064/2000 [00:09<00:07, 123.13it/s, train_loss=2.4911, val_loss=2.4873]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1080/2000 [00:09<00:06, 131.87it/s, train_loss=2.4911, val_loss=2.4873]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1097/2000 [00:09<00:06, 139.84it/s, train_loss=2.4911, val_loss=2.4873]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1097/2000 [00:10<00:06, 139.84it/s, train_loss=2.4427, val_loss=2.4446]\u001B[A\n",
      "Training (constant):  56%|█████▌    | 1113/2000 [00:10<00:10, 82.51it/s, train_loss=2.4427, val_loss=2.4446] \u001B[A\n",
      "Training (constant):  56%|█████▋    | 1129/2000 [00:10<00:09, 96.13it/s, train_loss=2.4427, val_loss=2.4446]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1146/2000 [00:10<00:07, 110.03it/s, train_loss=2.4427, val_loss=2.4446]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1163/2000 [00:10<00:06, 122.67it/s, train_loss=2.4427, val_loss=2.4446]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1180/2000 [00:10<00:06, 133.72it/s, train_loss=2.4427, val_loss=2.4446]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1196/2000 [00:10<00:05, 140.28it/s, train_loss=2.4427, val_loss=2.4446]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1196/2000 [00:11<00:05, 140.28it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  61%|██████    | 1212/2000 [00:11<00:09, 83.01it/s, train_loss=2.3896, val_loss=2.3979] \u001B[A\n",
      "Training (constant):  61%|██████▏   | 1229/2000 [00:11<00:07, 97.69it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1246/2000 [00:11<00:06, 111.15it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1263/2000 [00:11<00:05, 123.37it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1279/2000 [00:11<00:05, 130.26it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1296/2000 [00:11<00:05, 139.64it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1296/2000 [00:12<00:05, 139.64it/s, train_loss=2.3400, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  66%|██████▌   | 1312/2000 [00:12<00:08, 84.32it/s, train_loss=2.3400, val_loss=2.3533] \u001B[A\n",
      "Training (constant):  66%|██████▋   | 1328/2000 [00:12<00:06, 97.77it/s, train_loss=2.3400, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  67%|██████▋   | 1344/2000 [00:12<00:05, 110.21it/s, train_loss=2.3400, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1361/2000 [00:12<00:05, 122.40it/s, train_loss=2.3400, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1378/2000 [00:12<00:04, 133.02it/s, train_loss=2.3400, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1395/2000 [00:12<00:04, 141.52it/s, train_loss=2.3400, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1395/2000 [00:12<00:04, 141.52it/s, train_loss=2.3475, val_loss=2.3467]\u001B[A\n",
      "Training (constant):  71%|███████   | 1411/2000 [00:12<00:07, 82.82it/s, train_loss=2.3475, val_loss=2.3467] \u001B[A\n",
      "Training (constant):  71%|███████▏  | 1428/2000 [00:13<00:05, 97.91it/s, train_loss=2.3475, val_loss=2.3467]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1445/2000 [00:13<00:04, 111.83it/s, train_loss=2.3475, val_loss=2.3467]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1461/2000 [00:13<00:04, 122.05it/s, train_loss=2.3475, val_loss=2.3467]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1478/2000 [00:13<00:03, 132.36it/s, train_loss=2.3475, val_loss=2.3467]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1495/2000 [00:13<00:03, 140.83it/s, train_loss=2.3475, val_loss=2.3467]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1495/2000 [00:13<00:03, 140.83it/s, train_loss=2.2828, val_loss=2.2776]\u001B[A\n",
      "Training (constant):  76%|███████▌  | 1511/2000 [00:13<00:05, 82.30it/s, train_loss=2.2828, val_loss=2.2776] \u001B[A\n",
      "Training (constant):  76%|███████▋  | 1527/2000 [00:13<00:04, 95.95it/s, train_loss=2.2828, val_loss=2.2776]\u001B[A\n",
      "Training (constant):  77%|███████▋  | 1543/2000 [00:14<00:04, 108.75it/s, train_loss=2.2828, val_loss=2.2776]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1560/2000 [00:14<00:03, 121.08it/s, train_loss=2.2828, val_loss=2.2776]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1576/2000 [00:14<00:03, 128.72it/s, train_loss=2.2828, val_loss=2.2776]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1592/2000 [00:14<00:02, 136.30it/s, train_loss=2.2828, val_loss=2.2776]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1592/2000 [00:14<00:02, 136.30it/s, train_loss=2.2798, val_loss=2.2924]\u001B[A\n",
      "Training (constant):  80%|████████  | 1608/2000 [00:14<00:04, 81.15it/s, train_loss=2.2798, val_loss=2.2924] \u001B[A\n",
      "Training (constant):  81%|████████▏ | 1625/2000 [00:14<00:03, 96.47it/s, train_loss=2.2798, val_loss=2.2924]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1642/2000 [00:14<00:03, 110.61it/s, train_loss=2.2798, val_loss=2.2924]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1658/2000 [00:15<00:02, 121.53it/s, train_loss=2.2798, val_loss=2.2924]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1675/2000 [00:15<00:02, 132.08it/s, train_loss=2.2798, val_loss=2.2924]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1691/2000 [00:15<00:02, 138.33it/s, train_loss=2.2798, val_loss=2.2924]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1691/2000 [00:15<00:02, 138.33it/s, train_loss=2.2457, val_loss=2.2601]\u001B[A\n",
      "Training (constant):  85%|████████▌ | 1707/2000 [00:15<00:03, 84.18it/s, train_loss=2.2457, val_loss=2.2601] \u001B[A\n",
      "Training (constant):  86%|████████▌ | 1724/2000 [00:15<00:02, 98.87it/s, train_loss=2.2457, val_loss=2.2601]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1741/2000 [00:15<00:02, 112.03it/s, train_loss=2.2457, val_loss=2.2601]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1758/2000 [00:15<00:01, 124.11it/s, train_loss=2.2457, val_loss=2.2601]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1775/2000 [00:16<00:01, 133.36it/s, train_loss=2.2457, val_loss=2.2601]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1792/2000 [00:16<00:01, 141.32it/s, train_loss=2.2457, val_loss=2.2601]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1792/2000 [00:16<00:01, 141.32it/s, train_loss=2.1986, val_loss=2.2016]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1808/2000 [00:16<00:02, 81.42it/s, train_loss=2.1986, val_loss=2.2016] \u001B[A\n",
      "Training (constant):  91%|█████████ | 1823/2000 [00:16<00:01, 92.74it/s, train_loss=2.1986, val_loss=2.2016]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1837/2000 [00:16<00:01, 101.67it/s, train_loss=2.1986, val_loss=2.2016]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1853/2000 [00:16<00:01, 113.44it/s, train_loss=2.1986, val_loss=2.2016]\u001B[A\n",
      "Training (constant):  94%|█████████▎| 1870/2000 [00:17<00:01, 125.74it/s, train_loss=2.1986, val_loss=2.2016]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1887/2000 [00:17<00:00, 135.71it/s, train_loss=2.1986, val_loss=2.2016]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1887/2000 [00:17<00:00, 135.71it/s, train_loss=2.1750, val_loss=2.1655]\u001B[A\n",
      "Training (constant):  95%|█████████▌| 1903/2000 [00:17<00:01, 81.95it/s, train_loss=2.1750, val_loss=2.1655] \u001B[A\n",
      "Training (constant):  96%|█████████▌| 1920/2000 [00:17<00:00, 96.84it/s, train_loss=2.1750, val_loss=2.1655]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1937/2000 [00:17<00:00, 110.82it/s, train_loss=2.1750, val_loss=2.1655]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1954/2000 [00:17<00:00, 122.83it/s, train_loss=2.1750, val_loss=2.1655]\u001B[A\n",
      "Training (constant):  99%|█████████▊| 1971/2000 [00:17<00:00, 133.53it/s, train_loss=2.1750, val_loss=2.1655]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:18<00:00, 110.57it/s, train_loss=2.1750, val_loss=2.1655]\u001B[A\n",
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<08:50,  3.77it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   1%|          | 18/2000 [00:00<00:32, 61.33it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   2%|▏         | 35/2000 [00:00<00:20, 96.24it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   3%|▎         | 52/2000 [00:00<00:16, 118.21it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   3%|▎         | 69/2000 [00:00<00:14, 132.58it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   4%|▍         | 85/2000 [00:00<00:13, 140.34it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   4%|▍         | 85/2000 [00:01<00:13, 140.34it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   5%|▌         | 101/2000 [00:01<00:24, 79.09it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   6%|▌         | 117/2000 [00:01<00:20, 93.90it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   7%|▋         | 134/2000 [00:01<00:17, 109.34it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   8%|▊         | 151/2000 [00:01<00:15, 121.98it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   8%|▊         | 168/2000 [00:01<00:13, 132.30it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   9%|▉         | 184/2000 [00:01<00:13, 139.22it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):  10%|█         | 200/2000 [00:01<00:12, 144.21it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):  10%|█         | 200/2000 [00:02<00:12, 144.21it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  11%|█         | 216/2000 [00:02<00:20, 85.38it/s, train_loss=3.0934, val_loss=3.0878] \u001B[A\n",
      "Training (constant):  12%|█▏        | 232/2000 [00:02<00:17, 99.14it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  12%|█▏        | 249/2000 [00:02<00:15, 113.58it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  13%|█▎        | 266/2000 [00:02<00:13, 126.01it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  14%|█▍        | 283/2000 [00:02<00:12, 136.01it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  15%|█▌        | 300/2000 [00:02<00:11, 143.09it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  15%|█▌        | 300/2000 [00:02<00:11, 143.09it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  16%|█▌        | 316/2000 [00:03<00:20, 83.88it/s, train_loss=2.9572, val_loss=2.9528] \u001B[A\n",
      "Training (constant):  17%|█▋        | 334/2000 [00:03<00:16, 100.04it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  18%|█▊        | 351/2000 [00:03<00:14, 113.67it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  18%|█▊        | 368/2000 [00:03<00:12, 125.66it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  19%|█▉        | 385/2000 [00:03<00:11, 134.72it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  19%|█▉        | 385/2000 [00:03<00:11, 134.72it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  20%|██        | 401/2000 [00:03<00:19, 82.91it/s, train_loss=2.8577, val_loss=2.8565] \u001B[A\n",
      "Training (constant):  21%|██        | 418/2000 [00:03<00:16, 97.41it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  22%|██▏       | 435/2000 [00:04<00:14, 111.54it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  23%|██▎       | 452/2000 [00:04<00:12, 124.00it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  23%|██▎       | 468/2000 [00:04<00:11, 132.49it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  24%|██▍       | 485/2000 [00:04<00:10, 140.93it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  24%|██▍       | 485/2000 [00:04<00:10, 140.93it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  25%|██▌       | 501/2000 [00:04<00:17, 86.73it/s, train_loss=2.7661, val_loss=2.7745] \u001B[A\n",
      "Training (constant):  26%|██▌       | 517/2000 [00:04<00:14, 100.04it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  27%|██▋       | 534/2000 [00:04<00:12, 114.27it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  28%|██▊       | 552/2000 [00:05<00:11, 127.52it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  28%|██▊       | 569/2000 [00:05<00:10, 137.15it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  29%|██▉       | 586/2000 [00:05<00:09, 144.05it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  29%|██▉       | 586/2000 [00:05<00:09, 144.05it/s, train_loss=2.6926, val_loss=2.6948]\u001B[A\n",
      "Training (constant):  30%|███       | 602/2000 [00:05<00:16, 85.15it/s, train_loss=2.6926, val_loss=2.6948] \u001B[A\n",
      "Training (constant):  31%|███       | 619/2000 [00:05<00:13, 99.61it/s, train_loss=2.6926, val_loss=2.6948]\u001B[A\n",
      "Training (constant):  32%|███▏      | 635/2000 [00:05<00:12, 111.61it/s, train_loss=2.6926, val_loss=2.6948]\u001B[A\n",
      "Training (constant):  33%|███▎      | 652/2000 [00:05<00:10, 123.90it/s, train_loss=2.6926, val_loss=2.6948]\u001B[A\n",
      "Training (constant):  33%|███▎      | 669/2000 [00:06<00:09, 134.22it/s, train_loss=2.6926, val_loss=2.6948]\u001B[A\n",
      "Training (constant):  34%|███▍      | 686/2000 [00:06<00:09, 142.22it/s, train_loss=2.6926, val_loss=2.6948]\u001B[A\n",
      "Training (constant):  34%|███▍      | 686/2000 [00:06<00:09, 142.22it/s, train_loss=2.6325, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  35%|███▌      | 702/2000 [00:06<00:15, 84.51it/s, train_loss=2.6325, val_loss=2.6388] \u001B[A\n",
      "Training (constant):  36%|███▌      | 719/2000 [00:06<00:12, 98.91it/s, train_loss=2.6325, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  37%|███▋      | 736/2000 [00:06<00:11, 112.95it/s, train_loss=2.6325, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  38%|███▊      | 753/2000 [00:06<00:09, 124.73it/s, train_loss=2.6325, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  38%|███▊      | 769/2000 [00:06<00:09, 133.17it/s, train_loss=2.6325, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  39%|███▉      | 786/2000 [00:07<00:08, 140.92it/s, train_loss=2.6325, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  39%|███▉      | 786/2000 [00:07<00:08, 140.92it/s, train_loss=2.5565, val_loss=2.5608]\u001B[A\n",
      "Training (constant):  40%|████      | 802/2000 [00:07<00:14, 82.21it/s, train_loss=2.5565, val_loss=2.5608] \u001B[A\n",
      "Training (constant):  41%|████      | 819/2000 [00:07<00:12, 96.74it/s, train_loss=2.5565, val_loss=2.5608]\u001B[A\n",
      "Training (constant):  42%|████▏     | 836/2000 [00:07<00:10, 110.68it/s, train_loss=2.5565, val_loss=2.5608]\u001B[A\n",
      "Training (constant):  43%|████▎     | 853/2000 [00:07<00:09, 123.13it/s, train_loss=2.5565, val_loss=2.5608]\u001B[A\n",
      "Training (constant):  44%|████▎     | 870/2000 [00:07<00:08, 132.73it/s, train_loss=2.5565, val_loss=2.5608]\u001B[A\n",
      "Training (constant):  44%|████▍     | 887/2000 [00:07<00:07, 140.41it/s, train_loss=2.5565, val_loss=2.5608]\u001B[A\n",
      "Training (constant):  44%|████▍     | 887/2000 [00:08<00:07, 140.41it/s, train_loss=2.5182, val_loss=2.5275]\u001B[A\n",
      "Training (constant):  45%|████▌     | 903/2000 [00:08<00:13, 81.50it/s, train_loss=2.5182, val_loss=2.5275] \u001B[A\n",
      "Training (constant):  46%|████▌     | 918/2000 [00:08<00:11, 92.88it/s, train_loss=2.5182, val_loss=2.5275]\u001B[A\n",
      "Training (constant):  47%|████▋     | 935/2000 [00:08<00:09, 107.10it/s, train_loss=2.5182, val_loss=2.5275]\u001B[A\n",
      "Training (constant):  48%|████▊     | 952/2000 [00:08<00:08, 120.15it/s, train_loss=2.5182, val_loss=2.5275]\u001B[A\n",
      "Training (constant):  48%|████▊     | 969/2000 [00:08<00:07, 130.41it/s, train_loss=2.5182, val_loss=2.5275]\u001B[A\n",
      "Training (constant):  49%|████▉     | 985/2000 [00:08<00:07, 136.45it/s, train_loss=2.5182, val_loss=2.5275]\u001B[A\n",
      "Training (constant):  49%|████▉     | 985/2000 [00:09<00:07, 136.45it/s, train_loss=2.4611, val_loss=2.4623]\u001B[A\n",
      "Training (constant):  50%|█████     | 1001/2000 [00:09<00:11, 83.67it/s, train_loss=2.4611, val_loss=2.4623]\u001B[A\n",
      "Training (constant):  51%|█████     | 1016/2000 [00:09<00:10, 95.15it/s, train_loss=2.4611, val_loss=2.4623]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1032/2000 [00:09<00:09, 107.25it/s, train_loss=2.4611, val_loss=2.4623]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1049/2000 [00:09<00:07, 120.30it/s, train_loss=2.4611, val_loss=2.4623]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1066/2000 [00:09<00:07, 131.18it/s, train_loss=2.4611, val_loss=2.4623]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1083/2000 [00:09<00:06, 138.65it/s, train_loss=2.4611, val_loss=2.4623]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1099/2000 [00:09<00:06, 142.81it/s, train_loss=2.4611, val_loss=2.4623]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1099/2000 [00:10<00:06, 142.81it/s, train_loss=2.4116, val_loss=2.4138]\u001B[A\n",
      "Training (constant):  56%|█████▌    | 1115/2000 [00:10<00:10, 82.04it/s, train_loss=2.4116, val_loss=2.4138] \u001B[A\n",
      "Training (constant):  57%|█████▋    | 1131/2000 [00:10<00:09, 95.44it/s, train_loss=2.4116, val_loss=2.4138]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1148/2000 [00:10<00:07, 109.36it/s, train_loss=2.4116, val_loss=2.4138]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1164/2000 [00:10<00:07, 119.36it/s, train_loss=2.4116, val_loss=2.4138]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1181/2000 [00:10<00:06, 130.89it/s, train_loss=2.4116, val_loss=2.4138]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1198/2000 [00:10<00:05, 140.14it/s, train_loss=2.4116, val_loss=2.4138]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1198/2000 [00:11<00:05, 140.14it/s, train_loss=2.3801, val_loss=2.3843]\u001B[A\n",
      "Training (constant):  61%|██████    | 1214/2000 [00:11<00:09, 83.48it/s, train_loss=2.3801, val_loss=2.3843] \u001B[A\n",
      "Training (constant):  62%|██████▏   | 1231/2000 [00:11<00:07, 98.22it/s, train_loss=2.3801, val_loss=2.3843]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1248/2000 [00:11<00:06, 110.96it/s, train_loss=2.3801, val_loss=2.3843]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1265/2000 [00:11<00:05, 122.84it/s, train_loss=2.3801, val_loss=2.3843]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1282/2000 [00:11<00:05, 132.01it/s, train_loss=2.3801, val_loss=2.3843]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1299/2000 [00:11<00:05, 140.00it/s, train_loss=2.3801, val_loss=2.3843]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1299/2000 [00:11<00:05, 140.00it/s, train_loss=2.3368, val_loss=2.3503]\u001B[A\n",
      "Training (constant):  66%|██████▌   | 1315/2000 [00:12<00:08, 82.16it/s, train_loss=2.3368, val_loss=2.3503] \u001B[A\n",
      "Training (constant):  67%|██████▋   | 1332/2000 [00:12<00:06, 96.55it/s, train_loss=2.3368, val_loss=2.3503]\u001B[A\n",
      "Training (constant):  67%|██████▋   | 1349/2000 [00:12<00:05, 110.34it/s, train_loss=2.3368, val_loss=2.3503]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1366/2000 [00:12<00:05, 122.16it/s, train_loss=2.3368, val_loss=2.3503]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1383/2000 [00:12<00:04, 132.51it/s, train_loss=2.3368, val_loss=2.3503]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1399/2000 [00:12<00:04, 139.08it/s, train_loss=2.3368, val_loss=2.3503]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1399/2000 [00:12<00:04, 139.08it/s, train_loss=2.2982, val_loss=2.3103]\u001B[A\n",
      "Training (constant):  71%|███████   | 1415/2000 [00:12<00:07, 81.54it/s, train_loss=2.2982, val_loss=2.3103] \u001B[A\n",
      "Training (constant):  72%|███████▏  | 1432/2000 [00:13<00:05, 96.36it/s, train_loss=2.2982, val_loss=2.3103]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1448/2000 [00:13<00:05, 108.74it/s, train_loss=2.2982, val_loss=2.3103]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1464/2000 [00:13<00:04, 119.65it/s, train_loss=2.2982, val_loss=2.3103]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1479/2000 [00:13<00:04, 126.45it/s, train_loss=2.2982, val_loss=2.3103]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1494/2000 [00:13<00:03, 132.02it/s, train_loss=2.2982, val_loss=2.3103]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1494/2000 [00:13<00:03, 132.02it/s, train_loss=2.2800, val_loss=2.2807]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1509/2000 [00:13<00:06, 76.70it/s, train_loss=2.2800, val_loss=2.2807] \u001B[A\n",
      "Training (constant):  76%|███████▌  | 1523/2000 [00:13<00:05, 87.60it/s, train_loss=2.2800, val_loss=2.2807]\u001B[A\n",
      "Training (constant):  77%|███████▋  | 1536/2000 [00:14<00:05, 88.88it/s, train_loss=2.2800, val_loss=2.2807]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1553/2000 [00:14<00:04, 105.03it/s, train_loss=2.2800, val_loss=2.2807]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1569/2000 [00:14<00:03, 117.42it/s, train_loss=2.2800, val_loss=2.2807]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1585/2000 [00:14<00:03, 127.26it/s, train_loss=2.2800, val_loss=2.2807]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1585/2000 [00:14<00:03, 127.26it/s, train_loss=2.2329, val_loss=2.2496]\u001B[A\n",
      "Training (constant):  80%|████████  | 1601/2000 [00:14<00:05, 77.03it/s, train_loss=2.2329, val_loss=2.2496] \u001B[A\n",
      "Training (constant):  81%|████████  | 1616/2000 [00:14<00:04, 89.11it/s, train_loss=2.2329, val_loss=2.2496]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1633/2000 [00:15<00:03, 104.09it/s, train_loss=2.2329, val_loss=2.2496]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1649/2000 [00:15<00:03, 115.88it/s, train_loss=2.2329, val_loss=2.2496]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1665/2000 [00:15<00:02, 124.82it/s, train_loss=2.2329, val_loss=2.2496]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1681/2000 [00:15<00:02, 132.64it/s, train_loss=2.2329, val_loss=2.2496]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1697/2000 [00:15<00:02, 137.73it/s, train_loss=2.2329, val_loss=2.2496]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1697/2000 [00:15<00:02, 137.73it/s, train_loss=2.2354, val_loss=2.2398]\u001B[A\n",
      "Training (constant):  86%|████████▌ | 1712/2000 [00:15<00:03, 80.87it/s, train_loss=2.2354, val_loss=2.2398] \u001B[A\n",
      "Training (constant):  86%|████████▋ | 1728/2000 [00:15<00:02, 95.10it/s, train_loss=2.2354, val_loss=2.2398]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1745/2000 [00:16<00:02, 109.39it/s, train_loss=2.2354, val_loss=2.2398]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1760/2000 [00:16<00:02, 118.11it/s, train_loss=2.2354, val_loss=2.2398]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1777/2000 [00:16<00:01, 128.72it/s, train_loss=2.2354, val_loss=2.2398]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1794/2000 [00:16<00:01, 138.15it/s, train_loss=2.2354, val_loss=2.2398]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1794/2000 [00:16<00:01, 138.15it/s, train_loss=2.1777, val_loss=2.1762]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1810/2000 [00:16<00:02, 80.17it/s, train_loss=2.1777, val_loss=2.1762] \u001B[A\n",
      "Training (constant):  91%|█████████▏| 1827/2000 [00:16<00:01, 94.98it/s, train_loss=2.1777, val_loss=2.1762]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1844/2000 [00:16<00:01, 108.91it/s, train_loss=2.1777, val_loss=2.1762]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1861/2000 [00:17<00:01, 122.12it/s, train_loss=2.1777, val_loss=2.1762]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1878/2000 [00:17<00:00, 133.06it/s, train_loss=2.1777, val_loss=2.1762]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1895/2000 [00:17<00:00, 141.02it/s, train_loss=2.1777, val_loss=2.1762]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1895/2000 [00:17<00:00, 141.02it/s, train_loss=2.1852, val_loss=2.1897]\u001B[A\n",
      "Training (constant):  96%|█████████▌| 1911/2000 [00:17<00:01, 81.48it/s, train_loss=2.1852, val_loss=2.1897] \u001B[A\n",
      "Training (constant):  96%|█████████▋| 1928/2000 [00:17<00:00, 96.20it/s, train_loss=2.1852, val_loss=2.1897]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1945/2000 [00:17<00:00, 110.19it/s, train_loss=2.1852, val_loss=2.1897]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1961/2000 [00:17<00:00, 120.41it/s, train_loss=2.1852, val_loss=2.1897]\u001B[A\n",
      "Training (constant):  99%|█████████▉| 1978/2000 [00:18<00:00, 131.44it/s, train_loss=2.1852, val_loss=2.1897]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:18<00:00, 110.05it/s, train_loss=2.1852, val_loss=2.1897]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Constant: lr=0.005, avg_val_loss=2.1704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam LR:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<13:20,  2.50it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:18, 25.37it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 20/2000 [00:00<00:44, 44.34it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 29/2000 [00:00<00:34, 56.45it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 38/2000 [00:00<00:30, 65.22it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 47/2000 [00:00<00:27, 71.82it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:01<00:25, 76.96it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:01<00:23, 81.83it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 75/2000 [00:01<00:22, 83.80it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 85/2000 [00:01<00:22, 86.45it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:21, 88.17it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:21, 88.17it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 105/2000 [00:01<00:40, 47.29it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 115/2000 [00:01<00:33, 55.56it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 125/2000 [00:02<00:29, 63.84it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 135/2000 [00:02<00:26, 71.43it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:02<00:24, 77.02it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:24, 74.84it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 163/2000 [00:02<00:23, 77.65it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 173/2000 [00:02<00:22, 81.46it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:21, 85.82it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:02<00:20, 89.42it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:03<00:20, 89.42it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 203/2000 [00:03<00:35, 51.13it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 212/2000 [00:03<00:30, 57.82it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 222/2000 [00:03<00:26, 65.89it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 232/2000 [00:03<00:24, 72.58it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 242/2000 [00:03<00:22, 78.30it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 252/2000 [00:03<00:21, 82.02it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 262/2000 [00:03<00:20, 84.40it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 272/2000 [00:03<00:19, 86.48it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 282/2000 [00:04<00:19, 88.31it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 292/2000 [00:04<00:19, 89.10it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 292/2000 [00:04<00:19, 89.10it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 302/2000 [00:04<00:34, 48.94it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 312/2000 [00:04<00:29, 56.86it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 322/2000 [00:04<00:26, 64.29it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 332/2000 [00:04<00:23, 70.60it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 342/2000 [00:05<00:21, 75.94it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:05<00:20, 79.84it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 362/2000 [00:05<00:19, 83.15it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:05<00:19, 83.42it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 380/2000 [00:05<00:19, 85.13it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 390/2000 [00:05<00:18, 87.46it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 400/2000 [00:05<00:17, 89.50it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 400/2000 [00:05<00:17, 89.50it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 410/2000 [00:06<00:31, 50.25it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 420/2000 [00:06<00:27, 58.30it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 430/2000 [00:06<00:23, 65.74it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 440/2000 [00:06<00:21, 71.80it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▎       | 450/2000 [00:06<00:20, 76.55it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 460/2000 [00:06<00:19, 80.27it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 470/2000 [00:06<00:18, 83.69it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 480/2000 [00:06<00:17, 86.08it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:06<00:17, 88.73it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:07<00:16, 90.75it/s, train_loss=2.7887, val_loss=2.7960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:07<00:16, 90.75it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:07<00:28, 51.62it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 520/2000 [00:07<00:24, 60.11it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 530/2000 [00:07<00:21, 68.08it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 539/2000 [00:07<00:20, 72.24it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 549/2000 [00:07<00:18, 77.72it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 559/2000 [00:07<00:17, 83.14it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 569/2000 [00:08<00:16, 87.28it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 579/2000 [00:08<00:15, 90.18it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 589/2000 [00:08<00:15, 91.07it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 599/2000 [00:08<00:15, 92.21it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 599/2000 [00:08<00:15, 92.21it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 609/2000 [00:08<00:26, 53.00it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 619/2000 [00:08<00:22, 61.40it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 629/2000 [00:08<00:19, 69.26it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 639/2000 [00:09<00:17, 76.05it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 649/2000 [00:09<00:16, 81.40it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 659/2000 [00:09<00:15, 85.54it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 669/2000 [00:09<00:15, 88.61it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 679/2000 [00:09<00:14, 91.22it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 689/2000 [00:09<00:14, 93.18it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:09<00:13, 95.03it/s, train_loss=2.5454, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:09<00:13, 95.03it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 709/2000 [00:10<00:25, 50.32it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 719/2000 [00:10<00:21, 58.42it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 729/2000 [00:10<00:19, 66.16it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 739/2000 [00:10<00:17, 72.01it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 749/2000 [00:10<00:16, 76.49it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 758/2000 [00:10<00:15, 79.72it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 768/2000 [00:10<00:14, 82.88it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:10<00:14, 84.58it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 787/2000 [00:10<00:13, 86.75it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:11<00:13, 88.60it/s, train_loss=2.4268, val_loss=2.4429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:11<00:13, 88.60it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 807/2000 [00:11<00:23, 49.94it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 817/2000 [00:11<00:20, 57.83it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 827/2000 [00:11<00:18, 65.05it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 837/2000 [00:11<00:16, 71.86it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 846/2000 [00:11<00:15, 75.85it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 855/2000 [00:11<00:14, 79.10it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 865/2000 [00:12<00:13, 83.09it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:12<00:13, 85.78it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 885/2000 [00:12<00:12, 87.82it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:12<00:12, 89.13it/s, train_loss=2.3298, val_loss=2.3379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:12<00:12, 89.13it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:12<00:21, 49.77it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 914/2000 [00:12<00:19, 55.99it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 923/2000 [00:13<00:17, 62.17it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 933/2000 [00:13<00:15, 69.89it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 942/2000 [00:13<00:14, 73.23it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 951/2000 [00:13<00:14, 74.63it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 960/2000 [00:13<00:13, 76.20it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 969/2000 [00:13<00:13, 78.35it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 978/2000 [00:13<00:12, 79.97it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 987/2000 [00:13<00:12, 81.00it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 996/2000 [00:13<00:12, 81.88it/s, train_loss=2.2398, val_loss=2.2456]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 996/2000 [00:14<00:12, 81.88it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:14<00:21, 46.33it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1014/2000 [00:14<00:18, 53.73it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1023/2000 [00:14<00:16, 60.92it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1032/2000 [00:14<00:14, 67.22it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:14<00:13, 72.40it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▎    | 1050/2000 [00:14<00:12, 76.49it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1059/2000 [00:14<00:11, 79.55it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1068/2000 [00:15<00:11, 82.12it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:15<00:10, 84.53it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1088/2000 [00:15<00:10, 86.42it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:15<00:10, 87.20it/s, train_loss=2.1475, val_loss=2.1491]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:15<00:10, 87.20it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1106/2000 [00:15<00:18, 48.23it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1115/2000 [00:15<00:15, 55.72it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1124/2000 [00:15<00:14, 62.26it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1133/2000 [00:16<00:12, 68.37it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1142/2000 [00:16<00:11, 73.45it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1151/2000 [00:16<00:10, 77.62it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:16<00:10, 81.38it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1170/2000 [00:16<00:09, 83.16it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1179/2000 [00:16<00:09, 83.88it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1188/2000 [00:16<00:09, 85.11it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:16<00:09, 85.36it/s, train_loss=2.0552, val_loss=2.0612]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:17<00:09, 85.36it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1206/2000 [00:17<00:17, 46.64it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1216/2000 [00:17<00:14, 55.08it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1225/2000 [00:17<00:12, 61.78it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1234/2000 [00:17<00:11, 66.69it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1243/2000 [00:17<00:10, 71.36it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1252/2000 [00:17<00:09, 75.24it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1261/2000 [00:17<00:09, 77.10it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1270/2000 [00:17<00:09, 79.13it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:18<00:08, 80.74it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1288/2000 [00:18<00:08, 82.88it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:18<00:08, 83.58it/s, train_loss=1.9662, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:18<00:08, 83.58it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1306/2000 [00:18<00:15, 45.96it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1315/2000 [00:18<00:12, 53.62it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1324/2000 [00:18<00:11, 60.69it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1333/2000 [00:18<00:09, 66.98it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1342/2000 [00:19<00:09, 72.33it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:19<00:08, 77.51it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1361/2000 [00:19<00:07, 80.68it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1370/2000 [00:19<00:07, 83.09it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:19<00:07, 84.95it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1388/2000 [00:19<00:07, 86.36it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:19<00:06, 87.70it/s, train_loss=1.9072, val_loss=1.8994]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:19<00:06, 87.70it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:20<00:12, 49.03it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1416/2000 [00:20<00:10, 56.52it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1426/2000 [00:20<00:08, 64.41it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1436/2000 [00:20<00:07, 70.82it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1445/2000 [00:20<00:07, 74.90it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1454/2000 [00:20<00:06, 78.66it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1464/2000 [00:20<00:06, 82.37it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1474/2000 [00:20<00:06, 85.01it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1483/2000 [00:20<00:06, 86.08it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:20<00:05, 85.84it/s, train_loss=1.8184, val_loss=1.8263]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:21<00:05, 85.84it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1501/2000 [00:21<00:10, 47.28it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1510/2000 [00:21<00:08, 54.85it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1520/2000 [00:21<00:07, 62.77it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1530/2000 [00:21<00:06, 69.72it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1540/2000 [00:21<00:06, 75.19it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1550/2000 [00:21<00:05, 79.69it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1559/2000 [00:22<00:05, 82.06it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1569/2000 [00:22<00:05, 84.75it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1579/2000 [00:22<00:04, 87.20it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:22<00:04, 88.45it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:22<00:04, 88.95it/s, train_loss=1.7629, val_loss=1.7506]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:22<00:04, 88.95it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:22<00:07, 51.12it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:22<00:06, 59.00it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:23<00:05, 66.05it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1639/2000 [00:23<00:04, 72.27it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:23<00:04, 77.09it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1659/2000 [00:23<00:04, 80.93it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:23<00:03, 83.88it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1679/2000 [00:23<00:03, 86.10it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1689/2000 [00:23<00:03, 87.52it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:23<00:03, 88.31it/s, train_loss=1.6834, val_loss=1.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:24<00:03, 88.31it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1709/2000 [00:24<00:05, 50.67it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:24<00:04, 57.38it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1727/2000 [00:24<00:04, 63.58it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:24<00:03, 69.09it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:24<00:03, 74.56it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1756/2000 [00:24<00:03, 79.27it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1765/2000 [00:24<00:02, 81.91it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1774/2000 [00:24<00:02, 83.77it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1784/2000 [00:25<00:02, 85.93it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:25<00:02, 87.14it/s, train_loss=1.6169, val_loss=1.6102]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:25<00:02, 87.14it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1803/2000 [00:25<00:03, 49.32it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1813/2000 [00:25<00:03, 57.77it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1823/2000 [00:25<00:02, 65.33it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1833/2000 [00:25<00:02, 71.63it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1842/2000 [00:25<00:02, 75.72it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1852/2000 [00:26<00:01, 79.97it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1862/2000 [00:26<00:01, 83.60it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1872/2000 [00:26<00:01, 85.82it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1882/2000 [00:26<00:01, 87.53it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:26<00:01, 87.93it/s, train_loss=1.5446, val_loss=1.5463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:26<00:01, 87.93it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:26<00:01, 49.97it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1910/2000 [00:26<00:01, 57.11it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1920/2000 [00:27<00:01, 64.57it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1930/2000 [00:27<00:00, 71.15it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1940/2000 [00:27<00:00, 76.38it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1950/2000 [00:27<00:00, 80.34it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1959/2000 [00:27<00:00, 82.70it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1969/2000 [00:27<00:00, 85.10it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1979/2000 [00:27<00:00, 87.14it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1989/2000 [00:27<00:00, 88.23it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:27<00:00, 71.45it/s, train_loss=1.4861, val_loss=1.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:16,  3.59it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:56, 35.17it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:36, 54.10it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:29, 66.44it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:26, 74.39it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▎         | 50/2000 [00:00<00:24, 78.55it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 59/2000 [00:00<00:24, 79.55it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 68/2000 [00:01<00:23, 81.06it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 77/2000 [00:01<00:23, 82.82it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 86/2000 [00:01<00:22, 84.90it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:22, 85.83it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:22, 85.83it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 104/2000 [00:01<00:40, 47.32it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 113/2000 [00:01<00:34, 55.18it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 122/2000 [00:01<00:30, 62.37it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:02<00:27, 68.51it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:24, 74.61it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 150/2000 [00:02<00:23, 78.34it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 160/2000 [00:02<00:22, 81.81it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 169/2000 [00:02<00:21, 83.30it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 178/2000 [00:02<00:21, 83.81it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 188/2000 [00:02<00:21, 86.23it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:20, 87.62it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:20, 87.62it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:36, 49.19it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:31, 56.64it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:27, 63.49it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 235/2000 [00:03<00:25, 70.33it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 244/2000 [00:03<00:23, 74.72it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 254/2000 [00:03<00:21, 79.54it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 264/2000 [00:03<00:20, 82.75it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 273/2000 [00:03<00:20, 84.54it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 282/2000 [00:03<00:20, 85.11it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:19, 86.27it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:19, 87.33it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:19, 87.33it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 309/2000 [00:04<00:34, 48.35it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:04<00:30, 55.93it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 327/2000 [00:04<00:26, 62.68it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 336/2000 [00:04<00:24, 68.66it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 346/2000 [00:04<00:22, 74.30it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 355/2000 [00:05<00:21, 77.53it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 365/2000 [00:05<00:20, 81.16it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:05<00:19, 83.49it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 383/2000 [00:05<00:19, 84.48it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:18, 85.05it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:18, 85.05it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:33, 47.38it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:06<00:28, 55.83it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 420/2000 [00:06<00:25, 62.70it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 429/2000 [00:06<00:22, 68.70it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 439/2000 [00:06<00:20, 74.47it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 449/2000 [00:06<00:19, 79.28it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 458/2000 [00:06<00:18, 81.75it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 467/2000 [00:06<00:18, 83.76it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 477/2000 [00:06<00:17, 85.93it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 487/2000 [00:06<00:17, 87.53it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:06<00:16, 88.78it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:07<00:16, 88.78it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 507/2000 [00:07<00:29, 50.69it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 516/2000 [00:07<00:25, 57.76it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 525/2000 [00:07<00:23, 63.96it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 534/2000 [00:07<00:21, 69.53it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 544/2000 [00:07<00:19, 75.15it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 554/2000 [00:07<00:18, 79.61it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 563/2000 [00:07<00:17, 82.32it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 572/2000 [00:08<00:16, 84.03it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 581/2000 [00:08<00:16, 85.20it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:16, 87.03it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:16, 87.03it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:08<00:28, 49.56it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:08<00:24, 56.75it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 619/2000 [00:08<00:21, 63.47it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 629/2000 [00:09<00:19, 70.08it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 638/2000 [00:09<00:18, 74.75it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 647/2000 [00:09<00:17, 78.62it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:09<00:16, 82.08it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:09<00:15, 84.21it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 677/2000 [00:09<00:15, 85.96it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 686/2000 [00:09<00:15, 85.90it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:09<00:14, 87.44it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:10<00:14, 87.44it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 705/2000 [00:10<00:26, 49.34it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 715/2000 [00:10<00:22, 57.55it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 725/2000 [00:10<00:19, 64.88it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:10<00:17, 70.43it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 743/2000 [00:10<00:16, 75.02it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:10<00:15, 78.60it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 761/2000 [00:10<00:15, 78.88it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 770/2000 [00:10<00:15, 79.81it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 779/2000 [00:10<00:15, 80.52it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 788/2000 [00:11<00:14, 82.44it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:11<00:14, 84.10it/s, train_loss=2.4505, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:11<00:14, 84.10it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 806/2000 [00:11<00:25, 47.38it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 815/2000 [00:11<00:21, 54.99it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 825/2000 [00:11<00:18, 62.90it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 834/2000 [00:11<00:16, 68.82it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 843/2000 [00:11<00:15, 73.89it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 852/2000 [00:12<00:14, 77.65it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 861/2000 [00:12<00:14, 80.85it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 870/2000 [00:12<00:13, 83.35it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 879/2000 [00:12<00:13, 84.75it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 888/2000 [00:12<00:13, 85.41it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:12<00:12, 86.27it/s, train_loss=2.3453, val_loss=2.3430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:12<00:12, 86.27it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 906/2000 [00:12<00:22, 47.98it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 915/2000 [00:13<00:19, 55.64it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 924/2000 [00:13<00:17, 61.83it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 933/2000 [00:13<00:15, 67.95it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 942/2000 [00:13<00:14, 72.65it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 951/2000 [00:13<00:13, 76.79it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 960/2000 [00:13<00:12, 80.23it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 969/2000 [00:13<00:12, 82.48it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 978/2000 [00:13<00:12, 83.78it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 987/2000 [00:13<00:11, 85.17it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 996/2000 [00:14<00:11, 86.36it/s, train_loss=2.2536, val_loss=2.2527]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 996/2000 [00:14<00:11, 86.36it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:14<00:20, 47.50it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1014/2000 [00:14<00:17, 54.82it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1023/2000 [00:14<00:15, 61.44it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1032/2000 [00:14<00:14, 67.87it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:14<00:13, 73.06it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▎    | 1050/2000 [00:14<00:12, 77.15it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1060/2000 [00:15<00:11, 80.94it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1069/2000 [00:15<00:11, 83.07it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:15<00:10, 84.66it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1087/2000 [00:15<00:10, 85.94it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:15<00:10, 87.34it/s, train_loss=2.1773, val_loss=2.1745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:15<00:10, 87.34it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1106/2000 [00:15<00:18, 48.57it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1115/2000 [00:15<00:15, 56.05it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1124/2000 [00:16<00:14, 62.48it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1133/2000 [00:16<00:12, 68.06it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1142/2000 [00:16<00:11, 73.39it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1152/2000 [00:16<00:10, 78.19it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:16<00:10, 80.93it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1170/2000 [00:16<00:10, 82.98it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1179/2000 [00:16<00:09, 83.56it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1188/2000 [00:16<00:09, 85.35it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:16<00:09, 85.97it/s, train_loss=2.0761, val_loss=2.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:17<00:09, 85.97it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1206/2000 [00:17<00:17, 45.43it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1215/2000 [00:17<00:14, 53.09it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1225/2000 [00:17<00:12, 61.07it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1234/2000 [00:17<00:11, 67.10it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1243/2000 [00:17<00:10, 72.27it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1252/2000 [00:17<00:09, 76.40it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1262/2000 [00:17<00:09, 80.27it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1271/2000 [00:18<00:08, 82.85it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1281/2000 [00:18<00:08, 85.20it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1290/2000 [00:18<00:08, 86.18it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:18<00:07, 88.03it/s, train_loss=1.9982, val_loss=2.0094]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:18<00:07, 88.03it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:18<00:14, 48.70it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:18<00:12, 56.18it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:18<00:10, 63.84it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1337/2000 [00:19<00:09, 69.49it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1347/2000 [00:19<00:08, 74.72it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1356/2000 [00:19<00:08, 78.38it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1366/2000 [00:19<00:07, 81.94it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1375/2000 [00:19<00:07, 83.71it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1384/2000 [00:19<00:07, 85.16it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:19<00:07, 86.06it/s, train_loss=1.9105, val_loss=1.9200]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:20<00:07, 86.06it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:20<00:12, 47.70it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:20<00:10, 55.41it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1421/2000 [00:20<00:09, 63.32it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1431/2000 [00:20<00:08, 70.06it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1441/2000 [00:20<00:07, 75.86it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1451/2000 [00:20<00:06, 80.08it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1460/2000 [00:20<00:06, 82.02it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1469/2000 [00:20<00:06, 83.90it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1479/2000 [00:20<00:06, 85.81it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1489/2000 [00:21<00:05, 87.64it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:21<00:05, 88.61it/s, train_loss=1.8506, val_loss=1.8583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:21<00:05, 88.61it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1509/2000 [00:21<00:09, 49.23it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:21<00:08, 57.12it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1529/2000 [00:21<00:07, 64.26it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1539/2000 [00:21<00:06, 70.58it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1549/2000 [00:21<00:05, 75.65it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1558/2000 [00:22<00:05, 78.95it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1568/2000 [00:22<00:05, 82.22it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1578/2000 [00:22<00:04, 84.68it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1588/2000 [00:22<00:04, 86.87it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:22<00:04, 87.34it/s, train_loss=1.7622, val_loss=1.7666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:22<00:04, 87.34it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1606/2000 [00:22<00:08, 49.07it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1616/2000 [00:22<00:06, 57.55it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1626/2000 [00:23<00:05, 64.83it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:23<00:05, 70.07it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:23<00:04, 75.64it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:23<00:04, 79.09it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:23<00:04, 82.44it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:23<00:03, 85.43it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1683/2000 [00:23<00:03, 86.27it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:23<00:03, 88.34it/s, train_loss=1.6771, val_loss=1.6928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:24<00:03, 88.34it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1703/2000 [00:24<00:05, 50.06it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1713/2000 [00:24<00:04, 58.21it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1723/2000 [00:24<00:04, 65.62it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1733/2000 [00:24<00:03, 71.96it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1743/2000 [00:24<00:03, 77.12it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1752/2000 [00:24<00:03, 80.03it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1761/2000 [00:24<00:02, 82.45it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1770/2000 [00:24<00:02, 83.73it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1779/2000 [00:25<00:02, 84.38it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1788/2000 [00:25<00:02, 84.67it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:25<00:02, 85.38it/s, train_loss=1.6231, val_loss=1.6346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:25<00:02, 85.38it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1806/2000 [00:25<00:04, 45.94it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1816/2000 [00:25<00:03, 54.88it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1825/2000 [00:25<00:02, 61.35it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:26<00:02, 67.00it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1843/2000 [00:26<00:02, 72.00it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1853/2000 [00:26<00:01, 77.05it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1862/2000 [00:26<00:01, 78.31it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1871/2000 [00:26<00:01, 78.50it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:26<00:01, 80.37it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:26<00:01, 81.96it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:26<00:01, 81.97it/s, train_loss=1.5363, val_loss=1.5308]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:27<00:01, 81.97it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:27<00:01, 47.36it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1916/2000 [00:27<00:01, 54.78it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1925/2000 [00:27<00:01, 61.91it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1934/2000 [00:27<00:00, 67.90it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1943/2000 [00:27<00:00, 71.08it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1952/2000 [00:27<00:00, 74.95it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1961/2000 [00:27<00:00, 77.30it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1970/2000 [00:27<00:00, 80.22it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1979/2000 [00:27<00:00, 82.15it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1988/2000 [00:28<00:00, 83.30it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 70.85it/s, train_loss=1.4493, val_loss=1.4439]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:38,  3.46it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:02, 31.78it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:39, 50.01it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 28/2000 [00:00<00:32, 61.51it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 38/2000 [00:00<00:27, 70.95it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 48/2000 [00:00<00:25, 77.13it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 57/2000 [00:00<00:24, 80.49it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:01<00:23, 82.41it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 75/2000 [00:01<00:22, 84.42it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 84/2000 [00:01<00:22, 85.23it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:22, 85.82it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:22, 85.82it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 102/2000 [00:01<00:40, 46.57it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:34, 54.23it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 120/2000 [00:01<00:30, 61.61it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 129/2000 [00:02<00:27, 68.01it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 138/2000 [00:02<00:25, 72.41it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 147/2000 [00:02<00:24, 76.42it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 156/2000 [00:02<00:23, 79.18it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:22, 81.78it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 174/2000 [00:02<00:21, 83.21it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:21, 84.79it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:20, 86.22it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:03<00:20, 86.22it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:03<00:37, 47.73it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 210/2000 [00:03<00:32, 55.33it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 219/2000 [00:03<00:28, 62.40it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 228/2000 [00:03<00:25, 68.48it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 237/2000 [00:03<00:23, 73.60it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 246/2000 [00:03<00:22, 77.52it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:21, 80.36it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 264/2000 [00:03<00:20, 82.94it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 273/2000 [00:03<00:20, 84.57it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 282/2000 [00:04<00:19, 85.98it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:19, 85.98it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:19, 85.95it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:19, 85.95it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 309/2000 [00:04<00:36, 46.39it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:04<00:31, 54.05it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 327/2000 [00:04<00:27, 61.05it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 336/2000 [00:04<00:24, 66.88it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 345/2000 [00:05<00:22, 72.10it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 354/2000 [00:05<00:21, 75.76it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:05<00:20, 79.20it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:05<00:19, 81.55it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:19, 82.94it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 390/2000 [00:05<00:19, 81.31it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 399/2000 [00:05<00:20, 79.55it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 399/2000 [00:06<00:20, 79.55it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 408/2000 [00:06<00:36, 44.07it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 418/2000 [00:06<00:29, 52.81it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 427/2000 [00:06<00:26, 59.91it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 437/2000 [00:06<00:23, 67.10it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 446/2000 [00:06<00:21, 72.42it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 455/2000 [00:06<00:20, 76.49it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 464/2000 [00:06<00:19, 79.91it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:18, 82.14it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 482/2000 [00:06<00:18, 84.23it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:07<00:17, 85.39it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:07<00:17, 85.95it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:07<00:17, 85.95it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 509/2000 [00:07<00:30, 48.32it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 518/2000 [00:07<00:26, 56.10it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 527/2000 [00:07<00:23, 63.18it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:07<00:21, 69.35it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 545/2000 [00:07<00:19, 74.31it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 554/2000 [00:08<00:18, 78.25it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 563/2000 [00:08<00:17, 80.91it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 573/2000 [00:08<00:17, 83.92it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 582/2000 [00:08<00:16, 85.40it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:08<00:16, 87.13it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:08<00:16, 87.13it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:08<00:28, 48.65it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:08<00:24, 56.17it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 620/2000 [00:09<00:21, 63.88it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 629/2000 [00:09<00:19, 69.30it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 639/2000 [00:09<00:18, 74.96it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 648/2000 [00:09<00:17, 78.74it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:09<00:16, 81.69it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:09<00:15, 84.40it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:09<00:15, 85.54it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 685/2000 [00:09<00:15, 86.41it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:09<00:14, 87.38it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:10<00:14, 87.38it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 703/2000 [00:10<00:26, 48.10it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 713/2000 [00:10<00:22, 56.71it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 723/2000 [00:10<00:19, 64.33it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 733/2000 [00:10<00:17, 70.85it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:10<00:16, 75.23it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:10<00:15, 79.23it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 762/2000 [00:10<00:14, 82.66it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 771/2000 [00:11<00:14, 84.61it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:11<00:14, 86.40it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:11<00:13, 86.78it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:11<00:13, 87.56it/s, train_loss=2.4380, val_loss=2.4423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:11<00:13, 87.56it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 808/2000 [00:11<00:24, 48.76it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 818/2000 [00:11<00:20, 57.11it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 828/2000 [00:11<00:18, 64.58it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 837/2000 [00:12<00:16, 70.18it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 846/2000 [00:12<00:15, 74.96it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 855/2000 [00:12<00:14, 78.38it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 865/2000 [00:12<00:13, 81.92it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 874/2000 [00:12<00:13, 83.07it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 884/2000 [00:12<00:12, 85.85it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 893/2000 [00:12<00:12, 86.59it/s, train_loss=2.3392, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 893/2000 [00:13<00:12, 86.59it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 902/2000 [00:13<00:22, 48.35it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:13<00:19, 55.45it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 920/2000 [00:13<00:17, 61.89it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 929/2000 [00:13<00:15, 68.11it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 938/2000 [00:13<00:14, 72.94it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 947/2000 [00:13<00:13, 76.95it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 956/2000 [00:13<00:13, 80.22it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 965/2000 [00:13<00:12, 81.70it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:13<00:12, 84.38it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 985/2000 [00:14<00:11, 86.18it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:14<00:11, 87.15it/s, train_loss=2.2386, val_loss=2.2412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:14<00:11, 87.15it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1003/2000 [00:14<00:20, 48.59it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1013/2000 [00:14<00:17, 57.00it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1023/2000 [00:14<00:15, 64.73it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1032/2000 [00:14<00:13, 70.28it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1042/2000 [00:14<00:12, 76.02it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:15<00:11, 80.28it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1062/2000 [00:15<00:11, 83.36it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1072/2000 [00:15<00:10, 85.19it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1082/2000 [00:15<00:10, 87.11it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:15<00:10, 88.41it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:15<00:10, 88.41it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1102/2000 [00:15<00:17, 51.13it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1112/2000 [00:15<00:15, 58.93it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1122/2000 [00:16<00:13, 66.02it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1132/2000 [00:16<00:12, 72.18it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1141/2000 [00:16<00:11, 76.22it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1151/2000 [00:16<00:10, 80.12it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:16<00:10, 83.37it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1171/2000 [00:16<00:09, 85.78it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1181/2000 [00:16<00:09, 87.65it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:16<00:09, 88.21it/s, train_loss=2.0465, val_loss=2.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:17<00:09, 88.21it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:17<00:15, 51.10it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:17<00:13, 57.97it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1220/2000 [00:17<00:11, 65.33it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:17<00:10, 71.51it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:17<00:09, 76.58it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▎   | 1250/2000 [00:17<00:09, 80.57it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1260/2000 [00:17<00:08, 83.33it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1270/2000 [00:17<00:08, 85.65it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1280/2000 [00:18<00:08, 87.40it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1290/2000 [00:18<00:08, 88.63it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:18<00:07, 89.07it/s, train_loss=1.9613, val_loss=1.9719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:18<00:07, 89.07it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:18<00:13, 51.43it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:18<00:11, 58.38it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:18<00:10, 64.26it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1338/2000 [00:19<00:09, 70.74it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1348/2000 [00:19<00:08, 76.23it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1358/2000 [00:19<00:07, 80.51it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1368/2000 [00:19<00:07, 83.30it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1378/2000 [00:19<00:07, 85.72it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1388/2000 [00:19<00:06, 87.57it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:19<00:06, 89.05it/s, train_loss=1.8703, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:19<00:06, 89.05it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1408/2000 [00:20<00:11, 51.00it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1418/2000 [00:20<00:09, 58.85it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1428/2000 [00:20<00:08, 65.96it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:20<00:07, 71.01it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:20<00:07, 76.17it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:20<00:06, 80.34it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:20<00:06, 83.53it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:20<00:06, 85.75it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1486/2000 [00:20<00:05, 86.84it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:21<00:05, 88.29it/s, train_loss=1.7865, val_loss=1.8006]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:21<00:05, 88.29it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1506/2000 [00:21<00:09, 50.79it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1516/2000 [00:21<00:08, 58.84it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1526/2000 [00:21<00:07, 66.06it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1536/2000 [00:21<00:06, 72.19it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1546/2000 [00:21<00:05, 77.10it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1556/2000 [00:21<00:05, 80.88it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1566/2000 [00:22<00:05, 84.02it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1576/2000 [00:22<00:04, 86.32it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1586/2000 [00:22<00:04, 87.78it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:22<00:04, 88.33it/s, train_loss=1.6949, val_loss=1.7065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:22<00:04, 88.33it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1606/2000 [00:22<00:07, 51.26it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1615/2000 [00:22<00:06, 58.21it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1625/2000 [00:22<00:05, 65.52it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:23<00:05, 71.71it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:23<00:04, 76.86it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1655/2000 [00:23<00:04, 80.97it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1665/2000 [00:23<00:04, 83.66it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1675/2000 [00:23<00:03, 85.76it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1685/2000 [00:23<00:03, 87.50it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:23<00:03, 88.53it/s, train_loss=1.6137, val_loss=1.6198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:24<00:03, 88.53it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1705/2000 [00:24<00:05, 52.18it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1715/2000 [00:24<00:04, 60.03it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:24<00:04, 67.04it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1734/2000 [00:24<00:03, 71.92it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:24<00:03, 77.10it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:24<00:03, 81.04it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:24<00:02, 83.85it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1774/2000 [00:24<00:02, 85.88it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1783/2000 [00:24<00:02, 86.99it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:25<00:02, 88.43it/s, train_loss=1.5239, val_loss=1.5356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:25<00:02, 88.43it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1803/2000 [00:25<00:03, 50.75it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1812/2000 [00:25<00:03, 57.07it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1821/2000 [00:25<00:02, 62.39it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1830/2000 [00:25<00:02, 67.72it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1840/2000 [00:25<00:02, 73.47it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▎| 1850/2000 [00:26<00:01, 78.46it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1860/2000 [00:26<00:01, 82.46it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1870/2000 [00:26<00:01, 85.53it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:26<00:01, 87.51it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1890/2000 [00:26<00:01, 88.51it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:26<00:01, 89.55it/s, train_loss=1.4324, val_loss=1.4460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:26<00:01, 89.55it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1910/2000 [00:26<00:01, 51.37it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1920/2000 [00:27<00:01, 59.23it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1930/2000 [00:27<00:01, 66.24it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1940/2000 [00:27<00:00, 72.23it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1950/2000 [00:27<00:00, 77.05it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1960/2000 [00:27<00:00, 80.72it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1970/2000 [00:27<00:00, 83.54it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1980/2000 [00:27<00:00, 85.73it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|█████████▉| 1990/2000 [00:27<00:00, 87.41it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:27<00:00, 71.61it/s, train_loss=1.3487, val_loss=1.3596]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:24<01:24, 84.18s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0001, beta2=0.98, avg_val_loss=1.4356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:25,  3.54it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:56, 35.19it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:35, 55.33it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:28, 68.39it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 77.63it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:23, 82.97it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:00<00:22, 86.87it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:21, 89.30it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:20, 91.95it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 93.89it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 93.89it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:37, 50.50it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:32, 58.94it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:28, 66.80it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:01<00:25, 74.04it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:23, 79.64it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:21, 84.65it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:20, 88.46it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:20, 90.62it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:19, 92.31it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 93.38it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 93.38it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:02<00:34, 52.91it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 211/2000 [00:03<00:29, 61.07it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 221/2000 [00:03<00:25, 68.48it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:23, 74.91it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 241/2000 [00:03<00:21, 80.76it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:20, 85.33it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:19, 87.49it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:03<00:19, 88.00it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:03<00:19, 88.84it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:03<00:18, 90.51it/s, train_loss=3.0930, val_loss=3.0912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:18, 90.51it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 301/2000 [00:04<00:33, 51.46it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:28, 59.38it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 321/2000 [00:04<00:25, 66.63it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 331/2000 [00:04<00:22, 73.33it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:21, 78.63it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:04<00:19, 83.34it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 87.23it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:05<00:18, 89.55it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:17, 91.23it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 92.40it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 92.40it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:30, 51.91it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 410/2000 [00:05<00:27, 58.82it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 419/2000 [00:05<00:24, 64.89it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 428/2000 [00:05<00:22, 70.48it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 437/2000 [00:06<00:20, 74.81it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 446/2000 [00:06<00:19, 77.79it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 455/2000 [00:06<00:19, 80.54it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 464/2000 [00:06<00:18, 82.42it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:18, 83.38it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 482/2000 [00:06<00:17, 84.54it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:17, 85.38it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:07<00:17, 85.38it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:07<00:30, 48.76it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:07<00:26, 56.19it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 519/2000 [00:07<00:23, 63.08it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 529/2000 [00:07<00:21, 69.83it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 538/2000 [00:07<00:19, 74.69it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 547/2000 [00:07<00:18, 77.85it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 556/2000 [00:07<00:17, 80.95it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 566/2000 [00:07<00:17, 83.86it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 575/2000 [00:07<00:16, 85.43it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:07<00:16, 86.49it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 594/2000 [00:08<00:16, 87.14it/s, train_loss=2.6284, val_loss=2.6315]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 594/2000 [00:08<00:16, 87.14it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:08<00:28, 48.88it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 612/2000 [00:08<00:24, 55.94it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 621/2000 [00:08<00:21, 62.83it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 631/2000 [00:08<00:19, 69.76it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 640/2000 [00:08<00:18, 73.67it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▎      | 650/2000 [00:08<00:17, 78.35it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 659/2000 [00:09<00:16, 81.31it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 668/2000 [00:09<00:16, 82.95it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 678/2000 [00:09<00:15, 85.21it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:09<00:15, 86.99it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:09<00:14, 87.65it/s, train_loss=2.5038, val_loss=2.4998]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:09<00:14, 87.65it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:09<00:26, 48.30it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 716/2000 [00:10<00:22, 56.64it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 725/2000 [00:10<00:20, 63.04it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:10<00:18, 68.99it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 744/2000 [00:10<00:16, 74.26it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:10<00:15, 78.79it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 764/2000 [00:10<00:14, 82.45it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 773/2000 [00:10<00:14, 83.76it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 782/2000 [00:10<00:14, 85.13it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:14, 86.05it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 800/2000 [00:10<00:13, 86.81it/s, train_loss=2.3701, val_loss=2.3841]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 800/2000 [00:11<00:13, 86.81it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 809/2000 [00:11<00:24, 49.45it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 818/2000 [00:11<00:20, 57.08it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 827/2000 [00:11<00:18, 64.03it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 836/2000 [00:11<00:16, 69.72it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 846/2000 [00:11<00:15, 75.30it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 855/2000 [00:11<00:14, 78.73it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 865/2000 [00:11<00:13, 82.07it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:12<00:13, 84.61it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 884/2000 [00:12<00:13, 85.79it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:12<00:12, 87.11it/s, train_loss=2.2465, val_loss=2.2540]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:12<00:12, 87.11it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 903/2000 [00:12<00:22, 49.18it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 913/2000 [00:12<00:18, 57.37it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 923/2000 [00:12<00:16, 65.02it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 933/2000 [00:12<00:15, 71.08it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 943/2000 [00:13<00:13, 76.16it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 952/2000 [00:13<00:13, 79.03it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 961/2000 [00:13<00:12, 81.64it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 970/2000 [00:13<00:12, 83.60it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:13<00:12, 84.57it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:13<00:11, 86.11it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:13<00:11, 87.58it/s, train_loss=2.1316, val_loss=2.1346]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:14<00:11, 87.58it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1008/2000 [00:14<00:20, 48.89it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1017/2000 [00:14<00:17, 56.21it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1026/2000 [00:14<00:15, 63.11it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1035/2000 [00:14<00:14, 68.65it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1044/2000 [00:14<00:12, 73.78it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:14<00:12, 78.21it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1063/2000 [00:14<00:11, 81.11it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1072/2000 [00:14<00:11, 83.30it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1082/2000 [00:14<00:10, 85.55it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:15<00:10, 87.24it/s, train_loss=2.0075, val_loss=2.0140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:15<00:10, 87.24it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:15<00:18, 48.86it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:15<00:15, 56.23it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1119/2000 [00:15<00:14, 62.90it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1129/2000 [00:15<00:12, 69.92it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1139/2000 [00:15<00:11, 75.34it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1149/2000 [00:15<00:10, 79.65it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:16<00:10, 81.82it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1167/2000 [00:16<00:09, 83.91it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1176/2000 [00:16<00:09, 84.60it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1185/2000 [00:16<00:09, 86.07it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:16<00:09, 86.63it/s, train_loss=1.8871, val_loss=1.8951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:16<00:09, 86.63it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1203/2000 [00:16<00:16, 48.47it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1212/2000 [00:16<00:14, 56.01it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1221/2000 [00:17<00:12, 62.95it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:17<00:11, 69.13it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:17<00:10, 75.03it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▎   | 1250/2000 [00:17<00:09, 79.32it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1260/2000 [00:17<00:08, 82.36it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1270/2000 [00:17<00:08, 84.51it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:17<00:08, 85.11it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1288/2000 [00:17<00:08, 86.44it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:17<00:08, 87.00it/s, train_loss=1.7713, val_loss=1.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:18<00:08, 87.00it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1306/2000 [00:18<00:14, 48.16it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1315/2000 [00:18<00:12, 55.65it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1324/2000 [00:18<00:10, 62.45it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1333/2000 [00:18<00:09, 68.28it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1342/2000 [00:18<00:09, 72.96it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1351/2000 [00:18<00:08, 77.00it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1360/2000 [00:18<00:07, 80.40it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:18<00:07, 82.89it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1378/2000 [00:19<00:07, 84.47it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1387/2000 [00:19<00:07, 85.52it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:19<00:06, 87.20it/s, train_loss=1.6590, val_loss=1.6513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:19<00:06, 87.20it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1406/2000 [00:19<00:12, 47.84it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1415/2000 [00:19<00:10, 55.24it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1425/2000 [00:19<00:09, 63.08it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1434/2000 [00:20<00:08, 68.61it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1443/2000 [00:20<00:07, 72.95it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1452/2000 [00:20<00:07, 76.76it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1461/2000 [00:20<00:06, 80.09it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1471/2000 [00:20<00:06, 83.22it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1480/2000 [00:20<00:06, 84.14it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1490/2000 [00:20<00:05, 86.17it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:20<00:05, 85.58it/s, train_loss=1.5081, val_loss=1.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:21<00:05, 85.58it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:21<00:10, 48.11it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1517/2000 [00:21<00:08, 55.47it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1526/2000 [00:21<00:07, 62.52it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1535/2000 [00:21<00:06, 68.23it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1545/2000 [00:21<00:06, 74.09it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:21<00:05, 77.35it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1563/2000 [00:21<00:05, 80.10it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1572/2000 [00:21<00:05, 81.95it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1582/2000 [00:21<00:04, 83.95it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:22<00:04, 85.39it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:22<00:04, 84.79it/s, train_loss=1.4090, val_loss=1.3983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:22<00:04, 84.79it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:22<00:08, 48.12it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:22<00:06, 55.58it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1627/2000 [00:22<00:05, 62.19it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1636/2000 [00:22<00:05, 68.11it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:22<00:04, 73.44it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:23<00:04, 77.15it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1663/2000 [00:23<00:04, 80.15it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:23<00:03, 83.16it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1682/2000 [00:23<00:03, 85.00it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:23<00:03, 85.41it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1700/2000 [00:23<00:03, 86.26it/s, train_loss=1.2692, val_loss=1.2718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1700/2000 [00:23<00:03, 86.26it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1709/2000 [00:23<00:06, 48.41it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1719/2000 [00:24<00:04, 57.02it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:24<00:04, 63.53it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:24<00:03, 69.42it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:24<00:03, 73.93it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1755/2000 [00:24<00:03, 78.05it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:24<00:02, 80.18it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1773/2000 [00:24<00:02, 82.08it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1782/2000 [00:24<00:02, 81.82it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:24<00:02, 83.69it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:25<00:02, 85.33it/s, train_loss=1.1499, val_loss=1.1458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:25<00:02, 85.33it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1809/2000 [00:25<00:04, 46.48it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1819/2000 [00:25<00:03, 55.22it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1828/2000 [00:25<00:02, 62.13it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1837/2000 [00:25<00:02, 67.45it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:25<00:02, 72.44it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1855/2000 [00:25<00:01, 76.35it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1864/2000 [00:26<00:01, 79.88it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1874/2000 [00:26<00:01, 82.86it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1883/2000 [00:26<00:01, 84.48it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:26<00:01, 85.85it/s, train_loss=1.0482, val_loss=1.0515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:26<00:01, 85.85it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:26<00:02, 47.58it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1910/2000 [00:26<00:01, 55.07it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:26<00:01, 62.20it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1929/2000 [00:27<00:01, 69.28it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1939/2000 [00:27<00:00, 74.86it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1948/2000 [00:27<00:00, 77.52it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1958/2000 [00:27<00:00, 81.25it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1967/2000 [00:27<00:00, 82.97it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1976/2000 [00:27<00:00, 84.30it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1985/2000 [00:27<00:00, 85.41it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:27<00:00, 71.78it/s, train_loss=0.9608, val_loss=0.9696]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:27,  3.52it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:02, 32.08it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:39, 50.42it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 28/2000 [00:00<00:31, 62.70it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 37/2000 [00:00<00:27, 70.79it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 46/2000 [00:00<00:25, 76.29it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:24, 79.88it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 64/2000 [00:00<00:23, 82.77it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:22, 85.29it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 83/2000 [00:01<00:22, 85.39it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 92/2000 [00:01<00:22, 86.10it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 92/2000 [00:01<00:22, 86.10it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:40, 46.95it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 110/2000 [00:01<00:34, 54.77it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 119/2000 [00:01<00:30, 61.77it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 128/2000 [00:02<00:27, 67.48it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 138/2000 [00:02<00:25, 73.63it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 147/2000 [00:02<00:23, 77.76it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 156/2000 [00:02<00:22, 80.64it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:22, 81.65it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 174/2000 [00:02<00:21, 83.76it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:21, 84.14it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:21, 85.13it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:03<00:21, 85.13it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:03<00:37, 48.04it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 210/2000 [00:03<00:32, 55.70it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 219/2000 [00:03<00:28, 62.44it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 228/2000 [00:03<00:26, 67.80it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 237/2000 [00:03<00:24, 71.32it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 246/2000 [00:03<00:23, 73.93it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:22, 76.04it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 265/2000 [00:03<00:21, 80.39it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 274/2000 [00:03<00:20, 82.93it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 284/2000 [00:04<00:20, 85.40it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 293/2000 [00:04<00:19, 86.50it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 293/2000 [00:04<00:19, 86.50it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 302/2000 [00:04<00:35, 48.48it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 312/2000 [00:04<00:29, 56.92it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 321/2000 [00:04<00:26, 63.65it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 331/2000 [00:04<00:23, 70.26it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 340/2000 [00:04<00:22, 74.76it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 349/2000 [00:05<00:21, 78.39it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 358/2000 [00:05<00:20, 80.78it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 367/2000 [00:05<00:19, 82.89it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 376/2000 [00:05<00:19, 84.67it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 386/2000 [00:05<00:18, 86.65it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:18, 87.83it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:18, 87.83it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 405/2000 [00:05<00:32, 49.29it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 415/2000 [00:06<00:27, 57.53it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 425/2000 [00:06<00:24, 64.91it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 435/2000 [00:06<00:22, 71.01it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 444/2000 [00:06<00:20, 74.99it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 453/2000 [00:06<00:19, 78.51it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 463/2000 [00:06<00:18, 82.17it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 472/2000 [00:06<00:18, 84.22it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:06<00:17, 85.20it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:06<00:17, 86.46it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:07<00:20, 74.42it/s, train_loss=2.7659, val_loss=2.7614]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:07<00:20, 74.42it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 507/2000 [00:07<00:34, 42.78it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 516/2000 [00:07<00:32, 45.13it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 522/2000 [00:07<00:36, 40.48it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 529/2000 [00:07<00:32, 45.33it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 538/2000 [00:08<00:27, 53.13it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 546/2000 [00:08<00:24, 58.80it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 555/2000 [00:08<00:22, 65.42it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 563/2000 [00:08<00:21, 66.00it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:08<00:23, 61.69it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 578/2000 [00:08<00:22, 62.12it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:08<00:22, 64.11it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:08<00:25, 55.68it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:09<00:25, 55.51it/s, train_loss=2.6413, val_loss=2.6379]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:09<00:25, 55.51it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 604/2000 [00:09<01:05, 21.32it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:09<00:53, 25.74it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 616/2000 [00:10<00:45, 30.14it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:10<00:37, 36.42it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 630/2000 [00:10<00:32, 41.96it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 637/2000 [00:10<00:29, 46.22it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 643/2000 [00:10<00:28, 48.06it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▎      | 650/2000 [00:10<00:25, 53.13it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 658/2000 [00:10<00:22, 58.91it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:10<00:20, 65.70it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 675/2000 [00:10<00:19, 68.75it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 684/2000 [00:11<00:18, 72.44it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:11<00:17, 75.74it/s, train_loss=2.5154, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:11<00:17, 75.74it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 701/2000 [00:11<00:47, 27.46it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 707/2000 [00:12<00:42, 30.10it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 714/2000 [00:12<00:35, 35.82it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 723/2000 [00:12<00:28, 44.69it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 731/2000 [00:12<00:24, 51.00it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 740/2000 [00:12<00:21, 59.37it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 749/2000 [00:12<00:18, 65.91it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 758/2000 [00:12<00:17, 71.67it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 767/2000 [00:12<00:17, 70.23it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 775/2000 [00:12<00:17, 71.87it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:12<00:16, 73.50it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:13<00:15, 76.08it/s, train_loss=2.4065, val_loss=2.4015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:13<00:15, 76.08it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:13<00:41, 28.58it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 807/2000 [00:13<00:38, 30.60it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 813/2000 [00:14<00:36, 32.82it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:14<00:29, 40.10it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 829/2000 [00:14<00:24, 47.44it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 838/2000 [00:14<00:21, 55.09it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 846/2000 [00:14<00:19, 60.66it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 855/2000 [00:14<00:17, 65.89it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 863/2000 [00:14<00:16, 69.44it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 871/2000 [00:14<00:15, 71.70it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 879/2000 [00:14<00:15, 73.61it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 887/2000 [00:15<00:15, 74.07it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:15<00:15, 72.79it/s, train_loss=2.2727, val_loss=2.2698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:16<00:15, 72.79it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 903/2000 [00:16<01:13, 14.95it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 909/2000 [00:16<01:01, 17.85it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 915/2000 [00:16<00:51, 21.27it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:17<00:42, 25.10it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 928/2000 [00:17<00:34, 31.00it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 936/2000 [00:17<00:27, 38.40it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 944/2000 [00:17<00:23, 45.53it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 953/2000 [00:17<00:19, 53.72it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 962/2000 [00:17<00:17, 60.41it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 970/2000 [00:17<00:15, 65.05it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 978/2000 [00:17<00:14, 68.26it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 986/2000 [00:17<00:14, 71.07it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:18<00:13, 73.98it/s, train_loss=2.1567, val_loss=2.1559]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:18<00:13, 73.98it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1003/2000 [00:18<00:24, 41.12it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1011/2000 [00:18<00:20, 47.87it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:18<00:17, 55.36it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1029/2000 [00:18<00:15, 61.75it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:18<00:14, 67.43it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1047/2000 [00:18<00:13, 71.46it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1056/2000 [00:19<00:12, 75.14it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1065/2000 [00:19<00:12, 77.52it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:19<00:11, 79.42it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:19<00:11, 81.23it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:19<00:11, 81.75it/s, train_loss=2.0484, val_loss=2.0421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:19<00:11, 81.75it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:19<00:19, 46.86it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:20<00:16, 54.15it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1119/2000 [00:20<00:14, 61.00it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1128/2000 [00:20<00:13, 66.75it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1137/2000 [00:20<00:12, 70.83it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:20<00:11, 73.02it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1154/2000 [00:20<00:11, 76.52it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1163/2000 [00:20<00:10, 78.67it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1172/2000 [00:20<00:10, 78.49it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1181/2000 [00:20<00:10, 79.10it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1190/2000 [00:20<00:10, 79.31it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1199/2000 [00:21<00:10, 78.87it/s, train_loss=1.8960, val_loss=1.8983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1199/2000 [00:21<00:10, 78.87it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1207/2000 [00:21<00:17, 44.26it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1215/2000 [00:21<00:15, 49.88it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1223/2000 [00:21<00:13, 55.50it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:21<00:12, 60.72it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1239/2000 [00:21<00:11, 65.32it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1247/2000 [00:22<00:11, 67.85it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:22<00:10, 70.96it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1264/2000 [00:22<00:09, 74.28it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1273/2000 [00:22<00:09, 76.58it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1282/2000 [00:22<00:09, 79.20it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1291/2000 [00:22<00:08, 80.78it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:22<00:08, 81.10it/s, train_loss=1.7599, val_loss=1.7654]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:22<00:08, 81.10it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:23<00:15, 45.51it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:23<00:12, 53.13it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1327/2000 [00:23<00:11, 60.27it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1336/2000 [00:23<00:10, 66.05it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1345/2000 [00:23<00:09, 70.17it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1353/2000 [00:23<00:08, 72.35it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1362/2000 [00:23<00:08, 74.84it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1371/2000 [00:23<00:08, 77.43it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1380/2000 [00:23<00:07, 78.93it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:23<00:07, 80.71it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:24<00:07, 81.92it/s, train_loss=1.6072, val_loss=1.6132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:24<00:07, 81.92it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:24<00:12, 47.04it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1416/2000 [00:24<00:10, 54.47it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1425/2000 [00:24<00:09, 60.58it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1434/2000 [00:24<00:08, 65.85it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1442/2000 [00:24<00:08, 69.19it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1451/2000 [00:25<00:07, 72.92it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1460/2000 [00:25<00:07, 75.56it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1469/2000 [00:25<00:06, 78.61it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1478/2000 [00:25<00:06, 78.22it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1487/2000 [00:25<00:06, 79.96it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:25<00:06, 81.39it/s, train_loss=1.4755, val_loss=1.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:25<00:06, 81.39it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1505/2000 [00:25<00:10, 45.49it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1514/2000 [00:26<00:09, 52.90it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1523/2000 [00:26<00:08, 59.30it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1531/2000 [00:26<00:07, 63.73it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1539/2000 [00:26<00:07, 65.80it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1547/2000 [00:26<00:06, 67.69it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1555/2000 [00:26<00:06, 70.57it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:26<00:05, 73.95it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1573/2000 [00:26<00:05, 76.28it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1582/2000 [00:26<00:05, 78.70it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:27<00:05, 80.80it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:27<00:04, 80.32it/s, train_loss=1.3308, val_loss=1.3408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:27<00:04, 80.32it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:27<00:08, 45.38it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:27<00:07, 52.72it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1627/2000 [00:27<00:06, 59.25it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1636/2000 [00:27<00:05, 65.31it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:27<00:05, 70.71it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:28<00:04, 73.28it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1663/2000 [00:28<00:04, 76.35it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1672/2000 [00:28<00:04, 78.95it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1681/2000 [00:28<00:03, 81.24it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1690/2000 [00:28<00:03, 81.71it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:28<00:03, 81.06it/s, train_loss=1.2184, val_loss=1.2112]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:28<00:03, 81.06it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1708/2000 [00:29<00:06, 45.76it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1717/2000 [00:29<00:05, 53.24it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1726/2000 [00:29<00:04, 59.73it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1735/2000 [00:29<00:04, 65.60it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:29<00:03, 70.59it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1753/2000 [00:29<00:03, 74.98it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:29<00:03, 77.88it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1771/2000 [00:29<00:02, 78.55it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1780/2000 [00:29<00:02, 80.64it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1789/2000 [00:29<00:02, 82.14it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:30<00:02, 81.79it/s, train_loss=1.1102, val_loss=1.1193]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:30<00:02, 81.79it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1807/2000 [00:30<00:04, 46.87it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1816/2000 [00:30<00:03, 54.56it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1825/2000 [00:30<00:02, 60.68it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:30<00:02, 66.78it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1843/2000 [00:30<00:02, 70.57it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1852/2000 [00:30<00:01, 74.93it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1861/2000 [00:31<00:01, 77.57it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1870/2000 [00:31<00:01, 79.68it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:31<00:01, 80.62it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:31<00:01, 82.61it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:31<00:01, 83.89it/s, train_loss=1.0151, val_loss=1.0156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:31<00:01, 83.89it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1906/2000 [00:31<00:02, 46.73it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1915/2000 [00:32<00:01, 53.23it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1924/2000 [00:32<00:01, 60.16it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:32<00:01, 65.91it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1942/2000 [00:32<00:00, 69.95it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:32<00:00, 74.22it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1960/2000 [00:32<00:00, 76.76it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1969/2000 [00:32<00:00, 77.30it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1978/2000 [00:32<00:00, 78.71it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:32<00:00, 80.86it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:33<00:00, 60.53it/s, train_loss=0.9388, val_loss=0.9276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:17,  3.24it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 9/2000 [00:00<01:12, 27.39it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 18/2000 [00:00<00:43, 45.63it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 27/2000 [00:00<00:34, 57.12it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 36/2000 [00:00<00:29, 66.00it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:28, 68.01it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 52/2000 [00:00<00:27, 70.58it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:01<00:26, 74.22it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 70/2000 [00:01<00:24, 77.57it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 79/2000 [00:01<00:24, 79.46it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 88/2000 [00:01<00:24, 78.64it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:24, 77.62it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:24, 77.62it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 104/2000 [00:01<00:44, 42.30it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 112/2000 [00:02<00:38, 48.46it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:02<00:33, 55.87it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 129/2000 [00:02<00:30, 60.88it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 138/2000 [00:02<00:28, 66.28it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 146/2000 [00:02<00:26, 69.64it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 155/2000 [00:02<00:25, 73.10it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 163/2000 [00:02<00:24, 74.48it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 172/2000 [00:02<00:23, 76.50it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:23, 76.12it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 189/2000 [00:02<00:23, 78.47it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:22, 79.80it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:22, 79.80it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:41, 43.62it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:34, 51.44it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:30, 57.86it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:03<00:28, 62.68it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 241/2000 [00:03<00:26, 66.58it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▎        | 250/2000 [00:04<00:24, 70.27it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 258/2000 [00:04<00:24, 71.86it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 267/2000 [00:04<00:23, 74.37it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 275/2000 [00:04<00:23, 73.48it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 283/2000 [00:04<00:23, 74.07it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 292/2000 [00:04<00:22, 76.13it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 292/2000 [00:05<00:22, 76.13it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 301/2000 [00:05<00:42, 40.19it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:05<00:37, 45.05it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 316/2000 [00:05<00:32, 51.51it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 324/2000 [00:05<00:29, 57.42it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 332/2000 [00:05<00:26, 62.20it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 340/2000 [00:05<00:25, 65.80it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 348/2000 [00:05<00:24, 68.66it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 357/2000 [00:05<00:22, 72.63it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 366/2000 [00:05<00:21, 75.53it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 375/2000 [00:05<00:21, 76.80it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 383/2000 [00:06<00:21, 76.64it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:06<00:20, 78.98it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:06<00:20, 78.98it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:06<00:38, 41.64it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 409/2000 [00:06<00:33, 48.06it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 416/2000 [00:06<00:33, 47.88it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 423/2000 [00:06<00:30, 51.71it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 430/2000 [00:07<00:28, 54.17it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 437/2000 [00:07<00:27, 57.80it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 445/2000 [00:07<00:24, 62.91it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 454/2000 [00:07<00:22, 68.99it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 463/2000 [00:07<00:21, 72.77it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 472/2000 [00:07<00:19, 76.47it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:07<00:19, 78.77it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:07<00:19, 79.47it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:07<00:18, 79.52it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:08<00:18, 79.52it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 508/2000 [00:08<00:32, 45.28it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 517/2000 [00:08<00:28, 52.94it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 526/2000 [00:08<00:24, 60.22it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 535/2000 [00:08<00:22, 65.26it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 544/2000 [00:08<00:20, 71.05it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 553/2000 [00:08<00:19, 75.69it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 562/2000 [00:08<00:18, 78.05it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:09<00:18, 79.27it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 580/2000 [00:09<00:17, 81.90it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 589/2000 [00:09<00:17, 82.49it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:09<00:16, 84.37it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:09<00:16, 84.37it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 607/2000 [00:09<00:28, 48.97it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 616/2000 [00:09<00:24, 56.36it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 625/2000 [00:09<00:21, 62.71it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:10<00:19, 68.90it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 643/2000 [00:10<00:18, 73.64it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 652/2000 [00:10<00:17, 76.36it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 661/2000 [00:10<00:16, 79.23it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 670/2000 [00:10<00:16, 80.83it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 679/2000 [00:10<00:16, 81.73it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:10<00:15, 83.38it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:10<00:15, 83.15it/s, train_loss=2.5112, val_loss=2.5111]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:11<00:15, 83.15it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:11<00:27, 46.52it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 715/2000 [00:11<00:23, 53.54it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 723/2000 [00:11<00:22, 55.91it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 730/2000 [00:11<00:22, 56.57it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 738/2000 [00:11<00:20, 61.45it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 746/2000 [00:11<00:19, 65.33it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:11<00:19, 65.01it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 761/2000 [00:11<00:19, 62.95it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 770/2000 [00:12<00:18, 68.01it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 778/2000 [00:12<00:17, 68.38it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 787/2000 [00:12<00:16, 72.46it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:12<00:16, 74.29it/s, train_loss=2.3750, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:12<00:16, 74.29it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 803/2000 [00:12<00:29, 40.05it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 809/2000 [00:12<00:27, 42.93it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 815/2000 [00:13<00:26, 45.16it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:13<00:28, 41.75it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 826/2000 [00:13<00:28, 41.33it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:13<00:26, 44.40it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 839/2000 [00:13<00:23, 50.21it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 847/2000 [00:13<00:20, 55.98it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 856/2000 [00:13<00:18, 63.11it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:13<00:16, 67.10it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 873/2000 [00:14<00:15, 73.05it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 881/2000 [00:14<00:17, 65.60it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 888/2000 [00:14<00:17, 65.15it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:14<00:16, 66.05it/s, train_loss=2.2422, val_loss=2.2367]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:14<00:16, 66.05it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 903/2000 [00:14<00:29, 36.60it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 912/2000 [00:14<00:23, 45.49it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:15<00:20, 53.88it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 930/2000 [00:15<00:17, 61.13it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 939/2000 [00:15<00:15, 67.46it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:15<00:14, 72.36it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:15<00:13, 75.73it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:15<00:13, 78.10it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:15<00:13, 77.96it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:15<00:12, 79.96it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:15<00:12, 79.79it/s, train_loss=2.0978, val_loss=2.1001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:16<00:12, 79.79it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1002/2000 [00:16<00:21, 46.81it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1011/2000 [00:16<00:18, 54.23it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:16<00:16, 60.71it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1029/2000 [00:16<00:14, 66.49it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:16<00:13, 71.11it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1047/2000 [00:16<00:12, 75.88it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1056/2000 [00:16<00:11, 78.88it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1065/2000 [00:16<00:11, 80.32it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:17<00:11, 81.90it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:17<00:11, 81.24it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:17<00:10, 83.00it/s, train_loss=1.9423, val_loss=1.9481]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:17<00:10, 83.00it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:17<00:18, 48.10it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:17<00:16, 55.56it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1119/2000 [00:17<00:14, 62.49it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1127/2000 [00:17<00:13, 66.05it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1136/2000 [00:18<00:12, 70.91it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:18<00:11, 74.61it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1154/2000 [00:18<00:10, 77.23it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1163/2000 [00:18<00:10, 80.57it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1172/2000 [00:18<00:10, 82.54it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1181/2000 [00:18<00:09, 82.74it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1190/2000 [00:18<00:09, 83.27it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1199/2000 [00:18<00:09, 85.07it/s, train_loss=1.7709, val_loss=1.7854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1199/2000 [00:19<00:09, 85.07it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:19<00:16, 48.27it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1217/2000 [00:19<00:14, 55.58it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1226/2000 [00:19<00:12, 62.44it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1235/2000 [00:19<00:11, 67.36it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1244/2000 [00:19<00:10, 72.13it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1253/2000 [00:19<00:09, 74.88it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1262/2000 [00:19<00:09, 78.22it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1271/2000 [00:19<00:09, 79.77it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1280/2000 [00:20<00:09, 78.54it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:20<00:09, 75.05it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:20<00:09, 71.52it/s, train_loss=1.6139, val_loss=1.6186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:20<00:09, 71.52it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1305/2000 [00:20<00:15, 43.49it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1314/2000 [00:20<00:13, 51.49it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1323/2000 [00:20<00:11, 58.50it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1331/2000 [00:20<00:10, 63.11it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1340/2000 [00:21<00:09, 68.36it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1349/2000 [00:21<00:08, 72.56it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:21<00:08, 74.06it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1366/2000 [00:21<00:08, 76.91it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1375/2000 [00:21<00:07, 79.62it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1384/2000 [00:21<00:07, 81.22it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:21<00:07, 83.26it/s, train_loss=1.4558, val_loss=1.4608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:22<00:07, 83.26it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:22<00:12, 48.17it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:22<00:10, 55.75it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1420/2000 [00:22<00:09, 62.68it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:22<00:08, 68.17it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1438/2000 [00:22<00:07, 72.93it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:22<00:07, 76.94it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1456/2000 [00:22<00:06, 79.92it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1465/2000 [00:22<00:06, 81.72it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1474/2000 [00:22<00:06, 83.09it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1483/2000 [00:23<00:06, 84.43it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:23<00:06, 84.55it/s, train_loss=1.3133, val_loss=1.3212]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:23<00:06, 84.55it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1501/2000 [00:23<00:10, 47.99it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1510/2000 [00:23<00:08, 55.48it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:23<00:07, 62.49it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1528/2000 [00:23<00:06, 68.54it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1537/2000 [00:23<00:06, 72.84it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1546/2000 [00:24<00:05, 76.12it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1555/2000 [00:24<00:05, 77.29it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:24<00:05, 78.94it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1573/2000 [00:24<00:05, 81.10it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1582/2000 [00:24<00:05, 81.72it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:24<00:05, 81.55it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:24<00:04, 83.73it/s, train_loss=1.1851, val_loss=1.1956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:24<00:04, 83.73it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:25<00:08, 48.46it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:25<00:06, 56.13it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1627/2000 [00:25<00:05, 62.92it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1636/2000 [00:25<00:05, 68.67it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:25<00:04, 73.56it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:25<00:04, 75.88it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1663/2000 [00:25<00:04, 77.77it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1672/2000 [00:25<00:04, 80.04it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1681/2000 [00:25<00:03, 82.13it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1690/2000 [00:25<00:03, 83.53it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:26<00:03, 84.83it/s, train_loss=1.0808, val_loss=1.0805]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:26<00:03, 84.83it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1708/2000 [00:26<00:06, 48.60it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1717/2000 [00:26<00:05, 55.71it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1726/2000 [00:26<00:04, 62.82it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1735/2000 [00:26<00:03, 68.87it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:26<00:03, 73.74it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1753/2000 [00:26<00:03, 77.88it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:27<00:02, 80.15it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1771/2000 [00:27<00:02, 82.45it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1780/2000 [00:27<00:02, 84.19it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1789/2000 [00:27<00:02, 85.55it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:27<00:02, 85.22it/s, train_loss=0.9900, val_loss=1.0012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:27<00:02, 85.22it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1807/2000 [00:27<00:03, 48.92it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1816/2000 [00:27<00:03, 56.35it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1825/2000 [00:28<00:02, 63.15it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:28<00:02, 67.48it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1843/2000 [00:28<00:02, 72.47it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1852/2000 [00:28<00:01, 75.73it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1861/2000 [00:28<00:01, 78.23it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1870/2000 [00:28<00:01, 79.33it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:28<00:01, 81.81it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:28<00:01, 83.55it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:28<00:01, 83.84it/s, train_loss=0.9172, val_loss=0.9321]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:29<00:01, 83.84it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1906/2000 [00:29<00:01, 48.63it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1915/2000 [00:29<00:01, 56.06it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1924/2000 [00:29<00:01, 62.24it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:29<00:00, 67.38it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1942/2000 [00:29<00:00, 71.44it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:29<00:00, 75.06it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1960/2000 [00:29<00:00, 76.28it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1969/2000 [00:30<00:00, 78.40it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1978/2000 [00:30<00:00, 79.30it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:30<00:00, 79.98it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:30<00:00, 65.81it/s, train_loss=0.8516, val_loss=0.8613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:55<00:00, 88.39s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR:  33%|███▎      | 1/3 [02:55<05:51, 175.51s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0001, beta2=0.999, avg_val_loss=0.9195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<13:21,  2.49it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 8/2000 [00:00<01:38, 20.12it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 15/2000 [00:00<00:59, 33.41it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:44, 44.85it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:36, 54.11it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 39/2000 [00:00<00:32, 60.83it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 48/2000 [00:01<00:28, 67.89it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:01<00:27, 70.66it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 65/2000 [00:01<00:25, 74.54it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:24, 77.89it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 83/2000 [00:01<00:24, 77.38it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 92/2000 [00:01<00:24, 78.43it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 92/2000 [00:01<00:24, 78.43it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:02<00:45, 41.85it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 109/2000 [00:02<00:39, 48.18it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 117/2000 [00:02<00:35, 53.52it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 125/2000 [00:02<00:31, 59.18it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 133/2000 [00:02<00:29, 63.88it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 142/2000 [00:02<00:26, 69.60it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:24, 74.72it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 160/2000 [00:02<00:23, 76.92it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 169/2000 [00:02<00:23, 78.40it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 178/2000 [00:02<00:23, 78.89it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 187/2000 [00:03<00:22, 80.35it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 196/2000 [00:03<00:21, 82.91it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 196/2000 [00:03<00:21, 82.91it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 205/2000 [00:03<00:37, 48.32it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 215/2000 [00:03<00:31, 57.45it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 224/2000 [00:03<00:27, 63.50it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:03<00:25, 69.15it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 242/2000 [00:03<00:24, 71.63it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:04<00:23, 75.62it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 260/2000 [00:04<00:21, 79.18it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 269/2000 [00:04<00:21, 80.93it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 278/2000 [00:04<00:20, 82.95it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 288/2000 [00:04<00:20, 85.52it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:19, 85.66it/s, train_loss=2.2537, val_loss=2.2514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:19, 85.66it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:05<00:38, 44.50it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:05<00:32, 51.53it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 323/2000 [00:05<00:29, 56.71it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 332/2000 [00:05<00:26, 63.18it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 342/2000 [00:05<00:23, 70.08it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:05<00:22, 72.52it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:05<00:21, 77.93it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:05<00:19, 81.79it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:18, 85.42it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:18, 88.09it/s, train_loss=1.8930, val_loss=1.9096]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:06<00:18, 88.09it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:06<00:31, 50.62it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:06<00:26, 58.90it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 420/2000 [00:06<00:24, 64.92it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 430/2000 [00:06<00:22, 71.36it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 440/2000 [00:06<00:20, 76.45it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▎       | 450/2000 [00:06<00:19, 80.99it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 460/2000 [00:07<00:18, 84.13it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 470/2000 [00:07<00:17, 86.35it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 480/2000 [00:07<00:17, 88.21it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:07<00:16, 89.18it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:07<00:16, 91.23it/s, train_loss=1.6032, val_loss=1.6354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:07<00:16, 91.23it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:07<00:28, 52.66it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 520/2000 [00:07<00:24, 60.61it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 530/2000 [00:08<00:21, 67.60it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 540/2000 [00:08<00:19, 73.70it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 550/2000 [00:08<00:18, 78.43it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 559/2000 [00:08<00:18, 79.51it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 568/2000 [00:08<00:18, 79.48it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 577/2000 [00:08<00:18, 78.82it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 586/2000 [00:08<00:17, 79.46it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:17, 78.48it/s, train_loss=1.3902, val_loss=1.4001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:09<00:17, 78.48it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 604/2000 [00:09<00:30, 45.63it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 612/2000 [00:09<00:27, 51.08it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 621/2000 [00:09<00:23, 57.68it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 629/2000 [00:09<00:22, 62.29it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 637/2000 [00:09<00:20, 66.32it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 645/2000 [00:09<00:19, 69.66it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 653/2000 [00:09<00:18, 71.72it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 662/2000 [00:09<00:17, 74.41it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 671/2000 [00:10<00:17, 76.48it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 679/2000 [00:10<00:17, 76.62it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:10<00:16, 78.46it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:10<00:16, 79.52it/s, train_loss=1.1246, val_loss=1.1140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:10<00:16, 79.52it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:10<00:27, 46.31it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 715/2000 [00:10<00:23, 54.04it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 724/2000 [00:10<00:20, 61.30it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 733/2000 [00:11<00:18, 67.27it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:11<00:17, 72.40it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 751/2000 [00:11<00:16, 75.61it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 760/2000 [00:11<00:15, 78.41it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 769/2000 [00:11<00:15, 80.65it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 778/2000 [00:11<00:14, 83.05it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 787/2000 [00:11<00:14, 84.11it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:11<00:14, 83.56it/s, train_loss=0.8673, val_loss=0.8731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:12<00:14, 83.56it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 805/2000 [00:12<00:24, 48.40it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:12<00:21, 55.75it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 823/2000 [00:12<00:18, 62.55it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:12<00:17, 67.61it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 841/2000 [00:12<00:16, 72.19it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▎     | 850/2000 [00:12<00:15, 75.43it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 859/2000 [00:12<00:14, 78.26it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 868/2000 [00:12<00:14, 80.73it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 877/2000 [00:13<00:13, 82.27it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:13<00:13, 82.06it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:13<00:13, 81.18it/s, train_loss=0.6919, val_loss=0.7001]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:13<00:13, 81.18it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 904/2000 [00:13<00:23, 47.64it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 913/2000 [00:13<00:19, 55.10it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 922/2000 [00:13<00:17, 61.38it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 931/2000 [00:13<00:15, 66.94it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 940/2000 [00:14<00:14, 71.39it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 949/2000 [00:14<00:13, 75.20it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 958/2000 [00:14<00:13, 77.83it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 967/2000 [00:14<00:12, 80.25it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 976/2000 [00:14<00:12, 81.28it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 985/2000 [00:14<00:12, 82.04it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:14<00:12, 83.33it/s, train_loss=0.6296, val_loss=0.6238]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:15<00:12, 83.33it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1003/2000 [00:15<00:21, 46.75it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1012/2000 [00:15<00:18, 54.25it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:15<00:16, 59.90it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:15<00:14, 65.56it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1039/2000 [00:15<00:13, 70.52it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:15<00:12, 74.29it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1057/2000 [00:15<00:12, 76.69it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1066/2000 [00:15<00:11, 78.51it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1075/2000 [00:15<00:11, 79.98it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1084/2000 [00:16<00:11, 81.81it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:16<00:10, 83.30it/s, train_loss=0.5933, val_loss=0.5966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:16<00:10, 83.30it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1102/2000 [00:16<00:18, 47.74it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:16<00:16, 55.01it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:16<00:14, 62.05it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1129/2000 [00:16<00:12, 67.77it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:16<00:12, 71.79it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1147/2000 [00:17<00:11, 75.63it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1156/2000 [00:17<00:10, 79.04it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1165/2000 [00:17<00:10, 80.95it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1174/2000 [00:17<00:10, 81.90it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1183/2000 [00:17<00:09, 83.89it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:17<00:09, 85.11it/s, train_loss=0.5472, val_loss=0.5573]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:17<00:09, 85.11it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:17<00:16, 47.81it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:18<00:14, 55.14it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1219/2000 [00:18<00:12, 62.07it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1228/2000 [00:18<00:11, 68.23it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:18<00:10, 73.02it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1246/2000 [00:18<00:09, 77.03it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:18<00:09, 80.02it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1264/2000 [00:18<00:08, 82.32it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1273/2000 [00:18<00:08, 84.09it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1282/2000 [00:18<00:08, 84.13it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1291/2000 [00:18<00:08, 85.38it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:19<00:08, 85.91it/s, train_loss=0.5246, val_loss=0.5284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:19<00:08, 85.91it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:19<00:14, 47.81it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:19<00:12, 53.03it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1325/2000 [00:19<00:11, 58.14it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1333/2000 [00:19<00:10, 63.05it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1342/2000 [00:19<00:09, 69.51it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:19<00:08, 75.48it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1362/2000 [00:20<00:07, 79.90it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1372/2000 [00:20<00:07, 83.07it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1381/2000 [00:20<00:07, 84.71it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1390/2000 [00:20<00:07, 85.98it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:20<00:07, 84.88it/s, train_loss=0.5323, val_loss=0.5300]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:20<00:07, 84.88it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1408/2000 [00:20<00:12, 47.95it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:20<00:10, 55.26it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1427/2000 [00:21<00:09, 63.16it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1436/2000 [00:21<00:08, 68.97it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1445/2000 [00:21<00:07, 73.47it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1454/2000 [00:21<00:07, 76.79it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1463/2000 [00:21<00:06, 78.95it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1472/2000 [00:21<00:06, 81.20it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1481/2000 [00:21<00:06, 82.93it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1490/2000 [00:21<00:06, 83.96it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:21<00:05, 84.96it/s, train_loss=0.5099, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:22<00:05, 84.96it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:22<00:09, 49.42it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1517/2000 [00:22<00:08, 56.86it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1526/2000 [00:22<00:07, 63.85it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1536/2000 [00:22<00:06, 70.49it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1546/2000 [00:22<00:05, 75.78it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1555/2000 [00:22<00:05, 79.36it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:22<00:05, 81.91it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1574/2000 [00:23<00:05, 84.49it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1583/2000 [00:23<00:04, 85.13it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:23<00:04, 86.03it/s, train_loss=0.5108, val_loss=0.5092]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:23<00:04, 86.03it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1601/2000 [00:23<00:08, 49.55it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1610/2000 [00:23<00:06, 56.86it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:23<00:05, 63.73it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1628/2000 [00:23<00:05, 69.62it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1637/2000 [00:23<00:04, 74.54it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1646/2000 [00:24<00:04, 78.41it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1655/2000 [00:24<00:04, 81.54it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:24<00:04, 82.41it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:24<00:03, 83.82it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1682/2000 [00:24<00:03, 84.80it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:24<00:03, 84.81it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1700/2000 [00:24<00:03, 85.97it/s, train_loss=0.4891, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1700/2000 [00:24<00:03, 85.97it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1709/2000 [00:25<00:05, 49.27it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:25<00:04, 56.94it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1727/2000 [00:25<00:04, 63.95it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:25<00:03, 69.79it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1745/2000 [00:25<00:03, 73.38it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:25<00:03, 76.90it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1763/2000 [00:25<00:02, 79.41it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:25<00:02, 81.76it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1781/2000 [00:25<00:02, 83.23it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1790/2000 [00:26<00:02, 83.87it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:26<00:02, 85.29it/s, train_loss=0.4801, val_loss=0.4800]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:26<00:02, 85.29it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1808/2000 [00:26<00:03, 49.14it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1817/2000 [00:26<00:03, 56.38it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1826/2000 [00:26<00:02, 63.07it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1835/2000 [00:26<00:02, 69.02it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1844/2000 [00:26<00:02, 74.21it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1853/2000 [00:26<00:01, 77.77it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1862/2000 [00:27<00:01, 80.70it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1871/2000 [00:27<00:01, 82.62it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:27<00:01, 84.03it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:27<00:01, 84.85it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:27<00:01, 85.84it/s, train_loss=0.4809, val_loss=0.4965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:27<00:01, 85.84it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:27<00:01, 49.19it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1916/2000 [00:27<00:01, 56.69it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1925/2000 [00:28<00:01, 63.61it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1934/2000 [00:28<00:00, 69.74it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1943/2000 [00:28<00:00, 74.69it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1952/2000 [00:28<00:00, 76.97it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1961/2000 [00:28<00:00, 80.18it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1970/2000 [00:28<00:00, 82.41it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1979/2000 [00:28<00:00, 84.19it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1988/2000 [00:28<00:00, 85.25it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 69.13it/s, train_loss=0.4713, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:15,  3.60it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:00, 32.92it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:38, 51.35it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 29/2000 [00:00<00:30, 64.55it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 39/2000 [00:00<00:26, 73.13it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 49/2000 [00:00<00:24, 78.89it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 59/2000 [00:00<00:23, 82.67it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 68/2000 [00:01<00:22, 84.11it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 78/2000 [00:01<00:22, 86.17it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 88/2000 [00:01<00:21, 87.71it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 98/2000 [00:01<00:21, 88.58it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 98/2000 [00:01<00:21, 88.58it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 107/2000 [00:01<00:37, 50.66it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 117/2000 [00:01<00:31, 58.85it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 127/2000 [00:01<00:28, 66.02it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 136/2000 [00:02<00:26, 71.21it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:02<00:24, 75.68it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 155/2000 [00:02<00:23, 79.84it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:22, 82.97it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 174/2000 [00:02<00:21, 84.71it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:21, 85.96it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:02<00:20, 87.44it/s, train_loss=2.7083, val_loss=2.7135]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:03<00:20, 87.44it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 202/2000 [00:03<00:35, 50.37it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 212/2000 [00:03<00:30, 58.52it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 222/2000 [00:03<00:27, 65.70it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:24, 71.14it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 241/2000 [00:03<00:23, 76.19it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▎        | 250/2000 [00:03<00:22, 79.13it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 260/2000 [00:03<00:21, 82.32it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 269/2000 [00:03<00:20, 84.33it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 279/2000 [00:03<00:19, 86.32it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 289/2000 [00:04<00:19, 87.53it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:19, 87.68it/s, train_loss=2.2900, val_loss=2.2976]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:19, 87.68it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 307/2000 [00:04<00:33, 50.93it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 316/2000 [00:04<00:29, 57.98it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 326/2000 [00:04<00:25, 65.52it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 335/2000 [00:04<00:23, 71.02it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 344/2000 [00:04<00:21, 75.63it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 354/2000 [00:04<00:20, 79.74it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:05<00:19, 82.16it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 373/2000 [00:05<00:19, 84.68it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 382/2000 [00:05<00:18, 86.08it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:18, 87.50it/s, train_loss=1.9627, val_loss=1.9542]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:18, 87.50it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:31, 50.31it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 410/2000 [00:05<00:27, 57.59it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 419/2000 [00:05<00:24, 64.31it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 428/2000 [00:06<00:22, 69.75it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 438/2000 [00:06<00:20, 75.36it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 447/2000 [00:06<00:19, 79.01it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 457/2000 [00:06<00:18, 82.21it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 466/2000 [00:06<00:18, 84.21it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 475/2000 [00:06<00:17, 85.31it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 484/2000 [00:06<00:18, 82.01it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:19, 78.66it/s, train_loss=1.6527, val_loss=1.6294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:07<00:19, 78.66it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 502/2000 [00:07<00:32, 46.00it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:07<00:27, 53.84it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:07<00:23, 61.89it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 530/2000 [00:07<00:21, 67.84it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 539/2000 [00:07<00:20, 72.38it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 548/2000 [00:07<00:19, 75.81it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 557/2000 [00:07<00:18, 78.64it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 566/2000 [00:07<00:17, 81.20it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 575/2000 [00:08<00:17, 82.71it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:08<00:16, 85.34it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:16, 86.86it/s, train_loss=1.2410, val_loss=1.2366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:16, 86.86it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 604/2000 [00:08<00:28, 48.94it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 613/2000 [00:08<00:24, 56.41it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 622/2000 [00:08<00:21, 63.33it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 632/2000 [00:08<00:19, 70.40it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 642/2000 [00:09<00:17, 76.41it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 652/2000 [00:09<00:16, 81.62it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 662/2000 [00:09<00:15, 85.57it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 672/2000 [00:09<00:14, 88.59it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 682/2000 [00:09<00:14, 90.85it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 692/2000 [00:09<00:14, 92.56it/s, train_loss=0.9181, val_loss=0.9130]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 692/2000 [00:09<00:14, 92.56it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 702/2000 [00:09<00:24, 54.01it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 712/2000 [00:10<00:20, 62.05it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 722/2000 [00:10<00:18, 69.59it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 732/2000 [00:10<00:16, 75.86it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:10<00:15, 81.02it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:10<00:14, 85.04it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 762/2000 [00:10<00:14, 87.66it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:10<00:13, 89.82it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 782/2000 [00:10<00:13, 91.77it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:10<00:13, 91.50it/s, train_loss=0.7192, val_loss=0.7189]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:11<00:13, 91.50it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 802/2000 [00:11<00:22, 52.14it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 812/2000 [00:11<00:19, 60.34it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 822/2000 [00:11<00:17, 67.86it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:11<00:15, 74.03it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 842/2000 [00:11<00:14, 79.53it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 852/2000 [00:11<00:13, 83.85it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:11<00:13, 86.90it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 872/2000 [00:12<00:12, 88.65it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 882/2000 [00:12<00:12, 88.78it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:12<00:12, 88.48it/s, train_loss=0.6274, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:12<00:12, 88.48it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 902/2000 [00:12<00:22, 49.82it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:12<00:19, 56.60it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:12<00:16, 64.10it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 930/2000 [00:12<00:15, 69.31it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 939/2000 [00:13<00:14, 73.38it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:13<00:13, 77.27it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:13<00:13, 79.85it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:13<00:12, 81.91it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:13<00:12, 83.56it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:13<00:11, 85.38it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:13<00:11, 86.45it/s, train_loss=0.5784, val_loss=0.5864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:14<00:11, 86.45it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1003/2000 [00:14<00:19, 50.18it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1012/2000 [00:14<00:17, 57.59it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1022/2000 [00:14<00:14, 65.26it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1032/2000 [00:14<00:13, 71.78it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1042/2000 [00:14<00:12, 76.94it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:14<00:11, 80.65it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1062/2000 [00:14<00:11, 83.39it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1072/2000 [00:14<00:10, 85.88it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:14<00:10, 86.95it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:15<00:10, 88.33it/s, train_loss=0.5559, val_loss=0.5532]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:15<00:10, 88.33it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:15<00:17, 51.25it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:15<00:15, 59.09it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1121/2000 [00:15<00:13, 66.15it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1131/2000 [00:15<00:12, 72.14it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1141/2000 [00:15<00:11, 77.10it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1151/2000 [00:15<00:10, 81.21it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:16<00:09, 84.22it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1170/2000 [00:16<00:09, 85.54it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1180/2000 [00:16<00:09, 86.97it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1190/2000 [00:16<00:09, 88.29it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:16<00:08, 89.45it/s, train_loss=0.5368, val_loss=0.5344]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:16<00:08, 89.45it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:16<00:15, 52.22it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1220/2000 [00:16<00:12, 60.15it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:17<00:11, 67.15it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1239/2000 [00:17<00:10, 72.06it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:17<00:09, 76.35it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:17<00:09, 80.25it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1268/2000 [00:17<00:08, 83.48it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1278/2000 [00:17<00:08, 86.01it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1287/2000 [00:17<00:08, 86.58it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1296/2000 [00:17<00:08, 87.44it/s, train_loss=0.5168, val_loss=0.5244]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1296/2000 [00:18<00:08, 87.44it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1305/2000 [00:18<00:14, 49.57it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1314/2000 [00:18<00:12, 57.00it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1324/2000 [00:18<00:10, 64.65it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1334/2000 [00:18<00:09, 71.10it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1343/2000 [00:18<00:08, 75.44it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:18<00:08, 78.81it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1362/2000 [00:18<00:07, 82.23it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1372/2000 [00:18<00:07, 84.72it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1381/2000 [00:19<00:07, 86.08it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1390/2000 [00:19<00:07, 87.05it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:19<00:06, 87.19it/s, train_loss=0.5104, val_loss=0.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:19<00:06, 87.19it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1408/2000 [00:19<00:12, 49.16it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:19<00:10, 56.74it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1427/2000 [00:19<00:08, 64.47it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:19<00:07, 71.16it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:20<00:07, 76.22it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:20<00:06, 80.32it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:20<00:06, 83.38it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:20<00:06, 86.00it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1487/2000 [00:20<00:05, 87.81it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:20<00:05, 89.10it/s, train_loss=0.4766, val_loss=0.4920]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:20<00:05, 89.10it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:20<00:09, 52.64it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1517/2000 [00:21<00:08, 60.28it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1526/2000 [00:21<00:07, 66.09it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1536/2000 [00:21<00:06, 72.10it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1546/2000 [00:21<00:05, 77.11it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1556/2000 [00:21<00:05, 81.19it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1565/2000 [00:21<00:05, 79.72it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1574/2000 [00:21<00:05, 75.20it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1582/2000 [00:21<00:05, 73.82it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:21<00:05, 77.35it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:22<00:04, 80.30it/s, train_loss=0.4959, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:22<00:04, 80.30it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:22<00:08, 46.46it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:22<00:06, 55.18it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1628/2000 [00:22<00:05, 62.11it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1638/2000 [00:22<00:05, 69.13it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1647/2000 [00:22<00:04, 74.07it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1656/2000 [00:22<00:04, 78.08it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1666/2000 [00:23<00:04, 81.51it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1676/2000 [00:23<00:03, 84.07it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1685/2000 [00:23<00:03, 85.54it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:23<00:03, 87.48it/s, train_loss=0.4755, val_loss=0.4743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:23<00:03, 87.48it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1704/2000 [00:23<00:05, 50.66it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:23<00:04, 58.67it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1723/2000 [00:24<00:04, 64.78it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1733/2000 [00:24<00:03, 71.12it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1742/2000 [00:24<00:03, 75.22it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1751/2000 [00:24<00:03, 78.86it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1760/2000 [00:24<00:02, 81.60it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1769/2000 [00:24<00:02, 83.67it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1778/2000 [00:24<00:02, 85.18it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1787/2000 [00:24<00:02, 86.07it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:24<00:02, 87.47it/s, train_loss=0.4753, val_loss=0.4912]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:25<00:02, 87.47it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1806/2000 [00:25<00:03, 49.84it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1815/2000 [00:25<00:03, 57.29it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1824/2000 [00:25<00:02, 63.83it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1833/2000 [00:25<00:02, 69.83it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1842/2000 [00:25<00:02, 74.78it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1852/2000 [00:25<00:01, 79.45it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1862/2000 [00:25<00:01, 83.05it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1872/2000 [00:25<00:01, 86.48it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1882/2000 [00:26<00:01, 89.00it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:26<00:01, 90.91it/s, train_loss=0.4765, val_loss=0.4793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:26<00:01, 90.91it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1902/2000 [00:26<00:01, 52.55it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1912/2000 [00:26<00:01, 60.60it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:26<00:01, 67.59it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1932/2000 [00:26<00:00, 73.99it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1942/2000 [00:26<00:00, 79.41it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1952/2000 [00:27<00:00, 83.03it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:27<00:00, 85.67it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1972/2000 [00:27<00:00, 87.93it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1982/2000 [00:27<00:00, 89.60it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:27<00:00, 72.58it/s, train_loss=0.4850, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<21:11,  1.57it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 8/2000 [00:00<02:23, 13.91it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 16/2000 [00:00<01:13, 27.03it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:54, 36.60it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 30/2000 [00:01<00:51, 38.54it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 37/2000 [00:01<00:43, 44.87it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 43/2000 [00:01<00:40, 47.75it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▎         | 50/2000 [00:01<00:37, 52.02it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 58/2000 [00:01<00:33, 57.67it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:01<00:30, 63.05it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:29, 65.99it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 82/2000 [00:01<00:27, 69.24it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 90/2000 [00:01<00:27, 70.07it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 98/2000 [00:02<00:26, 71.24it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 98/2000 [00:02<00:26, 71.24it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 106/2000 [00:02<01:03, 29.71it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 112/2000 [00:02<00:56, 33.26it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 119/2000 [00:02<00:48, 39.01it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 127/2000 [00:03<00:40, 46.29it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 135/2000 [00:03<00:35, 52.52it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 142/2000 [00:03<00:34, 53.72it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 149/2000 [00:03<00:35, 52.69it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 155/2000 [00:03<00:34, 53.52it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 163/2000 [00:03<00:31, 58.87it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:03<00:29, 63.05it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 178/2000 [00:03<00:30, 59.43it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 185/2000 [00:04<00:31, 57.51it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:04<00:28, 63.10it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:04<00:28, 63.10it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:04<00:59, 30.19it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 208/2000 [00:04<00:50, 35.49it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 217/2000 [00:04<00:40, 43.79it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:04<00:35, 50.35it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:05<00:31, 56.44it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 240/2000 [00:05<00:30, 57.90it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 248/2000 [00:05<00:28, 61.26it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:05<00:28, 61.86it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 263/2000 [00:05<00:26, 65.74it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:05<00:24, 69.20it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 279/2000 [00:05<00:24, 70.42it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 288/2000 [00:05<00:23, 73.35it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:05<00:22, 74.77it/s, train_loss=2.2636, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:06<00:22, 74.77it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 304/2000 [00:06<00:48, 34.85it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 310/2000 [00:06<00:43, 38.69it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:06<00:36, 46.13it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:06<00:33, 50.19it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 332/2000 [00:06<00:31, 52.95it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:07<00:27, 60.20it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 350/2000 [00:07<00:24, 66.53it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 359/2000 [00:07<00:22, 71.67it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 368/2000 [00:07<00:21, 76.35it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 377/2000 [00:07<00:20, 78.33it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 387/2000 [00:07<00:19, 82.00it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:07<00:19, 83.00it/s, train_loss=1.8976, val_loss=1.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:07<00:19, 83.00it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 405/2000 [00:08<00:33, 47.62it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 414/2000 [00:08<00:29, 54.41it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 423/2000 [00:08<00:26, 60.37it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 432/2000 [00:08<00:23, 66.77it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:08<00:21, 72.03it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▎       | 450/2000 [00:08<00:21, 72.26it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 458/2000 [00:08<00:22, 69.75it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 466/2000 [00:08<00:21, 70.92it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 475/2000 [00:08<00:20, 74.97it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 484/2000 [00:09<00:19, 77.80it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:09<00:18, 80.82it/s, train_loss=1.5718, val_loss=1.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:09<00:18, 80.82it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 502/2000 [00:09<00:33, 44.83it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:09<00:28, 52.75it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:09<00:24, 61.47it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 530/2000 [00:09<00:21, 67.76it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 540/2000 [00:09<00:19, 74.30it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 550/2000 [00:10<00:18, 79.59it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 560/2000 [00:10<00:17, 83.50it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 570/2000 [00:10<00:16, 86.06it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 580/2000 [00:10<00:16, 88.58it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 590/2000 [00:10<00:15, 89.24it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:10<00:15, 90.53it/s, train_loss=1.1893, val_loss=1.2010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:10<00:15, 90.53it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:10<00:26, 52.63it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 620/2000 [00:11<00:22, 60.32it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 630/2000 [00:11<00:20, 67.81it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 640/2000 [00:11<00:18, 74.21it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▎      | 650/2000 [00:11<00:17, 78.89it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 660/2000 [00:11<00:16, 83.26it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 670/2000 [00:11<00:15, 84.80it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 680/2000 [00:11<00:15, 87.55it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 690/2000 [00:11<00:15, 86.23it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:11<00:15, 86.51it/s, train_loss=0.8938, val_loss=0.8836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:12<00:15, 86.51it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 708/2000 [00:12<00:25, 50.67it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 718/2000 [00:12<00:21, 58.95it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 728/2000 [00:12<00:19, 66.47it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 738/2000 [00:12<00:17, 72.74it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 748/2000 [00:12<00:16, 77.97it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 758/2000 [00:12<00:15, 81.90it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 768/2000 [00:12<00:14, 85.37it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 778/2000 [00:13<00:13, 88.13it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 788/2000 [00:13<00:13, 90.27it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 798/2000 [00:13<00:13, 91.19it/s, train_loss=0.7224, val_loss=0.7182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 798/2000 [00:13<00:13, 91.19it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 808/2000 [00:13<00:22, 53.41it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 818/2000 [00:13<00:19, 61.30it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 828/2000 [00:13<00:17, 68.10it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 838/2000 [00:13<00:15, 74.37it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 848/2000 [00:14<00:14, 79.87it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 858/2000 [00:14<00:13, 83.64it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 868/2000 [00:14<00:13, 86.85it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 878/2000 [00:14<00:12, 89.40it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 888/2000 [00:14<00:12, 91.07it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 898/2000 [00:14<00:11, 92.38it/s, train_loss=0.6292, val_loss=0.6240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 898/2000 [00:14<00:11, 92.38it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 908/2000 [00:14<00:20, 53.74it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 918/2000 [00:15<00:17, 61.81it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 928/2000 [00:15<00:15, 69.03it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 938/2000 [00:15<00:14, 74.66it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:15<00:13, 79.08it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 958/2000 [00:15<00:12, 82.62it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 968/2000 [00:15<00:12, 85.78it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 978/2000 [00:15<00:11, 87.67it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 988/2000 [00:15<00:11, 89.11it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:15<00:11, 90.42it/s, train_loss=0.5807, val_loss=0.5749]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:16<00:11, 90.42it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1008/2000 [00:16<00:19, 51.99it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1018/2000 [00:16<00:16, 59.99it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1028/2000 [00:16<00:14, 67.12it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:16<00:13, 73.38it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:16<00:12, 77.92it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1058/2000 [00:16<00:11, 82.32it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1068/2000 [00:16<00:10, 84.96it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:17<00:10, 86.83it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1088/2000 [00:17<00:10, 87.73it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:17<00:10, 89.85it/s, train_loss=0.5457, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:17<00:10, 89.85it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1108/2000 [00:17<00:16, 53.11it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1118/2000 [00:17<00:14, 60.90it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1128/2000 [00:17<00:12, 68.16it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:17<00:11, 74.30it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1148/2000 [00:18<00:10, 79.31it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:18<00:10, 83.37it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:18<00:09, 85.97it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1178/2000 [00:18<00:09, 87.83it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1188/2000 [00:18<00:09, 89.29it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:18<00:08, 89.34it/s, train_loss=0.5245, val_loss=0.5423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:18<00:08, 89.34it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:18<00:15, 51.88it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1217/2000 [00:19<00:13, 58.64it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1227/2000 [00:19<00:11, 66.04it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:19<00:10, 72.57it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1247/2000 [00:19<00:09, 78.12it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:19<00:09, 81.81it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:19<00:08, 85.25it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1277/2000 [00:19<00:08, 87.80it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1287/2000 [00:19<00:07, 90.09it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:19<00:07, 90.96it/s, train_loss=0.5202, val_loss=0.5324]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:20<00:07, 90.96it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:20<00:13, 53.23it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:20<00:11, 61.32it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1327/2000 [00:20<00:09, 68.49it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1337/2000 [00:20<00:08, 74.86it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1347/2000 [00:20<00:08, 79.72it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:20<00:07, 83.57it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1367/2000 [00:20<00:07, 85.11it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1377/2000 [00:21<00:07, 87.25it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1387/2000 [00:21<00:06, 88.48it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:21<00:06, 88.84it/s, train_loss=0.4926, val_loss=0.5150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:21<00:06, 88.84it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:21<00:11, 53.14it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:21<00:09, 61.15it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1427/2000 [00:21<00:08, 68.51it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:21<00:07, 74.65it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:22<00:06, 79.42it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:22<00:06, 82.77it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:22<00:06, 85.75it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:22<00:05, 87.94it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1487/2000 [00:22<00:05, 89.37it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:22<00:05, 90.81it/s, train_loss=0.4982, val_loss=0.5071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:22<00:05, 90.81it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:22<00:09, 53.34it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1517/2000 [00:23<00:07, 61.69it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1527/2000 [00:23<00:06, 69.15it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1537/2000 [00:23<00:06, 75.40it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1547/2000 [00:23<00:05, 80.20it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1557/2000 [00:23<00:05, 82.93it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1567/2000 [00:23<00:05, 78.47it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1576/2000 [00:23<00:05, 79.01it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1585/2000 [00:23<00:05, 81.01it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:23<00:04, 84.21it/s, train_loss=0.5032, val_loss=0.5084]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:24<00:04, 84.21it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:24<00:07, 50.40it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1614/2000 [00:24<00:06, 59.28it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1624/2000 [00:24<00:05, 67.23it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1634/2000 [00:24<00:04, 74.00it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1644/2000 [00:24<00:04, 79.42it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:24<00:04, 83.92it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:24<00:03, 86.28it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:25<00:03, 88.42it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1684/2000 [00:25<00:03, 90.58it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:25<00:03, 90.94it/s, train_loss=0.4944, val_loss=0.4887]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:25<00:03, 90.94it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1704/2000 [00:25<00:05, 53.35it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:25<00:04, 61.63it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1724/2000 [00:25<00:03, 69.01it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1734/2000 [00:25<00:03, 75.27it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:26<00:03, 79.93it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:26<00:02, 84.03it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:26<00:02, 86.13it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1774/2000 [00:26<00:02, 88.80it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1784/2000 [00:26<00:02, 89.76it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:26<00:02, 90.87it/s, train_loss=0.4823, val_loss=0.5020]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:26<00:02, 90.87it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:26<00:03, 53.49it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1814/2000 [00:27<00:03, 61.45it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1824/2000 [00:27<00:02, 68.10it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:27<00:02, 74.06it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1844/2000 [00:27<00:01, 78.99it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1854/2000 [00:27<00:01, 82.84it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1864/2000 [00:27<00:01, 86.11it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1874/2000 [00:27<00:01, 89.02it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1884/2000 [00:27<00:01, 90.61it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:27<00:01, 87.83it/s, train_loss=0.4732, val_loss=0.4835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:28<00:01, 87.83it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1903/2000 [00:28<00:01, 51.08it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1913/2000 [00:28<00:01, 59.50it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1923/2000 [00:28<00:01, 67.29it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:28<00:00, 73.99it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1943/2000 [00:28<00:00, 79.36it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1953/2000 [00:28<00:00, 83.36it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1963/2000 [00:28<00:00, 86.34it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1973/2000 [00:29<00:00, 89.01it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1983/2000 [00:29<00:00, 90.71it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:29<00:00, 68.21it/s, train_loss=0.4693, val_loss=0.4825]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:25<01:25, 85.85s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0005, beta2=0.98, avg_val_loss=0.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:38,  3.46it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:57, 34.57it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:36, 54.82it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:28, 68.42it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 77.27it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:23, 83.31it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:00<00:22, 86.70it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:21, 89.15it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:20, 91.64it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 93.38it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 93.38it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:36, 52.55it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:31, 60.88it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:27, 68.64it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:01<00:24, 75.37it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:22, 80.90it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:21, 84.46it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:20, 88.22it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:20, 90.92it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:19, 93.05it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 94.27it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 94.27it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:02<00:34, 52.28it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 211/2000 [00:03<00:29, 60.31it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 221/2000 [00:03<00:26, 68.06it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:23, 74.69it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 241/2000 [00:03<00:21, 80.28it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:20, 83.80it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:19, 86.95it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:03<00:19, 89.71it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:03<00:18, 91.94it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:03<00:18, 93.42it/s, train_loss=2.2390, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:18, 93.42it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 301/2000 [00:04<00:32, 52.67it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:27, 60.96it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 321/2000 [00:04<00:24, 68.78it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 331/2000 [00:04<00:22, 75.51it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:20, 80.96it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:04<00:19, 85.18it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 88.17it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:04<00:17, 90.86it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:17, 93.00it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 94.17it/s, train_loss=1.8323, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 94.17it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:30, 53.15it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:05<00:25, 61.22it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:22, 68.81it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:05<00:20, 75.28it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:05<00:19, 80.15it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:06<00:18, 84.33it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 461/2000 [00:06<00:17, 88.13it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 471/2000 [00:06<00:16, 90.32it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:06<00:16, 92.38it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:16, 93.45it/s, train_loss=1.4899, val_loss=1.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:16, 93.45it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:06<00:28, 53.02it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:06<00:24, 61.49it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:07<00:21, 69.10it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:07<00:19, 75.49it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:18, 80.99it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:07<00:17, 84.91it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 561/2000 [00:07<00:16, 88.39it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:07<00:15, 90.54it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 581/2000 [00:07<00:15, 92.27it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:07<00:15, 93.46it/s, train_loss=1.0817, val_loss=1.0835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:15, 93.46it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:08<00:26, 52.44it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 611/2000 [00:08<00:22, 60.71it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 621/2000 [00:08<00:20, 68.52it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 631/2000 [00:08<00:18, 75.30it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 641/2000 [00:08<00:16, 80.56it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 651/2000 [00:08<00:15, 84.79it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 661/2000 [00:08<00:15, 87.64it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 671/2000 [00:08<00:14, 90.11it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 681/2000 [00:08<00:14, 92.13it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:09<00:14, 93.28it/s, train_loss=0.7983, val_loss=0.7832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:09<00:14, 93.28it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 701/2000 [00:09<00:25, 51.40it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 711/2000 [00:09<00:21, 59.81it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 721/2000 [00:09<00:18, 67.53it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 731/2000 [00:09<00:17, 74.13it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 741/2000 [00:09<00:15, 79.62it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 751/2000 [00:10<00:14, 84.06it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 761/2000 [00:10<00:14, 87.56it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 771/2000 [00:10<00:13, 90.55it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:10<00:13, 92.88it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 94.07it/s, train_loss=0.6670, val_loss=0.6721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 94.07it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:22, 52.98it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:10<00:19, 61.39it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:11<00:17, 68.25it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 831/2000 [00:11<00:15, 74.71it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 841/2000 [00:11<00:14, 79.97it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 851/2000 [00:11<00:13, 84.63it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 861/2000 [00:11<00:12, 88.20it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 871/2000 [00:11<00:12, 90.78it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 881/2000 [00:11<00:12, 92.84it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 92.56it/s, train_loss=0.5969, val_loss=0.6048]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:12<00:11, 92.56it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 901/2000 [00:12<00:20, 53.02it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:12<00:17, 61.46it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:12<00:15, 69.15it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 931/2000 [00:12<00:14, 75.32it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 941/2000 [00:12<00:13, 80.64it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 951/2000 [00:12<00:12, 85.05it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 961/2000 [00:12<00:11, 87.08it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 971/2000 [00:12<00:12, 83.98it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 980/2000 [00:12<00:12, 83.06it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:13<00:12, 84.11it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:13<00:11, 87.10it/s, train_loss=0.5674, val_loss=0.5617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:13<00:11, 87.10it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1008/2000 [00:13<00:19, 51.55it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1018/2000 [00:13<00:16, 60.60it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1028/2000 [00:13<00:14, 68.46it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:13<00:12, 75.66it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:13<00:11, 80.64it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1058/2000 [00:14<00:11, 84.57it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1068/2000 [00:14<00:10, 88.34it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:14<00:10, 91.19it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1088/2000 [00:14<00:09, 93.11it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:14<00:09, 94.46it/s, train_loss=0.5511, val_loss=0.5550]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:14<00:09, 94.46it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1108/2000 [00:14<00:16, 54.49it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1118/2000 [00:14<00:13, 63.03it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1128/2000 [00:15<00:12, 70.63it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:15<00:11, 76.97it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1148/2000 [00:15<00:10, 82.59it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:15<00:09, 86.94it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:15<00:09, 89.53it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1178/2000 [00:15<00:08, 92.26it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1188/2000 [00:15<00:08, 93.67it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 95.06it/s, train_loss=0.5238, val_loss=0.5294]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:16<00:08, 95.06it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:16<00:14, 54.87it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1218/2000 [00:16<00:12, 63.31it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1228/2000 [00:16<00:10, 71.14it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1238/2000 [00:16<00:09, 77.65it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:16<00:09, 80.08it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:16<00:08, 83.72it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1268/2000 [00:16<00:08, 86.92it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1278/2000 [00:16<00:08, 89.89it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1288/2000 [00:16<00:07, 90.54it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:17<00:07, 92.50it/s, train_loss=0.4948, val_loss=0.4962]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:17<00:07, 92.50it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1308/2000 [00:17<00:12, 53.95it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:17<00:10, 62.43it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:17<00:09, 69.93it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1338/2000 [00:17<00:08, 76.74it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1348/2000 [00:17<00:08, 81.32it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1358/2000 [00:17<00:07, 85.49it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1368/2000 [00:18<00:07, 89.12it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1378/2000 [00:18<00:06, 91.49it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1388/2000 [00:18<00:06, 92.71it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:18<00:06, 94.46it/s, train_loss=0.4933, val_loss=0.4910]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:18<00:06, 94.46it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1408/2000 [00:18<00:10, 54.13it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1418/2000 [00:18<00:09, 62.66it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1428/2000 [00:18<00:08, 70.32it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1438/2000 [00:19<00:07, 76.62it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1448/2000 [00:19<00:06, 81.73it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1458/2000 [00:19<00:06, 86.01it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1468/2000 [00:19<00:05, 89.06it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1478/2000 [00:19<00:05, 91.29it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1488/2000 [00:19<00:05, 93.15it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 94.46it/s, train_loss=0.4870, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 94.46it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:20<00:09, 54.57it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1518/2000 [00:20<00:07, 62.98it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1528/2000 [00:20<00:06, 70.58it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1538/2000 [00:20<00:05, 77.25it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1548/2000 [00:20<00:05, 82.72it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1558/2000 [00:20<00:05, 86.81it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1568/2000 [00:20<00:04, 89.41it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1578/2000 [00:20<00:04, 91.95it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1588/2000 [00:20<00:04, 93.62it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1598/2000 [00:20<00:04, 95.08it/s, train_loss=0.4927, val_loss=0.4934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1598/2000 [00:21<00:04, 95.08it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1608/2000 [00:21<00:07, 54.44it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:21<00:06, 62.76it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1628/2000 [00:21<00:05, 70.51it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1638/2000 [00:21<00:04, 76.56it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1648/2000 [00:21<00:04, 81.91it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:21<00:03, 85.73it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1668/2000 [00:21<00:03, 88.41it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1678/2000 [00:22<00:03, 89.74it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1688/2000 [00:22<00:03, 91.35it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:22<00:03, 92.05it/s, train_loss=0.4762, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:22<00:03, 92.05it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1708/2000 [00:22<00:05, 53.76it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:22<00:04, 62.25it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:22<00:03, 69.88it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1738/2000 [00:22<00:03, 76.46it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1748/2000 [00:23<00:03, 81.65it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1758/2000 [00:23<00:02, 84.84it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1768/2000 [00:23<00:02, 88.07it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1778/2000 [00:23<00:02, 90.96it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1788/2000 [00:23<00:02, 92.72it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:23<00:02, 93.95it/s, train_loss=0.4862, val_loss=0.4868]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:23<00:02, 93.95it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1808/2000 [00:23<00:03, 54.46it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:23<00:02, 62.76it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1828/2000 [00:24<00:02, 70.22it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:24<00:02, 76.57it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1848/2000 [00:24<00:01, 81.87it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1858/2000 [00:24<00:01, 86.23it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1868/2000 [00:24<00:01, 89.25it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1878/2000 [00:24<00:01, 91.23it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:24<00:01, 93.16it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:24<00:01, 94.75it/s, train_loss=0.4609, val_loss=0.4734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:25<00:01, 94.75it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1908/2000 [00:25<00:01, 54.34it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1918/2000 [00:25<00:01, 62.52it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1928/2000 [00:25<00:01, 69.90it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1938/2000 [00:25<00:00, 76.42it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1948/2000 [00:25<00:00, 81.77it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1958/2000 [00:25<00:00, 85.61it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1968/2000 [00:25<00:00, 88.18it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1978/2000 [00:25<00:00, 90.63it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1988/2000 [00:26<00:00, 92.22it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 76.56it/s, train_loss=0.4662, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:17,  3.59it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:55, 35.87it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:35, 55.73it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:28, 68.44it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 77.49it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:23, 83.36it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:00<00:22, 86.99it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:21, 89.90it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:20, 91.89it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 92.87it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 92.87it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:36, 51.48it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:31, 59.47it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:28, 66.86it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:01<00:25, 73.47it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:23, 78.79it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:22, 83.20it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:21, 86.80it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:20, 89.85it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:19, 91.17it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 92.11it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 92.11it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:02<00:34, 51.41it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 211/2000 [00:03<00:29, 59.67it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 221/2000 [00:03<00:26, 67.42it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:23, 74.39it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 241/2000 [00:03<00:22, 79.64it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:20, 83.69it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:20, 86.53it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:03<00:19, 88.93it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:03<00:18, 90.86it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:03<00:18, 92.19it/s, train_loss=2.2631, val_loss=2.2711]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:18, 92.19it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 301/2000 [00:04<00:33, 51.46it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:28, 59.23it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 321/2000 [00:04<00:25, 66.47it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 331/2000 [00:04<00:22, 72.86it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:21, 78.66it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:04<00:19, 82.87it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:19, 85.01it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:05<00:18, 87.80it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:18, 89.81it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 90.65it/s, train_loss=1.8648, val_loss=1.8580]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 90.65it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:32, 48.96it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:05<00:27, 57.38it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:24, 65.32it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:05<00:21, 72.26it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 442/2000 [00:06<00:19, 79.38it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 452/2000 [00:06<00:18, 83.95it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 462/2000 [00:06<00:17, 88.05it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 472/2000 [00:06<00:16, 90.96it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 482/2000 [00:06<00:16, 92.92it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:06<00:16, 90.99it/s, train_loss=1.3229, val_loss=1.3028]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:06<00:16, 90.99it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 502/2000 [00:07<00:31, 47.40it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:07<00:30, 49.59it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 518/2000 [00:07<00:26, 55.10it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 526/2000 [00:07<00:24, 60.11it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:07<00:21, 67.85it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 546/2000 [00:07<00:19, 73.94it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 556/2000 [00:07<00:18, 79.51it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 566/2000 [00:07<00:17, 82.91it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 575/2000 [00:07<00:16, 84.79it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:08<00:16, 87.65it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:16, 87.58it/s, train_loss=0.9163, val_loss=0.9153]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:16, 87.58it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 604/2000 [00:08<00:27, 50.13it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 614/2000 [00:08<00:23, 58.70it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 624/2000 [00:08<00:20, 66.37it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:08<00:18, 72.98it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 644/2000 [00:08<00:17, 78.27it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 654/2000 [00:09<00:16, 81.39it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 664/2000 [00:09<00:15, 84.32it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 674/2000 [00:09<00:15, 87.00it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 684/2000 [00:09<00:14, 89.41it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:09<00:14, 91.02it/s, train_loss=0.7222, val_loss=0.7270]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:09<00:14, 91.02it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 704/2000 [00:09<00:24, 52.51it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 714/2000 [00:09<00:21, 60.69it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 724/2000 [00:10<00:18, 68.10it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:10<00:16, 74.74it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 744/2000 [00:10<00:15, 79.17it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:10<00:15, 82.14it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 763/2000 [00:10<00:14, 83.99it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 773/2000 [00:10<00:14, 86.29it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:10<00:13, 87.95it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 793/2000 [00:10<00:13, 89.35it/s, train_loss=0.6365, val_loss=0.6323]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 793/2000 [00:11<00:13, 89.35it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 803/2000 [00:11<00:23, 51.79it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 813/2000 [00:11<00:19, 60.04it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 823/2000 [00:11<00:17, 67.67it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 833/2000 [00:11<00:15, 74.44it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 842/2000 [00:11<00:15, 76.45it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 851/2000 [00:11<00:15, 75.94it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 860/2000 [00:11<00:15, 75.77it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 869/2000 [00:11<00:14, 77.74it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 879/2000 [00:12<00:13, 82.25it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 889/2000 [00:12<00:13, 84.73it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 899/2000 [00:12<00:12, 86.71it/s, train_loss=0.5910, val_loss=0.5886]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 899/2000 [00:12<00:12, 86.71it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 908/2000 [00:12<00:21, 51.38it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 918/2000 [00:12<00:18, 59.67it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 928/2000 [00:12<00:15, 67.28it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 938/2000 [00:12<00:14, 73.82it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:13<00:13, 78.66it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 958/2000 [00:13<00:12, 82.86it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 968/2000 [00:13<00:11, 86.38it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 978/2000 [00:13<00:11, 89.06it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 988/2000 [00:13<00:11, 89.27it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:13<00:11, 89.12it/s, train_loss=0.5579, val_loss=0.5658]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:13<00:11, 89.12it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1008/2000 [00:13<00:18, 52.78it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1018/2000 [00:14<00:16, 60.98it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1028/2000 [00:14<00:14, 68.49it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:14<00:12, 74.75it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:14<00:11, 80.31it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1058/2000 [00:14<00:11, 84.15it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1068/2000 [00:14<00:10, 85.61it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:14<00:10, 86.81it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1088/2000 [00:14<00:10, 89.27it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:14<00:09, 91.38it/s, train_loss=0.5442, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:15<00:09, 91.38it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1108/2000 [00:15<00:17, 52.35it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1118/2000 [00:15<00:14, 60.09it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1127/2000 [00:15<00:13, 66.12it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1137/2000 [00:15<00:11, 72.09it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1146/2000 [00:15<00:11, 76.22it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1156/2000 [00:15<00:10, 80.22it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1166/2000 [00:15<00:10, 83.24it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1175/2000 [00:16<00:09, 84.76it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1184/2000 [00:16<00:09, 84.95it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:16<00:09, 86.63it/s, train_loss=0.5267, val_loss=0.5275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:16<00:09, 86.63it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1203/2000 [00:16<00:16, 47.87it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1212/2000 [00:16<00:14, 54.76it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1221/2000 [00:16<00:12, 61.43it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:17<00:11, 66.02it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1239/2000 [00:17<00:10, 70.24it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:17<00:10, 74.07it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:17<00:09, 77.27it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1266/2000 [00:17<00:09, 80.03it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1275/2000 [00:17<00:08, 81.87it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1284/2000 [00:17<00:08, 80.85it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1293/2000 [00:17<00:08, 79.38it/s, train_loss=0.5080, val_loss=0.5128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1293/2000 [00:18<00:08, 79.38it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1302/2000 [00:18<00:15, 44.73it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1311/2000 [00:18<00:13, 51.89it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1320/2000 [00:18<00:11, 58.98it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1329/2000 [00:18<00:10, 65.34it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1338/2000 [00:18<00:09, 70.62it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1347/2000 [00:18<00:08, 74.82it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1356/2000 [00:18<00:08, 78.25it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1365/2000 [00:18<00:07, 80.92it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1374/2000 [00:19<00:07, 82.48it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1383/2000 [00:19<00:07, 84.07it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:19<00:07, 84.81it/s, train_loss=0.4951, val_loss=0.5024]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:19<00:07, 84.81it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1401/2000 [00:19<00:13, 46.00it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:19<00:11, 51.85it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1418/2000 [00:19<00:09, 58.79it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1426/2000 [00:19<00:09, 60.05it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1434/2000 [00:20<00:09, 60.94it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1443/2000 [00:20<00:08, 66.60it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1452/2000 [00:20<00:07, 70.66it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1460/2000 [00:20<00:07, 71.58it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1468/2000 [00:20<00:07, 73.02it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:20<00:06, 75.86it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1486/2000 [00:20<00:06, 78.29it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1495/2000 [00:20<00:06, 80.54it/s, train_loss=0.4782, val_loss=0.4916]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1495/2000 [00:21<00:06, 80.54it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1504/2000 [00:21<00:11, 43.17it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1513/2000 [00:21<00:09, 51.06it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1522/2000 [00:21<00:08, 58.47it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1531/2000 [00:21<00:07, 65.04it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1540/2000 [00:21<00:06, 70.62it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1549/2000 [00:21<00:05, 75.34it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1558/2000 [00:21<00:05, 78.79it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1567/2000 [00:21<00:05, 80.84it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1576/2000 [00:22<00:05, 81.38it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1585/2000 [00:22<00:05, 82.82it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:22<00:04, 84.70it/s, train_loss=0.4844, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:22<00:04, 84.70it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1603/2000 [00:22<00:08, 48.00it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1612/2000 [00:22<00:06, 55.66it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1621/2000 [00:22<00:06, 62.75it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1631/2000 [00:22<00:05, 69.75it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:23<00:04, 74.57it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:23<00:04, 77.90it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:23<00:04, 80.83it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1667/2000 [00:23<00:04, 82.79it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1676/2000 [00:23<00:03, 84.59it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1685/2000 [00:23<00:03, 85.99it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:23<00:03, 87.10it/s, train_loss=0.4780, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:24<00:03, 87.10it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1703/2000 [00:24<00:06, 47.83it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1712/2000 [00:24<00:05, 55.43it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1721/2000 [00:24<00:04, 62.48it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1730/2000 [00:24<00:03, 68.64it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1739/2000 [00:24<00:03, 73.60it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1748/2000 [00:24<00:03, 77.66it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1757/2000 [00:24<00:03, 80.66it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1766/2000 [00:24<00:02, 82.97it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1775/2000 [00:24<00:02, 84.40it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1784/2000 [00:24<00:02, 84.94it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:25<00:02, 85.44it/s, train_loss=0.4879, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:25<00:02, 85.44it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1802/2000 [00:25<00:04, 47.76it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1811/2000 [00:25<00:03, 55.11it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1820/2000 [00:25<00:02, 61.95it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1829/2000 [00:25<00:02, 68.14it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:25<00:02, 73.14it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1847/2000 [00:25<00:01, 77.17it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1857/2000 [00:26<00:01, 80.94it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1866/2000 [00:26<00:01, 83.25it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1875/2000 [00:26<00:01, 84.88it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1884/2000 [00:26<00:01, 85.97it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1893/2000 [00:26<00:01, 86.50it/s, train_loss=0.4717, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1893/2000 [00:26<00:01, 86.50it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1902/2000 [00:26<00:02, 42.74it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:27<00:01, 50.16it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1920/2000 [00:27<00:01, 57.47it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1929/2000 [00:27<00:01, 64.18it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1938/2000 [00:27<00:00, 70.17it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1947/2000 [00:27<00:00, 75.13it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1956/2000 [00:27<00:00, 78.65it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1965/2000 [00:27<00:00, 81.58it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1974/2000 [00:27<00:00, 83.51it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1983/2000 [00:27<00:00, 85.15it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 71.23it/s, train_loss=0.4734, val_loss=0.4584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:45,  3.42it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:03, 31.35it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:39, 49.57it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 28/2000 [00:00<00:31, 61.71it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 36/2000 [00:00<00:29, 66.75it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:26, 73.20it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 54/2000 [00:00<00:25, 77.69it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 63/2000 [00:01<00:23, 80.81it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 72/2000 [00:01<00:23, 82.93it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:22, 84.75it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 90/2000 [00:01<00:22, 85.85it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:21, 86.71it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:21, 86.71it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 108/2000 [00:01<00:39, 48.23it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 117/2000 [00:01<00:33, 55.69it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 126/2000 [00:01<00:29, 62.69it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 135/2000 [00:02<00:27, 68.43it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 144/2000 [00:02<00:25, 73.06it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:23, 77.15it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 162/2000 [00:02<00:22, 80.07it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:22, 82.57it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:21, 84.03it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 189/2000 [00:02<00:21, 85.45it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:21, 85.75it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:21, 85.75it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:37, 47.62it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:32, 55.12it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:28, 62.18it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 234/2000 [00:03<00:25, 68.49it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:23, 73.61it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 252/2000 [00:03<00:22, 77.51it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:21, 80.06it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 270/2000 [00:03<00:21, 82.38it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 279/2000 [00:04<00:20, 83.79it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 288/2000 [00:04<00:20, 85.31it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:19, 86.35it/s, train_loss=2.2317, val_loss=2.2334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:19, 86.35it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:04<00:35, 47.31it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:04<00:30, 54.99it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:26, 62.85it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 334/2000 [00:04<00:24, 68.53it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 343/2000 [00:05<00:22, 73.44it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:05<00:21, 77.28it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:05<00:20, 79.92it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 370/2000 [00:05<00:19, 82.56it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 379/2000 [00:05<00:19, 84.31it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 388/2000 [00:05<00:18, 85.54it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 397/2000 [00:05<00:18, 86.24it/s, train_loss=1.7763, val_loss=1.7734]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 397/2000 [00:05<00:18, 86.24it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 406/2000 [00:06<00:32, 48.79it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 415/2000 [00:06<00:28, 56.37it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 424/2000 [00:06<00:24, 63.17it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 433/2000 [00:06<00:22, 69.04it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 442/2000 [00:06<00:21, 74.09it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:06<00:19, 77.99it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 460/2000 [00:06<00:19, 80.74it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 469/2000 [00:06<00:18, 82.91it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 478/2000 [00:06<00:18, 84.13it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 487/2000 [00:06<00:18, 83.73it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:07<00:17, 84.62it/s, train_loss=1.2460, val_loss=1.2630]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:07<00:17, 84.62it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 505/2000 [00:07<00:30, 48.47it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 514/2000 [00:07<00:26, 56.15it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 523/2000 [00:07<00:23, 62.47it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 532/2000 [00:07<00:21, 68.63it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:19, 73.49it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 550/2000 [00:07<00:18, 77.52it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 559/2000 [00:08<00:17, 80.71it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 568/2000 [00:08<00:17, 83.27it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 577/2000 [00:08<00:16, 84.77it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 586/2000 [00:08<00:16, 86.06it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:16, 86.44it/s, train_loss=0.8775, val_loss=0.8943]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:16, 86.44it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 604/2000 [00:08<00:28, 48.84it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 612/2000 [00:08<00:25, 54.54it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 621/2000 [00:09<00:22, 61.52it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 630/2000 [00:09<00:20, 68.00it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 639/2000 [00:09<00:18, 73.30it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 648/2000 [00:09<00:17, 77.49it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:09<00:16, 80.75it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:09<00:16, 82.91it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 675/2000 [00:09<00:15, 84.24it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 684/2000 [00:09<00:15, 85.53it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:09<00:15, 86.13it/s, train_loss=0.7194, val_loss=0.7059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:10<00:15, 86.13it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 702/2000 [00:10<00:27, 47.87it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 711/2000 [00:10<00:23, 55.51it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 720/2000 [00:10<00:20, 62.57it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 729/2000 [00:10<00:18, 68.66it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 738/2000 [00:10<00:17, 73.63it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 747/2000 [00:10<00:16, 77.60it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 756/2000 [00:10<00:15, 80.63it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 765/2000 [00:10<00:14, 82.90it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 774/2000 [00:11<00:14, 84.69it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:11<00:14, 85.65it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:11<00:13, 86.54it/s, train_loss=0.6386, val_loss=0.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:11<00:13, 86.54it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:11<00:24, 48.21it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 810/2000 [00:11<00:21, 55.81it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 819/2000 [00:11<00:18, 62.66it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 827/2000 [00:11<00:17, 65.61it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 835/2000 [00:12<00:17, 66.24it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 843/2000 [00:12<00:17, 67.13it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 851/2000 [00:12<00:16, 69.96it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 860/2000 [00:12<00:15, 74.72it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 869/2000 [00:12<00:14, 78.55it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 878/2000 [00:12<00:13, 81.53it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 887/2000 [00:12<00:13, 83.35it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:12<00:12, 85.11it/s, train_loss=0.5869, val_loss=0.5806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:13<00:12, 85.11it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:13<00:22, 48.19it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 914/2000 [00:13<00:19, 55.77it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 923/2000 [00:13<00:17, 62.08it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 932/2000 [00:13<00:15, 68.25it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 941/2000 [00:13<00:14, 73.45it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 950/2000 [00:13<00:13, 77.19it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 959/2000 [00:13<00:12, 80.31it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 968/2000 [00:13<00:12, 82.72it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 977/2000 [00:13<00:12, 83.96it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 986/2000 [00:14<00:11, 84.83it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:14<00:11, 85.75it/s, train_loss=0.5531, val_loss=0.5470]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:14<00:11, 85.75it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1004/2000 [00:14<00:20, 48.44it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1013/2000 [00:14<00:17, 56.10it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1022/2000 [00:14<00:15, 63.04it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1031/2000 [00:14<00:14, 68.76it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1040/2000 [00:14<00:13, 73.47it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1049/2000 [00:15<00:12, 77.36it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1058/2000 [00:15<00:11, 80.35it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1067/2000 [00:15<00:11, 82.41it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1076/2000 [00:15<00:10, 84.34it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1085/2000 [00:15<00:10, 85.29it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1094/2000 [00:15<00:10, 86.20it/s, train_loss=0.5280, val_loss=0.5355]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1094/2000 [00:15<00:10, 86.20it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1103/2000 [00:15<00:18, 48.34it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1112/2000 [00:16<00:15, 56.06it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1121/2000 [00:16<00:13, 62.98it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1130/2000 [00:16<00:12, 68.78it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1139/2000 [00:16<00:11, 73.73it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1148/2000 [00:16<00:10, 77.71it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1157/2000 [00:16<00:10, 80.27it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1166/2000 [00:16<00:10, 82.66it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1175/2000 [00:16<00:09, 84.39it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1184/2000 [00:16<00:09, 85.57it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1193/2000 [00:16<00:09, 85.18it/s, train_loss=0.5176, val_loss=0.5311]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1193/2000 [00:17<00:09, 85.18it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1202/2000 [00:17<00:16, 47.74it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1211/2000 [00:17<00:14, 55.24it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1220/2000 [00:17<00:12, 62.09it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1229/2000 [00:17<00:11, 68.34it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1238/2000 [00:17<00:10, 73.40it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1247/2000 [00:17<00:09, 76.57it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1256/2000 [00:17<00:09, 79.37it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1265/2000 [00:18<00:09, 81.57it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1274/2000 [00:18<00:08, 83.26it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1283/2000 [00:18<00:08, 84.89it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:18<00:08, 84.56it/s, train_loss=0.5020, val_loss=0.5132]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:18<00:08, 84.56it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1301/2000 [00:18<00:14, 46.70it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:18<00:12, 54.22it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:18<00:11, 61.48it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:19<00:09, 67.40it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1337/2000 [00:19<00:09, 72.67it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1346/2000 [00:19<00:08, 76.46it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1355/2000 [00:19<00:08, 79.80it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1364/2000 [00:19<00:07, 82.10it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1373/2000 [00:19<00:07, 84.00it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1382/2000 [00:19<00:07, 85.32it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:19<00:07, 85.87it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1400/2000 [00:19<00:06, 86.64it/s, train_loss=0.4827, val_loss=0.5080]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1400/2000 [00:20<00:06, 86.64it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:20<00:12, 48.65it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1418/2000 [00:20<00:10, 55.86it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1427/2000 [00:20<00:09, 63.02it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1436/2000 [00:20<00:08, 68.93it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1445/2000 [00:20<00:07, 73.51it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1454/2000 [00:20<00:07, 77.03it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1463/2000 [00:20<00:06, 80.35it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1472/2000 [00:20<00:06, 82.82it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1481/2000 [00:21<00:06, 84.47it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1490/2000 [00:21<00:05, 85.85it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:21<00:05, 86.65it/s, train_loss=0.4801, val_loss=0.4908]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:21<00:05, 86.65it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:21<00:10, 48.64it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1517/2000 [00:21<00:08, 56.26it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1526/2000 [00:21<00:07, 63.38it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1535/2000 [00:21<00:06, 68.56it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1544/2000 [00:22<00:06, 73.67it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1553/2000 [00:22<00:05, 77.47it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1562/2000 [00:22<00:05, 80.40it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1571/2000 [00:22<00:05, 82.32it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1580/2000 [00:22<00:05, 83.77it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:22<00:04, 84.04it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1598/2000 [00:22<00:04, 84.71it/s, train_loss=0.4799, val_loss=0.4865]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1598/2000 [00:22<00:04, 84.71it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1607/2000 [00:23<00:08, 47.68it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1616/2000 [00:23<00:06, 55.24it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1625/2000 [00:23<00:06, 62.27it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1634/2000 [00:23<00:05, 68.57it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1643/2000 [00:23<00:04, 73.34it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1652/2000 [00:23<00:04, 77.30it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1661/2000 [00:23<00:04, 80.43it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1670/2000 [00:23<00:04, 82.43it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1679/2000 [00:23<00:03, 83.69it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1688/2000 [00:23<00:03, 84.98it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:24<00:03, 86.05it/s, train_loss=0.4752, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:24<00:03, 86.05it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1706/2000 [00:24<00:06, 47.75it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:24<00:05, 52.38it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1721/2000 [00:24<00:04, 55.97it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1729/2000 [00:24<00:04, 60.42it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1738/2000 [00:24<00:03, 67.00it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1747/2000 [00:24<00:03, 72.33it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1756/2000 [00:25<00:03, 76.96it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1765/2000 [00:25<00:02, 80.10it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1774/2000 [00:25<00:02, 82.51it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1783/2000 [00:25<00:02, 84.43it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:25<00:02, 85.49it/s, train_loss=0.4778, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:25<00:02, 85.49it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1801/2000 [00:25<00:04, 47.13it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1810/2000 [00:25<00:03, 54.90it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1819/2000 [00:26<00:02, 62.02it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1829/2000 [00:26<00:02, 69.05it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:26<00:02, 73.70it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1847/2000 [00:26<00:01, 77.60it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:26<00:01, 80.67it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1865/2000 [00:26<00:01, 82.88it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1874/2000 [00:26<00:01, 84.52it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1883/2000 [00:26<00:01, 85.27it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:26<00:01, 85.18it/s, train_loss=0.4652, val_loss=0.4772]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:27<00:01, 85.18it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:27<00:02, 48.16it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1910/2000 [00:27<00:01, 55.48it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:27<00:01, 62.54it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1928/2000 [00:27<00:01, 68.71it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1937/2000 [00:27<00:00, 73.67it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1946/2000 [00:27<00:00, 77.52it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1955/2000 [00:27<00:00, 80.48it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1964/2000 [00:28<00:00, 82.58it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1973/2000 [00:28<00:00, 84.18it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1982/2000 [00:28<00:00, 85.19it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|█████████▉| 1991/2000 [00:28<00:00, 86.09it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 70.36it/s, train_loss=0.4651, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:48<00:00, 83.97s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR:  67%|██████▋   | 2/3 [05:44<02:51, 171.39s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0005, beta2=0.999, avg_val_loss=0.4694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:13,  3.26it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:04, 30.74it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 20/2000 [00:00<00:38, 51.03it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 30/2000 [00:00<00:30, 64.54it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 40/2000 [00:00<00:26, 73.17it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▎         | 50/2000 [00:00<00:24, 78.91it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 59/2000 [00:00<00:23, 81.94it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 68/2000 [00:01<00:22, 84.07it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 77/2000 [00:01<00:22, 85.81it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 86/2000 [00:01<00:22, 86.93it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:21, 87.84it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:21, 87.84it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 104/2000 [00:01<00:39, 48.56it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 113/2000 [00:01<00:33, 56.38it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 123/2000 [00:01<00:29, 64.28it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 133/2000 [00:02<00:26, 70.80it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:02<00:24, 75.94it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:23, 80.02it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 162/2000 [00:02<00:22, 82.45it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 172/2000 [00:02<00:21, 84.62it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 182/2000 [00:02<00:21, 86.37it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:20, 87.35it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 200/2000 [00:02<00:20, 87.98it/s, train_loss=2.3304, val_loss=2.3326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 200/2000 [00:03<00:20, 87.98it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 209/2000 [00:03<00:35, 50.02it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 219/2000 [00:03<00:30, 58.28it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 229/2000 [00:03<00:27, 65.54it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 238/2000 [00:03<00:24, 70.73it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 247/2000 [00:03<00:23, 74.99it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 256/2000 [00:03<00:22, 78.67it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 266/2000 [00:03<00:21, 82.05it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 276/2000 [00:03<00:20, 84.58it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 286/2000 [00:04<00:19, 86.49it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:04<00:19, 86.57it/s, train_loss=1.8182, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:04<00:19, 86.57it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 304/2000 [00:04<00:35, 47.74it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 314/2000 [00:04<00:30, 56.08it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 323/2000 [00:04<00:26, 62.62it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 332/2000 [00:04<00:24, 68.62it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:22, 73.72it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 350/2000 [00:05<00:21, 77.74it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 359/2000 [00:05<00:20, 80.98it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 368/2000 [00:05<00:19, 82.71it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 377/2000 [00:05<00:19, 84.54it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 386/2000 [00:05<00:18, 85.94it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:18, 87.17it/s, train_loss=1.4685, val_loss=1.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:18, 87.17it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 405/2000 [00:05<00:32, 49.72it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 414/2000 [00:06<00:27, 56.65it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 423/2000 [00:06<00:24, 63.38it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 432/2000 [00:06<00:22, 69.45it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 442/2000 [00:06<00:20, 74.98it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:06<00:19, 78.78it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 461/2000 [00:06<00:18, 82.17it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 470/2000 [00:06<00:18, 84.09it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 479/2000 [00:06<00:18, 82.75it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 488/2000 [00:06<00:17, 84.01it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:06<00:17, 84.90it/s, train_loss=1.1706, val_loss=1.1980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:07<00:17, 84.90it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:07<00:30, 48.84it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 515/2000 [00:07<00:26, 56.47it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 524/2000 [00:07<00:23, 63.48it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 533/2000 [00:07<00:21, 69.61it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 542/2000 [00:07<00:19, 74.63it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:07<00:18, 78.18it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 560/2000 [00:07<00:17, 80.98it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 570/2000 [00:08<00:17, 83.72it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 579/2000 [00:08<00:16, 85.47it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 589/2000 [00:08<00:16, 86.93it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:08<00:15, 87.73it/s, train_loss=0.8376, val_loss=0.8423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:08<00:15, 87.73it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 607/2000 [00:08<00:28, 49.71it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 616/2000 [00:08<00:24, 57.03it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 626/2000 [00:08<00:21, 64.67it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 636/2000 [00:09<00:19, 70.98it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:09<00:17, 76.00it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 655/2000 [00:09<00:16, 79.45it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 665/2000 [00:09<00:15, 83.64it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 675/2000 [00:09<00:15, 87.80it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 686/2000 [00:09<00:14, 91.57it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:09<00:13, 93.77it/s, train_loss=0.6524, val_loss=0.6395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:09<00:13, 93.77it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:10<00:24, 53.32it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 717/2000 [00:10<00:20, 62.75it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 727/2000 [00:10<00:18, 70.25it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 737/2000 [00:10<00:16, 76.95it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 747/2000 [00:10<00:15, 81.98it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 757/2000 [00:10<00:14, 85.94it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 767/2000 [00:10<00:13, 89.64it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:10<00:13, 92.48it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 787/2000 [00:10<00:12, 94.01it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:10<00:12, 95.59it/s, train_loss=0.5867, val_loss=0.5853]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:11<00:12, 95.59it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 807/2000 [00:11<00:22, 53.26it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 818/2000 [00:11<00:18, 62.66it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 829/2000 [00:11<00:16, 71.03it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 840/2000 [00:11<00:14, 78.25it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 851/2000 [00:11<00:13, 83.96it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:11<00:12, 88.36it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 873/2000 [00:12<00:12, 91.96it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 883/2000 [00:12<00:11, 93.64it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:12<00:11, 95.66it/s, train_loss=0.5392, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:12<00:11, 95.66it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 904/2000 [00:12<00:20, 54.70it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 914/2000 [00:12<00:17, 62.78it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 924/2000 [00:12<00:15, 70.38it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 935/2000 [00:12<00:13, 77.90it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 946/2000 [00:13<00:12, 83.78it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 956/2000 [00:13<00:11, 87.68it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:13<00:11, 90.88it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 976/2000 [00:13<00:11, 92.99it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 987/2000 [00:13<00:10, 95.44it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:13<00:10, 96.48it/s, train_loss=0.5366, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:13<00:10, 96.48it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1007/2000 [00:13<00:18, 53.93it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1017/2000 [00:14<00:15, 62.32it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1028/2000 [00:14<00:13, 71.01it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:14<00:12, 77.30it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:14<00:11, 82.82it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1059/2000 [00:14<00:10, 87.71it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1070/2000 [00:14<00:10, 91.43it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1080/2000 [00:14<00:09, 93.44it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1090/2000 [00:14<00:09, 95.14it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:14<00:09, 96.33it/s, train_loss=0.5185, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:15<00:09, 96.33it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:15<00:16, 53.62it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:15<00:14, 62.09it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1130/2000 [00:15<00:12, 69.87it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1140/2000 [00:15<00:11, 76.71it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1151/2000 [00:15<00:10, 83.19it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1162/2000 [00:15<00:09, 88.26it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1173/2000 [00:15<00:08, 91.91it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1183/2000 [00:15<00:08, 93.68it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1193/2000 [00:16<00:08, 95.40it/s, train_loss=0.4973, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1193/2000 [00:16<00:08, 95.40it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1203/2000 [00:16<00:14, 54.01it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1214/2000 [00:16<00:12, 63.38it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1224/2000 [00:16<00:10, 70.90it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1235/2000 [00:16<00:09, 78.10it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1245/2000 [00:16<00:09, 83.32it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1256/2000 [00:16<00:08, 87.97it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:17<00:08, 91.46it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1277/2000 [00:17<00:07, 93.72it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1287/2000 [00:17<00:07, 95.39it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:17<00:07, 96.82it/s, train_loss=0.4774, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:17<00:07, 96.82it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1308/2000 [00:17<00:12, 54.29it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:17<00:10, 62.56it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:17<00:09, 69.97it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1338/2000 [00:18<00:08, 76.62it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1349/2000 [00:18<00:07, 82.92it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1359/2000 [00:18<00:07, 87.10it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:18<00:06, 90.28it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:18<00:06, 92.86it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:18<00:06, 94.64it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:18<00:06, 95.95it/s, train_loss=0.4855, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:18<00:06, 95.95it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:19<00:11, 53.26it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1419/2000 [00:19<00:09, 61.85it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:19<00:08, 69.59it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1439/2000 [00:19<00:07, 76.41it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▎  | 1450/2000 [00:19<00:06, 82.86it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1461/2000 [00:19<00:06, 87.64it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1472/2000 [00:19<00:05, 91.23it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1482/2000 [00:19<00:05, 93.02it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1493/2000 [00:19<00:05, 95.52it/s, train_loss=0.4808, val_loss=0.4838]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1493/2000 [00:20<00:05, 95.52it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1503/2000 [00:20<00:09, 54.15it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1513/2000 [00:20<00:07, 62.49it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1523/2000 [00:20<00:06, 70.16it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1533/2000 [00:20<00:06, 76.91it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1544/2000 [00:20<00:05, 82.98it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:20<00:05, 87.26it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:20<00:04, 90.39it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1575/2000 [00:21<00:04, 93.33it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1586/2000 [00:21<00:04, 95.58it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:21<00:04, 96.77it/s, train_loss=0.4941, val_loss=0.4927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:21<00:04, 96.77it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1606/2000 [00:21<00:07, 54.08it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1616/2000 [00:21<00:06, 62.44it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1627/2000 [00:21<00:05, 70.83it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1638/2000 [00:21<00:04, 78.12it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:22<00:04, 83.87it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1659/2000 [00:22<00:03, 87.88it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1670/2000 [00:22<00:03, 91.34it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1680/2000 [00:22<00:03, 93.53it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:22<00:03, 95.68it/s, train_loss=0.4802, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:22<00:03, 95.68it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:22<00:05, 53.96it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1710/2000 [00:22<00:04, 60.21it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1720/2000 [00:23<00:04, 68.20it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1730/2000 [00:23<00:03, 74.79it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1740/2000 [00:23<00:03, 80.29it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1750/2000 [00:23<00:02, 84.76it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1761/2000 [00:23<00:02, 89.16it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1771/2000 [00:23<00:02, 92.04it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1782/2000 [00:23<00:02, 94.59it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:23<00:02, 95.82it/s, train_loss=0.4784, val_loss=0.4807]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:24<00:02, 95.82it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1802/2000 [00:24<00:03, 53.64it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1812/2000 [00:24<00:03, 61.30it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1822/2000 [00:24<00:02, 69.16it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1832/2000 [00:24<00:02, 74.50it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1842/2000 [00:24<00:01, 80.55it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1853/2000 [00:24<00:01, 86.09it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1863/2000 [00:24<00:01, 89.62it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1873/2000 [00:24<00:01, 92.07it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1884/2000 [00:25<00:01, 94.62it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1895/2000 [00:25<00:01, 96.70it/s, train_loss=0.4668, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1895/2000 [00:25<00:01, 96.70it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1905/2000 [00:25<00:01, 54.30it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1916/2000 [00:25<00:01, 63.35it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1927/2000 [00:25<00:01, 71.61it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1938/2000 [00:25<00:00, 78.64it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1949/2000 [00:25<00:00, 84.18it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1960/2000 [00:26<00:00, 88.57it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1970/2000 [00:26<00:00, 91.24it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1981/2000 [00:26<00:00, 93.79it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 75.64it/s, train_loss=0.4624, val_loss=0.4667]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:10,  3.63it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:54, 36.37it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:34, 57.15it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:27, 70.42it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 42/2000 [00:00<00:24, 80.51it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 52/2000 [00:00<00:22, 86.12it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 62/2000 [00:00<00:21, 89.67it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 73/2000 [00:01<00:20, 93.06it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 83/2000 [00:01<00:20, 95.02it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:20, 94.48it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:20, 94.48it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 103/2000 [00:01<00:36, 51.98it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 113/2000 [00:01<00:31, 60.42it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 123/2000 [00:01<00:27, 68.51it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 133/2000 [00:01<00:24, 75.39it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:02<00:22, 81.10it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:21, 85.31it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 163/2000 [00:02<00:20, 88.72it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 173/2000 [00:02<00:20, 90.64it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:19, 91.67it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:02<00:19, 92.95it/s, train_loss=2.3622, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:02<00:19, 92.95it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 203/2000 [00:02<00:34, 51.81it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 213/2000 [00:03<00:29, 60.17it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 223/2000 [00:03<00:26, 68.19it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:03<00:23, 75.08it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:21, 80.58it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 253/2000 [00:03<00:20, 84.90it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 263/2000 [00:03<00:19, 88.37it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 273/2000 [00:03<00:19, 90.58it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 283/2000 [00:03<00:18, 92.19it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 293/2000 [00:03<00:18, 92.76it/s, train_loss=1.8786, val_loss=1.8843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 293/2000 [00:04<00:18, 92.76it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 303/2000 [00:04<00:32, 52.59it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 313/2000 [00:04<00:27, 60.71it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 323/2000 [00:04<00:24, 68.50it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 333/2000 [00:04<00:22, 75.33it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 343/2000 [00:04<00:20, 80.95it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 353/2000 [00:04<00:19, 84.93it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:04<00:18, 88.25it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 373/2000 [00:04<00:17, 91.14it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 383/2000 [00:05<00:17, 92.83it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:05<00:17, 94.06it/s, train_loss=1.4048, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:05<00:17, 94.06it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 403/2000 [00:05<00:31, 51.12it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 413/2000 [00:05<00:26, 59.71it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 423/2000 [00:05<00:23, 67.32it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 433/2000 [00:05<00:21, 74.16it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 443/2000 [00:05<00:19, 79.58it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 453/2000 [00:06<00:18, 84.41it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 463/2000 [00:06<00:17, 88.28it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:16, 90.81it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 483/2000 [00:06<00:16, 92.37it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:16, 92.90it/s, train_loss=0.9221, val_loss=0.9072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:16, 92.90it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 503/2000 [00:06<00:29, 51.19it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 513/2000 [00:06<00:25, 59.21it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 523/2000 [00:07<00:22, 66.49it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 533/2000 [00:07<00:19, 73.45it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 543/2000 [00:07<00:18, 79.27it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 553/2000 [00:07<00:17, 83.57it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 563/2000 [00:07<00:16, 87.04it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 573/2000 [00:07<00:15, 89.30it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 583/2000 [00:07<00:15, 91.41it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:15, 93.42it/s, train_loss=0.6810, val_loss=0.6808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:08<00:15, 93.42it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:08<00:26, 52.24it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 613/2000 [00:08<00:22, 60.49it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:08<00:20, 68.13it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 633/2000 [00:08<00:18, 74.99it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 643/2000 [00:08<00:16, 80.16it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 653/2000 [00:08<00:15, 84.93it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 664/2000 [00:08<00:14, 89.38it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 674/2000 [00:08<00:14, 91.93it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 684/2000 [00:09<00:14, 93.99it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:09<00:13, 95.46it/s, train_loss=0.5889, val_loss=0.5900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:09<00:13, 95.46it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 704/2000 [00:09<00:24, 53.49it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 715/2000 [00:09<00:20, 62.90it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 725/2000 [00:09<00:18, 70.33it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 735/2000 [00:09<00:16, 76.77it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 746/2000 [00:09<00:15, 83.04it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 756/2000 [00:10<00:14, 87.27it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 766/2000 [00:10<00:13, 90.32it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 776/2000 [00:10<00:13, 92.90it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 786/2000 [00:10<00:12, 94.84it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:10<00:12, 95.77it/s, train_loss=0.5525, val_loss=0.5515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:10<00:12, 95.77it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 806/2000 [00:10<00:22, 53.39it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 816/2000 [00:10<00:19, 61.85it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 826/2000 [00:11<00:16, 69.66it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 836/2000 [00:11<00:15, 76.23it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 846/2000 [00:11<00:14, 81.04it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 856/2000 [00:11<00:13, 85.33it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 866/2000 [00:11<00:12, 88.46it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 876/2000 [00:11<00:12, 90.84it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:11<00:12, 91.46it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:11<00:11, 92.15it/s, train_loss=0.5308, val_loss=0.5262]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:12<00:11, 92.15it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 906/2000 [00:12<00:21, 52.07it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 916/2000 [00:12<00:17, 60.66it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 926/2000 [00:12<00:15, 68.55it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 936/2000 [00:12<00:14, 75.13it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 946/2000 [00:12<00:13, 80.68it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 956/2000 [00:12<00:12, 85.45it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 967/2000 [00:12<00:11, 89.81it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 977/2000 [00:12<00:11, 92.45it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 988/2000 [00:12<00:10, 94.91it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:13<00:10, 96.25it/s, train_loss=0.5412, val_loss=0.5434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:13<00:10, 96.25it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1008/2000 [00:13<00:18, 53.61it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1018/2000 [00:13<00:15, 62.10it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1028/2000 [00:13<00:13, 69.92it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:13<00:12, 76.71it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1049/2000 [00:13<00:11, 82.77it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1059/2000 [00:13<00:10, 87.17it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1069/2000 [00:14<00:10, 90.57it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1079/2000 [00:14<00:09, 92.99it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1089/2000 [00:14<00:09, 94.79it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:14<00:09, 96.50it/s, train_loss=0.5133, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:14<00:09, 96.50it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:14<00:16, 54.25it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:14<00:14, 62.53it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1130/2000 [00:14<00:12, 70.12it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1140/2000 [00:15<00:11, 76.44it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▊    | 1150/2000 [00:15<00:10, 82.16it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:15<00:09, 87.47it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1171/2000 [00:15<00:09, 90.20it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1181/2000 [00:15<00:08, 92.88it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:15<00:08, 93.94it/s, train_loss=0.5162, val_loss=0.5197]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:15<00:08, 93.94it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:15<00:15, 52.91it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1211/2000 [00:16<00:12, 61.48it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1221/2000 [00:16<00:11, 69.36it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1232/2000 [00:16<00:09, 76.98it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1242/2000 [00:16<00:09, 82.35it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1252/2000 [00:16<00:08, 86.04it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1262/2000 [00:16<00:08, 89.66it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1273/2000 [00:16<00:07, 93.05it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1283/2000 [00:16<00:07, 94.91it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1293/2000 [00:16<00:07, 95.48it/s, train_loss=0.5054, val_loss=0.5139]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1293/2000 [00:17<00:07, 95.48it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1303/2000 [00:17<00:13, 53.15it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1313/2000 [00:17<00:11, 61.75it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1323/2000 [00:17<00:09, 69.36it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1333/2000 [00:17<00:08, 76.20it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1343/2000 [00:17<00:08, 81.80it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1353/2000 [00:17<00:07, 85.96it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1363/2000 [00:17<00:07, 89.55it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1373/2000 [00:18<00:06, 91.29it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1383/2000 [00:18<00:06, 92.64it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1394/2000 [00:18<00:06, 94.89it/s, train_loss=0.4769, val_loss=0.4837]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1394/2000 [00:18<00:06, 94.89it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1404/2000 [00:18<00:11, 53.52it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1414/2000 [00:18<00:09, 61.57it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1424/2000 [00:18<00:08, 69.29it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1434/2000 [00:18<00:07, 76.16it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1444/2000 [00:19<00:06, 81.78it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1454/2000 [00:19<00:06, 85.87it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1464/2000 [00:19<00:05, 89.58it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1474/2000 [00:19<00:05, 92.41it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1484/2000 [00:19<00:05, 94.28it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1494/2000 [00:19<00:05, 95.71it/s, train_loss=0.4753, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1494/2000 [00:19<00:05, 95.71it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1504/2000 [00:19<00:09, 53.54it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1514/2000 [00:19<00:07, 62.18it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1524/2000 [00:20<00:06, 69.69it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1534/2000 [00:20<00:06, 76.47it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1544/2000 [00:20<00:05, 81.83it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:20<00:05, 86.26it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:20<00:04, 89.56it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1574/2000 [00:20<00:04, 92.37it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1584/2000 [00:20<00:04, 93.88it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 94.34it/s, train_loss=0.4847, val_loss=0.4849]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:21<00:04, 94.34it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:21<00:07, 52.91it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1615/2000 [00:21<00:06, 62.07it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1625/2000 [00:21<00:05, 69.68it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:21<00:04, 75.95it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:21<00:04, 81.37it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1655/2000 [00:21<00:04, 85.08it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1665/2000 [00:21<00:03, 88.93it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1675/2000 [00:21<00:03, 91.69it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1685/2000 [00:22<00:03, 93.37it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:22<00:03, 93.87it/s, train_loss=0.4644, val_loss=0.4646]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:22<00:03, 93.87it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1705/2000 [00:22<00:05, 53.25it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1715/2000 [00:22<00:04, 61.48it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:22<00:04, 68.47it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1734/2000 [00:22<00:03, 73.06it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:22<00:03, 79.45it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:23<00:02, 84.08it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:23<00:02, 88.02it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1774/2000 [00:23<00:02, 90.09it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1784/2000 [00:23<00:02, 92.34it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:23<00:02, 93.87it/s, train_loss=0.4743, val_loss=0.4900]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:23<00:02, 93.87it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:23<00:03, 52.74it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1814/2000 [00:23<00:03, 61.04it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1824/2000 [00:24<00:02, 68.98it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:24<00:02, 75.09it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1844/2000 [00:24<00:01, 80.90it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1854/2000 [00:24<00:01, 85.11it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1864/2000 [00:24<00:01, 88.57it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1874/2000 [00:24<00:01, 90.74it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1884/2000 [00:24<00:01, 93.31it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:24<00:01, 95.13it/s, train_loss=0.4760, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:25<00:01, 95.13it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1904/2000 [00:25<00:01, 53.47it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1915/2000 [00:25<00:01, 62.97it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1925/2000 [00:25<00:01, 70.25it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1935/2000 [00:25<00:00, 76.91it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1945/2000 [00:25<00:00, 82.35it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1955/2000 [00:25<00:00, 86.42it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1966/2000 [00:25<00:00, 90.46it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1976/2000 [00:25<00:00, 92.79it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1986/2000 [00:25<00:00, 94.59it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 76.69it/s, train_loss=0.4864, val_loss=0.4737]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:21,  3.56it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:55, 36.05it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:35, 55.07it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:28, 68.43it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 78.07it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:23, 84.58it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 62/2000 [00:00<00:21, 89.84it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 72/2000 [00:01<00:20, 92.24it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 82/2000 [00:01<00:20, 93.92it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 92/2000 [00:01<00:19, 95.46it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 92/2000 [00:01<00:19, 95.46it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 102/2000 [00:01<00:35, 52.84it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 112/2000 [00:01<00:30, 61.31it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 122/2000 [00:01<00:27, 69.41it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 132/2000 [00:01<00:24, 76.06it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 142/2000 [00:02<00:23, 80.06it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 152/2000 [00:02<00:21, 84.45it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 162/2000 [00:02<00:20, 87.58it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 172/2000 [00:02<00:20, 90.55it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:19, 93.53it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:02<00:18, 95.13it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:02<00:18, 95.13it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 203/2000 [00:02<00:33, 53.50it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 214/2000 [00:03<00:28, 62.99it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:24, 71.23it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 235/2000 [00:03<00:22, 77.64it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 245/2000 [00:03<00:21, 83.01it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 256/2000 [00:03<00:19, 87.77it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 267/2000 [00:03<00:18, 91.55it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 277/2000 [00:03<00:18, 93.72it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 287/2000 [00:03<00:17, 95.21it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:03<00:17, 96.65it/s, train_loss=1.8445, val_loss=1.8503]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:17, 96.65it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:30, 54.65it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:26, 63.86it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 329/2000 [00:04<00:23, 71.21it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 340/2000 [00:04<00:21, 78.41it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:04<00:19, 83.96it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 87.64it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:04<00:18, 90.49it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:04<00:17, 92.58it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 94.45it/s, train_loss=1.3815, val_loss=1.3732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 94.45it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:29, 53.70it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:05<00:25, 62.27it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:22, 69.99it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:05<00:20, 76.68it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:05<00:19, 81.91it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:05<00:17, 86.40it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 462/2000 [00:06<00:16, 90.63it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:16, 93.86it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 484/2000 [00:06<00:15, 96.03it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:15, 97.28it/s, train_loss=0.8852, val_loss=0.9059]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:15, 97.28it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 505/2000 [00:06<00:27, 54.63it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 515/2000 [00:06<00:23, 62.85it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 526/2000 [00:06<00:20, 71.17it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 537/2000 [00:07<00:18, 78.30it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 547/2000 [00:07<00:17, 83.31it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 557/2000 [00:07<00:16, 87.05it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 567/2000 [00:07<00:15, 89.68it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 577/2000 [00:07<00:15, 91.68it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 587/2000 [00:07<00:15, 93.82it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 597/2000 [00:07<00:14, 95.04it/s, train_loss=0.6779, val_loss=0.6953]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 597/2000 [00:08<00:14, 95.04it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 607/2000 [00:08<00:25, 53.60it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 617/2000 [00:08<00:22, 62.17it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 627/2000 [00:08<00:19, 69.74it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 638/2000 [00:08<00:17, 77.29it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 648/2000 [00:08<00:16, 81.55it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 658/2000 [00:08<00:15, 86.11it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 668/2000 [00:08<00:14, 89.63it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 678/2000 [00:08<00:14, 92.10it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:08<00:14, 93.46it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 94.30it/s, train_loss=0.5895, val_loss=0.5781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 94.30it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 708/2000 [00:09<00:24, 53.32it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 718/2000 [00:09<00:20, 61.87it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 728/2000 [00:09<00:18, 69.64it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 738/2000 [00:09<00:16, 76.48it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 748/2000 [00:09<00:15, 82.05it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 759/2000 [00:09<00:14, 87.26it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 769/2000 [00:09<00:13, 90.48it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 780/2000 [00:10<00:13, 93.44it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:10<00:12, 94.92it/s, train_loss=0.5423, val_loss=0.5384]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:10<00:12, 94.92it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:21, 54.91it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:10<00:18, 62.98it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 822/2000 [00:10<00:16, 71.42it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:10<00:15, 77.70it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 842/2000 [00:11<00:14, 81.98it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 852/2000 [00:11<00:13, 84.34it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:11<00:13, 87.33it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 872/2000 [00:11<00:12, 90.60it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 882/2000 [00:11<00:12, 92.36it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:11<00:11, 93.38it/s, train_loss=0.5397, val_loss=0.5412]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:11<00:11, 93.38it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 902/2000 [00:11<00:20, 52.78it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 912/2000 [00:12<00:17, 60.98it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 922/2000 [00:12<00:15, 68.62it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 932/2000 [00:12<00:14, 75.46it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 942/2000 [00:12<00:12, 81.41it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 952/2000 [00:12<00:12, 85.77it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 962/2000 [00:12<00:11, 88.78it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 972/2000 [00:12<00:11, 91.75it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 982/2000 [00:12<00:10, 93.81it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:12<00:10, 95.95it/s, train_loss=0.5226, val_loss=0.5181]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:13<00:10, 95.95it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1003/2000 [00:13<00:18, 53.89it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1013/2000 [00:13<00:15, 62.16it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1023/2000 [00:13<00:13, 69.87it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1033/2000 [00:13<00:12, 76.39it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1043/2000 [00:13<00:11, 81.77it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1053/2000 [00:13<00:10, 86.35it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1063/2000 [00:13<00:10, 89.53it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1073/2000 [00:13<00:10, 91.79it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:14<00:09, 93.49it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:14<00:09, 95.00it/s, train_loss=0.5337, val_loss=0.5333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:14<00:09, 95.00it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1103/2000 [00:14<00:16, 53.35it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1114/2000 [00:14<00:14, 62.83it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1125/2000 [00:14<00:12, 71.24it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1136/2000 [00:14<00:11, 78.31it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1147/2000 [00:14<00:10, 84.03it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:15<00:09, 88.79it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:15<00:09, 91.64it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1178/2000 [00:15<00:08, 93.39it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1188/2000 [00:15<00:08, 94.56it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 96.02it/s, train_loss=0.4923, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 96.02it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:15<00:14, 54.19it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1219/2000 [00:15<00:12, 63.40it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:16<00:10, 71.66it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:16<00:09, 77.81it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1251/2000 [00:16<00:08, 83.82it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1261/2000 [00:16<00:08, 87.49it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1271/2000 [00:16<00:08, 90.26it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1282/2000 [00:16<00:07, 93.45it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:16<00:07, 94.74it/s, train_loss=0.4955, val_loss=0.5019]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:17<00:07, 94.74it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1302/2000 [00:17<00:12, 53.97it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1312/2000 [00:17<00:11, 62.17it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1322/2000 [00:17<00:09, 69.88it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1332/2000 [00:17<00:08, 76.57it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1342/2000 [00:17<00:08, 82.21it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:17<00:07, 86.82it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1362/2000 [00:17<00:07, 89.97it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1373/2000 [00:17<00:06, 93.01it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1383/2000 [00:17<00:06, 93.40it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1394/2000 [00:17<00:06, 95.62it/s, train_loss=0.4790, val_loss=0.4968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1394/2000 [00:18<00:06, 95.62it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1404/2000 [00:18<00:10, 54.25it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1414/2000 [00:18<00:09, 62.55it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1424/2000 [00:18<00:08, 70.27it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1435/2000 [00:18<00:07, 77.61it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1445/2000 [00:18<00:06, 82.87it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1456/2000 [00:18<00:06, 87.65it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1466/2000 [00:19<00:05, 90.33it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:19<00:05, 93.28it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1487/2000 [00:19<00:05, 94.93it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 96.57it/s, train_loss=0.4827, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 96.57it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:19<00:09, 54.45it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1518/2000 [00:19<00:07, 62.74it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1529/2000 [00:19<00:06, 71.24it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1539/2000 [00:20<00:05, 77.40it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1550/2000 [00:20<00:05, 83.61it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1561/2000 [00:20<00:04, 88.30it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1571/2000 [00:20<00:04, 91.26it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1582/2000 [00:20<00:04, 93.84it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:20<00:04, 95.16it/s, train_loss=0.4854, val_loss=0.4928]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:20<00:04, 95.16it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1602/2000 [00:20<00:07, 54.12it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1612/2000 [00:21<00:06, 62.47it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1622/2000 [00:21<00:05, 69.75it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1632/2000 [00:21<00:04, 76.53it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1643/2000 [00:21<00:04, 82.76it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1653/2000 [00:21<00:04, 86.12it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1663/2000 [00:21<00:03, 88.91it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:21<00:03, 91.86it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1683/2000 [00:21<00:03, 93.32it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:21<00:03, 94.89it/s, train_loss=0.4811, val_loss=0.4735]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:22<00:03, 94.89it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1703/2000 [00:22<00:05, 53.12it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:22<00:04, 62.43it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1724/2000 [00:22<00:03, 69.94it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1734/2000 [00:22<00:03, 76.48it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:22<00:03, 81.76it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:22<00:02, 85.95it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:22<00:02, 89.51it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1775/2000 [00:22<00:02, 92.76it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1785/2000 [00:23<00:02, 94.33it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1796/2000 [00:23<00:02, 96.60it/s, train_loss=0.4706, val_loss=0.4867]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1796/2000 [00:23<00:02, 96.60it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1806/2000 [00:23<00:03, 54.46it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1817/2000 [00:23<00:02, 63.78it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1827/2000 [00:23<00:02, 71.15it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1837/2000 [00:23<00:02, 77.59it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1848/2000 [00:23<00:01, 83.74it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1859/2000 [00:24<00:01, 88.49it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1870/2000 [00:24<00:01, 91.99it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:24<00:01, 93.84it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:24<00:01, 95.96it/s, train_loss=0.4792, val_loss=0.4949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:24<00:01, 95.96it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:24<00:01, 55.28it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:24<00:01, 63.49it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:24<00:01, 71.92it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1932/2000 [00:25<00:00, 78.27it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1943/2000 [00:25<00:00, 84.23it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1954/2000 [00:25<00:00, 88.64it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1965/2000 [00:25<00:00, 91.89it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1976/2000 [00:25<00:00, 94.58it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:25<00:00, 96.62it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 77.67it/s, train_loss=0.4671, val_loss=0.4783]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:18<01:18, 78.30s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.001, beta2=0.98, avg_val_loss=0.4729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:03,  3.68it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:54, 36.73it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 59.12it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:26, 72.96it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:23, 81.81it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:22, 87.69it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:00<00:21, 91.86it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 77/2000 [00:01<00:20, 94.40it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 88/2000 [00:01<00:19, 96.61it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:19, 98.00it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:19, 98.00it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 109/2000 [00:01<00:34, 55.26it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 119/2000 [00:01<00:29, 63.43it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 130/2000 [00:01<00:26, 71.85it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 140/2000 [00:01<00:23, 78.20it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:21, 84.27it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 162/2000 [00:02<00:20, 88.90it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 172/2000 [00:02<00:19, 91.81it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 182/2000 [00:02<00:19, 93.94it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:02<00:18, 95.95it/s, train_loss=2.3258, val_loss=2.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:02<00:18, 95.95it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 203/2000 [00:02<00:32, 54.74it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 213/2000 [00:02<00:28, 63.06it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 224/2000 [00:03<00:24, 71.46it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 235/2000 [00:03<00:22, 78.58it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 246/2000 [00:03<00:20, 84.42it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 257/2000 [00:03<00:19, 88.79it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 268/2000 [00:03<00:18, 92.36it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 278/2000 [00:03<00:18, 94.20it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 289/2000 [00:03<00:17, 96.44it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:03<00:17, 97.79it/s, train_loss=1.8048, val_loss=1.8152]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:17, 97.79it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:29, 56.49it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 322/2000 [00:04<00:25, 65.26it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 333/2000 [00:04<00:22, 73.19it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 344/2000 [00:04<00:20, 79.91it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 355/2000 [00:04<00:19, 85.25it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 366/2000 [00:04<00:18, 89.28it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 377/2000 [00:04<00:17, 92.56it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 387/2000 [00:04<00:17, 94.32it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 397/2000 [00:05<00:16, 94.40it/s, train_loss=1.4028, val_loss=1.4090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 397/2000 [00:05<00:16, 94.40it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 407/2000 [00:05<00:30, 52.70it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 417/2000 [00:05<00:25, 61.01it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 427/2000 [00:05<00:22, 68.82it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 438/2000 [00:05<00:20, 76.23it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 448/2000 [00:05<00:18, 81.81it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 458/2000 [00:05<00:17, 85.90it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 469/2000 [00:06<00:16, 90.39it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 480/2000 [00:06<00:16, 93.69it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:06<00:16, 94.25it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:06<00:15, 95.66it/s, train_loss=0.9050, val_loss=0.9182]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:06<00:15, 95.66it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:06<00:27, 54.31it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 520/2000 [00:06<00:23, 62.74it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:06<00:20, 71.31it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:18, 77.62it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:07<00:17, 83.02it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 562/2000 [00:07<00:16, 87.52it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 573/2000 [00:07<00:15, 91.16it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 584/2000 [00:07<00:15, 94.08it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:07<00:14, 95.38it/s, train_loss=0.6751, val_loss=0.6795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:07<00:14, 95.38it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 605/2000 [00:07<00:25, 54.45it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 616/2000 [00:08<00:21, 63.38it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 627/2000 [00:08<00:19, 71.71it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 638/2000 [00:08<00:17, 78.90it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 649/2000 [00:08<00:15, 84.66it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 660/2000 [00:08<00:15, 89.26it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 671/2000 [00:08<00:14, 92.79it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 682/2000 [00:08<00:13, 95.41it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:08<00:13, 97.08it/s, train_loss=0.5833, val_loss=0.5699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:09<00:13, 97.08it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 704/2000 [00:09<00:23, 54.96it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 714/2000 [00:09<00:20, 62.88it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 724/2000 [00:09<00:18, 70.33it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 735/2000 [00:09<00:16, 77.73it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 746/2000 [00:09<00:14, 83.76it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 757/2000 [00:09<00:14, 88.49it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 767/2000 [00:09<00:13, 91.26it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 778/2000 [00:09<00:12, 94.18it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 788/2000 [00:10<00:12, 95.56it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:12, 97.47it/s, train_loss=0.5414, val_loss=0.5433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:12, 97.47it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 809/2000 [00:10<00:21, 55.57it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 820/2000 [00:10<00:18, 64.70it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 831/2000 [00:10<00:16, 72.83it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 841/2000 [00:10<00:14, 78.85it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 851/2000 [00:10<00:13, 83.03it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 861/2000 [00:11<00:13, 86.67it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 871/2000 [00:11<00:12, 89.95it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 881/2000 [00:11<00:12, 92.47it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 94.20it/s, train_loss=0.5033, val_loss=0.5099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 94.20it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 901/2000 [00:11<00:20, 53.87it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:11<00:17, 62.08it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:11<00:15, 69.32it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 931/2000 [00:12<00:14, 75.93it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 941/2000 [00:12<00:13, 81.41it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 952/2000 [00:12<00:12, 86.72it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 963/2000 [00:12<00:11, 90.26it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 974/2000 [00:12<00:10, 93.29it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:12<00:10, 94.69it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:12<00:10, 96.88it/s, train_loss=0.5105, val_loss=0.5078]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:13<00:10, 96.88it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:13<00:18, 54.97it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1015/2000 [00:13<00:15, 63.00it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1026/2000 [00:13<00:13, 71.30it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1037/2000 [00:13<00:12, 78.55it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:13<00:11, 84.31it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1059/2000 [00:13<00:10, 88.86it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1070/2000 [00:13<00:10, 92.48it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:13<00:09, 95.06it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:13<00:09, 96.29it/s, train_loss=0.5093, val_loss=0.5140]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:14<00:09, 96.29it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:14<00:16, 53.90it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:14<00:14, 62.11it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1122/2000 [00:14<00:12, 70.46it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1133/2000 [00:14<00:11, 77.66it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1143/2000 [00:14<00:10, 82.86it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1154/2000 [00:14<00:09, 87.64it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1165/2000 [00:14<00:09, 91.20it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1176/2000 [00:15<00:08, 93.96it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1186/2000 [00:15<00:08, 95.48it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1196/2000 [00:15<00:08, 96.16it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1196/2000 [00:15<00:08, 96.16it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1206/2000 [00:15<00:14, 54.67it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1216/2000 [00:15<00:12, 63.06it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1226/2000 [00:15<00:10, 70.74it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:15<00:09, 78.09it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1247/2000 [00:16<00:09, 83.40it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:16<00:08, 88.41it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1268/2000 [00:16<00:08, 91.36it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:16<00:07, 94.26it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:16<00:07, 95.60it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 97.22it/s, train_loss=0.4733, val_loss=0.4795]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 97.22it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:16<00:12, 55.43it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:17<00:10, 64.39it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1332/2000 [00:17<00:09, 72.57it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1343/2000 [00:17<00:08, 79.56it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1353/2000 [00:17<00:07, 84.38it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1364/2000 [00:17<00:07, 89.11it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1374/2000 [00:17<00:06, 91.81it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1385/2000 [00:17<00:06, 94.29it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1395/2000 [00:17<00:06, 95.08it/s, train_loss=0.4683, val_loss=0.4693]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1395/2000 [00:18<00:06, 95.08it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1405/2000 [00:18<00:10, 54.29it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1415/2000 [00:18<00:09, 62.53it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1425/2000 [00:18<00:08, 70.29it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1436/2000 [00:18<00:07, 77.73it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:18<00:06, 83.68it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1458/2000 [00:18<00:06, 88.25it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1468/2000 [00:18<00:05, 90.89it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1478/2000 [00:18<00:05, 93.24it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1488/2000 [00:19<00:05, 95.07it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 96.13it/s, train_loss=0.4665, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 96.13it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:19<00:09, 54.16it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1518/2000 [00:19<00:07, 62.47it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1528/2000 [00:19<00:06, 70.10it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1539/2000 [00:19<00:05, 77.56it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1550/2000 [00:19<00:05, 83.55it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1561/2000 [00:20<00:04, 88.44it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1572/2000 [00:20<00:04, 92.16it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1583/2000 [00:20<00:04, 94.64it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.46it/s, train_loss=0.4760, val_loss=0.4778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.46it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:20<00:07, 54.78it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1614/2000 [00:20<00:06, 62.86it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1625/2000 [00:20<00:05, 71.23it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1636/2000 [00:21<00:04, 78.41it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1647/2000 [00:21<00:04, 84.39it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:21<00:03, 88.59it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:21<00:03, 92.16it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1680/2000 [00:21<00:03, 94.71it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:21<00:03, 96.45it/s, train_loss=0.4634, val_loss=0.4621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:21<00:03, 96.45it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:21<00:05, 54.20it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1711/2000 [00:22<00:04, 62.13it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1721/2000 [00:22<00:04, 69.53it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1731/2000 [00:22<00:03, 76.25it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1742/2000 [00:22<00:03, 82.46it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1752/2000 [00:22<00:02, 86.77it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:22<00:02, 90.03it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:22<00:02, 92.32it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1782/2000 [00:22<00:02, 94.34it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:22<00:02, 95.62it/s, train_loss=0.4816, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:23<00:02, 95.62it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1802/2000 [00:23<00:03, 52.69it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1812/2000 [00:23<00:03, 61.10it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1823/2000 [00:23<00:02, 69.81it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:23<00:02, 77.17it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1845/2000 [00:23<00:01, 83.06it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:23<00:01, 87.65it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1866/2000 [00:23<00:01, 90.78it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1877/2000 [00:24<00:01, 93.69it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1887/2000 [00:24<00:01, 95.36it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:24<00:01, 97.17it/s, train_loss=0.4572, val_loss=0.4745]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:24<00:01, 97.17it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1908/2000 [00:24<00:01, 55.27it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:24<00:01, 64.26it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1928/2000 [00:24<00:01, 68.18it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1938/2000 [00:24<00:00, 73.56it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1948/2000 [00:25<00:00, 78.06it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1958/2000 [00:25<00:00, 81.57it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1968/2000 [00:25<00:00, 85.31it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1978/2000 [00:25<00:00, 76.68it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:25<00:00, 79.14it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 77.82it/s, train_loss=0.4615, val_loss=0.4632]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<08:55,  3.73it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:54, 36.17it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:35, 55.33it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:29, 67.67it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 75.81it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▎         | 50/2000 [00:00<00:24, 79.76it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 59/2000 [00:00<00:23, 82.78it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 68/2000 [00:01<00:23, 83.56it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 77/2000 [00:01<00:24, 79.13it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 86/2000 [00:01<00:25, 76.37it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 94/2000 [00:01<00:24, 76.47it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 94/2000 [00:01<00:24, 76.47it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 102/2000 [00:01<00:47, 40.03it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:39, 47.91it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 120/2000 [00:02<00:34, 55.17it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 129/2000 [00:02<00:30, 62.07it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 138/2000 [00:02<00:27, 67.38it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 147/2000 [00:02<00:25, 72.70it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 156/2000 [00:02<00:24, 76.51it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:23, 77.40it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 174/2000 [00:02<00:23, 77.25it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:23, 78.08it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:22, 78.84it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:03<00:22, 78.84it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:03<00:41, 43.77it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 210/2000 [00:03<00:34, 51.19it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 219/2000 [00:03<00:30, 57.97it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 228/2000 [00:03<00:27, 64.13it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 237/2000 [00:03<00:25, 69.46it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 246/2000 [00:03<00:23, 73.55it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:22, 77.22it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 264/2000 [00:04<00:21, 79.93it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 273/2000 [00:04<00:21, 81.91it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 282/2000 [00:04<00:20, 82.25it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:20, 83.59it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:20, 84.17it/s, train_loss=1.8518, val_loss=1.8623]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:20, 84.17it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 309/2000 [00:04<00:35, 47.66it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:04<00:30, 55.13it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 327/2000 [00:05<00:27, 61.68it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 336/2000 [00:05<00:24, 67.13it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 345/2000 [00:05<00:23, 71.36it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 354/2000 [00:05<00:21, 74.84it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:05<00:21, 76.84it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:05<00:20, 79.15it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:20, 80.60it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 390/2000 [00:05<00:19, 82.34it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 399/2000 [00:05<00:19, 83.38it/s, train_loss=1.1218, val_loss=1.1208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 399/2000 [00:06<00:19, 83.38it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 408/2000 [00:06<00:33, 47.79it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 417/2000 [00:06<00:28, 55.52it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 426/2000 [00:06<00:25, 62.43it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 435/2000 [00:06<00:23, 67.94it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 444/2000 [00:06<00:21, 72.11it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 453/2000 [00:06<00:20, 75.20it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 462/2000 [00:06<00:19, 77.76it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 471/2000 [00:07<00:19, 79.70it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 480/2000 [00:07<00:18, 81.23it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 489/2000 [00:07<00:18, 82.90it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 498/2000 [00:07<00:17, 83.81it/s, train_loss=0.7417, val_loss=0.7333]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 498/2000 [00:07<00:17, 83.81it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 507/2000 [00:07<00:30, 48.70it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 516/2000 [00:07<00:26, 56.18it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 525/2000 [00:07<00:23, 63.24it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 534/2000 [00:07<00:21, 69.32it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 543/2000 [00:08<00:19, 74.17it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 552/2000 [00:08<00:18, 78.17it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 561/2000 [00:08<00:17, 80.81it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 570/2000 [00:08<00:17, 83.32it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 579/2000 [00:08<00:16, 84.50it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 588/2000 [00:08<00:16, 84.76it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 597/2000 [00:08<00:16, 83.25it/s, train_loss=0.6036, val_loss=0.6115]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 597/2000 [00:09<00:16, 83.25it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 606/2000 [00:09<00:28, 48.63it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 615/2000 [00:09<00:24, 56.30it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 624/2000 [00:09<00:22, 62.24it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 633/2000 [00:09<00:20, 67.84it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 642/2000 [00:09<00:18, 72.95it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 651/2000 [00:09<00:17, 76.72it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 660/2000 [00:09<00:16, 79.77it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 669/2000 [00:09<00:16, 81.36it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 678/2000 [00:09<00:16, 81.65it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 687/2000 [00:10<00:15, 82.93it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:10<00:15, 83.77it/s, train_loss=0.5639, val_loss=0.5671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:10<00:15, 83.77it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 705/2000 [00:10<00:27, 47.15it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 714/2000 [00:10<00:23, 54.02it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 723/2000 [00:10<00:21, 60.79it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 732/2000 [00:10<00:19, 66.55it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 741/2000 [00:10<00:17, 71.52it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 750/2000 [00:11<00:16, 76.18it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 759/2000 [00:11<00:15, 79.34it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 768/2000 [00:11<00:15, 82.02it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:11<00:14, 83.91it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 786/2000 [00:11<00:14, 85.61it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:11<00:13, 87.68it/s, train_loss=0.5450, val_loss=0.5429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:11<00:13, 87.68it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 805/2000 [00:11<00:24, 47.88it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:12<00:21, 54.78it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 823/2000 [00:12<00:19, 61.53it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:12<00:17, 67.25it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 841/2000 [00:12<00:16, 71.45it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▎     | 850/2000 [00:12<00:15, 74.66it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 859/2000 [00:12<00:14, 77.40it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 868/2000 [00:12<00:14, 79.33it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 877/2000 [00:12<00:13, 80.28it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:12<00:13, 81.56it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:13<00:13, 83.88it/s, train_loss=0.5235, val_loss=0.5208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:13<00:13, 83.88it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 904/2000 [00:13<00:22, 48.56it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 914/2000 [00:13<00:19, 57.05it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 923/2000 [00:13<00:16, 63.90it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 931/2000 [00:13<00:15, 67.26it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 940/2000 [00:13<00:14, 71.66it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 949/2000 [00:13<00:14, 74.64it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 958/2000 [00:14<00:13, 77.80it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 967/2000 [00:14<00:12, 81.01it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 976/2000 [00:14<00:12, 78.80it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 985/2000 [00:14<00:12, 78.48it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:14<00:12, 80.18it/s, train_loss=0.4989, val_loss=0.5032]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:14<00:12, 80.18it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1003/2000 [00:14<00:24, 40.34it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1012/2000 [00:15<00:20, 48.25it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:15<00:17, 56.02it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:15<00:15, 63.08it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1039/2000 [00:15<00:14, 68.16it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:15<00:13, 70.52it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1057/2000 [00:15<00:12, 74.18it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1066/2000 [00:15<00:12, 76.99it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1075/2000 [00:15<00:11, 79.02it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1084/2000 [00:15<00:11, 81.44it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:15<00:10, 83.54it/s, train_loss=0.5004, val_loss=0.4993]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:16<00:10, 83.54it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1102/2000 [00:16<00:19, 46.57it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:16<00:16, 54.32it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:16<00:14, 61.28it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1129/2000 [00:16<00:12, 67.60it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:16<00:11, 72.57it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1147/2000 [00:16<00:11, 76.73it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1156/2000 [00:16<00:10, 80.24it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1165/2000 [00:17<00:10, 82.63it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1174/2000 [00:17<00:09, 84.43it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1183/2000 [00:17<00:09, 85.86it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:17<00:09, 87.02it/s, train_loss=0.4938, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:17<00:09, 87.02it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:17<00:16, 48.76it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:17<00:14, 56.32it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1219/2000 [00:17<00:12, 63.34it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1228/2000 [00:18<00:11, 69.38it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:18<00:10, 74.31it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1246/2000 [00:18<00:09, 78.15it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:18<00:09, 80.37it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1264/2000 [00:18<00:08, 82.97it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1273/2000 [00:18<00:08, 84.30it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1282/2000 [00:18<00:08, 85.41it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1291/2000 [00:18<00:08, 86.31it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:18<00:08, 86.71it/s, train_loss=0.4789, val_loss=0.4844]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:19<00:08, 86.71it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:19<00:14, 48.18it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:19<00:12, 55.98it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1327/2000 [00:19<00:10, 62.95it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1336/2000 [00:19<00:09, 68.78it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1345/2000 [00:19<00:08, 73.53it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1354/2000 [00:19<00:08, 76.61it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1363/2000 [00:19<00:08, 79.21it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1372/2000 [00:19<00:07, 81.85it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1381/2000 [00:20<00:07, 83.15it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1390/2000 [00:20<00:07, 84.91it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:20<00:06, 86.00it/s, train_loss=0.4776, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:20<00:06, 86.00it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1408/2000 [00:20<00:12, 48.63it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:20<00:10, 56.33it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1426/2000 [00:20<00:09, 63.39it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1435/2000 [00:20<00:08, 69.03it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1444/2000 [00:21<00:07, 73.88it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1453/2000 [00:21<00:07, 77.01it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1462/2000 [00:21<00:06, 80.27it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1471/2000 [00:21<00:06, 82.86it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1480/2000 [00:21<00:06, 83.51it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1489/2000 [00:21<00:06, 84.72it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:21<00:05, 85.59it/s, train_loss=0.4680, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:21<00:05, 85.59it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:22<00:10, 47.53it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1516/2000 [00:22<00:08, 55.25it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1525/2000 [00:22<00:07, 62.33it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1534/2000 [00:22<00:06, 68.54it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1543/2000 [00:22<00:06, 73.50it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1552/2000 [00:22<00:05, 76.15it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1561/2000 [00:22<00:05, 79.64it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1570/2000 [00:22<00:05, 82.07it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1579/2000 [00:22<00:05, 84.05it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1588/2000 [00:22<00:04, 85.32it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:23<00:04, 85.64it/s, train_loss=0.4754, val_loss=0.4719]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:23<00:04, 85.64it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1606/2000 [00:23<00:08, 48.66it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1615/2000 [00:23<00:06, 56.29it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1624/2000 [00:23<00:05, 62.88it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1633/2000 [00:23<00:05, 69.01it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1642/2000 [00:23<00:04, 73.91it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1651/2000 [00:23<00:04, 77.93it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1660/2000 [00:24<00:04, 80.89it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:24<00:03, 83.39it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1678/2000 [00:24<00:03, 83.98it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1687/2000 [00:24<00:03, 85.26it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:24<00:03, 85.09it/s, train_loss=0.4598, val_loss=0.4593]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:24<00:03, 85.09it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1705/2000 [00:24<00:06, 48.64it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:24<00:05, 56.10it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1723/2000 [00:25<00:04, 62.89it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1732/2000 [00:25<00:03, 68.73it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1741/2000 [00:25<00:03, 73.14it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1750/2000 [00:25<00:03, 76.70it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1759/2000 [00:25<00:03, 79.41it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1768/2000 [00:25<00:02, 81.58it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1777/2000 [00:25<00:02, 83.38it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1786/2000 [00:25<00:02, 84.51it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:25<00:02, 85.21it/s, train_loss=0.4568, val_loss=0.4703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:26<00:02, 85.21it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:26<00:04, 48.36it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1813/2000 [00:26<00:03, 56.09it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1822/2000 [00:26<00:02, 62.98it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1831/2000 [00:26<00:02, 68.37it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1840/2000 [00:26<00:02, 73.48it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1849/2000 [00:26<00:01, 77.24it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1858/2000 [00:26<00:01, 80.39it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1867/2000 [00:26<00:01, 82.61it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1876/2000 [00:27<00:01, 84.26it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1885/2000 [00:27<00:01, 85.21it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:27<00:01, 86.10it/s, train_loss=0.4689, val_loss=0.4702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:27<00:01, 86.10it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1903/2000 [00:27<00:02, 47.73it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1912/2000 [00:27<00:01, 55.26it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1921/2000 [00:27<00:01, 62.05it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1930/2000 [00:27<00:01, 67.97it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1939/2000 [00:28<00:00, 72.15it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1948/2000 [00:28<00:00, 71.23it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1956/2000 [00:28<00:00, 70.01it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1964/2000 [00:28<00:00, 71.77it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1973/2000 [00:28<00:00, 75.82it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1982/2000 [00:28<00:00, 78.74it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|█████████▉| 1991/2000 [00:28<00:00, 80.52it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 69.28it/s, train_loss=0.4678, val_loss=0.4551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:54,  3.36it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:04, 30.94it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:40, 48.82it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 28/2000 [00:00<00:32, 60.86it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 37/2000 [00:00<00:28, 69.39it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 46/2000 [00:00<00:26, 75.08it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:24, 78.56it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 64/2000 [00:01<00:23, 81.48it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 73/2000 [00:01<00:23, 83.63it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 82/2000 [00:01<00:22, 84.73it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:22, 85.55it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:22, 85.70it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:22, 85.70it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 109/2000 [00:01<00:39, 47.90it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 118/2000 [00:01<00:33, 55.55it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 127/2000 [00:02<00:29, 62.70it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 136/2000 [00:02<00:27, 68.55it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:02<00:25, 73.26it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:23, 76.99it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 163/2000 [00:02<00:23, 79.38it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 172/2000 [00:02<00:22, 81.84it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:21, 83.58it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 190/2000 [00:02<00:21, 84.52it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 200/2000 [00:02<00:20, 86.23it/s, train_loss=2.3316, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 200/2000 [00:03<00:20, 86.23it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 209/2000 [00:03<00:36, 48.98it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 218/2000 [00:03<00:31, 56.49it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 227/2000 [00:03<00:28, 63.22it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 236/2000 [00:03<00:25, 68.53it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 245/2000 [00:03<00:23, 73.17it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 254/2000 [00:03<00:22, 77.09it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 263/2000 [00:03<00:21, 79.92it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 272/2000 [00:03<00:21, 82.18it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:04<00:20, 83.67it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 290/2000 [00:04<00:20, 85.34it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 299/2000 [00:04<00:19, 86.05it/s, train_loss=1.8075, val_loss=1.8117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 299/2000 [00:04<00:19, 86.05it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:34, 48.38it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 317/2000 [00:04<00:30, 55.86it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 326/2000 [00:04<00:26, 62.87it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 335/2000 [00:04<00:24, 68.96it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 344/2000 [00:05<00:22, 73.94it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 353/2000 [00:05<00:21, 77.91it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 362/2000 [00:05<00:20, 80.63it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:05<00:19, 82.89it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 380/2000 [00:05<00:19, 83.82it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 389/2000 [00:05<00:18, 85.21it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 398/2000 [00:05<00:18, 85.88it/s, train_loss=1.1380, val_loss=1.1293]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 398/2000 [00:05<00:18, 85.88it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 407/2000 [00:06<00:33, 48.24it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 416/2000 [00:06<00:28, 55.83it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 425/2000 [00:06<00:25, 62.80it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 434/2000 [00:06<00:22, 69.00it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 443/2000 [00:06<00:21, 73.87it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 452/2000 [00:06<00:19, 77.79it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 461/2000 [00:06<00:19, 80.63it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 470/2000 [00:06<00:18, 82.56it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 479/2000 [00:06<00:18, 84.18it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 488/2000 [00:06<00:17, 84.76it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:07<00:17, 85.28it/s, train_loss=0.7571, val_loss=0.7738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:07<00:17, 85.28it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:07<00:30, 48.44it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 515/2000 [00:07<00:26, 55.69it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 524/2000 [00:07<00:23, 62.26it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 533/2000 [00:07<00:21, 68.24it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 542/2000 [00:07<00:19, 73.34it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:07<00:18, 76.92it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 560/2000 [00:08<00:18, 79.78it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 569/2000 [00:08<00:17, 82.11it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 578/2000 [00:08<00:16, 83.71it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 587/2000 [00:08<00:16, 84.60it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 596/2000 [00:08<00:16, 84.72it/s, train_loss=0.6423, val_loss=0.6591]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 596/2000 [00:08<00:16, 84.72it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 605/2000 [00:08<00:29, 47.92it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 614/2000 [00:08<00:25, 55.23it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:09<00:22, 62.21it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 632/2000 [00:09<00:20, 68.16it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 641/2000 [00:09<00:18, 73.12it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▎      | 650/2000 [00:09<00:17, 77.05it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 659/2000 [00:09<00:16, 80.04it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 668/2000 [00:09<00:16, 82.23it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 677/2000 [00:09<00:15, 83.85it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 686/2000 [00:09<00:15, 85.12it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 695/2000 [00:09<00:15, 85.67it/s, train_loss=0.5725, val_loss=0.5600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 695/2000 [00:10<00:15, 85.67it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 704/2000 [00:10<00:27, 47.85it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 713/2000 [00:10<00:23, 55.44it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 722/2000 [00:10<00:20, 61.96it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 731/2000 [00:10<00:18, 67.60it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 740/2000 [00:10<00:17, 72.72it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 749/2000 [00:10<00:16, 76.76it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 758/2000 [00:10<00:15, 79.93it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 767/2000 [00:10<00:15, 82.00it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 776/2000 [00:11<00:14, 83.59it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 785/2000 [00:11<00:14, 84.74it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:11<00:14, 85.65it/s, train_loss=0.5380, val_loss=0.5329]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:11<00:14, 85.65it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 803/2000 [00:11<00:24, 48.09it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 812/2000 [00:11<00:21, 55.61it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:11<00:19, 61.46it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 830/2000 [00:11<00:17, 67.71it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 839/2000 [00:12<00:15, 72.79it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 848/2000 [00:12<00:15, 72.29it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 856/2000 [00:12<00:16, 70.80it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:12<00:16, 70.90it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 873/2000 [00:12<00:14, 75.15it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 882/2000 [00:12<00:14, 78.92it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:12<00:13, 82.26it/s, train_loss=0.5074, val_loss=0.5052]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:13<00:13, 82.26it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 901/2000 [00:13<00:24, 45.40it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 910/2000 [00:13<00:20, 52.96it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 919/2000 [00:13<00:18, 59.96it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 928/2000 [00:13<00:16, 66.57it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 937/2000 [00:13<00:14, 71.71it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 946/2000 [00:13<00:13, 75.90it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 955/2000 [00:13<00:13, 78.94it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 964/2000 [00:13<00:12, 81.52it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 973/2000 [00:13<00:12, 83.14it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 982/2000 [00:14<00:12, 84.40it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 991/2000 [00:14<00:11, 84.99it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:14<00:11, 85.29it/s, train_loss=0.4951, val_loss=0.4921]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:14<00:11, 85.29it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1009/2000 [00:14<00:20, 47.98it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1018/2000 [00:14<00:17, 55.57it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1027/2000 [00:14<00:15, 62.47it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1036/2000 [00:14<00:14, 68.03it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1045/2000 [00:15<00:13, 72.93it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:15<00:12, 77.05it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1063/2000 [00:15<00:11, 79.82it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1072/2000 [00:15<00:11, 81.25it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:15<00:11, 82.77it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1090/2000 [00:15<00:10, 83.86it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1099/2000 [00:15<00:10, 85.03it/s, train_loss=0.4974, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1099/2000 [00:15<00:10, 85.03it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1108/2000 [00:16<00:18, 47.95it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1117/2000 [00:16<00:15, 55.35it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1126/2000 [00:16<00:14, 61.52it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1135/2000 [00:16<00:12, 67.59it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1144/2000 [00:16<00:11, 72.09it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1153/2000 [00:16<00:11, 76.05it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1162/2000 [00:16<00:10, 78.92it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1171/2000 [00:16<00:10, 80.84it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1180/2000 [00:16<00:09, 82.89it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1189/2000 [00:17<00:09, 83.98it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:17<00:09, 84.21it/s, train_loss=0.4827, val_loss=0.4967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:17<00:09, 84.21it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1207/2000 [00:17<00:16, 47.19it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1216/2000 [00:17<00:14, 54.87it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1225/2000 [00:17<00:12, 61.99it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1234/2000 [00:17<00:11, 68.30it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1243/2000 [00:17<00:10, 73.29it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1252/2000 [00:18<00:09, 77.31it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1261/2000 [00:18<00:09, 80.12it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1270/2000 [00:18<00:08, 82.42it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:18<00:08, 83.94it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1288/2000 [00:18<00:08, 85.45it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:18<00:08, 85.90it/s, train_loss=0.4741, val_loss=0.4826]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:18<00:08, 85.90it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1306/2000 [00:18<00:14, 48.16it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1315/2000 [00:18<00:12, 55.71it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1324/2000 [00:19<00:10, 62.64it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1333/2000 [00:19<00:09, 68.53it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1342/2000 [00:19<00:09, 72.94it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1351/2000 [00:19<00:08, 75.98it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1360/2000 [00:19<00:08, 79.04it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:19<00:07, 80.91it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1378/2000 [00:19<00:07, 82.92it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1387/2000 [00:19<00:07, 84.28it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:19<00:07, 85.19it/s, train_loss=0.4658, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:20<00:07, 85.19it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1405/2000 [00:20<00:12, 47.85it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1414/2000 [00:20<00:10, 55.06it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1423/2000 [00:20<00:09, 61.29it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1432/2000 [00:20<00:08, 67.43it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1441/2000 [00:20<00:07, 72.34it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▎  | 1450/2000 [00:20<00:07, 76.33it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1459/2000 [00:20<00:06, 79.66it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1468/2000 [00:21<00:06, 81.90it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:21<00:06, 83.38it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1486/2000 [00:21<00:06, 84.30it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1495/2000 [00:21<00:05, 85.05it/s, train_loss=0.4728, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1495/2000 [00:21<00:05, 85.05it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1504/2000 [00:21<00:10, 47.98it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1513/2000 [00:21<00:08, 55.43it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1522/2000 [00:21<00:07, 62.30it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1531/2000 [00:22<00:06, 68.29it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1540/2000 [00:22<00:06, 70.51it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1549/2000 [00:22<00:06, 74.28it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1558/2000 [00:22<00:05, 77.22it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1567/2000 [00:22<00:05, 80.37it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1576/2000 [00:22<00:05, 82.38it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1585/2000 [00:22<00:04, 84.42it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:22<00:04, 85.38it/s, train_loss=0.4718, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:23<00:04, 85.38it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1603/2000 [00:23<00:08, 47.63it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1611/2000 [00:23<00:07, 53.49it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1620/2000 [00:23<00:06, 60.76it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:23<00:05, 67.22it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1638/2000 [00:23<00:05, 72.17it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1647/2000 [00:23<00:04, 76.41it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1656/2000 [00:23<00:04, 79.64it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1665/2000 [00:23<00:04, 82.15it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:23<00:03, 83.55it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1683/2000 [00:24<00:03, 85.14it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:24<00:03, 83.23it/s, train_loss=0.4734, val_loss=0.4712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:24<00:03, 83.23it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:24<00:06, 42.77it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1710/2000 [00:24<00:05, 50.34it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1719/2000 [00:24<00:04, 57.92it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:24<00:04, 64.72it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:25<00:03, 70.44it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:25<00:03, 75.19it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1755/2000 [00:25<00:03, 78.79it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:25<00:02, 81.26it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1773/2000 [00:25<00:02, 82.50it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1782/2000 [00:25<00:02, 83.56it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:25<00:02, 85.34it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:25<00:02, 85.74it/s, train_loss=0.4736, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:26<00:02, 85.74it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1809/2000 [00:26<00:03, 48.10it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:26<00:03, 55.89it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1827/2000 [00:26<00:02, 63.00it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1837/2000 [00:26<00:02, 70.29it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1847/2000 [00:26<00:01, 77.33it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1857/2000 [00:26<00:01, 82.83it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1867/2000 [00:26<00:01, 87.11it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1877/2000 [00:26<00:01, 90.53it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:26<00:01, 93.63it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:27<00:01, 95.99it/s, train_loss=0.4743, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:27<00:01, 95.99it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1909/2000 [00:27<00:01, 54.06it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:27<00:01, 62.49it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1929/2000 [00:27<00:01, 69.97it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1939/2000 [00:27<00:00, 76.31it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1949/2000 [00:27<00:00, 82.02it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1959/2000 [00:27<00:00, 86.32it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1970/2000 [00:28<00:00, 90.43it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1980/2000 [00:28<00:00, 92.47it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|█████████▉| 1990/2000 [00:28<00:00, 94.27it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 70.48it/s, train_loss=0.4681, val_loss=0.4810]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:41<00:00, 81.05s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR: 100%|██████████| 3/3 [08:25<00:00, 166.78s/it]\u001B[A\n",
      "Constant LR:  33%|███▎      | 1/3 [09:20<18:41, 560.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.001, beta2=0.999, avg_val_loss=0.4665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<09:04,  3.67it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   1%|          | 19/2000 [00:00<00:31, 63.87it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   2%|▏         | 37/2000 [00:00<00:19, 101.61it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   3%|▎         | 55/2000 [00:00<00:15, 126.11it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   4%|▎         | 73/2000 [00:00<00:13, 141.76it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   5%|▍         | 91/2000 [00:00<00:12, 152.42it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   5%|▍         | 91/2000 [00:01<00:12, 152.42it/s, train_loss=3.0932, val_loss=3.0989]\u001B[A\n",
      "Training (constant):   5%|▌         | 108/2000 [00:01<00:22, 85.73it/s, train_loss=3.0932, val_loss=3.0989]\u001B[A\n",
      "Training (constant):   6%|▋         | 126/2000 [00:01<00:18, 103.26it/s, train_loss=3.0932, val_loss=3.0989]\u001B[A\n",
      "Training (constant):   7%|▋         | 144/2000 [00:01<00:15, 119.16it/s, train_loss=3.0932, val_loss=3.0989]\u001B[A\n",
      "Training (constant):   8%|▊         | 162/2000 [00:01<00:13, 132.88it/s, train_loss=3.0932, val_loss=3.0989]\u001B[A\n",
      "Training (constant):   9%|▉         | 180/2000 [00:01<00:12, 144.15it/s, train_loss=3.0932, val_loss=3.0989]\u001B[A\n",
      "Training (constant):  10%|▉         | 198/2000 [00:01<00:11, 152.61it/s, train_loss=3.0932, val_loss=3.0989]\u001B[A\n",
      "Training (constant):  10%|▉         | 198/2000 [00:01<00:11, 152.61it/s, train_loss=2.8576, val_loss=2.8589]\u001B[A\n",
      "Training (constant):  11%|█         | 215/2000 [00:02<00:19, 89.50it/s, train_loss=2.8576, val_loss=2.8589] \u001B[A\n",
      "Training (constant):  12%|█▏        | 233/2000 [00:02<00:16, 105.40it/s, train_loss=2.8576, val_loss=2.8589]\u001B[A\n",
      "Training (constant):  12%|█▎        | 250/2000 [00:02<00:14, 118.49it/s, train_loss=2.8576, val_loss=2.8589]\u001B[A\n",
      "Training (constant):  13%|█▎        | 268/2000 [00:02<00:13, 131.50it/s, train_loss=2.8576, val_loss=2.8589]\u001B[A\n",
      "Training (constant):  14%|█▍        | 285/2000 [00:02<00:12, 140.66it/s, train_loss=2.8576, val_loss=2.8589]\u001B[A\n",
      "Training (constant):  14%|█▍        | 285/2000 [00:02<00:12, 140.66it/s, train_loss=2.6852, val_loss=2.6920]\u001B[A\n",
      "Training (constant):  15%|█▌        | 302/2000 [00:02<00:19, 86.01it/s, train_loss=2.6852, val_loss=2.6920] \u001B[A\n",
      "Training (constant):  16%|█▌        | 320/2000 [00:02<00:16, 101.87it/s, train_loss=2.6852, val_loss=2.6920]\u001B[A\n",
      "Training (constant):  17%|█▋        | 338/2000 [00:03<00:14, 116.26it/s, train_loss=2.6852, val_loss=2.6920]\u001B[A\n",
      "Training (constant):  18%|█▊        | 356/2000 [00:03<00:12, 128.97it/s, train_loss=2.6852, val_loss=2.6920]\u001B[A\n",
      "Training (constant):  19%|█▊        | 374/2000 [00:03<00:11, 139.38it/s, train_loss=2.6852, val_loss=2.6920]\u001B[A\n",
      "Training (constant):  20%|█▉        | 392/2000 [00:03<00:10, 148.59it/s, train_loss=2.6852, val_loss=2.6920]\u001B[A\n",
      "Training (constant):  20%|█▉        | 392/2000 [00:03<00:10, 148.59it/s, train_loss=2.5654, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  20%|██        | 409/2000 [00:03<00:17, 88.69it/s, train_loss=2.5654, val_loss=2.5797] \u001B[A\n",
      "Training (constant):  21%|██▏       | 427/2000 [00:03<00:15, 104.51it/s, train_loss=2.5654, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  22%|██▏       | 445/2000 [00:03<00:13, 119.01it/s, train_loss=2.5654, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  23%|██▎       | 462/2000 [00:04<00:11, 130.23it/s, train_loss=2.5654, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  24%|██▍       | 479/2000 [00:04<00:10, 139.13it/s, train_loss=2.5654, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  25%|██▍       | 497/2000 [00:04<00:10, 147.68it/s, train_loss=2.5654, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  25%|██▍       | 497/2000 [00:04<00:10, 147.68it/s, train_loss=2.4805, val_loss=2.4816]\u001B[A\n",
      "Training (constant):  26%|██▌       | 514/2000 [00:04<00:16, 88.04it/s, train_loss=2.4805, val_loss=2.4816] \u001B[A\n",
      "Training (constant):  27%|██▋       | 532/2000 [00:04<00:14, 103.74it/s, train_loss=2.4805, val_loss=2.4816]\u001B[A\n",
      "Training (constant):  27%|██▋       | 549/2000 [00:04<00:12, 116.91it/s, train_loss=2.4805, val_loss=2.4816]\u001B[A\n",
      "Training (constant):  28%|██▊       | 567/2000 [00:04<00:10, 130.56it/s, train_loss=2.4805, val_loss=2.4816]\u001B[A\n",
      "Training (constant):  29%|██▉       | 585/2000 [00:05<00:10, 141.27it/s, train_loss=2.4805, val_loss=2.4816]\u001B[A\n",
      "Training (constant):  29%|██▉       | 585/2000 [00:05<00:10, 141.27it/s, train_loss=2.3974, val_loss=2.3971]\u001B[A\n",
      "Training (constant):  30%|███       | 602/2000 [00:05<00:16, 85.68it/s, train_loss=2.3974, val_loss=2.3971] \u001B[A\n",
      "Training (constant):  31%|███       | 619/2000 [00:05<00:13, 99.76it/s, train_loss=2.3974, val_loss=2.3971]\u001B[A\n",
      "Training (constant):  32%|███▏      | 636/2000 [00:05<00:12, 113.57it/s, train_loss=2.3974, val_loss=2.3971]\u001B[A\n",
      "Training (constant):  33%|███▎      | 654/2000 [00:05<00:10, 126.53it/s, train_loss=2.3974, val_loss=2.3971]\u001B[A\n",
      "Training (constant):  34%|███▎      | 672/2000 [00:05<00:09, 137.54it/s, train_loss=2.3974, val_loss=2.3971]\u001B[A\n",
      "Training (constant):  34%|███▍      | 690/2000 [00:05<00:08, 146.84it/s, train_loss=2.3974, val_loss=2.3971]\u001B[A\n",
      "Training (constant):  34%|███▍      | 690/2000 [00:06<00:08, 146.84it/s, train_loss=2.3404, val_loss=2.3606]\u001B[A\n",
      "Training (constant):  35%|███▌      | 707/2000 [00:06<00:14, 88.53it/s, train_loss=2.3404, val_loss=2.3606] \u001B[A\n",
      "Training (constant):  36%|███▋      | 725/2000 [00:06<00:12, 104.39it/s, train_loss=2.3404, val_loss=2.3606]\u001B[A\n",
      "Training (constant):  37%|███▋      | 742/2000 [00:06<00:10, 117.57it/s, train_loss=2.3404, val_loss=2.3606]\u001B[A\n",
      "Training (constant):  38%|███▊      | 760/2000 [00:06<00:09, 130.02it/s, train_loss=2.3404, val_loss=2.3606]\u001B[A\n",
      "Training (constant):  39%|███▉      | 778/2000 [00:06<00:08, 141.01it/s, train_loss=2.3404, val_loss=2.3606]\u001B[A\n",
      "Training (constant):  40%|███▉      | 796/2000 [00:06<00:08, 149.97it/s, train_loss=2.3404, val_loss=2.3606]\u001B[A\n",
      "Training (constant):  40%|███▉      | 796/2000 [00:07<00:08, 149.97it/s, train_loss=2.2767, val_loss=2.2864]\u001B[A\n",
      "Training (constant):  41%|████      | 813/2000 [00:07<00:13, 88.74it/s, train_loss=2.2767, val_loss=2.2864] \u001B[A\n",
      "Training (constant):  42%|████▏     | 831/2000 [00:07<00:11, 104.10it/s, train_loss=2.2767, val_loss=2.2864]\u001B[A\n",
      "Training (constant):  42%|████▏     | 849/2000 [00:07<00:09, 118.75it/s, train_loss=2.2767, val_loss=2.2864]\u001B[A\n",
      "Training (constant):  43%|████▎     | 867/2000 [00:07<00:08, 131.14it/s, train_loss=2.2767, val_loss=2.2864]\u001B[A\n",
      "Training (constant):  44%|████▍     | 885/2000 [00:07<00:07, 141.55it/s, train_loss=2.2767, val_loss=2.2864]\u001B[A\n",
      "Training (constant):  44%|████▍     | 885/2000 [00:07<00:07, 141.55it/s, train_loss=2.2520, val_loss=2.2591]\u001B[A\n",
      "Training (constant):  45%|████▌     | 902/2000 [00:08<00:12, 88.51it/s, train_loss=2.2520, val_loss=2.2591] \u001B[A\n",
      "Training (constant):  46%|████▌     | 918/2000 [00:08<00:10, 101.00it/s, train_loss=2.2520, val_loss=2.2591]\u001B[A\n",
      "Training (constant):  47%|████▋     | 935/2000 [00:08<00:09, 114.30it/s, train_loss=2.2520, val_loss=2.2591]\u001B[A\n",
      "Training (constant):  48%|████▊     | 952/2000 [00:08<00:08, 126.57it/s, train_loss=2.2520, val_loss=2.2591]\u001B[A\n",
      "Training (constant):  48%|████▊     | 969/2000 [00:08<00:07, 135.81it/s, train_loss=2.2520, val_loss=2.2591]\u001B[A\n",
      "Training (constant):  49%|████▉     | 987/2000 [00:08<00:06, 145.96it/s, train_loss=2.2520, val_loss=2.2591]\u001B[A\n",
      "Training (constant):  49%|████▉     | 987/2000 [00:08<00:06, 145.96it/s, train_loss=2.2175, val_loss=2.2175]\u001B[A\n",
      "Training (constant):  50%|█████     | 1004/2000 [00:08<00:11, 86.84it/s, train_loss=2.2175, val_loss=2.2175]\u001B[A\n",
      "Training (constant):  51%|█████     | 1021/2000 [00:09<00:09, 100.92it/s, train_loss=2.2175, val_loss=2.2175]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1039/2000 [00:09<00:08, 116.60it/s, train_loss=2.2175, val_loss=2.2175]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1057/2000 [00:09<00:07, 129.34it/s, train_loss=2.2175, val_loss=2.2175]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1075/2000 [00:09<00:06, 139.98it/s, train_loss=2.2175, val_loss=2.2175]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1093/2000 [00:09<00:06, 149.47it/s, train_loss=2.2175, val_loss=2.2175]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1093/2000 [00:09<00:06, 149.47it/s, train_loss=2.1252, val_loss=2.1240]\u001B[A\n",
      "Training (constant):  56%|█████▌    | 1110/2000 [00:09<00:10, 88.85it/s, train_loss=2.1252, val_loss=2.1240] \u001B[A\n",
      "Training (constant):  56%|█████▋    | 1128/2000 [00:09<00:08, 104.98it/s, train_loss=2.1252, val_loss=2.1240]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1146/2000 [00:09<00:07, 119.82it/s, train_loss=2.1252, val_loss=2.1240]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1164/2000 [00:10<00:06, 132.67it/s, train_loss=2.1252, val_loss=2.1240]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1182/2000 [00:10<00:05, 142.30it/s, train_loss=2.1252, val_loss=2.1240]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1199/2000 [00:10<00:05, 149.20it/s, train_loss=2.1252, val_loss=2.1240]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1199/2000 [00:10<00:05, 149.20it/s, train_loss=2.0755, val_loss=2.0860]\u001B[A\n",
      "Training (constant):  61%|██████    | 1216/2000 [00:10<00:08, 88.57it/s, train_loss=2.0755, val_loss=2.0860] \u001B[A\n",
      "Training (constant):  62%|██████▏   | 1234/2000 [00:10<00:07, 103.92it/s, train_loss=2.0755, val_loss=2.0860]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1251/2000 [00:10<00:06, 116.81it/s, train_loss=2.0755, val_loss=2.0860]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1269/2000 [00:11<00:05, 129.53it/s, train_loss=2.0755, val_loss=2.0860]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1287/2000 [00:11<00:05, 139.81it/s, train_loss=2.0755, val_loss=2.0860]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1287/2000 [00:11<00:05, 139.81it/s, train_loss=2.0787, val_loss=2.0741]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1304/2000 [00:11<00:08, 85.75it/s, train_loss=2.0787, val_loss=2.0741] \u001B[A\n",
      "Training (constant):  66%|██████▌   | 1321/2000 [00:11<00:06, 99.85it/s, train_loss=2.0787, val_loss=2.0741]\u001B[A\n",
      "Training (constant):  67%|██████▋   | 1338/2000 [00:11<00:05, 113.47it/s, train_loss=2.0787, val_loss=2.0741]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1356/2000 [00:11<00:05, 126.84it/s, train_loss=2.0787, val_loss=2.0741]\u001B[A\n",
      "Training (constant):  69%|██████▊   | 1374/2000 [00:11<00:04, 137.52it/s, train_loss=2.0787, val_loss=2.0741]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1392/2000 [00:12<00:04, 146.64it/s, train_loss=2.0787, val_loss=2.0741]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1392/2000 [00:12<00:04, 146.64it/s, train_loss=2.0151, val_loss=2.0201]\u001B[A\n",
      "Training (constant):  70%|███████   | 1409/2000 [00:12<00:06, 87.84it/s, train_loss=2.0151, val_loss=2.0201] \u001B[A\n",
      "Training (constant):  71%|███████▏  | 1427/2000 [00:12<00:05, 103.71it/s, train_loss=2.0151, val_loss=2.0201]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1445/2000 [00:12<00:04, 118.01it/s, train_loss=2.0151, val_loss=2.0201]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1463/2000 [00:12<00:04, 130.98it/s, train_loss=2.0151, val_loss=2.0201]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1481/2000 [00:12<00:03, 141.66it/s, train_loss=2.0151, val_loss=2.0201]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1499/2000 [00:12<00:03, 149.79it/s, train_loss=2.0151, val_loss=2.0201]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1499/2000 [00:13<00:03, 149.79it/s, train_loss=1.9660, val_loss=1.9610]\u001B[A\n",
      "Training (constant):  76%|███████▌  | 1516/2000 [00:13<00:05, 89.00it/s, train_loss=1.9660, val_loss=1.9610] \u001B[A\n",
      "Training (constant):  77%|███████▋  | 1534/2000 [00:13<00:04, 104.35it/s, train_loss=1.9660, val_loss=1.9610]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1551/2000 [00:13<00:03, 116.35it/s, train_loss=1.9660, val_loss=1.9610]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1569/2000 [00:13<00:03, 129.16it/s, train_loss=1.9660, val_loss=1.9610]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1587/2000 [00:13<00:02, 140.70it/s, train_loss=1.9660, val_loss=1.9610]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1587/2000 [00:14<00:02, 140.70it/s, train_loss=2.0507, val_loss=2.0461]\u001B[A\n",
      "Training (constant):  80%|████████  | 1604/2000 [00:14<00:04, 85.78it/s, train_loss=2.0507, val_loss=2.0461] \u001B[A\n",
      "Training (constant):  81%|████████  | 1621/2000 [00:14<00:03, 99.93it/s, train_loss=2.0507, val_loss=2.0461]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1638/2000 [00:14<00:03, 113.43it/s, train_loss=2.0507, val_loss=2.0461]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1656/2000 [00:14<00:02, 127.09it/s, train_loss=2.0507, val_loss=2.0461]\u001B[A\n",
      "Training (constant):  84%|████████▎ | 1674/2000 [00:14<00:02, 138.40it/s, train_loss=2.0507, val_loss=2.0461]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1692/2000 [00:14<00:02, 148.09it/s, train_loss=2.0507, val_loss=2.0461]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1692/2000 [00:14<00:02, 148.09it/s, train_loss=1.9955, val_loss=1.9888]\u001B[A\n",
      "Training (constant):  85%|████████▌ | 1709/2000 [00:14<00:03, 90.20it/s, train_loss=1.9955, val_loss=1.9888] \u001B[A\n",
      "Training (constant):  86%|████████▋ | 1727/2000 [00:15<00:02, 105.87it/s, train_loss=1.9955, val_loss=1.9888]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1745/2000 [00:15<00:02, 120.40it/s, train_loss=1.9955, val_loss=1.9888]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1763/2000 [00:15<00:01, 132.51it/s, train_loss=1.9955, val_loss=1.9888]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1781/2000 [00:15<00:01, 143.44it/s, train_loss=1.9955, val_loss=1.9888]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1799/2000 [00:15<00:01, 151.58it/s, train_loss=1.9955, val_loss=1.9888]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1799/2000 [00:15<00:01, 151.58it/s, train_loss=1.8198, val_loss=1.8297]\u001B[A\n",
      "Training (constant):  91%|█████████ | 1816/2000 [00:15<00:02, 90.04it/s, train_loss=1.8198, val_loss=1.8297] \u001B[A\n",
      "Training (constant):  92%|█████████▏| 1833/2000 [00:15<00:01, 104.33it/s, train_loss=1.8198, val_loss=1.8297]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1851/2000 [00:16<00:01, 119.03it/s, train_loss=1.8198, val_loss=1.8297]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1869/2000 [00:16<00:00, 131.63it/s, train_loss=1.8198, val_loss=1.8297]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1887/2000 [00:16<00:00, 141.33it/s, train_loss=1.8198, val_loss=1.8297]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1887/2000 [00:16<00:00, 141.33it/s, train_loss=1.7297, val_loss=1.7441]\u001B[A\n",
      "Training (constant):  95%|█████████▌| 1904/2000 [00:16<00:01, 86.77it/s, train_loss=1.7297, val_loss=1.7441] \u001B[A\n",
      "Training (constant):  96%|█████████▌| 1921/2000 [00:16<00:00, 100.59it/s, train_loss=1.7297, val_loss=1.7441]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1939/2000 [00:16<00:00, 115.61it/s, train_loss=1.7297, val_loss=1.7441]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1957/2000 [00:16<00:00, 128.27it/s, train_loss=1.7297, val_loss=1.7441]\u001B[A\n",
      "Training (constant):  99%|█████████▉| 1975/2000 [00:17<00:00, 139.10it/s, train_loss=1.7297, val_loss=1.7441]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:17<00:00, 116.16it/s, train_loss=1.7297, val_loss=1.7441]\u001B[A\n",
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<09:02,  3.68it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   1%|          | 19/2000 [00:00<00:31, 63.81it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   2%|▏         | 37/2000 [00:00<00:19, 101.25it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   3%|▎         | 55/2000 [00:00<00:15, 125.75it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   4%|▎         | 72/2000 [00:00<00:13, 138.96it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   4%|▍         | 90/2000 [00:00<00:12, 149.50it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   4%|▍         | 90/2000 [00:01<00:12, 149.50it/s, train_loss=3.0714, val_loss=3.0756]\u001B[A\n",
      "Training (constant):   5%|▌         | 107/2000 [00:01<00:22, 85.20it/s, train_loss=3.0714, val_loss=3.0756]\u001B[A\n",
      "Training (constant):   6%|▋         | 125/2000 [00:01<00:18, 103.11it/s, train_loss=3.0714, val_loss=3.0756]\u001B[A\n",
      "Training (constant):   7%|▋         | 143/2000 [00:01<00:15, 119.54it/s, train_loss=3.0714, val_loss=3.0756]\u001B[A\n",
      "Training (constant):   8%|▊         | 161/2000 [00:01<00:13, 133.41it/s, train_loss=3.0714, val_loss=3.0756]\u001B[A\n",
      "Training (constant):   9%|▉         | 179/2000 [00:01<00:12, 144.98it/s, train_loss=3.0714, val_loss=3.0756]\u001B[A\n",
      "Training (constant):  10%|▉         | 197/2000 [00:01<00:11, 154.18it/s, train_loss=3.0714, val_loss=3.0756]\u001B[A\n",
      "Training (constant):  10%|▉         | 197/2000 [00:01<00:11, 154.18it/s, train_loss=2.8543, val_loss=2.8632]\u001B[A\n",
      "Training (constant):  11%|█         | 215/2000 [00:02<00:19, 90.27it/s, train_loss=2.8543, val_loss=2.8632] \u001B[A\n",
      "Training (constant):  12%|█▏        | 233/2000 [00:02<00:16, 106.16it/s, train_loss=2.8543, val_loss=2.8632]\u001B[A\n",
      "Training (constant):  13%|█▎        | 251/2000 [00:02<00:14, 120.88it/s, train_loss=2.8543, val_loss=2.8632]\u001B[A\n",
      "Training (constant):  13%|█▎        | 269/2000 [00:02<00:12, 133.27it/s, train_loss=2.8543, val_loss=2.8632]\u001B[A\n",
      "Training (constant):  14%|█▍        | 287/2000 [00:02<00:11, 143.86it/s, train_loss=2.8543, val_loss=2.8632]\u001B[A\n",
      "Training (constant):  14%|█▍        | 287/2000 [00:02<00:11, 143.86it/s, train_loss=2.7225, val_loss=2.7264]\u001B[A\n",
      "Training (constant):  15%|█▌        | 304/2000 [00:02<00:19, 88.77it/s, train_loss=2.7225, val_loss=2.7264] \u001B[A\n",
      "Training (constant):  16%|█▌        | 322/2000 [00:02<00:16, 104.03it/s, train_loss=2.7225, val_loss=2.7264]\u001B[A\n",
      "Training (constant):  17%|█▋        | 340/2000 [00:03<00:13, 118.96it/s, train_loss=2.7225, val_loss=2.7264]\u001B[A\n",
      "Training (constant):  18%|█▊        | 358/2000 [00:03<00:12, 131.17it/s, train_loss=2.7225, val_loss=2.7264]\u001B[A\n",
      "Training (constant):  19%|█▉        | 376/2000 [00:03<00:11, 142.50it/s, train_loss=2.7225, val_loss=2.7264]\u001B[A\n",
      "Training (constant):  20%|█▉        | 394/2000 [00:03<00:10, 150.95it/s, train_loss=2.7225, val_loss=2.7264]\u001B[A\n",
      "Training (constant):  20%|█▉        | 394/2000 [00:03<00:10, 150.95it/s, train_loss=2.6033, val_loss=2.6020]\u001B[A\n",
      "Training (constant):  21%|██        | 411/2000 [00:03<00:17, 90.23it/s, train_loss=2.6033, val_loss=2.6020] \u001B[A\n",
      "Training (constant):  21%|██▏       | 429/2000 [00:03<00:14, 106.32it/s, train_loss=2.6033, val_loss=2.6020]\u001B[A\n",
      "Training (constant):  22%|██▏       | 447/2000 [00:03<00:12, 120.79it/s, train_loss=2.6033, val_loss=2.6020]\u001B[A\n",
      "Training (constant):  23%|██▎       | 465/2000 [00:04<00:11, 133.54it/s, train_loss=2.6033, val_loss=2.6020]\u001B[A\n",
      "Training (constant):  24%|██▍       | 482/2000 [00:04<00:10, 142.29it/s, train_loss=2.6033, val_loss=2.6020]\u001B[A\n",
      "Training (constant):  25%|██▌       | 500/2000 [00:04<00:09, 150.63it/s, train_loss=2.6033, val_loss=2.6020]\u001B[A\n",
      "Training (constant):  25%|██▌       | 500/2000 [00:04<00:09, 150.63it/s, train_loss=2.5061, val_loss=2.5003]\u001B[A\n",
      "Training (constant):  26%|██▌       | 517/2000 [00:04<00:16, 89.90it/s, train_loss=2.5061, val_loss=2.5003] \u001B[A\n",
      "Training (constant):  27%|██▋       | 535/2000 [00:04<00:13, 105.94it/s, train_loss=2.5061, val_loss=2.5003]\u001B[A\n",
      "Training (constant):  28%|██▊       | 553/2000 [00:04<00:12, 119.84it/s, train_loss=2.5061, val_loss=2.5003]\u001B[A\n",
      "Training (constant):  29%|██▊       | 571/2000 [00:04<00:10, 132.87it/s, train_loss=2.5061, val_loss=2.5003]\u001B[A\n",
      "Training (constant):  29%|██▉       | 589/2000 [00:05<00:09, 143.83it/s, train_loss=2.5061, val_loss=2.5003]\u001B[A\n",
      "Training (constant):  29%|██▉       | 589/2000 [00:05<00:09, 143.83it/s, train_loss=2.4180, val_loss=2.4198]\u001B[A\n",
      "Training (constant):  30%|███       | 606/2000 [00:05<00:15, 88.00it/s, train_loss=2.4180, val_loss=2.4198] \u001B[A\n",
      "Training (constant):  31%|███       | 624/2000 [00:05<00:13, 103.51it/s, train_loss=2.4180, val_loss=2.4198]\u001B[A\n",
      "Training (constant):  32%|███▏      | 642/2000 [00:05<00:11, 118.32it/s, train_loss=2.4180, val_loss=2.4198]\u001B[A\n",
      "Training (constant):  33%|███▎      | 659/2000 [00:05<00:10, 129.35it/s, train_loss=2.4180, val_loss=2.4198]\u001B[A\n",
      "Training (constant):  34%|███▍      | 677/2000 [00:05<00:09, 140.23it/s, train_loss=2.4180, val_loss=2.4198]\u001B[A\n",
      "Training (constant):  35%|███▍      | 695/2000 [00:05<00:08, 148.62it/s, train_loss=2.4180, val_loss=2.4198]\u001B[A\n",
      "Training (constant):  35%|███▍      | 695/2000 [00:06<00:08, 148.62it/s, train_loss=2.3799, val_loss=2.3738]\u001B[A\n",
      "Training (constant):  36%|███▌      | 712/2000 [00:06<00:14, 89.28it/s, train_loss=2.3799, val_loss=2.3738] \u001B[A\n",
      "Training (constant):  36%|███▋      | 730/2000 [00:06<00:12, 105.32it/s, train_loss=2.3799, val_loss=2.3738]\u001B[A\n",
      "Training (constant):  37%|███▋      | 748/2000 [00:06<00:10, 119.96it/s, train_loss=2.3799, val_loss=2.3738]\u001B[A\n",
      "Training (constant):  38%|███▊      | 766/2000 [00:06<00:09, 133.26it/s, train_loss=2.3799, val_loss=2.3738]\u001B[A\n",
      "Training (constant):  39%|███▉      | 785/2000 [00:06<00:08, 145.27it/s, train_loss=2.3799, val_loss=2.3738]\u001B[A\n",
      "Training (constant):  39%|███▉      | 785/2000 [00:07<00:08, 145.27it/s, train_loss=2.3035, val_loss=2.3032]\u001B[A\n",
      "Training (constant):  40%|████      | 802/2000 [00:07<00:13, 89.10it/s, train_loss=2.3035, val_loss=2.3032] \u001B[A\n",
      "Training (constant):  41%|████      | 820/2000 [00:07<00:11, 104.96it/s, train_loss=2.3035, val_loss=2.3032]\u001B[A\n",
      "Training (constant):  42%|████▏     | 838/2000 [00:07<00:09, 120.03it/s, train_loss=2.3035, val_loss=2.3032]\u001B[A\n",
      "Training (constant):  43%|████▎     | 856/2000 [00:07<00:08, 133.07it/s, train_loss=2.3035, val_loss=2.3032]\u001B[A\n",
      "Training (constant):  44%|████▎     | 874/2000 [00:07<00:07, 143.80it/s, train_loss=2.3035, val_loss=2.3032]\u001B[A\n",
      "Training (constant):  45%|████▍     | 893/2000 [00:07<00:07, 153.85it/s, train_loss=2.3035, val_loss=2.3032]\u001B[A\n",
      "Training (constant):  45%|████▍     | 893/2000 [00:07<00:07, 153.85it/s, train_loss=2.2329, val_loss=2.2281]\u001B[A\n",
      "Training (constant):  46%|████▌     | 911/2000 [00:07<00:11, 92.14it/s, train_loss=2.2329, val_loss=2.2281] \u001B[A\n",
      "Training (constant):  46%|████▋     | 929/2000 [00:08<00:09, 107.62it/s, train_loss=2.2329, val_loss=2.2281]\u001B[A\n",
      "Training (constant):  47%|████▋     | 947/2000 [00:08<00:08, 121.97it/s, train_loss=2.2329, val_loss=2.2281]\u001B[A\n",
      "Training (constant):  48%|████▊     | 965/2000 [00:08<00:07, 134.19it/s, train_loss=2.2329, val_loss=2.2281]\u001B[A\n",
      "Training (constant):  49%|████▉     | 984/2000 [00:08<00:06, 146.10it/s, train_loss=2.2329, val_loss=2.2281]\u001B[A\n",
      "Training (constant):  49%|████▉     | 984/2000 [00:08<00:06, 146.10it/s, train_loss=2.1963, val_loss=2.1873]\u001B[A\n",
      "Training (constant):  50%|█████     | 1001/2000 [00:08<00:11, 90.05it/s, train_loss=2.1963, val_loss=2.1873]\u001B[A\n",
      "Training (constant):  51%|█████     | 1019/2000 [00:08<00:09, 105.80it/s, train_loss=2.1963, val_loss=2.1873]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1037/2000 [00:08<00:08, 120.36it/s, train_loss=2.1963, val_loss=2.1873]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1055/2000 [00:09<00:07, 133.18it/s, train_loss=2.1963, val_loss=2.1873]\u001B[A\n",
      "Training (constant):  54%|█████▎    | 1073/2000 [00:09<00:06, 144.24it/s, train_loss=2.1963, val_loss=2.1873]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1091/2000 [00:09<00:05, 152.51it/s, train_loss=2.1963, val_loss=2.1873]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1091/2000 [00:09<00:05, 152.51it/s, train_loss=2.1613, val_loss=2.1675]\u001B[A\n",
      "Training (constant):  55%|█████▌    | 1108/2000 [00:09<00:09, 90.73it/s, train_loss=2.1613, val_loss=2.1675] \u001B[A\n",
      "Training (constant):  56%|█████▋    | 1126/2000 [00:09<00:08, 106.66it/s, train_loss=2.1613, val_loss=2.1675]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1144/2000 [00:09<00:07, 121.64it/s, train_loss=2.1613, val_loss=2.1675]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1162/2000 [00:09<00:06, 134.39it/s, train_loss=2.1613, val_loss=2.1675]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1181/2000 [00:10<00:05, 146.26it/s, train_loss=2.1613, val_loss=2.1675]\u001B[A\n",
      "Training (constant):  60%|██████    | 1200/2000 [00:10<00:05, 155.12it/s, train_loss=2.1613, val_loss=2.1675]\u001B[A\n",
      "Training (constant):  60%|██████    | 1200/2000 [00:10<00:05, 155.12it/s, train_loss=2.1155, val_loss=2.1233]\u001B[A\n",
      "Training (constant):  61%|██████    | 1218/2000 [00:10<00:08, 92.47it/s, train_loss=2.1155, val_loss=2.1233] \u001B[A\n",
      "Training (constant):  62%|██████▏   | 1237/2000 [00:10<00:07, 108.81it/s, train_loss=2.1155, val_loss=2.1233]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1255/2000 [00:10<00:06, 122.69it/s, train_loss=2.1155, val_loss=2.1233]\u001B[A\n",
      "Training (constant):  64%|██████▎   | 1273/2000 [00:10<00:05, 134.54it/s, train_loss=2.1155, val_loss=2.1233]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1291/2000 [00:10<00:04, 144.53it/s, train_loss=2.1155, val_loss=2.1233]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1291/2000 [00:11<00:04, 144.53it/s, train_loss=2.0770, val_loss=2.0973]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1308/2000 [00:11<00:07, 88.86it/s, train_loss=2.0770, val_loss=2.0973] \u001B[A\n",
      "Training (constant):  66%|██████▋   | 1326/2000 [00:11<00:06, 104.72it/s, train_loss=2.0770, val_loss=2.0973]\u001B[A\n",
      "Training (constant):  67%|██████▋   | 1344/2000 [00:11<00:05, 119.49it/s, train_loss=2.0770, val_loss=2.0973]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1362/2000 [00:11<00:04, 132.55it/s, train_loss=2.0770, val_loss=2.0973]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1380/2000 [00:11<00:04, 142.71it/s, train_loss=2.0770, val_loss=2.0973]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1399/2000 [00:11<00:03, 152.74it/s, train_loss=2.0770, val_loss=2.0973]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1399/2000 [00:12<00:03, 152.74it/s, train_loss=2.0495, val_loss=2.0511]\u001B[A\n",
      "Training (constant):  71%|███████   | 1416/2000 [00:12<00:06, 91.16it/s, train_loss=2.0495, val_loss=2.0511] \u001B[A\n",
      "Training (constant):  72%|███████▏  | 1434/2000 [00:12<00:05, 107.07it/s, train_loss=2.0495, val_loss=2.0511]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1452/2000 [00:12<00:04, 121.43it/s, train_loss=2.0495, val_loss=2.0511]\u001B[A\n",
      "Training (constant):  74%|███████▎  | 1470/2000 [00:12<00:03, 133.94it/s, train_loss=2.0495, val_loss=2.0511]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1489/2000 [00:12<00:03, 145.69it/s, train_loss=2.0495, val_loss=2.0511]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1489/2000 [00:12<00:03, 145.69it/s, train_loss=1.9551, val_loss=1.9542]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1506/2000 [00:12<00:05, 89.29it/s, train_loss=1.9551, val_loss=1.9542] \u001B[A\n",
      "Training (constant):  76%|███████▌  | 1524/2000 [00:13<00:04, 105.02it/s, train_loss=1.9551, val_loss=1.9542]\u001B[A\n",
      "Training (constant):  77%|███████▋  | 1542/2000 [00:13<00:03, 120.02it/s, train_loss=1.9551, val_loss=1.9542]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1560/2000 [00:13<00:03, 133.19it/s, train_loss=1.9551, val_loss=1.9542]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1579/2000 [00:13<00:02, 145.26it/s, train_loss=1.9551, val_loss=1.9542]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1597/2000 [00:13<00:02, 153.56it/s, train_loss=1.9551, val_loss=1.9542]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1597/2000 [00:13<00:02, 153.56it/s, train_loss=1.9137, val_loss=1.9281]\u001B[A\n",
      "Training (constant):  81%|████████  | 1615/2000 [00:13<00:04, 91.79it/s, train_loss=1.9137, val_loss=1.9281] \u001B[A\n",
      "Training (constant):  82%|████████▏ | 1633/2000 [00:13<00:03, 106.80it/s, train_loss=1.9137, val_loss=1.9281]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1651/2000 [00:14<00:02, 120.25it/s, train_loss=1.9137, val_loss=1.9281]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1669/2000 [00:14<00:02, 132.91it/s, train_loss=1.9137, val_loss=1.9281]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1687/2000 [00:14<00:02, 143.81it/s, train_loss=1.9137, val_loss=1.9281]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1687/2000 [00:14<00:02, 143.81it/s, train_loss=1.9494, val_loss=1.9705]\u001B[A\n",
      "Training (constant):  85%|████████▌ | 1704/2000 [00:14<00:03, 90.78it/s, train_loss=1.9494, val_loss=1.9705] \u001B[A\n",
      "Training (constant):  86%|████████▌ | 1722/2000 [00:14<00:02, 106.24it/s, train_loss=1.9494, val_loss=1.9705]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1740/2000 [00:14<00:02, 120.46it/s, train_loss=1.9494, val_loss=1.9705]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1758/2000 [00:14<00:01, 132.19it/s, train_loss=1.9494, val_loss=1.9705]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1776/2000 [00:15<00:01, 142.89it/s, train_loss=1.9494, val_loss=1.9705]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1795/2000 [00:15<00:01, 152.82it/s, train_loss=1.9494, val_loss=1.9705]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1795/2000 [00:15<00:01, 152.82it/s, train_loss=1.8938, val_loss=1.8950]\u001B[A\n",
      "Training (constant):  91%|█████████ | 1812/2000 [00:15<00:02, 91.17it/s, train_loss=1.8938, val_loss=1.8950] \u001B[A\n",
      "Training (constant):  92%|█████████▏| 1830/2000 [00:15<00:01, 106.24it/s, train_loss=1.8938, val_loss=1.8950]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1848/2000 [00:15<00:01, 120.10it/s, train_loss=1.8938, val_loss=1.8950]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1866/2000 [00:15<00:01, 132.79it/s, train_loss=1.8938, val_loss=1.8950]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1884/2000 [00:15<00:00, 142.90it/s, train_loss=1.8938, val_loss=1.8950]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1884/2000 [00:16<00:00, 142.90it/s, train_loss=1.9209, val_loss=1.9065]\u001B[A\n",
      "Training (constant):  95%|█████████▌| 1901/2000 [00:16<00:01, 82.13it/s, train_loss=1.9209, val_loss=1.9065] \u001B[A\n",
      "Training (constant):  96%|█████████▌| 1917/2000 [00:16<00:00, 94.74it/s, train_loss=1.9209, val_loss=1.9065]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1935/2000 [00:16<00:00, 109.98it/s, train_loss=1.9209, val_loss=1.9065]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1953/2000 [00:16<00:00, 124.58it/s, train_loss=1.9209, val_loss=1.9065]\u001B[A\n",
      "Training (constant):  99%|█████████▊| 1971/2000 [00:16<00:00, 136.39it/s, train_loss=1.9209, val_loss=1.9065]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:16<00:00, 118.08it/s, train_loss=1.9209, val_loss=1.9065]\u001B[A\n",
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<09:04,  3.67it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   1%|          | 19/2000 [00:00<00:30, 64.20it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   2%|▏         | 37/2000 [00:00<00:19, 102.14it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   3%|▎         | 55/2000 [00:00<00:15, 126.33it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   4%|▎         | 73/2000 [00:00<00:13, 141.67it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   5%|▍         | 91/2000 [00:00<00:12, 151.99it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   5%|▍         | 91/2000 [00:01<00:12, 151.99it/s, train_loss=3.0924, val_loss=3.0953]\u001B[A\n",
      "Training (constant):   5%|▌         | 108/2000 [00:01<00:22, 85.52it/s, train_loss=3.0924, val_loss=3.0953]\u001B[A\n",
      "Training (constant):   6%|▋         | 126/2000 [00:01<00:18, 102.85it/s, train_loss=3.0924, val_loss=3.0953]\u001B[A\n",
      "Training (constant):   7%|▋         | 144/2000 [00:01<00:15, 118.61it/s, train_loss=3.0924, val_loss=3.0953]\u001B[A\n",
      "Training (constant):   8%|▊         | 163/2000 [00:01<00:13, 133.67it/s, train_loss=3.0924, val_loss=3.0953]\u001B[A\n",
      "Training (constant):   9%|▉         | 181/2000 [00:01<00:12, 144.43it/s, train_loss=3.0924, val_loss=3.0953]\u001B[A\n",
      "Training (constant):  10%|▉         | 199/2000 [00:01<00:11, 153.62it/s, train_loss=3.0924, val_loss=3.0953]\u001B[A\n",
      "Training (constant):  10%|▉         | 199/2000 [00:01<00:11, 153.62it/s, train_loss=2.8587, val_loss=2.8533]\u001B[A\n",
      "Training (constant):  11%|█         | 217/2000 [00:02<00:19, 91.58it/s, train_loss=2.8587, val_loss=2.8533] \u001B[A\n",
      "Training (constant):  12%|█▏        | 236/2000 [00:02<00:16, 108.29it/s, train_loss=2.8587, val_loss=2.8533]\u001B[A\n",
      "Training (constant):  13%|█▎        | 255/2000 [00:02<00:14, 123.59it/s, train_loss=2.8587, val_loss=2.8533]\u001B[A\n",
      "Training (constant):  14%|█▎        | 273/2000 [00:02<00:12, 135.75it/s, train_loss=2.8587, val_loss=2.8533]\u001B[A\n",
      "Training (constant):  15%|█▍        | 291/2000 [00:02<00:11, 145.68it/s, train_loss=2.8587, val_loss=2.8533]\u001B[A\n",
      "Training (constant):  15%|█▍        | 291/2000 [00:02<00:11, 145.68it/s, train_loss=2.6968, val_loss=2.6955]\u001B[A\n",
      "Training (constant):  15%|█▌        | 308/2000 [00:02<00:18, 89.11it/s, train_loss=2.6968, val_loss=2.6955] \u001B[A\n",
      "Training (constant):  16%|█▋        | 326/2000 [00:02<00:15, 104.97it/s, train_loss=2.6968, val_loss=2.6955]\u001B[A\n",
      "Training (constant):  17%|█▋        | 344/2000 [00:03<00:13, 119.78it/s, train_loss=2.6968, val_loss=2.6955]\u001B[A\n",
      "Training (constant):  18%|█▊        | 362/2000 [00:03<00:12, 132.61it/s, train_loss=2.6968, val_loss=2.6955]\u001B[A\n",
      "Training (constant):  19%|█▉        | 380/2000 [00:03<00:11, 143.57it/s, train_loss=2.6968, val_loss=2.6955]\u001B[A\n",
      "Training (constant):  20%|█▉        | 398/2000 [00:03<00:10, 152.06it/s, train_loss=2.6968, val_loss=2.6955]\u001B[A\n",
      "Training (constant):  20%|█▉        | 398/2000 [00:03<00:10, 152.06it/s, train_loss=2.6086, val_loss=2.6071]\u001B[A\n",
      "Training (constant):  21%|██        | 415/2000 [00:03<00:17, 90.55it/s, train_loss=2.6086, val_loss=2.6071] \u001B[A\n",
      "Training (constant):  22%|██▏       | 433/2000 [00:03<00:14, 105.80it/s, train_loss=2.6086, val_loss=2.6071]\u001B[A\n",
      "Training (constant):  23%|██▎       | 451/2000 [00:03<00:12, 120.49it/s, train_loss=2.6086, val_loss=2.6071]\u001B[A\n",
      "Training (constant):  23%|██▎       | 469/2000 [00:04<00:11, 133.05it/s, train_loss=2.6086, val_loss=2.6071]\u001B[A\n",
      "Training (constant):  24%|██▍       | 487/2000 [00:04<00:10, 143.33it/s, train_loss=2.6086, val_loss=2.6071]\u001B[A\n",
      "Training (constant):  24%|██▍       | 487/2000 [00:04<00:10, 143.33it/s, train_loss=2.4723, val_loss=2.4827]\u001B[A\n",
      "Training (constant):  25%|██▌       | 504/2000 [00:04<00:17, 87.96it/s, train_loss=2.4723, val_loss=2.4827] \u001B[A\n",
      "Training (constant):  26%|██▌       | 523/2000 [00:04<00:14, 104.83it/s, train_loss=2.4723, val_loss=2.4827]\u001B[A\n",
      "Training (constant):  27%|██▋       | 541/2000 [00:04<00:12, 118.92it/s, train_loss=2.4723, val_loss=2.4827]\u001B[A\n",
      "Training (constant):  28%|██▊       | 559/2000 [00:04<00:10, 132.28it/s, train_loss=2.4723, val_loss=2.4827]\u001B[A\n",
      "Training (constant):  29%|██▉       | 577/2000 [00:04<00:09, 142.45it/s, train_loss=2.4723, val_loss=2.4827]\u001B[A\n",
      "Training (constant):  30%|██▉       | 595/2000 [00:05<00:09, 151.59it/s, train_loss=2.4723, val_loss=2.4827]\u001B[A\n",
      "Training (constant):  30%|██▉       | 595/2000 [00:05<00:09, 151.59it/s, train_loss=2.3951, val_loss=2.3969]\u001B[A\n",
      "Training (constant):  31%|███       | 612/2000 [00:05<00:15, 89.74it/s, train_loss=2.3951, val_loss=2.3969] \u001B[A\n",
      "Training (constant):  32%|███▏      | 630/2000 [00:05<00:13, 105.01it/s, train_loss=2.3951, val_loss=2.3969]\u001B[A\n",
      "Training (constant):  32%|███▏      | 648/2000 [00:05<00:11, 119.50it/s, train_loss=2.3951, val_loss=2.3969]\u001B[A\n",
      "Training (constant):  33%|███▎      | 666/2000 [00:05<00:10, 132.09it/s, train_loss=2.3951, val_loss=2.3969]\u001B[A\n",
      "Training (constant):  34%|███▍      | 685/2000 [00:05<00:09, 144.19it/s, train_loss=2.3951, val_loss=2.3969]\u001B[A\n",
      "Training (constant):  34%|███▍      | 685/2000 [00:06<00:09, 144.19it/s, train_loss=2.3497, val_loss=2.3557]\u001B[A\n",
      "Training (constant):  35%|███▌      | 702/2000 [00:06<00:14, 88.45it/s, train_loss=2.3497, val_loss=2.3557] \u001B[A\n",
      "Training (constant):  36%|███▌      | 721/2000 [00:06<00:12, 105.48it/s, train_loss=2.3497, val_loss=2.3557]\u001B[A\n",
      "Training (constant):  37%|███▋      | 739/2000 [00:06<00:10, 119.90it/s, train_loss=2.3497, val_loss=2.3557]\u001B[A\n",
      "Training (constant):  38%|███▊      | 757/2000 [00:06<00:09, 133.12it/s, train_loss=2.3497, val_loss=2.3557]\u001B[A\n",
      "Training (constant):  39%|███▉      | 775/2000 [00:06<00:08, 144.09it/s, train_loss=2.3497, val_loss=2.3557]\u001B[A\n",
      "Training (constant):  40%|███▉      | 793/2000 [00:06<00:07, 152.68it/s, train_loss=2.3497, val_loss=2.3557]\u001B[A\n",
      "Training (constant):  40%|███▉      | 793/2000 [00:07<00:07, 152.68it/s, train_loss=2.2657, val_loss=2.2647]\u001B[A\n",
      "Training (constant):  41%|████      | 811/2000 [00:07<00:12, 91.88it/s, train_loss=2.2657, val_loss=2.2647] \u001B[A\n",
      "Training (constant):  41%|████▏     | 829/2000 [00:07<00:10, 107.56it/s, train_loss=2.2657, val_loss=2.2647]\u001B[A\n",
      "Training (constant):  42%|████▏     | 847/2000 [00:07<00:09, 122.07it/s, train_loss=2.2657, val_loss=2.2647]\u001B[A\n",
      "Training (constant):  43%|████▎     | 866/2000 [00:07<00:08, 135.77it/s, train_loss=2.2657, val_loss=2.2647]\u001B[A\n",
      "Training (constant):  44%|████▍     | 885/2000 [00:07<00:07, 147.12it/s, train_loss=2.2657, val_loss=2.2647]\u001B[A\n",
      "Training (constant):  44%|████▍     | 885/2000 [00:07<00:07, 147.12it/s, train_loss=2.2224, val_loss=2.2260]\u001B[A\n",
      "Training (constant):  45%|████▌     | 902/2000 [00:07<00:12, 89.94it/s, train_loss=2.2224, val_loss=2.2260] \u001B[A\n",
      "Training (constant):  46%|████▌     | 919/2000 [00:07<00:10, 103.13it/s, train_loss=2.2224, val_loss=2.2260]\u001B[A\n",
      "Training (constant):  47%|████▋     | 937/2000 [00:08<00:09, 118.11it/s, train_loss=2.2224, val_loss=2.2260]\u001B[A\n",
      "Training (constant):  48%|████▊     | 955/2000 [00:08<00:07, 131.74it/s, train_loss=2.2224, val_loss=2.2260]\u001B[A\n",
      "Training (constant):  49%|████▊     | 973/2000 [00:08<00:07, 143.14it/s, train_loss=2.2224, val_loss=2.2260]\u001B[A\n",
      "Training (constant):  50%|████▉     | 992/2000 [00:08<00:06, 153.26it/s, train_loss=2.2224, val_loss=2.2260]\u001B[A\n",
      "Training (constant):  50%|████▉     | 992/2000 [00:08<00:06, 153.26it/s, train_loss=2.1771, val_loss=2.1763]\u001B[A\n",
      "Training (constant):  50%|█████     | 1010/2000 [00:08<00:10, 91.50it/s, train_loss=2.1771, val_loss=2.1763]\u001B[A\n",
      "Training (constant):  51%|█████▏    | 1029/2000 [00:08<00:08, 108.10it/s, train_loss=2.1771, val_loss=2.1763]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1048/2000 [00:08<00:07, 123.39it/s, train_loss=2.1771, val_loss=2.1763]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1066/2000 [00:09<00:06, 135.03it/s, train_loss=2.1771, val_loss=2.1763]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1084/2000 [00:09<00:06, 145.37it/s, train_loss=2.1771, val_loss=2.1763]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1084/2000 [00:09<00:06, 145.37it/s, train_loss=2.1252, val_loss=2.1283]\u001B[A\n",
      "Training (constant):  55%|█████▌    | 1101/2000 [00:09<00:10, 89.67it/s, train_loss=2.1252, val_loss=2.1283] \u001B[A\n",
      "Training (constant):  56%|█████▌    | 1119/2000 [00:09<00:08, 104.88it/s, train_loss=2.1252, val_loss=2.1283]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1138/2000 [00:09<00:07, 120.55it/s, train_loss=2.1252, val_loss=2.1283]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1156/2000 [00:09<00:06, 133.45it/s, train_loss=2.1252, val_loss=2.1283]\u001B[A\n",
      "Training (constant):  59%|█████▊    | 1174/2000 [00:09<00:05, 143.28it/s, train_loss=2.1252, val_loss=2.1283]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1192/2000 [00:10<00:05, 152.13it/s, train_loss=2.1252, val_loss=2.1283]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1192/2000 [00:10<00:05, 152.13it/s, train_loss=2.0817, val_loss=2.0862]\u001B[A\n",
      "Training (constant):  60%|██████    | 1209/2000 [00:10<00:08, 90.96it/s, train_loss=2.0817, val_loss=2.0862] \u001B[A\n",
      "Training (constant):  61%|██████▏   | 1227/2000 [00:10<00:07, 106.89it/s, train_loss=2.0817, val_loss=2.0862]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1245/2000 [00:10<00:06, 121.13it/s, train_loss=2.0817, val_loss=2.0862]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1264/2000 [00:10<00:05, 135.33it/s, train_loss=2.0817, val_loss=2.0862]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1282/2000 [00:10<00:04, 145.71it/s, train_loss=2.0817, val_loss=2.0862]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1300/2000 [00:10<00:04, 154.24it/s, train_loss=2.0817, val_loss=2.0862]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1300/2000 [00:11<00:04, 154.24it/s, train_loss=2.0339, val_loss=2.0533]\u001B[A\n",
      "Training (constant):  66%|██████▌   | 1318/2000 [00:11<00:07, 91.74it/s, train_loss=2.0339, val_loss=2.0533] \u001B[A\n",
      "Training (constant):  67%|██████▋   | 1336/2000 [00:11<00:06, 107.49it/s, train_loss=2.0339, val_loss=2.0533]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1354/2000 [00:11<00:05, 122.11it/s, train_loss=2.0339, val_loss=2.0533]\u001B[A\n",
      "Training (constant):  69%|██████▊   | 1372/2000 [00:11<00:04, 134.93it/s, train_loss=2.0339, val_loss=2.0533]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1390/2000 [00:11<00:04, 145.69it/s, train_loss=2.0339, val_loss=2.0533]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1390/2000 [00:12<00:04, 145.69it/s, train_loss=2.0222, val_loss=2.0320]\u001B[A\n",
      "Training (constant):  70%|███████   | 1407/2000 [00:12<00:06, 88.81it/s, train_loss=2.0222, val_loss=2.0320] \u001B[A\n",
      "Training (constant):  71%|███████▏  | 1425/2000 [00:12<00:05, 104.64it/s, train_loss=2.0222, val_loss=2.0320]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1443/2000 [00:12<00:04, 119.80it/s, train_loss=2.0222, val_loss=2.0320]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1462/2000 [00:12<00:04, 134.19it/s, train_loss=2.0222, val_loss=2.0320]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1480/2000 [00:12<00:03, 144.75it/s, train_loss=2.0222, val_loss=2.0320]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1499/2000 [00:12<00:03, 154.58it/s, train_loss=2.0222, val_loss=2.0320]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1499/2000 [00:12<00:03, 154.58it/s, train_loss=1.9805, val_loss=1.9854]\u001B[A\n",
      "Training (constant):  76%|███████▌  | 1517/2000 [00:13<00:05, 93.02it/s, train_loss=1.9805, val_loss=1.9854] \u001B[A\n",
      "Training (constant):  77%|███████▋  | 1536/2000 [00:13<00:04, 109.39it/s, train_loss=1.9805, val_loss=1.9854]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1555/2000 [00:13<00:03, 125.00it/s, train_loss=1.9805, val_loss=1.9854]\u001B[A\n",
      "Training (constant):  79%|███████▊  | 1574/2000 [00:13<00:03, 138.03it/s, train_loss=1.9805, val_loss=1.9854]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1593/2000 [00:13<00:02, 148.61it/s, train_loss=1.9805, val_loss=1.9854]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1593/2000 [00:13<00:02, 148.61it/s, train_loss=1.8801, val_loss=1.8889]\u001B[A\n",
      "Training (constant):  81%|████████  | 1611/2000 [00:13<00:04, 91.92it/s, train_loss=1.8801, val_loss=1.8889] \u001B[A\n",
      "Training (constant):  81%|████████▏ | 1629/2000 [00:13<00:03, 107.29it/s, train_loss=1.8801, val_loss=1.8889]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1648/2000 [00:14<00:02, 122.57it/s, train_loss=1.8801, val_loss=1.8889]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1667/2000 [00:14<00:02, 135.88it/s, train_loss=1.8801, val_loss=1.8889]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1685/2000 [00:14<00:02, 146.13it/s, train_loss=1.8801, val_loss=1.8889]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1685/2000 [00:14<00:02, 146.13it/s, train_loss=1.9049, val_loss=1.9126]\u001B[A\n",
      "Training (constant):  85%|████████▌ | 1702/2000 [00:14<00:03, 89.88it/s, train_loss=1.9049, val_loss=1.9126] \u001B[A\n",
      "Training (constant):  86%|████████▌ | 1720/2000 [00:14<00:02, 105.36it/s, train_loss=1.9049, val_loss=1.9126]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1738/2000 [00:14<00:02, 120.05it/s, train_loss=1.9049, val_loss=1.9126]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1756/2000 [00:14<00:01, 133.18it/s, train_loss=1.9049, val_loss=1.9126]\u001B[A\n",
      "Training (constant):  89%|████████▊ | 1774/2000 [00:14<00:01, 142.85it/s, train_loss=1.9049, val_loss=1.9126]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1792/2000 [00:15<00:01, 152.20it/s, train_loss=1.9049, val_loss=1.9126]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1792/2000 [00:15<00:01, 152.20it/s, train_loss=1.9461, val_loss=1.9613]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1809/2000 [00:15<00:02, 90.82it/s, train_loss=1.9461, val_loss=1.9613] \u001B[A\n",
      "Training (constant):  91%|█████████▏| 1828/2000 [00:15<00:01, 107.82it/s, train_loss=1.9461, val_loss=1.9613]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1846/2000 [00:15<00:01, 122.48it/s, train_loss=1.9461, val_loss=1.9613]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1864/2000 [00:15<00:01, 135.43it/s, train_loss=1.9461, val_loss=1.9613]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1882/2000 [00:15<00:00, 146.17it/s, train_loss=1.9461, val_loss=1.9613]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1882/2000 [00:16<00:00, 146.17it/s, train_loss=1.8351, val_loss=1.8502]\u001B[A\n",
      "Training (constant):  95%|█████████▌| 1901/2000 [00:16<00:01, 90.30it/s, train_loss=1.8351, val_loss=1.8502] \u001B[A\n",
      "Training (constant):  96%|█████████▌| 1920/2000 [00:16<00:00, 106.85it/s, train_loss=1.8351, val_loss=1.8502]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1937/2000 [00:16<00:00, 118.83it/s, train_loss=1.8351, val_loss=1.8502]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1955/2000 [00:16<00:00, 131.63it/s, train_loss=1.8351, val_loss=1.8502]\u001B[A\n",
      "Training (constant):  99%|█████████▊| 1973/2000 [00:16<00:00, 142.90it/s, train_loss=1.8351, val_loss=1.8502]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:16<00:00, 118.89it/s, train_loss=1.8351, val_loss=1.8502]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Constant: lr=0.01, avg_val_loss=1.8336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam LR:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:18,  3.58it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:55, 35.98it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 58.43it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 32/2000 [00:00<00:27, 71.02it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 42/2000 [00:00<00:24, 79.86it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 53/2000 [00:00<00:22, 86.69it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 64/2000 [00:00<00:21, 91.37it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:20, 93.75it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 84/2000 [00:01<00:20, 94.89it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 94/2000 [00:01<00:19, 96.18it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 94/2000 [00:01<00:19, 96.18it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 104/2000 [00:01<00:36, 52.58it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 114/2000 [00:01<00:30, 61.11it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 125/2000 [00:01<00:26, 70.04it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 136/2000 [00:01<00:24, 77.63it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 146/2000 [00:02<00:22, 82.99it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 156/2000 [00:02<00:21, 87.18it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 167/2000 [00:02<00:20, 91.25it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 177/2000 [00:02<00:19, 93.55it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 188/2000 [00:02<00:18, 95.59it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:18, 96.82it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:18, 96.82it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 208/2000 [00:02<00:32, 54.64it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 219/2000 [00:03<00:27, 63.74it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 229/2000 [00:03<00:24, 71.21it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 240/2000 [00:03<00:22, 78.34it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▎        | 250/2000 [00:03<00:21, 83.07it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 260/2000 [00:03<00:20, 86.88it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 270/2000 [00:03<00:19, 90.35it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:03<00:18, 93.42it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 292/2000 [00:03<00:17, 95.36it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 292/2000 [00:04<00:17, 95.36it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 302/2000 [00:04<00:32, 52.87it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 313/2000 [00:04<00:27, 62.01it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 323/2000 [00:04<00:24, 69.58it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 334/2000 [00:04<00:21, 76.82it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 344/2000 [00:04<00:20, 79.92it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 354/2000 [00:04<00:19, 82.87it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 364/2000 [00:04<00:19, 85.00it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:04<00:18, 86.57it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 384/2000 [00:05<00:18, 88.14it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:17, 89.57it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:17, 89.57it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 404/2000 [00:05<00:32, 48.95it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 414/2000 [00:05<00:27, 56.77it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 423/2000 [00:05<00:25, 62.99it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 432/2000 [00:05<00:22, 68.85it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 442/2000 [00:05<00:20, 75.93it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 452/2000 [00:06<00:18, 81.84it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 463/2000 [00:06<00:17, 87.24it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:16, 90.38it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 483/2000 [00:06<00:16, 92.77it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:15, 94.32it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:15, 94.32it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 503/2000 [00:06<00:29, 50.97it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 513/2000 [00:07<00:25, 58.81it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 523/2000 [00:07<00:22, 66.77it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 532/2000 [00:07<00:21, 66.98it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:21, 68.95it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 550/2000 [00:07<00:19, 72.85it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 560/2000 [00:07<00:18, 77.59it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 570/2000 [00:07<00:17, 82.22it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 580/2000 [00:07<00:16, 85.41it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 590/2000 [00:07<00:15, 88.23it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:08<00:15, 88.84it/s, train_loss=2.6611, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:08<00:15, 88.84it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:08<00:31, 43.66it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 618/2000 [00:08<00:28, 49.19it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 626/2000 [00:08<00:25, 54.57it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:08<00:23, 59.20it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 642/2000 [00:08<00:21, 63.36it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▎      | 650/2000 [00:09<00:20, 66.89it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 659/2000 [00:09<00:18, 71.36it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 668/2000 [00:09<00:17, 74.57it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 677/2000 [00:09<00:17, 77.01it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 686/2000 [00:09<00:16, 78.62it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 695/2000 [00:09<00:16, 79.09it/s, train_loss=2.5453, val_loss=2.5415]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 695/2000 [00:09<00:16, 79.09it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 704/2000 [00:09<00:28, 45.60it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 713/2000 [00:10<00:24, 53.35it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 722/2000 [00:10<00:21, 60.50it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 731/2000 [00:10<00:19, 66.51it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 740/2000 [00:10<00:17, 71.37it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 749/2000 [00:10<00:16, 75.41it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 758/2000 [00:10<00:15, 78.64it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 767/2000 [00:10<00:15, 81.01it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 776/2000 [00:10<00:14, 82.47it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 785/2000 [00:10<00:14, 83.66it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:14, 85.11it/s, train_loss=2.4267, val_loss=2.4428]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:11<00:14, 85.11it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 803/2000 [00:11<00:24, 48.08it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 812/2000 [00:11<00:21, 55.53it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:11<00:18, 62.20it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 830/2000 [00:11<00:17, 68.17it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 839/2000 [00:11<00:15, 72.95it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 848/2000 [00:11<00:15, 76.34it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 857/2000 [00:11<00:14, 79.13it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 866/2000 [00:12<00:14, 80.99it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:12<00:13, 83.17it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 884/2000 [00:12<00:13, 83.41it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 893/2000 [00:12<00:13, 83.81it/s, train_loss=2.3297, val_loss=2.3377]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 893/2000 [00:12<00:13, 83.81it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 902/2000 [00:12<00:24, 45.33it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:12<00:20, 52.98it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:13<00:17, 61.07it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 931/2000 [00:13<00:15, 68.58it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 941/2000 [00:13<00:14, 74.58it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 951/2000 [00:13<00:13, 78.90it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 961/2000 [00:13<00:12, 82.43it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 970/2000 [00:13<00:12, 84.37it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 980/2000 [00:13<00:11, 86.25it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 990/2000 [00:13<00:11, 87.33it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:13<00:11, 88.62it/s, train_loss=2.2397, val_loss=2.2455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:14<00:11, 88.62it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1010/2000 [00:14<00:19, 52.07it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:14<00:16, 59.83it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:14<00:14, 67.04it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1040/2000 [00:14<00:13, 73.39it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▎    | 1050/2000 [00:14<00:12, 78.74it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1060/2000 [00:14<00:11, 81.84it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1069/2000 [00:14<00:11, 83.47it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:15<00:10, 85.06it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1088/2000 [00:15<00:10, 87.58it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:15<00:10, 89.52it/s, train_loss=2.1472, val_loss=2.1488]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:15<00:10, 89.52it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1108/2000 [00:15<00:17, 51.63it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1118/2000 [00:15<00:14, 59.97it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1128/2000 [00:15<00:12, 67.14it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:15<00:11, 73.66it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1148/2000 [00:16<00:10, 78.57it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:16<00:10, 83.13it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:16<00:09, 85.98it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1178/2000 [00:16<00:09, 87.38it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1188/2000 [00:16<00:09, 88.66it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:16<00:08, 90.27it/s, train_loss=2.0554, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:16<00:08, 90.27it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:16<00:15, 52.61it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1218/2000 [00:17<00:12, 60.88it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1228/2000 [00:17<00:11, 68.54it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1238/2000 [00:17<00:10, 74.85it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1247/2000 [00:17<00:09, 78.41it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:17<00:09, 82.29it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:17<00:08, 85.65it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1277/2000 [00:17<00:08, 87.34it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1287/2000 [00:17<00:07, 89.14it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:17<00:07, 90.29it/s, train_loss=1.9665, val_loss=1.9724]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:18<00:07, 90.29it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:18<00:13, 53.25it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:18<00:11, 61.14it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1327/2000 [00:18<00:09, 68.27it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1337/2000 [00:18<00:08, 74.82it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1347/2000 [00:18<00:08, 80.22it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:18<00:07, 84.59it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1367/2000 [00:18<00:07, 86.29it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1377/2000 [00:19<00:07, 87.20it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1387/2000 [00:19<00:06, 88.62it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:19<00:06, 89.72it/s, train_loss=1.9073, val_loss=1.8995]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:19<00:06, 89.72it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:19<00:11, 53.06it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:19<00:09, 61.11it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1427/2000 [00:19<00:08, 68.57it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1436/2000 [00:19<00:07, 72.67it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1445/2000 [00:20<00:07, 76.63it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1455/2000 [00:20<00:06, 80.92it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1465/2000 [00:20<00:06, 84.63it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1475/2000 [00:20<00:06, 87.29it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1485/2000 [00:20<00:05, 88.49it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1495/2000 [00:20<00:05, 90.60it/s, train_loss=1.8184, val_loss=1.8265]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1495/2000 [00:20<00:05, 90.60it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1505/2000 [00:20<00:09, 52.76it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1515/2000 [00:21<00:07, 60.78it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1525/2000 [00:21<00:06, 68.43it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1535/2000 [00:21<00:06, 74.81it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1545/2000 [00:21<00:05, 80.08it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1555/2000 [00:21<00:05, 83.80it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1565/2000 [00:21<00:05, 86.82it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1575/2000 [00:21<00:04, 89.16it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1585/2000 [00:21<00:04, 91.03it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:21<00:04, 92.10it/s, train_loss=1.7634, val_loss=1.7514]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:22<00:04, 92.10it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1605/2000 [00:22<00:07, 53.85it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1615/2000 [00:22<00:06, 62.02it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1625/2000 [00:22<00:05, 69.28it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:22<00:04, 75.46it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1644/2000 [00:22<00:04, 78.66it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:22<00:04, 83.16it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:22<00:03, 85.78it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:22<00:03, 86.72it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1684/2000 [00:23<00:03, 87.41it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:23<00:03, 88.00it/s, train_loss=1.6829, val_loss=1.6848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:23<00:03, 88.00it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1702/2000 [00:23<00:05, 50.63it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1712/2000 [00:23<00:04, 58.64it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1721/2000 [00:23<00:04, 64.80it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1730/2000 [00:23<00:03, 69.24it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1739/2000 [00:23<00:03, 72.02it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1748/2000 [00:24<00:03, 73.58it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1757/2000 [00:24<00:03, 76.44it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1766/2000 [00:24<00:02, 78.58it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1775/2000 [00:24<00:02, 81.38it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1784/2000 [00:24<00:02, 83.45it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:24<00:02, 80.36it/s, train_loss=1.6159, val_loss=1.6091]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:25<00:02, 80.36it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1802/2000 [00:25<00:04, 44.82it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1812/2000 [00:25<00:03, 53.64it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1822/2000 [00:25<00:02, 61.89it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1832/2000 [00:25<00:02, 68.84it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1841/2000 [00:25<00:02, 73.73it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1851/2000 [00:25<00:01, 78.57it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1861/2000 [00:25<00:01, 82.07it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1871/2000 [00:25<00:01, 84.34it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:25<00:01, 85.53it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:26<00:01, 86.52it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:26<00:01, 87.83it/s, train_loss=1.5449, val_loss=1.5466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:26<00:01, 87.83it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1908/2000 [00:26<00:01, 51.35it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1918/2000 [00:26<00:01, 59.96it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1928/2000 [00:26<00:01, 67.61it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1938/2000 [00:26<00:00, 74.15it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1948/2000 [00:26<00:00, 79.03it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1958/2000 [00:27<00:00, 82.74it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1968/2000 [00:27<00:00, 85.87it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1978/2000 [00:27<00:00, 88.34it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1988/2000 [00:27<00:00, 90.01it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:27<00:00, 72.86it/s, train_loss=1.4861, val_loss=1.5038]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:14,  3.60it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:55, 35.88it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:35, 55.65it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:28, 68.22it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 76.14it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:23, 81.81it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:00<00:22, 85.66it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:21, 88.42it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:21, 90.06it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 91.69it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 91.69it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:36, 52.11it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:31, 60.43it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:27, 67.65it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:01<00:25, 73.87it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:23, 79.35it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:22, 83.57it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:21, 86.67it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:20, 88.57it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:20, 90.38it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 91.69it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 91.69it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:02<00:34, 52.33it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 211/2000 [00:03<00:29, 60.19it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 221/2000 [00:03<00:26, 67.42it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:23, 73.77it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 241/2000 [00:03<00:22, 79.25it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:20, 83.37it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:20, 85.67it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:03<00:19, 88.37it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:03<00:19, 90.46it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:03<00:18, 91.88it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:18, 91.88it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 301/2000 [00:04<00:32, 52.85it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:27, 61.05it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 321/2000 [00:04<00:24, 68.03it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 331/2000 [00:04<00:22, 73.98it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:21, 78.77it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:04<00:19, 83.30it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 86.84it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:05<00:18, 89.11it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:17, 90.54it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 91.63it/s, train_loss=2.9239, val_loss=2.9240]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 91.63it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:30, 52.36it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:05<00:26, 60.24it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:23, 67.68it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:05<00:21, 74.01it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:06<00:19, 79.11it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:06<00:18, 82.91it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 461/2000 [00:06<00:17, 85.87it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 471/2000 [00:06<00:17, 88.26it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:06<00:16, 90.26it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:16, 91.47it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:16, 91.47it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:06<00:28, 52.19it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:07<00:24, 60.22it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 520/2000 [00:07<00:22, 66.13it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 529/2000 [00:07<00:20, 70.70it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 538/2000 [00:07<00:19, 74.87it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 547/2000 [00:07<00:18, 77.89it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 556/2000 [00:07<00:18, 79.46it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 565/2000 [00:07<00:17, 80.97it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 574/2000 [00:07<00:17, 81.56it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 583/2000 [00:07<00:17, 81.38it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:08<00:17, 82.63it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:08<00:17, 82.63it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:08<00:30, 46.43it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:08<00:25, 53.63it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 619/2000 [00:08<00:22, 60.25it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 628/2000 [00:08<00:20, 65.64it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 637/2000 [00:08<00:19, 70.32it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:08<00:18, 73.82it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 655/2000 [00:09<00:17, 76.76it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 664/2000 [00:09<00:16, 79.43it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 673/2000 [00:09<00:16, 81.56it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 682/2000 [00:09<00:15, 83.54it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:09<00:15, 84.68it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:09<00:15, 84.27it/s, train_loss=2.5460, val_loss=2.5473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:09<00:15, 84.27it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 709/2000 [00:09<00:26, 48.69it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 718/2000 [00:10<00:22, 56.04it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 727/2000 [00:10<00:20, 62.32it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 736/2000 [00:10<00:18, 67.99it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 745/2000 [00:10<00:17, 72.37it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:10<00:16, 75.92it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 763/2000 [00:10<00:15, 78.47it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:10<00:15, 78.77it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:10<00:15, 80.43it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:10<00:14, 81.99it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:14, 83.71it/s, train_loss=2.4505, val_loss=2.4462]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:11<00:14, 83.71it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 808/2000 [00:11<00:25, 47.48it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 817/2000 [00:11<00:21, 55.17it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 826/2000 [00:11<00:18, 61.97it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 835/2000 [00:11<00:17, 67.60it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 844/2000 [00:11<00:15, 72.28it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:11<00:15, 76.13it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:12<00:15, 75.25it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 871/2000 [00:12<00:15, 71.59it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 879/2000 [00:12<00:15, 71.41it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 888/2000 [00:12<00:14, 74.52it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:12<00:14, 77.31it/s, train_loss=2.3452, val_loss=2.3429]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:12<00:14, 77.31it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:12<00:25, 43.35it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 914/2000 [00:12<00:21, 51.43it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 923/2000 [00:13<00:18, 58.94it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 932/2000 [00:13<00:16, 65.45it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 942/2000 [00:13<00:14, 72.11it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 951/2000 [00:13<00:13, 75.61it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 960/2000 [00:13<00:13, 79.18it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 970/2000 [00:13<00:12, 83.16it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:13<00:12, 84.23it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:13<00:11, 87.02it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:13<00:12, 82.65it/s, train_loss=2.2534, val_loss=2.2524]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:14<00:12, 82.65it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1007/2000 [00:14<00:22, 43.81it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1015/2000 [00:14<00:20, 49.22it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1024/2000 [00:14<00:17, 55.84it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1033/2000 [00:14<00:15, 62.37it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1043/2000 [00:14<00:13, 69.55it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:14<00:12, 74.46it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1061/2000 [00:15<00:12, 77.61it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1070/2000 [00:15<00:11, 80.79it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1080/2000 [00:15<00:10, 84.01it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1089/2000 [00:15<00:10, 85.64it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:15<00:10, 86.65it/s, train_loss=2.1772, val_loss=2.1744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:15<00:10, 86.65it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1107/2000 [00:15<00:18, 49.40it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1116/2000 [00:15<00:15, 57.01it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1126/2000 [00:15<00:13, 64.79it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1136/2000 [00:16<00:12, 71.10it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:16<00:11, 75.07it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1154/2000 [00:16<00:10, 78.80it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1164/2000 [00:16<00:10, 82.22it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1173/2000 [00:16<00:09, 84.26it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1183/2000 [00:16<00:09, 86.70it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1193/2000 [00:16<00:09, 87.89it/s, train_loss=2.0762, val_loss=2.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1193/2000 [00:17<00:09, 87.89it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1202/2000 [00:17<00:16, 48.73it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1211/2000 [00:17<00:14, 56.03it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1220/2000 [00:17<00:12, 62.77it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1229/2000 [00:17<00:11, 68.84it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1239/2000 [00:17<00:10, 75.03it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1249/2000 [00:17<00:09, 79.92it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1259/2000 [00:17<00:08, 83.34it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1269/2000 [00:17<00:08, 86.06it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:17<00:08, 88.37it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:18<00:07, 89.95it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:18<00:07, 91.26it/s, train_loss=1.9982, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:18<00:07, 91.26it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:18<00:12, 53.30it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:18<00:11, 61.06it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1329/2000 [00:18<00:09, 68.22it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1339/2000 [00:18<00:08, 74.00it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1349/2000 [00:18<00:08, 78.71it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1359/2000 [00:19<00:07, 82.84it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:19<00:07, 85.62it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:19<00:07, 88.44it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:19<00:06, 90.38it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:19<00:06, 91.26it/s, train_loss=1.9107, val_loss=1.9199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:19<00:06, 91.26it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:19<00:11, 53.45it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1419/2000 [00:19<00:09, 61.55it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:20<00:08, 68.71it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1439/2000 [00:20<00:07, 74.91it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1449/2000 [00:20<00:06, 79.83it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1459/2000 [00:20<00:06, 83.78it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1469/2000 [00:20<00:06, 86.59it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1479/2000 [00:20<00:05, 88.98it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1489/2000 [00:20<00:05, 90.29it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:20<00:05, 91.25it/s, train_loss=1.8508, val_loss=1.8589]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:21<00:05, 91.25it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1509/2000 [00:21<00:09, 53.25it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:21<00:07, 61.17it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1529/2000 [00:21<00:06, 68.10it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1539/2000 [00:21<00:06, 74.10it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1549/2000 [00:21<00:05, 79.35it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1559/2000 [00:21<00:05, 82.92it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1569/2000 [00:21<00:05, 86.10it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1579/2000 [00:21<00:04, 88.81it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:22<00:04, 89.70it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:22<00:04, 90.82it/s, train_loss=1.7628, val_loss=1.7673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:22<00:04, 90.82it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:22<00:07, 53.29it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:22<00:06, 59.97it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1628/2000 [00:22<00:05, 67.56it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1638/2000 [00:22<00:04, 73.69it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1648/2000 [00:22<00:04, 79.04it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:23<00:04, 82.62it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1668/2000 [00:23<00:03, 85.66it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1678/2000 [00:23<00:03, 87.82it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1688/2000 [00:23<00:03, 89.04it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:23<00:03, 90.17it/s, train_loss=1.6775, val_loss=1.6934]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:23<00:03, 90.17it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1708/2000 [00:23<00:05, 52.62it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1717/2000 [00:23<00:04, 59.39it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1727/2000 [00:24<00:04, 67.19it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:24<00:03, 73.47it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1747/2000 [00:24<00:03, 78.74it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1757/2000 [00:24<00:02, 82.44it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1767/2000 [00:24<00:02, 84.71it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1777/2000 [00:24<00:02, 87.04it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1787/2000 [00:24<00:02, 88.95it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:24<00:02, 89.97it/s, train_loss=1.6234, val_loss=1.6352]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:25<00:02, 89.97it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1807/2000 [00:25<00:03, 52.52it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1817/2000 [00:25<00:03, 60.43it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1827/2000 [00:25<00:02, 67.37it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1837/2000 [00:25<00:02, 73.83it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1847/2000 [00:25<00:01, 78.97it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1857/2000 [00:25<00:01, 83.29it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1867/2000 [00:25<00:01, 86.28it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1877/2000 [00:25<00:01, 88.69it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1887/2000 [00:26<00:01, 90.61it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:26<00:01, 91.69it/s, train_loss=1.5360, val_loss=1.5305]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:26<00:01, 91.69it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:26<00:01, 53.51it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1917/2000 [00:26<00:01, 61.64it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1927/2000 [00:26<00:01, 68.70it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1937/2000 [00:26<00:00, 74.86it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1947/2000 [00:26<00:00, 80.23it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1957/2000 [00:27<00:00, 83.80it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1967/2000 [00:27<00:00, 86.62it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1977/2000 [00:27<00:00, 88.69it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:27<00:00, 90.46it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:27<00:00, 72.68it/s, train_loss=1.4499, val_loss=1.4451]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:37,  3.46it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 9/2000 [00:00<01:10, 28.18it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 17/2000 [00:00<00:45, 43.44it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 26/2000 [00:00<00:34, 56.77it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 36/2000 [00:00<00:28, 68.18it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 46/2000 [00:00<00:25, 76.49it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:23, 81.87it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:01<00:22, 85.23it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 76/2000 [00:01<00:21, 88.61it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 86/2000 [00:01<00:21, 90.37it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:20, 91.51it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:20, 91.51it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 106/2000 [00:01<00:36, 52.36it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 116/2000 [00:01<00:31, 60.74it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 126/2000 [00:01<00:27, 68.35it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 136/2000 [00:02<00:24, 74.97it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:02<00:23, 78.60it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 155/2000 [00:02<00:22, 82.02it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:21, 84.96it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 175/2000 [00:02<00:20, 88.29it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 185/2000 [00:02<00:20, 90.48it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 195/2000 [00:02<00:19, 91.13it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 195/2000 [00:02<00:19, 91.13it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 205/2000 [00:03<00:33, 53.17it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 215/2000 [00:03<00:28, 61.57it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:25, 68.62it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 235/2000 [00:03<00:23, 74.24it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 245/2000 [00:03<00:22, 79.35it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:20, 83.56it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 265/2000 [00:03<00:19, 86.86it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 275/2000 [00:03<00:19, 89.59it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 285/2000 [00:03<00:18, 91.47it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:03<00:18, 92.46it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:04<00:18, 92.46it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 305/2000 [00:04<00:31, 53.46it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:04<00:27, 61.36it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:24, 68.71it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 335/2000 [00:04<00:22, 74.95it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 345/2000 [00:04<00:20, 80.30it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 355/2000 [00:04<00:19, 83.43it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 365/2000 [00:04<00:18, 86.40it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 375/2000 [00:05<00:18, 88.76it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:05<00:17, 90.77it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 395/2000 [00:05<00:17, 91.86it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 395/2000 [00:05<00:17, 91.86it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 405/2000 [00:05<00:30, 51.89it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 415/2000 [00:05<00:26, 60.00it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 425/2000 [00:05<00:23, 67.58it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 435/2000 [00:06<00:21, 74.19it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 445/2000 [00:06<00:19, 79.59it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 455/2000 [00:06<00:18, 83.75it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 465/2000 [00:06<00:17, 87.07it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 475/2000 [00:06<00:17, 89.18it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 485/2000 [00:06<00:16, 90.92it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:16, 92.58it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:16, 92.58it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 505/2000 [00:07<00:27, 54.26it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 515/2000 [00:07<00:23, 62.20it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 524/2000 [00:07<00:21, 67.73it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 534/2000 [00:07<00:19, 74.48it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 544/2000 [00:07<00:18, 78.80it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 554/2000 [00:07<00:17, 82.83it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 564/2000 [00:07<00:16, 86.66it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 574/2000 [00:07<00:16, 89.01it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 584/2000 [00:07<00:15, 90.71it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 594/2000 [00:07<00:15, 91.76it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 594/2000 [00:08<00:15, 91.76it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 604/2000 [00:08<00:25, 53.86it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 614/2000 [00:08<00:22, 62.00it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 624/2000 [00:08<00:19, 69.34it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:08<00:18, 75.75it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 644/2000 [00:08<00:16, 81.06it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 654/2000 [00:08<00:15, 84.76it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 664/2000 [00:08<00:15, 88.18it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 674/2000 [00:09<00:14, 88.67it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 684/2000 [00:09<00:14, 90.23it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:09<00:14, 91.74it/s, train_loss=2.5535, val_loss=2.5522]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:09<00:14, 91.74it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 704/2000 [00:09<00:24, 53.64it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 714/2000 [00:09<00:20, 61.87it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 724/2000 [00:09<00:18, 69.35it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:09<00:16, 75.75it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 744/2000 [00:10<00:15, 80.27it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:10<00:14, 83.25it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 764/2000 [00:10<00:14, 86.71it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 774/2000 [00:10<00:13, 89.76it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 784/2000 [00:10<00:13, 91.64it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 92.77it/s, train_loss=2.4379, val_loss=2.4422]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 92.77it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 804/2000 [00:10<00:22, 54.30it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:11<00:18, 62.43it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 824/2000 [00:11<00:16, 69.87it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 834/2000 [00:11<00:15, 75.90it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 844/2000 [00:11<00:14, 80.60it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 854/2000 [00:11<00:13, 83.94it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:11<00:13, 86.77it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 874/2000 [00:11<00:12, 89.62it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 884/2000 [00:11<00:12, 91.58it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:11<00:11, 92.77it/s, train_loss=2.3393, val_loss=2.3343]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:12<00:11, 92.77it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 904/2000 [00:12<00:20, 54.27it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 914/2000 [00:12<00:17, 62.52it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 924/2000 [00:12<00:15, 69.62it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 934/2000 [00:12<00:14, 75.76it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 944/2000 [00:12<00:13, 80.99it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 954/2000 [00:12<00:12, 84.93it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 964/2000 [00:12<00:11, 87.94it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 974/2000 [00:12<00:11, 90.09it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:13<00:11, 91.78it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:13<00:10, 92.66it/s, train_loss=2.2389, val_loss=2.2414]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:13<00:10, 92.66it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1004/2000 [00:13<00:18, 53.97it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1014/2000 [00:13<00:15, 62.13it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1024/2000 [00:13<00:14, 69.39it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1034/2000 [00:13<00:12, 75.66it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1044/2000 [00:13<00:11, 80.94it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:14<00:11, 85.05it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1064/2000 [00:14<00:10, 87.80it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:14<00:10, 90.28it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1084/2000 [00:14<00:09, 91.79it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1094/2000 [00:14<00:09, 93.20it/s, train_loss=2.1368, val_loss=2.1425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1094/2000 [00:14<00:09, 93.20it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1104/2000 [00:14<00:16, 54.36it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1112/2000 [00:14<00:15, 59.03it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:15<00:14, 62.73it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1129/2000 [00:15<00:12, 67.32it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1139/2000 [00:15<00:11, 74.27it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1149/2000 [00:15<00:10, 80.11it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1159/2000 [00:15<00:10, 83.80it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1169/2000 [00:15<00:09, 87.32it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1179/2000 [00:15<00:09, 90.15it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1189/2000 [00:15<00:08, 92.19it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1199/2000 [00:15<00:08, 93.71it/s, train_loss=2.0466, val_loss=2.0517]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1199/2000 [00:16<00:08, 93.71it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1209/2000 [00:16<00:14, 54.56it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1219/2000 [00:16<00:12, 63.00it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1229/2000 [00:16<00:10, 70.11it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1239/2000 [00:16<00:09, 76.33it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1249/2000 [00:16<00:09, 81.27it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1259/2000 [00:16<00:08, 85.04it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1269/2000 [00:16<00:08, 87.95it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:16<00:07, 90.42it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:17<00:07, 91.90it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:17<00:07, 93.37it/s, train_loss=1.9613, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:17<00:07, 93.37it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:17<00:12, 54.31it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:17<00:10, 62.55it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1329/2000 [00:17<00:09, 70.00it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1339/2000 [00:17<00:08, 76.17it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1349/2000 [00:17<00:08, 81.37it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1359/2000 [00:18<00:07, 85.15it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:18<00:07, 88.23it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:18<00:06, 90.27it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:18<00:06, 92.07it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:18<00:06, 93.21it/s, train_loss=1.8704, val_loss=1.8830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:18<00:06, 93.21it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:18<00:10, 54.26it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1419/2000 [00:18<00:09, 62.23it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:19<00:08, 69.22it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1439/2000 [00:19<00:07, 75.23it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1449/2000 [00:19<00:06, 80.25it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1459/2000 [00:19<00:06, 84.30it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1469/2000 [00:19<00:06, 86.84it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1479/2000 [00:19<00:05, 89.50it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1489/2000 [00:19<00:05, 91.52it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:19<00:05, 92.89it/s, train_loss=1.7866, val_loss=1.8005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:20<00:05, 92.89it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1509/2000 [00:20<00:09, 54.36it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:20<00:07, 62.29it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1529/2000 [00:20<00:06, 69.91it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1539/2000 [00:20<00:06, 76.23it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1549/2000 [00:20<00:05, 81.41it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1559/2000 [00:20<00:05, 85.23it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1569/2000 [00:20<00:04, 87.97it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1579/2000 [00:20<00:04, 90.67it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:20<00:04, 92.15it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:21<00:04, 93.09it/s, train_loss=1.6941, val_loss=1.7058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:21<00:04, 93.09it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:21<00:07, 54.03it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:21<00:06, 62.13it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:21<00:05, 69.48it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1639/2000 [00:21<00:04, 75.41it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:21<00:04, 80.62it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1659/2000 [00:21<00:04, 84.94it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:22<00:03, 87.70it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1679/2000 [00:22<00:03, 90.19it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1689/2000 [00:22<00:03, 91.94it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:22<00:03, 92.87it/s, train_loss=1.6135, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:22<00:03, 92.87it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1709/2000 [00:22<00:05, 53.47it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1719/2000 [00:22<00:04, 61.58it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1729/2000 [00:22<00:03, 68.87it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1739/2000 [00:23<00:03, 74.80it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1749/2000 [00:23<00:03, 79.83it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1759/2000 [00:23<00:02, 83.80it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1769/2000 [00:23<00:02, 86.01it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1779/2000 [00:23<00:02, 88.37it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1789/2000 [00:23<00:02, 90.51it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:23<00:02, 91.93it/s, train_loss=1.5250, val_loss=1.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:23<00:02, 91.93it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1809/2000 [00:24<00:03, 53.89it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1819/2000 [00:24<00:02, 62.02it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1829/2000 [00:24<00:02, 69.32it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1839/2000 [00:24<00:02, 75.42it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1849/2000 [00:24<00:01, 80.35it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1859/2000 [00:24<00:01, 84.10it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1869/2000 [00:24<00:01, 87.09it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:24<00:01, 89.30it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:24<00:01, 91.20it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:25<00:01, 92.68it/s, train_loss=1.4327, val_loss=1.4465]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:25<00:01, 92.68it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1909/2000 [00:25<00:01, 54.12it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:25<00:01, 62.42it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1929/2000 [00:25<00:01, 69.75it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1939/2000 [00:25<00:00, 76.03it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1949/2000 [00:25<00:00, 80.99it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1959/2000 [00:25<00:00, 85.11it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1969/2000 [00:26<00:00, 87.78it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1979/2000 [00:26<00:00, 89.69it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1989/2000 [00:26<00:00, 91.15it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 75.89it/s, train_loss=1.3499, val_loss=1.3611]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:21<01:21, 81.35s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0001, beta2=0.98, avg_val_loss=1.4367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:36,  3.46it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:57, 34.86it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:35, 55.04it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:28, 68.18it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 77.30it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:23, 83.85it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:00<00:21, 88.22it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:21, 90.94it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:20, 92.73it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 93.77it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 93.77it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:37, 51.05it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:31, 59.58it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:27, 67.57it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:01<00:25, 74.55it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:23, 80.25it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:21, 84.69it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:20, 88.01it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:20, 89.99it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:19, 92.22it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 93.21it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 93.21it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:02<00:36, 49.62it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 209/2000 [00:03<00:32, 54.40it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 218/2000 [00:03<00:29, 60.25it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 226/2000 [00:03<00:28, 61.44it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 236/2000 [00:03<00:25, 68.84it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 246/2000 [00:03<00:23, 75.55it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 256/2000 [00:03<00:21, 80.42it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 266/2000 [00:03<00:20, 84.61it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 276/2000 [00:03<00:19, 88.05it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 286/2000 [00:03<00:19, 89.59it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:04<00:18, 90.37it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:04<00:18, 90.37it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:04<00:40, 41.63it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:04<00:34, 48.73it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:29, 56.88it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 335/2000 [00:04<00:25, 64.51it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 345/2000 [00:05<00:23, 71.11it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 355/2000 [00:05<00:21, 76.15it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 365/2000 [00:05<00:20, 80.83it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:05<00:19, 82.19it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 383/2000 [00:05<00:20, 78.63it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:19, 81.00it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:19, 81.00it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:34, 46.54it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:06<00:28, 55.04it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:06<00:25, 63.15it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:06<00:22, 69.69it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:06<00:20, 75.79it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:06<00:19, 81.12it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 461/2000 [00:06<00:18, 84.95it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 471/2000 [00:06<00:17, 87.32it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:06<00:17, 89.08it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:16, 90.80it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:07<00:16, 90.80it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:07<00:29, 50.84it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:07<00:25, 58.82it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:07<00:22, 66.00it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:07<00:20, 72.26it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:18, 77.46it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:07<00:17, 81.44it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 561/2000 [00:07<00:16, 84.97it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:08<00:16, 87.87it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 581/2000 [00:08<00:15, 90.00it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:15, 91.32it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:15, 91.32it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:08<00:27, 51.10it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 611/2000 [00:08<00:23, 58.80it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 621/2000 [00:08<00:20, 65.99it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 631/2000 [00:08<00:18, 72.74it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 641/2000 [00:09<00:17, 78.07it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 651/2000 [00:09<00:16, 81.65it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 661/2000 [00:09<00:15, 85.11it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 671/2000 [00:09<00:15, 87.54it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 681/2000 [00:09<00:14, 89.45it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:09<00:14, 90.66it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:10<00:14, 90.66it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 701/2000 [00:10<00:25, 51.90it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 711/2000 [00:10<00:21, 59.59it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 721/2000 [00:10<00:19, 66.78it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 731/2000 [00:10<00:17, 72.56it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 741/2000 [00:10<00:16, 78.01it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 751/2000 [00:10<00:15, 82.04it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 761/2000 [00:10<00:14, 85.49it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 771/2000 [00:10<00:13, 88.03it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:10<00:13, 90.63it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:13, 91.66it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:11<00:13, 91.66it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:11<00:23, 51.52it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:11<00:19, 59.51it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:11<00:17, 66.64it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 831/2000 [00:11<00:15, 73.42it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 841/2000 [00:11<00:14, 78.47it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 851/2000 [00:11<00:13, 82.71it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 861/2000 [00:12<00:13, 85.94it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 871/2000 [00:12<00:12, 88.62it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 881/2000 [00:12<00:12, 90.70it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:12<00:12, 92.23it/s, train_loss=2.2465, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:12<00:12, 92.23it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 901/2000 [00:12<00:21, 51.92it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:12<00:18, 59.79it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:12<00:16, 67.11it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 931/2000 [00:13<00:14, 73.01it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 941/2000 [00:13<00:13, 77.93it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 951/2000 [00:13<00:12, 83.05it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 961/2000 [00:13<00:12, 86.06it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 971/2000 [00:13<00:11, 88.49it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 981/2000 [00:13<00:11, 89.91it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 991/2000 [00:13<00:11, 90.82it/s, train_loss=2.1319, val_loss=2.1351]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 991/2000 [00:14<00:11, 90.82it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1001/2000 [00:14<00:19, 51.19it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1011/2000 [00:14<00:16, 59.07it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:14<00:14, 66.48it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1031/2000 [00:14<00:13, 73.09it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:14<00:12, 78.77it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1051/2000 [00:14<00:11, 82.27it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1061/2000 [00:14<00:10, 85.46it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1071/2000 [00:14<00:10, 88.25it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:14<00:10, 89.90it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:15<00:09, 91.16it/s, train_loss=2.0073, val_loss=2.0142]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:15<00:09, 91.16it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:15<00:17, 51.06it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:15<00:15, 58.90it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1121/2000 [00:15<00:13, 66.26it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1131/2000 [00:15<00:11, 73.45it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1142/2000 [00:15<00:10, 80.52it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1152/2000 [00:15<00:09, 85.37it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1163/2000 [00:16<00:09, 89.72it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1173/2000 [00:16<00:08, 92.16it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1184/2000 [00:16<00:08, 94.60it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:16<00:08, 95.93it/s, train_loss=1.8866, val_loss=1.8948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:16<00:08, 95.93it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1204/2000 [00:16<00:14, 53.87it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1214/2000 [00:16<00:12, 62.08it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1225/2000 [00:16<00:10, 70.58it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1236/2000 [00:17<00:09, 77.80it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1247/2000 [00:17<00:08, 83.67it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:17<00:08, 88.30it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1268/2000 [00:17<00:08, 91.33it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1278/2000 [00:17<00:07, 93.48it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:17<00:07, 95.64it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:17<00:07, 96.50it/s, train_loss=1.7706, val_loss=1.7707]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:17<00:07, 96.50it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:18<00:12, 55.09it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:18<00:10, 63.35it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1330/2000 [00:18<00:09, 71.83it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1341/2000 [00:18<00:08, 78.81it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:18<00:07, 84.54it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1363/2000 [00:18<00:07, 88.93it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1374/2000 [00:18<00:06, 92.39it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1385/2000 [00:18<00:06, 94.82it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:18<00:06, 96.56it/s, train_loss=1.6594, val_loss=1.6520]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:19<00:06, 96.56it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1406/2000 [00:19<00:10, 55.88it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:19<00:09, 64.70it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1427/2000 [00:19<00:08, 71.43it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:19<00:07, 77.46it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:19<00:06, 82.70it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:19<00:06, 87.00it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1468/2000 [00:19<00:05, 91.09it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1479/2000 [00:20<00:05, 93.89it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1489/2000 [00:20<00:05, 95.47it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1500/2000 [00:20<00:05, 97.02it/s, train_loss=1.5087, val_loss=1.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1500/2000 [00:20<00:05, 97.02it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1510/2000 [00:20<00:08, 55.32it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1521/2000 [00:20<00:07, 64.45it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1532/2000 [00:20<00:06, 72.49it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1543/2000 [00:20<00:05, 79.51it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:21<00:05, 84.87it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1565/2000 [00:21<00:04, 89.27it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1575/2000 [00:21<00:04, 91.65it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1586/2000 [00:21<00:04, 94.16it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:21<00:04, 96.18it/s, train_loss=1.4095, val_loss=1.3990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:21<00:04, 96.18it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1607/2000 [00:21<00:07, 55.60it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1617/2000 [00:21<00:06, 63.73it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1628/2000 [00:22<00:05, 72.09it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1638/2000 [00:22<00:04, 78.37it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:22<00:04, 84.10it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1659/2000 [00:22<00:03, 87.88it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1670/2000 [00:22<00:03, 91.67it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1681/2000 [00:22<00:03, 94.35it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:22<00:03, 95.87it/s, train_loss=1.2687, val_loss=1.2699]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:23<00:03, 95.87it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:23<00:05, 52.96it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1711/2000 [00:23<00:04, 61.41it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1721/2000 [00:23<00:04, 69.23it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1731/2000 [00:23<00:03, 76.09it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1741/2000 [00:23<00:03, 81.54it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1752/2000 [00:23<00:02, 86.82it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:23<00:02, 90.12it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:23<00:02, 92.53it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1783/2000 [00:23<00:02, 94.92it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:24<00:02, 96.61it/s, train_loss=1.1494, val_loss=1.1460]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:24<00:02, 96.61it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:24<00:03, 54.17it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1814/2000 [00:24<00:02, 62.28it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1825/2000 [00:24<00:02, 70.86it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1835/2000 [00:24<00:02, 77.35it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1845/2000 [00:24<00:01, 82.58it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1855/2000 [00:24<00:01, 87.02it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1866/2000 [00:25<00:01, 91.12it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1877/2000 [00:25<00:01, 93.88it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1887/2000 [00:25<00:01, 95.49it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:25<00:01, 97.27it/s, train_loss=1.0473, val_loss=1.0512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:25<00:01, 97.27it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1908/2000 [00:25<00:01, 55.56it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:25<00:01, 64.58it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1930/2000 [00:25<00:00, 72.57it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1940/2000 [00:26<00:00, 78.68it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:26<00:00, 84.57it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1961/2000 [00:26<00:00, 88.47it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1972/2000 [00:26<00:00, 92.27it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1983/2000 [00:26<00:00, 95.00it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 75.09it/s, train_loss=0.9598, val_loss=0.9685]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:02,  3.69it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.08it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.19it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:27, 72.25it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:23, 81.58it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 54/2000 [00:00<00:22, 86.96it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 64/2000 [00:00<00:21, 90.76it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:20, 93.36it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 85/2000 [00:01<00:19, 95.80it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:19, 96.84it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:19, 96.84it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 105/2000 [00:01<00:35, 53.70it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 115/2000 [00:01<00:30, 62.24it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 125/2000 [00:01<00:26, 70.07it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 136/2000 [00:01<00:24, 77.59it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 146/2000 [00:02<00:22, 82.91it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 156/2000 [00:02<00:21, 87.01it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 166/2000 [00:02<00:20, 90.27it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 176/2000 [00:02<00:19, 92.76it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 186/2000 [00:02<00:19, 94.79it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:18, 96.65it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:18, 96.65it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:02<00:32, 55.05it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 217/2000 [00:02<00:28, 63.40it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 226/2000 [00:03<00:25, 68.94it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 236/2000 [00:03<00:23, 74.79it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 247/2000 [00:03<00:21, 81.80it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 258/2000 [00:03<00:20, 87.09it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 268/2000 [00:03<00:19, 90.42it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 278/2000 [00:03<00:18, 92.67it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 289/2000 [00:03<00:17, 95.07it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:03<00:17, 96.64it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:17, 96.64it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 310/2000 [00:04<00:30, 55.05it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 320/2000 [00:04<00:26, 63.23it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 331/2000 [00:04<00:23, 71.64it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 342/2000 [00:04<00:21, 78.69it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:04<00:19, 83.68it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 362/2000 [00:04<00:18, 87.49it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:04<00:18, 89.54it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 382/2000 [00:04<00:17, 91.95it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:05<00:16, 94.54it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:05<00:16, 94.54it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 403/2000 [00:05<00:29, 53.51it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 413/2000 [00:05<00:25, 61.85it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 424/2000 [00:05<00:22, 70.50it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 435/2000 [00:05<00:20, 77.84it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 446/2000 [00:05<00:18, 83.97it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 456/2000 [00:05<00:17, 87.93it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 466/2000 [00:06<00:16, 90.97it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 477/2000 [00:06<00:16, 93.97it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 488/2000 [00:06<00:15, 96.16it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:06<00:15, 97.87it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:06<00:15, 97.87it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:06<00:26, 56.33it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:06<00:22, 65.16it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 532/2000 [00:06<00:20, 73.13it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 543/2000 [00:07<00:18, 79.98it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 553/2000 [00:07<00:17, 84.69it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 564/2000 [00:07<00:16, 89.01it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 575/2000 [00:07<00:15, 92.49it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:07<00:14, 94.45it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 596/2000 [00:07<00:14, 96.59it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 596/2000 [00:07<00:14, 96.59it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 606/2000 [00:07<00:25, 55.15it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 616/2000 [00:08<00:21, 63.39it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 627/2000 [00:08<00:19, 71.46it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 638/2000 [00:08<00:17, 78.72it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 649/2000 [00:08<00:16, 84.21it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 659/2000 [00:08<00:15, 87.85it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 669/2000 [00:08<00:14, 90.97it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 680/2000 [00:08<00:14, 94.26it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:08<00:13, 96.08it/s, train_loss=2.5157, val_loss=2.5196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:09<00:13, 96.08it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 701/2000 [00:09<00:24, 53.93it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 711/2000 [00:09<00:20, 61.77it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 721/2000 [00:09<00:18, 69.54it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 732/2000 [00:09<00:16, 76.91it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:09<00:15, 82.43it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 753/2000 [00:09<00:14, 87.34it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 763/2000 [00:09<00:13, 90.57it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 773/2000 [00:09<00:13, 93.02it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 784/2000 [00:10<00:12, 95.44it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:10<00:12, 96.92it/s, train_loss=2.4063, val_loss=2.4013]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:10<00:12, 96.92it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 805/2000 [00:10<00:21, 54.36it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 815/2000 [00:10<00:18, 62.64it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 826/2000 [00:10<00:16, 71.03it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 837/2000 [00:10<00:14, 78.21it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 848/2000 [00:10<00:13, 84.10it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 859/2000 [00:11<00:12, 88.59it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 870/2000 [00:11<00:12, 91.86it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 880/2000 [00:11<00:11, 93.91it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 96.08it/s, train_loss=2.2730, val_loss=2.2700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 96.08it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 901/2000 [00:11<00:20, 53.53it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:11<00:17, 61.75it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:12<00:15, 69.27it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 932/2000 [00:12<00:13, 76.75it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 942/2000 [00:12<00:12, 82.16it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 953/2000 [00:12<00:12, 87.13it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 963/2000 [00:12<00:11, 90.19it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 973/2000 [00:12<00:11, 92.70it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:12<00:10, 95.07it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:12<00:10, 96.20it/s, train_loss=2.1566, val_loss=2.1558]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:13<00:10, 96.20it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1004/2000 [00:13<00:18, 53.72it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1014/2000 [00:13<00:16, 61.54it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1024/2000 [00:13<00:14, 69.27it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1035/2000 [00:13<00:12, 76.97it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1045/2000 [00:13<00:11, 82.32it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1056/2000 [00:13<00:10, 87.40it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1067/2000 [00:13<00:10, 91.02it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1077/2000 [00:13<00:09, 93.37it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1088/2000 [00:13<00:09, 95.64it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:14<00:09, 96.76it/s, train_loss=2.0476, val_loss=2.0417]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1098/2000 [00:14<00:09, 96.76it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1108/2000 [00:14<00:16, 54.74it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1119/2000 [00:14<00:13, 63.84it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1130/2000 [00:14<00:12, 72.16it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1141/2000 [00:14<00:10, 79.07it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1151/2000 [00:14<00:10, 83.89it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1162/2000 [00:14<00:09, 88.43it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1172/2000 [00:15<00:09, 91.36it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1182/2000 [00:15<00:08, 93.63it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:15<00:08, 95.40it/s, train_loss=1.8949, val_loss=1.8974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:15<00:08, 95.40it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1202/2000 [00:15<00:15, 53.13it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1212/2000 [00:15<00:12, 61.42it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1222/2000 [00:15<00:11, 69.22it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1233/2000 [00:15<00:09, 76.93it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1244/2000 [00:16<00:09, 83.10it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:16<00:08, 87.95it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1265/2000 [00:16<00:08, 90.57it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1276/2000 [00:16<00:07, 93.50it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1286/2000 [00:16<00:07, 95.16it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1296/2000 [00:16<00:07, 95.76it/s, train_loss=1.7585, val_loss=1.7639]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1296/2000 [00:16<00:07, 95.76it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1306/2000 [00:16<00:12, 54.06it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1316/2000 [00:17<00:10, 62.37it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1327/2000 [00:17<00:09, 71.06it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1338/2000 [00:17<00:08, 78.39it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1348/2000 [00:17<00:07, 83.57it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1358/2000 [00:17<00:07, 87.58it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:17<00:06, 91.24it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1380/2000 [00:17<00:06, 94.28it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:17<00:06, 96.04it/s, train_loss=1.6071, val_loss=1.6131]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:18<00:06, 96.04it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1401/2000 [00:18<00:11, 53.89it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:18<00:09, 62.11it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1422/2000 [00:18<00:08, 70.64it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1433/2000 [00:18<00:07, 77.84it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1443/2000 [00:18<00:06, 82.98it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1453/2000 [00:18<00:06, 87.16it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1464/2000 [00:18<00:05, 91.22it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1475/2000 [00:18<00:05, 93.75it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1486/2000 [00:19<00:05, 95.87it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 97.42it/s, train_loss=1.4723, val_loss=1.4764]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 97.42it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:19<00:08, 55.76it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1517/2000 [00:19<00:07, 63.88it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1528/2000 [00:19<00:06, 72.13it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1539/2000 [00:19<00:05, 79.28it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1549/2000 [00:19<00:05, 84.24it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1559/2000 [00:20<00:05, 88.13it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1569/2000 [00:20<00:04, 91.01it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1579/2000 [00:20<00:04, 93.15it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:20<00:04, 94.91it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:20<00:04, 95.96it/s, train_loss=1.3291, val_loss=1.3396]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:20<00:04, 95.96it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:20<00:07, 54.07it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:20<00:06, 62.51it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1630/2000 [00:21<00:05, 71.21it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1641/2000 [00:21<00:04, 78.44it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1652/2000 [00:21<00:04, 84.07it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1662/2000 [00:21<00:03, 87.98it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:21<00:03, 91.51it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1683/2000 [00:21<00:03, 93.73it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:21<00:03, 95.77it/s, train_loss=1.2160, val_loss=1.2081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:22<00:03, 95.77it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1704/2000 [00:22<00:05, 54.03it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:22<00:04, 62.23it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:22<00:03, 70.78it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:22<00:03, 77.96it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:22<00:03, 83.15it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1757/2000 [00:22<00:02, 87.84it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1767/2000 [00:22<00:02, 90.75it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1777/2000 [00:22<00:02, 92.72it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1787/2000 [00:22<00:02, 94.71it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:23<00:02, 96.14it/s, train_loss=1.1081, val_loss=1.1173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:23<00:02, 96.14it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1807/2000 [00:23<00:03, 53.90it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:23<00:02, 63.36it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1829/2000 [00:23<00:02, 71.64it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1840/2000 [00:23<00:02, 78.75it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1851/2000 [00:23<00:01, 84.46it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1862/2000 [00:23<00:01, 88.73it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1873/2000 [00:24<00:01, 92.21it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1883/2000 [00:24<00:01, 94.07it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:24<00:01, 96.22it/s, train_loss=1.0113, val_loss=1.0128]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:24<00:01, 96.22it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1904/2000 [00:24<00:01, 54.40it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1914/2000 [00:24<00:01, 62.58it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1925/2000 [00:24<00:01, 71.06it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1935/2000 [00:24<00:00, 77.25it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1945/2000 [00:25<00:00, 82.64it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1955/2000 [00:25<00:00, 87.06it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1965/2000 [00:25<00:00, 89.85it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1976/2000 [00:25<00:00, 93.17it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:25<00:00, 95.31it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.09it/s, train_loss=0.9346, val_loss=0.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:02,  3.69it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:53, 36.88it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:34, 57.46it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 32/2000 [00:00<00:27, 72.14it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 43/2000 [00:00<00:24, 81.45it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 53/2000 [00:00<00:22, 86.48it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 63/2000 [00:00<00:21, 89.92it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:20, 93.29it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 85/2000 [00:01<00:20, 95.64it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:19, 96.08it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:19, 96.08it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 105/2000 [00:01<00:35, 53.61it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 115/2000 [00:01<00:30, 61.90it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 125/2000 [00:01<00:26, 69.83it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 135/2000 [00:01<00:24, 76.21it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:02<00:22, 81.52it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 156/2000 [00:02<00:21, 87.04it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 167/2000 [00:02<00:20, 90.95it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 177/2000 [00:02<00:19, 93.28it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 188/2000 [00:02<00:18, 95.62it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:20, 86.93it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:20, 86.93it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 208/2000 [00:02<00:36, 49.23it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:32, 54.40it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:29, 60.40it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 235/2000 [00:03<00:26, 67.62it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 245/2000 [00:03<00:23, 74.27it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:21, 79.47it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 265/2000 [00:03<00:20, 83.90it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 275/2000 [00:03<00:19, 87.42it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 285/2000 [00:03<00:19, 89.65it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:03<00:19, 85.69it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:04<00:19, 85.69it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 304/2000 [00:04<00:40, 41.72it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:37, 45.52it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:04<00:33, 49.57it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:31, 52.95it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 332/2000 [00:04<00:30, 55.46it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 340/2000 [00:05<00:27, 59.87it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 348/2000 [00:05<00:26, 63.43it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 356/2000 [00:05<00:24, 67.51it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 365/2000 [00:05<00:22, 71.75it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:05<00:21, 74.85it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 382/2000 [00:05<00:22, 71.94it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 390/2000 [00:05<00:21, 73.23it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 399/2000 [00:05<00:20, 77.11it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 399/2000 [00:06<00:20, 77.11it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 407/2000 [00:06<00:38, 40.89it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 417/2000 [00:06<00:31, 50.35it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 427/2000 [00:06<00:26, 59.05it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 436/2000 [00:06<00:23, 65.48it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 446/2000 [00:06<00:21, 71.61it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 455/2000 [00:06<00:20, 74.83it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 464/2000 [00:06<00:20, 76.49it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:19, 79.63it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 482/2000 [00:07<00:18, 81.96it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:07<00:17, 86.38it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:07<00:17, 86.38it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:07<00:33, 44.28it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:07<00:28, 51.72it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 519/2000 [00:07<00:25, 58.73it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 528/2000 [00:07<00:22, 64.09it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:08<00:21, 67.60it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 545/2000 [00:08<00:19, 73.11it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 555/2000 [00:08<00:18, 78.18it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 564/2000 [00:08<00:18, 78.90it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 573/2000 [00:08<00:17, 79.46it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 582/2000 [00:08<00:17, 81.61it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:16, 83.68it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:08<00:16, 83.41it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:09<00:16, 83.41it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 609/2000 [00:09<00:31, 43.65it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 619/2000 [00:09<00:26, 52.67it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 629/2000 [00:09<00:22, 60.79it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 638/2000 [00:09<00:20, 66.83it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 647/2000 [00:09<00:18, 72.03it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 656/2000 [00:09<00:17, 75.20it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:09<00:16, 80.41it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:09<00:15, 84.81it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 686/2000 [00:10<00:15, 87.51it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:10<00:14, 90.73it/s, train_loss=2.5111, val_loss=2.5109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:10<00:14, 90.73it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:10<00:27, 46.37it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 716/2000 [00:10<00:23, 55.10it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 726/2000 [00:10<00:20, 62.90it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 735/2000 [00:10<00:18, 68.33it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 745/2000 [00:11<00:16, 74.44it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 755/2000 [00:11<00:15, 79.78it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 765/2000 [00:11<00:14, 83.15it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 775/2000 [00:11<00:15, 81.51it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 785/2000 [00:11<00:14, 84.55it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:11<00:13, 87.24it/s, train_loss=2.3748, val_loss=2.3791]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:11<00:13, 87.24it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 805/2000 [00:11<00:25, 47.59it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 815/2000 [00:12<00:21, 56.01it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 825/2000 [00:12<00:18, 64.11it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 835/2000 [00:12<00:16, 71.50it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 845/2000 [00:12<00:14, 78.16it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 855/2000 [00:12<00:13, 82.89it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 865/2000 [00:12<00:13, 86.28it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:12<00:12, 89.25it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 885/2000 [00:12<00:12, 91.64it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:12<00:11, 93.30it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:13<00:11, 93.30it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:13<00:21, 50.09it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 915/2000 [00:13<00:18, 58.14it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 925/2000 [00:13<00:16, 66.06it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 935/2000 [00:13<00:14, 73.17it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 945/2000 [00:13<00:13, 78.18it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 955/2000 [00:13<00:12, 82.90it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 965/2000 [00:13<00:12, 86.06it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:14<00:11, 89.16it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 985/2000 [00:14<00:11, 91.55it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:14<00:10, 93.80it/s, train_loss=2.0980, val_loss=2.1004]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:14<00:10, 93.80it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:14<00:20, 48.15it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1014/2000 [00:14<00:17, 55.23it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1024/2000 [00:14<00:15, 63.45it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1034/2000 [00:15<00:13, 70.37it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1043/2000 [00:15<00:13, 72.43it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:15<00:12, 76.17it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1061/2000 [00:15<00:11, 78.83it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1071/2000 [00:15<00:11, 82.94it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:15<00:10, 86.95it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:15<00:10, 89.73it/s, train_loss=1.9417, val_loss=1.9476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:16<00:10, 89.73it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:16<00:19, 46.36it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:16<00:16, 54.48it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1121/2000 [00:16<00:13, 63.04it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1131/2000 [00:16<00:12, 70.37it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1141/2000 [00:16<00:11, 76.37it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1151/2000 [00:16<00:10, 80.98it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:16<00:09, 85.32it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1171/2000 [00:16<00:09, 89.00it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1181/2000 [00:16<00:08, 91.29it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:17<00:08, 93.04it/s, train_loss=1.7730, val_loss=1.7875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:17<00:08, 93.04it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:17<00:16, 48.21it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:17<00:14, 55.31it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1220/2000 [00:17<00:12, 63.61it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:17<00:10, 70.72it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:17<00:09, 76.74it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▎   | 1250/2000 [00:18<00:09, 81.30it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1260/2000 [00:18<00:08, 84.92it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1270/2000 [00:18<00:08, 88.07it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1280/2000 [00:18<00:07, 90.48it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1290/2000 [00:18<00:07, 90.65it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:18<00:07, 89.15it/s, train_loss=1.6153, val_loss=1.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:18<00:07, 89.15it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:19<00:14, 46.35it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:19<00:13, 49.50it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1326/2000 [00:19<00:12, 54.19it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1335/2000 [00:19<00:11, 60.16it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1344/2000 [00:19<00:10, 65.41it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1353/2000 [00:19<00:09, 70.17it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1362/2000 [00:19<00:08, 74.49it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1371/2000 [00:19<00:08, 78.51it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1380/2000 [00:19<00:07, 81.64it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:19<00:07, 83.85it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:20<00:06, 86.05it/s, train_loss=1.4577, val_loss=1.4625]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:20<00:06, 86.05it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1408/2000 [00:20<00:14, 42.00it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:20<00:11, 49.81it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1426/2000 [00:20<00:10, 57.00it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1435/2000 [00:20<00:08, 63.23it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1444/2000 [00:20<00:08, 69.33it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1453/2000 [00:21<00:07, 74.39it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1463/2000 [00:21<00:06, 78.91it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1472/2000 [00:21<00:06, 81.57it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1481/2000 [00:21<00:06, 83.69it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1490/2000 [00:21<00:06, 83.23it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:21<00:06, 76.23it/s, train_loss=1.3139, val_loss=1.3220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:22<00:06, 76.23it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:22<00:12, 39.39it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1516/2000 [00:22<00:10, 47.33it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1525/2000 [00:22<00:08, 54.69it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1534/2000 [00:22<00:07, 61.93it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1544/2000 [00:22<00:06, 69.69it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:22<00:05, 74.75it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:22<00:05, 79.59it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1574/2000 [00:22<00:05, 84.09it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1583/2000 [00:22<00:04, 84.16it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:23<00:04, 84.56it/s, train_loss=1.1843, val_loss=1.1944]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:23<00:04, 84.56it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1601/2000 [00:23<00:08, 45.12it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1610/2000 [00:23<00:07, 52.69it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1620/2000 [00:23<00:06, 61.16it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1630/2000 [00:23<00:05, 68.13it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1639/2000 [00:23<00:04, 72.72it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:24<00:04, 77.63it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1659/2000 [00:24<00:04, 81.76it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:24<00:03, 85.29it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1679/2000 [00:24<00:03, 87.08it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1689/2000 [00:24<00:03, 87.59it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:24<00:03, 90.64it/s, train_loss=1.0796, val_loss=1.0793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:24<00:03, 90.64it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1709/2000 [00:25<00:06, 47.00it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:25<00:05, 53.86it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:25<00:04, 62.01it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1738/2000 [00:25<00:03, 68.78it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1747/2000 [00:25<00:03, 73.58it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1757/2000 [00:25<00:03, 79.17it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1766/2000 [00:25<00:03, 76.11it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1775/2000 [00:25<00:02, 76.33it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1784/2000 [00:25<00:02, 76.58it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:25<00:02, 75.69it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:26<00:02, 75.69it/s, train_loss=0.9893, val_loss=1.0008]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:26<00:02, 75.69it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1808/2000 [00:26<00:04, 38.87it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:26<00:03, 48.48it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1828/2000 [00:26<00:02, 57.59it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:26<00:02, 65.48it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1848/2000 [00:26<00:02, 71.82it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1858/2000 [00:27<00:01, 77.09it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1868/2000 [00:27<00:01, 82.09it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1878/2000 [00:27<00:01, 85.76it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:27<00:01, 86.78it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:27<00:01, 86.68it/s, train_loss=0.9183, val_loss=0.9337]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:27<00:01, 86.68it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:27<00:01, 48.64it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1916/2000 [00:28<00:01, 55.90it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1925/2000 [00:28<00:01, 62.65it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1935/2000 [00:28<00:00, 69.42it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1944/2000 [00:28<00:00, 73.49it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1953/2000 [00:28<00:00, 77.46it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:28<00:00, 80.66it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1971/2000 [00:28<00:00, 82.35it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1981/2000 [00:28<00:00, 85.43it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|█████████▉| 1990/2000 [00:28<00:00, 86.58it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 69.00it/s, train_loss=0.8523, val_loss=0.8617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:42<00:00, 81.30s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR:  33%|███▎      | 1/3 [02:42<05:25, 162.62s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0001, beta2=0.999, avg_val_loss=0.9183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:00,  3.33it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:03, 31.23it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 20/2000 [00:00<00:38, 51.59it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 29/2000 [00:00<00:31, 63.06it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 38/2000 [00:00<00:28, 69.64it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 47/2000 [00:00<00:26, 72.40it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:26, 73.53it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 63/2000 [00:01<00:25, 74.61it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:25, 75.24it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 80/2000 [00:01<00:24, 76.89it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:23, 79.75it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 98/2000 [00:01<00:23, 81.35it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 98/2000 [00:01<00:23, 81.35it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 107/2000 [00:01<00:40, 46.60it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 116/2000 [00:01<00:34, 54.09it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 125/2000 [00:02<00:30, 60.91it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 134/2000 [00:02<00:28, 66.53it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:02<00:26, 70.79it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 152/2000 [00:02<00:24, 74.58it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:23, 77.26it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 170/2000 [00:02<00:22, 80.16it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 179/2000 [00:02<00:22, 81.49it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 188/2000 [00:02<00:21, 82.59it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:21, 82.43it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:03<00:21, 82.43it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 206/2000 [00:03<00:37, 47.81it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 215/2000 [00:03<00:32, 55.41it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 224/2000 [00:03<00:28, 62.13it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:03<00:26, 67.88it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 242/2000 [00:03<00:24, 72.45it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:22, 76.37it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 260/2000 [00:03<00:21, 79.15it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 269/2000 [00:04<00:21, 81.23it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 278/2000 [00:04<00:20, 82.68it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 287/2000 [00:04<00:20, 83.78it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:04<00:20, 84.95it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:04<00:20, 84.95it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 305/2000 [00:04<00:35, 47.59it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 312/2000 [00:04<00:34, 48.83it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:32, 52.34it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 328/2000 [00:05<00:28, 59.26it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 337/2000 [00:05<00:25, 65.52it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 346/2000 [00:05<00:23, 69.33it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 354/2000 [00:05<00:25, 63.84it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:05<00:23, 68.41it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:05<00:22, 71.33it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 380/2000 [00:05<00:22, 72.21it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 388/2000 [00:05<00:22, 72.95it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:23, 67.96it/s, train_loss=1.8928, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:06<00:23, 67.96it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 403/2000 [00:06<00:43, 36.57it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 410/2000 [00:06<00:37, 41.95it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 417/2000 [00:06<00:33, 46.77it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 425/2000 [00:06<00:29, 53.78it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 433/2000 [00:06<00:26, 59.22it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:06<00:24, 63.07it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 449/2000 [00:07<00:23, 66.40it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 457/2000 [00:07<00:22, 69.71it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 465/2000 [00:07<00:21, 71.88it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:07<00:20, 73.86it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:07<00:20, 72.89it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 489/2000 [00:07<00:21, 68.83it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:07<00:23, 64.30it/s, train_loss=1.6047, val_loss=1.6361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:08<00:23, 64.30it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 504/2000 [00:08<00:47, 31.41it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:08<00:40, 36.59it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 519/2000 [00:08<00:33, 43.69it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 527/2000 [00:08<00:29, 50.64it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 535/2000 [00:08<00:26, 56.33it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 543/2000 [00:08<00:23, 60.96it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:08<00:22, 64.38it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 559/2000 [00:09<00:21, 67.90it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 567/2000 [00:09<00:22, 63.18it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 575/2000 [00:09<00:21, 66.31it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 583/2000 [00:09<00:20, 68.65it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:09<00:19, 72.44it/s, train_loss=1.3913, val_loss=1.4005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:09<00:19, 72.44it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:09<00:34, 40.40it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:10<00:28, 48.56it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 619/2000 [00:10<00:24, 56.13it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 628/2000 [00:10<00:22, 61.83it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 637/2000 [00:10<00:20, 67.10it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:10<00:18, 71.76it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 655/2000 [00:10<00:17, 75.81it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 664/2000 [00:10<00:16, 78.89it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 673/2000 [00:10<00:16, 81.52it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 682/2000 [00:10<00:16, 81.10it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:10<00:15, 82.91it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:11<00:15, 83.58it/s, train_loss=1.1325, val_loss=1.1196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:11<00:15, 83.58it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 709/2000 [00:11<00:27, 46.65it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 718/2000 [00:11<00:23, 54.27it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 727/2000 [00:11<00:20, 61.34it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 736/2000 [00:11<00:18, 66.96it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 745/2000 [00:11<00:17, 72.04it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:11<00:16, 75.79it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 763/2000 [00:12<00:15, 78.95it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:12<00:15, 81.61it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:12<00:14, 81.54it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:12<00:14, 83.05it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:12<00:14, 84.49it/s, train_loss=0.8814, val_loss=0.8835]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:12<00:14, 84.49it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 808/2000 [00:12<00:25, 45.92it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 817/2000 [00:13<00:22, 53.17it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 826/2000 [00:13<00:19, 59.90it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 835/2000 [00:13<00:17, 66.11it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 844/2000 [00:13<00:16, 71.28it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:13<00:15, 75.80it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:13<00:14, 78.91it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 871/2000 [00:13<00:13, 80.73it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 880/2000 [00:13<00:13, 81.78it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 889/2000 [00:13<00:13, 82.90it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 898/2000 [00:13<00:13, 83.81it/s, train_loss=0.6882, val_loss=0.6967]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 898/2000 [00:14<00:13, 83.81it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 907/2000 [00:14<00:24, 44.66it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 916/2000 [00:14<00:20, 52.20it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 925/2000 [00:14<00:18, 59.39it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 934/2000 [00:14<00:16, 65.83it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 943/2000 [00:14<00:15, 70.30it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 952/2000 [00:14<00:14, 74.43it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 961/2000 [00:15<00:13, 78.18it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 970/2000 [00:15<00:12, 80.25it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:15<00:12, 81.43it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 988/2000 [00:15<00:12, 81.38it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:15<00:12, 83.11it/s, train_loss=0.6293, val_loss=0.6234]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:15<00:12, 83.11it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1006/2000 [00:15<00:22, 43.68it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1015/2000 [00:15<00:19, 50.85it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1024/2000 [00:16<00:16, 57.80it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1033/2000 [00:16<00:15, 64.46it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1042/2000 [00:16<00:13, 70.16it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1051/2000 [00:16<00:12, 74.64it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1060/2000 [00:16<00:11, 78.38it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1069/2000 [00:16<00:11, 80.56it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:16<00:11, 82.67it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1087/2000 [00:16<00:10, 83.99it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:16<00:10, 85.44it/s, train_loss=0.5998, val_loss=0.6010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:17<00:10, 85.44it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1105/2000 [00:17<00:20, 44.38it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1114/2000 [00:17<00:17, 51.49it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1123/2000 [00:17<00:14, 58.69it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1132/2000 [00:17<00:13, 64.76it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1141/2000 [00:17<00:12, 70.02it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▊    | 1150/2000 [00:17<00:11, 73.05it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1159/2000 [00:17<00:10, 76.60it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:18<00:10, 78.88it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1177/2000 [00:18<00:10, 80.87it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1186/2000 [00:18<00:09, 81.65it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:18<00:09, 82.82it/s, train_loss=0.5454, val_loss=0.5539]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:18<00:09, 82.82it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1204/2000 [00:18<00:18, 43.23it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1213/2000 [00:18<00:15, 50.48it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1222/2000 [00:19<00:13, 57.72it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:19<00:12, 63.34it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:19<00:11, 68.04it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1249/2000 [00:19<00:10, 71.84it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:19<00:09, 76.09it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:19<00:09, 79.40it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1276/2000 [00:19<00:08, 82.08it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1285/2000 [00:19<00:08, 82.60it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1294/2000 [00:19<00:08, 83.65it/s, train_loss=0.5211, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1294/2000 [00:20<00:08, 83.65it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1303/2000 [00:20<00:15, 43.92it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1312/2000 [00:20<00:13, 51.45it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:20<00:11, 58.99it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1330/2000 [00:20<00:10, 65.34it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1339/2000 [00:20<00:09, 70.55it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1348/2000 [00:20<00:08, 73.97it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:20<00:08, 76.12it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1366/2000 [00:21<00:08, 79.07it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1375/2000 [00:21<00:07, 81.14it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1384/2000 [00:21<00:07, 82.56it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:21<00:07, 82.77it/s, train_loss=0.5282, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:21<00:07, 82.77it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:21<00:13, 43.44it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:21<00:11, 51.25it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1420/2000 [00:22<00:09, 58.35it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:22<00:08, 64.93it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:22<00:08, 67.76it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1445/2000 [00:22<00:08, 66.71it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1453/2000 [00:22<00:08, 65.43it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1461/2000 [00:22<00:08, 66.75it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1470/2000 [00:22<00:07, 71.39it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1479/2000 [00:22<00:06, 75.68it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1488/2000 [00:22<00:06, 78.32it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:23<00:06, 80.87it/s, train_loss=0.5072, val_loss=0.5136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:23<00:06, 80.87it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1506/2000 [00:23<00:11, 42.70it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1515/2000 [00:23<00:09, 50.41it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1524/2000 [00:23<00:08, 57.97it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1533/2000 [00:23<00:07, 64.36it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1542/2000 [00:23<00:06, 69.44it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1551/2000 [00:23<00:06, 72.17it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1560/2000 [00:24<00:05, 76.16it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1569/2000 [00:24<00:05, 79.18it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1578/2000 [00:24<00:05, 81.48it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1587/2000 [00:24<00:04, 82.97it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:24<00:04, 83.95it/s, train_loss=0.5031, val_loss=0.5040]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:24<00:04, 83.95it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1605/2000 [00:24<00:08, 46.02it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1614/2000 [00:25<00:07, 53.87it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1623/2000 [00:25<00:06, 60.45it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1632/2000 [00:25<00:05, 66.33it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:25<00:05, 69.53it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:25<00:04, 73.70it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:25<00:04, 76.63it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1667/2000 [00:25<00:04, 79.18it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1676/2000 [00:25<00:04, 80.96it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1685/2000 [00:25<00:03, 82.52it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:25<00:03, 83.52it/s, train_loss=0.4957, val_loss=0.4991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:26<00:03, 83.52it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1703/2000 [00:26<00:06, 42.89it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1712/2000 [00:26<00:05, 50.01it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1721/2000 [00:26<00:04, 57.17it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1730/2000 [00:26<00:04, 63.31it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1739/2000 [00:26<00:03, 68.89it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1748/2000 [00:26<00:03, 72.62it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1757/2000 [00:27<00:03, 75.81it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1766/2000 [00:27<00:02, 78.81it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1775/2000 [00:27<00:02, 81.11it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1784/2000 [00:27<00:02, 82.53it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:27<00:02, 83.80it/s, train_loss=0.4879, val_loss=0.4898]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:27<00:02, 83.80it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1802/2000 [00:27<00:04, 43.77it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1811/2000 [00:27<00:03, 51.24it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1820/2000 [00:28<00:03, 57.85it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1829/2000 [00:28<00:02, 63.97it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:28<00:02, 69.52it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1847/2000 [00:28<00:02, 73.38it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:28<00:01, 75.68it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1865/2000 [00:28<00:01, 77.32it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1874/2000 [00:28<00:01, 79.09it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1883/2000 [00:28<00:01, 80.68it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:28<00:01, 81.90it/s, train_loss=0.4748, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:29<00:01, 81.90it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:29<00:02, 42.72it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1909/2000 [00:29<00:01, 48.89it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1918/2000 [00:29<00:01, 56.42it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1927/2000 [00:29<00:01, 63.03it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1936/2000 [00:29<00:00, 69.02it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1945/2000 [00:29<00:00, 72.35it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1954/2000 [00:30<00:00, 74.51it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1963/2000 [00:30<00:00, 76.28it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1972/2000 [00:30<00:00, 77.03it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1981/2000 [00:30<00:00, 79.85it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|█████████▉| 1990/2000 [00:30<00:00, 81.03it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:30<00:00, 65.39it/s, train_loss=0.4740, val_loss=0.4802]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:55,  3.05it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 9/2000 [00:00<01:15, 26.40it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 18/2000 [00:00<00:43, 45.21it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 27/2000 [00:00<00:33, 58.33it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 36/2000 [00:00<00:29, 66.28it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:27, 72.36it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 54/2000 [00:00<00:25, 75.35it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 63/2000 [00:01<00:24, 79.06it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 72/2000 [00:01<00:23, 81.22it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:23, 82.98it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 90/2000 [00:01<00:22, 83.07it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:22, 83.87it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:22, 83.87it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 108/2000 [00:01<00:39, 47.45it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 117/2000 [00:01<00:34, 54.96it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 126/2000 [00:02<00:30, 61.79it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 135/2000 [00:02<00:27, 67.61it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 144/2000 [00:02<00:25, 71.56it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:24, 75.34it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 162/2000 [00:02<00:23, 78.60it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:22, 80.10it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:22, 81.07it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 189/2000 [00:02<00:21, 83.08it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:21, 84.01it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:21, 84.01it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:39, 45.76it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:33, 53.42it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:29, 60.42it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 234/2000 [00:03<00:26, 65.89it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:24, 70.58it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 252/2000 [00:03<00:23, 73.61it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:22, 76.99it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 270/2000 [00:04<00:21, 79.36it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 279/2000 [00:04<00:21, 81.57it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 288/2000 [00:04<00:20, 82.03it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:20, 83.11it/s, train_loss=2.2895, val_loss=2.2972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:20, 83.11it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:04<00:35, 47.25it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:04<00:30, 54.37it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 324/2000 [00:04<00:27, 61.16it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 333/2000 [00:05<00:24, 66.86it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 342/2000 [00:05<00:23, 71.17it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:05<00:21, 75.16it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 360/2000 [00:05<00:20, 78.64it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 369/2000 [00:05<00:21, 75.78it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 377/2000 [00:05<00:22, 70.63it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:05<00:23, 67.86it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:22, 72.86it/s, train_loss=1.9620, val_loss=1.9541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:06<00:22, 72.86it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 402/2000 [00:06<00:40, 39.76it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:06<00:33, 47.30it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 420/2000 [00:06<00:28, 54.87it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 429/2000 [00:06<00:25, 61.42it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 438/2000 [00:06<00:23, 66.84it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 447/2000 [00:06<00:21, 71.64it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 456/2000 [00:06<00:20, 75.03it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 465/2000 [00:07<00:19, 78.45it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 474/2000 [00:07<00:19, 80.25it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 483/2000 [00:07<00:18, 81.78it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:07<00:18, 83.35it/s, train_loss=1.6536, val_loss=1.6306]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:07<00:18, 83.35it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:07<00:33, 44.74it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:07<00:28, 52.05it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 519/2000 [00:07<00:25, 59.20it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 528/2000 [00:08<00:22, 65.50it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 537/2000 [00:08<00:20, 69.95it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 546/2000 [00:08<00:19, 73.32it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 555/2000 [00:08<00:18, 76.62it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 564/2000 [00:08<00:18, 78.55it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 573/2000 [00:08<00:18, 79.08it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 582/2000 [00:08<00:17, 81.45it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:17, 82.32it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:08<00:16, 83.83it/s, train_loss=1.2459, val_loss=1.2433]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:09<00:16, 83.83it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 609/2000 [00:09<00:30, 45.40it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 618/2000 [00:09<00:26, 52.86it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 627/2000 [00:09<00:23, 59.44it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 636/2000 [00:09<00:21, 64.69it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 645/2000 [00:09<00:19, 68.89it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 653/2000 [00:09<00:18, 71.50it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 661/2000 [00:09<00:18, 73.42it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 670/2000 [00:10<00:17, 75.85it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 679/2000 [00:10<00:16, 79.34it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:10<00:16, 80.25it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:10<00:16, 80.85it/s, train_loss=0.9272, val_loss=0.9217]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:10<00:16, 80.85it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:10<00:29, 44.15it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 715/2000 [00:10<00:24, 51.71it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 724/2000 [00:11<00:21, 58.91it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 733/2000 [00:11<00:19, 65.08it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:11<00:18, 69.79it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 751/2000 [00:11<00:17, 73.07it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 760/2000 [00:11<00:16, 75.70it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 769/2000 [00:11<00:15, 77.44it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 778/2000 [00:11<00:15, 79.59it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 787/2000 [00:11<00:14, 81.50it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:11<00:14, 82.91it/s, train_loss=0.7246, val_loss=0.7229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:12<00:14, 82.91it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 805/2000 [00:12<00:26, 44.49it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:12<00:22, 52.29it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 823/2000 [00:12<00:19, 59.25it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:12<00:17, 65.40it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 841/2000 [00:12<00:16, 69.82it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▎     | 850/2000 [00:12<00:15, 74.07it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 859/2000 [00:12<00:14, 76.66it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 868/2000 [00:13<00:14, 79.07it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 877/2000 [00:13<00:14, 79.83it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:13<00:13, 80.24it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:13<00:13, 81.60it/s, train_loss=0.6304, val_loss=0.6301]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:13<00:13, 81.60it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 904/2000 [00:13<00:24, 45.31it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 913/2000 [00:13<00:20, 52.64it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 922/2000 [00:14<00:18, 58.90it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 931/2000 [00:14<00:16, 64.78it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 940/2000 [00:14<00:15, 69.13it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:14<00:14, 71.77it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:14<00:13, 75.57it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:14<00:13, 78.13it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:14<00:12, 79.18it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:14<00:12, 80.81it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:14<00:12, 81.70it/s, train_loss=0.5855, val_loss=0.5937]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:15<00:12, 81.70it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1002/2000 [00:15<00:22, 43.98it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1011/2000 [00:15<00:19, 51.30it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:15<00:16, 58.10it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1029/2000 [00:15<00:15, 64.01it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:15<00:13, 69.06it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1047/2000 [00:15<00:13, 72.26it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1056/2000 [00:15<00:12, 75.24it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1065/2000 [00:16<00:12, 77.46it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:16<00:11, 79.76it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:16<00:11, 81.14it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:16<00:11, 79.83it/s, train_loss=0.5649, val_loss=0.5613]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:16<00:11, 79.83it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:16<00:21, 41.90it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:16<00:18, 49.12it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1119/2000 [00:17<00:15, 56.41it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1128/2000 [00:17<00:13, 62.81it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1137/2000 [00:17<00:12, 67.93it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1146/2000 [00:17<00:11, 71.76it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1155/2000 [00:17<00:11, 74.55it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1164/2000 [00:17<00:11, 75.90it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1173/2000 [00:17<00:10, 77.51it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1182/2000 [00:17<00:10, 78.47it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:17<00:10, 79.58it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:18<00:09, 80.73it/s, train_loss=0.5377, val_loss=0.5354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:18<00:09, 80.73it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1209/2000 [00:18<00:16, 46.74it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1216/2000 [00:18<00:15, 50.87it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1223/2000 [00:18<00:14, 54.08it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:18<00:13, 56.37it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1238/2000 [00:18<00:12, 60.78it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1246/2000 [00:18<00:11, 65.50it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:19<00:10, 70.40it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1264/2000 [00:19<00:09, 74.76it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1272/2000 [00:19<00:09, 73.24it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1280/2000 [00:19<00:09, 75.06it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:19<00:09, 77.82it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:19<00:08, 80.69it/s, train_loss=0.5359, val_loss=0.5399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:19<00:08, 80.69it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:19<00:15, 45.42it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1316/2000 [00:20<00:12, 53.38it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1324/2000 [00:20<00:12, 54.74it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1331/2000 [00:20<00:12, 53.71it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1338/2000 [00:20<00:12, 53.58it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1344/2000 [00:20<00:11, 54.91it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1351/2000 [00:20<00:11, 56.98it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1358/2000 [00:20<00:10, 59.65it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1366/2000 [00:20<00:09, 64.24it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1375/2000 [00:21<00:09, 68.58it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1383/2000 [00:21<00:09, 68.28it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:21<00:08, 68.78it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:21<00:08, 71.62it/s, train_loss=0.5047, val_loss=0.5145]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:21<00:08, 71.62it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:21<00:14, 40.38it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1415/2000 [00:21<00:12, 46.45it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1423/2000 [00:21<00:11, 52.39it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1431/2000 [00:22<00:09, 58.34it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1438/2000 [00:22<00:09, 60.62it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1445/2000 [00:22<00:08, 62.34it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1452/2000 [00:22<00:08, 61.40it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1459/2000 [00:22<00:09, 57.02it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:22<00:08, 61.63it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1475/2000 [00:22<00:07, 66.08it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1483/2000 [00:22<00:07, 67.86it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1491/2000 [00:22<00:07, 70.46it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:23<00:07, 67.62it/s, train_loss=0.4928, val_loss=0.5056]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:23<00:07, 67.62it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1506/2000 [00:23<00:19, 25.15it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1512/2000 [00:23<00:16, 29.42it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:24<00:13, 35.17it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1527/2000 [00:24<00:11, 42.37it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1535/2000 [00:24<00:09, 48.75it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1542/2000 [00:24<00:08, 52.10it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1549/2000 [00:24<00:08, 55.88it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1556/2000 [00:24<00:07, 57.11it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1563/2000 [00:24<00:07, 58.28it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1572/2000 [00:24<00:06, 63.57it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1581/2000 [00:24<00:06, 69.46it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:25<00:05, 76.69it/s, train_loss=0.4890, val_loss=0.4917]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:25<00:05, 76.69it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1601/2000 [00:25<00:09, 41.34it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1610/2000 [00:25<00:08, 48.57it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:25<00:07, 54.24it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1626/2000 [00:25<00:06, 59.42it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:25<00:05, 63.94it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1643/2000 [00:26<00:05, 66.11it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1651/2000 [00:26<00:05, 69.36it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1659/2000 [00:26<00:05, 64.42it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1667/2000 [00:26<00:04, 67.36it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1675/2000 [00:26<00:04, 70.63it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1683/2000 [00:26<00:04, 72.78it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:26<00:04, 73.09it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:26<00:04, 73.16it/s, train_loss=0.4882, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:27<00:04, 73.16it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1707/2000 [00:27<00:07, 39.53it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1716/2000 [00:27<00:05, 47.79it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:27<00:04, 55.15it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1734/2000 [00:27<00:04, 61.64it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1742/2000 [00:27<00:03, 65.61it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1750/2000 [00:27<00:03, 68.58it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1758/2000 [00:27<00:03, 67.83it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1766/2000 [00:27<00:03, 70.63it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1774/2000 [00:28<00:03, 70.45it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1782/2000 [00:28<00:03, 71.30it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1790/2000 [00:28<00:03, 59.82it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:28<00:03, 61.62it/s, train_loss=0.4734, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:28<00:03, 61.62it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:28<00:05, 37.37it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1812/2000 [00:28<00:04, 44.64it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1820/2000 [00:29<00:03, 51.60it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1829/2000 [00:29<00:02, 59.43it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:29<00:02, 65.21it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1847/2000 [00:29<00:02, 69.38it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:29<00:01, 73.16it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1865/2000 [00:29<00:01, 76.27it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1874/2000 [00:29<00:01, 78.47it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1883/2000 [00:29<00:01, 80.08it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:29<00:01, 80.74it/s, train_loss=0.4755, val_loss=0.4784]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1892/2000 [00:30<00:01, 80.74it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:30<00:02, 45.52it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1910/2000 [00:30<00:01, 52.68it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:30<00:01, 59.65it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1928/2000 [00:30<00:01, 65.83it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1937/2000 [00:30<00:00, 70.89it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1946/2000 [00:30<00:00, 73.79it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1955/2000 [00:30<00:00, 75.52it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1964/2000 [00:31<00:00, 77.72it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1973/2000 [00:31<00:00, 79.24it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1982/2000 [00:31<00:00, 81.01it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|█████████▉| 1991/2000 [00:31<00:00, 81.97it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:31<00:00, 63.47it/s, train_loss=0.4851, val_loss=0.4675]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:15,  3.25it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:06, 30.03it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:41, 47.29it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 27/2000 [00:00<00:35, 56.33it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:33, 58.43it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:32, 60.46it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 48/2000 [00:00<00:31, 61.79it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 57/2000 [00:01<00:28, 68.10it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:01<00:26, 72.24it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:25, 74.42it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 83/2000 [00:01<00:25, 76.57it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 92/2000 [00:01<00:24, 78.56it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 92/2000 [00:01<00:24, 78.56it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:43, 43.76it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 110/2000 [00:02<00:37, 51.04it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 119/2000 [00:02<00:32, 58.27it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 128/2000 [00:02<00:29, 64.47it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 137/2000 [00:02<00:27, 68.56it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:02<00:26, 69.89it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:26, 70.42it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:25, 71.84it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 169/2000 [00:02<00:26, 69.26it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 177/2000 [00:02<00:25, 70.82it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 185/2000 [00:02<00:25, 71.80it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:03<00:24, 72.72it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 193/2000 [00:03<00:24, 72.72it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:03<00:50, 35.28it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:47, 37.97it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 213/2000 [00:03<00:43, 41.24it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 220/2000 [00:03<00:38, 45.83it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 226/2000 [00:04<00:36, 48.72it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:04<00:32, 53.69it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 240/2000 [00:04<00:31, 56.17it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 247/2000 [00:04<00:29, 59.75it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 254/2000 [00:04<00:29, 59.36it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:04<00:29, 58.74it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 268/2000 [00:04<00:30, 57.21it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 276/2000 [00:04<00:27, 61.76it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 283/2000 [00:04<00:29, 57.69it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:05<00:27, 62.46it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 299/2000 [00:05<00:25, 66.04it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 299/2000 [00:05<00:25, 66.04it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:05<00:44, 38.45it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:05<00:35, 47.45it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 324/2000 [00:05<00:30, 55.69it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 333/2000 [00:05<00:26, 62.45it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 342/2000 [00:05<00:24, 68.42it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:06<00:22, 72.98it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 360/2000 [00:06<00:22, 71.82it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 368/2000 [00:06<00:22, 71.77it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 376/2000 [00:06<00:22, 71.41it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:06<00:21, 75.09it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:06<00:20, 77.72it/s, train_loss=1.8973, val_loss=1.8942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:07<00:20, 77.72it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 402/2000 [00:07<00:36, 44.04it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:07<00:30, 51.83it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 420/2000 [00:07<00:26, 59.20it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 429/2000 [00:07<00:24, 65.00it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 438/2000 [00:07<00:22, 70.10it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 447/2000 [00:07<00:21, 73.63it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 456/2000 [00:07<00:20, 76.73it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 465/2000 [00:07<00:19, 79.22it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 474/2000 [00:07<00:18, 81.07it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 483/2000 [00:08<00:18, 81.12it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:08<00:18, 81.93it/s, train_loss=1.5715, val_loss=1.5847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:08<00:18, 81.93it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:08<00:33, 44.45it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 509/2000 [00:08<00:29, 50.15it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 518/2000 [00:08<00:25, 57.17it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 527/2000 [00:08<00:23, 63.42it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:08<00:21, 68.72it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 545/2000 [00:09<00:19, 73.42it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 554/2000 [00:09<00:19, 76.05it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 563/2000 [00:09<00:18, 79.10it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 572/2000 [00:09<00:17, 81.24it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 581/2000 [00:09<00:17, 82.38it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 590/2000 [00:09<00:16, 83.17it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 599/2000 [00:09<00:16, 84.10it/s, train_loss=1.1839, val_loss=1.1966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 599/2000 [00:09<00:16, 84.10it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 608/2000 [00:10<00:29, 47.78it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 617/2000 [00:10<00:25, 55.24it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 626/2000 [00:10<00:22, 61.95it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 635/2000 [00:10<00:20, 67.44it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 644/2000 [00:10<00:18, 72.00it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 653/2000 [00:10<00:17, 75.48it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 662/2000 [00:10<00:17, 78.32it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 671/2000 [00:10<00:16, 80.68it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 680/2000 [00:10<00:15, 82.55it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 689/2000 [00:11<00:15, 83.40it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:11<00:15, 84.34it/s, train_loss=0.8909, val_loss=0.8778]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:11<00:15, 84.34it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 707/2000 [00:11<00:27, 47.33it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 716/2000 [00:11<00:23, 55.00it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 725/2000 [00:11<00:20, 61.48it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:11<00:18, 67.41it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 743/2000 [00:11<00:17, 71.49it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:12<00:16, 75.40it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 761/2000 [00:12<00:15, 78.10it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 770/2000 [00:12<00:15, 80.08it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 779/2000 [00:12<00:15, 79.99it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 788/2000 [00:12<00:15, 77.33it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:12<00:16, 73.29it/s, train_loss=0.7025, val_loss=0.6990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:12<00:16, 73.29it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 804/2000 [00:13<00:29, 41.03it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 813/2000 [00:13<00:24, 49.06it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 822/2000 [00:13<00:20, 56.53it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 831/2000 [00:13<00:18, 63.26it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 840/2000 [00:13<00:16, 68.52it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 849/2000 [00:13<00:15, 73.14it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 858/2000 [00:13<00:15, 75.28it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 867/2000 [00:13<00:14, 77.58it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 876/2000 [00:13<00:14, 79.18it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 885/2000 [00:13<00:13, 80.65it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:14<00:13, 80.91it/s, train_loss=0.6170, val_loss=0.6106]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:14<00:13, 80.91it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 903/2000 [00:14<00:23, 45.78it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 912/2000 [00:14<00:20, 53.00it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:14<00:18, 59.72it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 930/2000 [00:14<00:16, 65.63it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 939/2000 [00:14<00:14, 70.75it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:14<00:14, 74.52it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:15<00:13, 77.29it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:15<00:13, 79.15it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:15<00:12, 80.51it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:15<00:12, 82.24it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:15<00:12, 82.82it/s, train_loss=0.5875, val_loss=0.5823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:15<00:12, 82.82it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1002/2000 [00:15<00:21, 46.21it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1011/2000 [00:16<00:18, 53.38it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:16<00:16, 60.36it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1029/2000 [00:16<00:14, 66.35it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:16<00:13, 70.93it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1047/2000 [00:16<00:12, 74.13it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1056/2000 [00:16<00:12, 77.46it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1065/2000 [00:16<00:11, 79.39it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:16<00:11, 80.45it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:16<00:11, 81.99it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:16<00:11, 81.58it/s, train_loss=0.5377, val_loss=0.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:17<00:11, 81.58it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:17<00:19, 45.78it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:17<00:16, 52.98it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1119/2000 [00:17<00:14, 60.00it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1128/2000 [00:17<00:13, 66.01it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1137/2000 [00:17<00:12, 71.08it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1146/2000 [00:17<00:11, 75.08it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1155/2000 [00:18<00:10, 78.29it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1164/2000 [00:18<00:10, 80.50it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1173/2000 [00:18<00:10, 81.58it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1182/2000 [00:18<00:09, 82.94it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:18<00:09, 83.71it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:18<00:09, 84.38it/s, train_loss=0.5207, val_loss=0.5370]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:18<00:09, 84.38it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1209/2000 [00:18<00:16, 47.34it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1218/2000 [00:19<00:14, 54.85it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1227/2000 [00:19<00:12, 61.20it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1236/2000 [00:19<00:11, 67.01it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1245/2000 [00:19<00:10, 71.96it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1254/2000 [00:19<00:09, 76.17it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1263/2000 [00:19<00:09, 79.02it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1272/2000 [00:19<00:08, 81.29it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1281/2000 [00:19<00:08, 82.41it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1290/2000 [00:19<00:08, 83.44it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:19<00:08, 83.38it/s, train_loss=0.5102, val_loss=0.5195]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:20<00:08, 83.38it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1308/2000 [00:20<00:14, 47.37it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:20<00:12, 54.65it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1326/2000 [00:20<00:10, 61.39it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1335/2000 [00:20<00:09, 67.44it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1344/2000 [00:20<00:09, 72.37it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1353/2000 [00:20<00:08, 76.09it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1362/2000 [00:20<00:08, 78.77it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1371/2000 [00:21<00:07, 80.64it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1380/2000 [00:21<00:07, 82.40it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:21<00:07, 83.42it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:21<00:07, 84.37it/s, train_loss=0.5045, val_loss=0.5279]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:21<00:07, 84.37it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:21<00:12, 47.81it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1416/2000 [00:21<00:10, 55.17it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1425/2000 [00:21<00:09, 61.89it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1434/2000 [00:22<00:08, 67.46it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1443/2000 [00:22<00:07, 72.28it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1452/2000 [00:22<00:07, 75.84it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1461/2000 [00:22<00:06, 78.80it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1470/2000 [00:22<00:06, 80.26it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1479/2000 [00:22<00:06, 82.09it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1488/2000 [00:22<00:06, 83.28it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:22<00:05, 84.47it/s, train_loss=0.4913, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:23<00:05, 84.47it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1506/2000 [00:23<00:10, 47.01it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1515/2000 [00:23<00:08, 54.60it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1524/2000 [00:23<00:07, 61.41it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1533/2000 [00:23<00:06, 67.05it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1542/2000 [00:23<00:06, 71.73it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1551/2000 [00:23<00:06, 74.67it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1560/2000 [00:23<00:06, 73.24it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1568/2000 [00:23<00:06, 71.60it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1576/2000 [00:24<00:06, 70.12it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1585/2000 [00:24<00:05, 73.68it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:24<00:05, 76.65it/s, train_loss=0.4959, val_loss=0.4987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:24<00:05, 76.65it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1602/2000 [00:24<00:09, 43.97it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1611/2000 [00:24<00:07, 51.80it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1620/2000 [00:24<00:06, 59.37it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:25<00:05, 65.15it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1638/2000 [00:25<00:05, 69.44it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1647/2000 [00:25<00:04, 72.72it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1656/2000 [00:25<00:04, 76.06it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1665/2000 [00:25<00:04, 78.91it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:25<00:04, 80.85it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1683/2000 [00:25<00:03, 82.11it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:25<00:03, 82.03it/s, train_loss=0.4887, val_loss=0.4855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:26<00:03, 82.03it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:26<00:06, 45.40it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1710/2000 [00:26<00:05, 52.63it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1719/2000 [00:26<00:04, 59.51it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:26<00:04, 65.48it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:26<00:03, 70.19it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:26<00:03, 74.19it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1755/2000 [00:26<00:03, 77.67it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:26<00:02, 80.37it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1773/2000 [00:27<00:02, 81.64it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1782/2000 [00:27<00:02, 82.38it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:27<00:02, 82.90it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:27<00:02, 83.98it/s, train_loss=0.4800, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:27<00:02, 83.98it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1809/2000 [00:27<00:04, 47.49it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:27<00:03, 54.96it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1827/2000 [00:27<00:02, 61.87it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1836/2000 [00:28<00:02, 67.69it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1845/2000 [00:28<00:02, 72.48it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1854/2000 [00:28<00:01, 75.82it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1863/2000 [00:28<00:01, 78.45it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1872/2000 [00:28<00:01, 80.10it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1881/2000 [00:28<00:01, 81.77it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1890/2000 [00:28<00:01, 82.35it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:28<00:01, 82.89it/s, train_loss=0.4715, val_loss=0.4828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:29<00:01, 82.89it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1908/2000 [00:29<00:01, 47.41it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1917/2000 [00:29<00:01, 55.13it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1926/2000 [00:29<00:01, 61.96it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1935/2000 [00:29<00:00, 67.79it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1944/2000 [00:29<00:00, 72.77it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1953/2000 [00:29<00:00, 76.63it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:29<00:00, 79.53it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1971/2000 [00:29<00:00, 81.32it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1980/2000 [00:29<00:00, 82.84it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1989/2000 [00:30<00:00, 83.72it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:30<00:00, 66.22it/s, train_loss=0.4701, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:32<01:32, 92.35s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0005, beta2=0.98, avg_val_loss=0.4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:18,  3.23it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:04, 30.64it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:40, 48.97it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 28/2000 [00:00<00:32, 61.29it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 38/2000 [00:00<00:27, 70.89it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 47/2000 [00:00<00:25, 76.00it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:24, 80.11it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:01<00:23, 83.35it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 75/2000 [00:01<00:22, 84.22it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 84/2000 [00:01<00:22, 85.74it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:22, 86.15it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:22, 86.15it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 102/2000 [00:01<00:39, 47.74it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:34, 54.79it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 120/2000 [00:01<00:30, 61.90it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 129/2000 [00:02<00:27, 68.06it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 138/2000 [00:02<00:25, 73.12it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 147/2000 [00:02<00:24, 76.83it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 156/2000 [00:02<00:23, 79.28it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:22, 81.08it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 174/2000 [00:02<00:21, 83.06it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:21, 84.79it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:20, 86.24it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:03<00:20, 86.24it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:03<00:38, 46.87it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 210/2000 [00:03<00:32, 54.41it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 219/2000 [00:03<00:28, 61.50it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 228/2000 [00:03<00:26, 67.25it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 237/2000 [00:03<00:24, 72.25it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 246/2000 [00:03<00:23, 76.19it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:22, 78.85it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 264/2000 [00:03<00:21, 80.81it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 273/2000 [00:03<00:20, 83.12it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 282/2000 [00:04<00:20, 84.22it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:20, 84.68it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:19, 85.64it/s, train_loss=2.2391, val_loss=2.2354]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:19, 85.64it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 309/2000 [00:04<00:34, 48.52it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:29, 57.04it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 328/2000 [00:04<00:26, 63.81it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 337/2000 [00:04<00:23, 69.47it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 346/2000 [00:05<00:22, 74.09it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 355/2000 [00:05<00:21, 77.88it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 364/2000 [00:05<00:20, 80.28it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 373/2000 [00:05<00:19, 82.77it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 382/2000 [00:05<00:19, 84.30it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:18, 85.16it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 400/2000 [00:05<00:18, 86.16it/s, train_loss=1.8332, val_loss=1.8473]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 400/2000 [00:05<00:18, 86.16it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 409/2000 [00:06<00:32, 48.67it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 418/2000 [00:06<00:28, 56.42it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 427/2000 [00:06<00:24, 63.26it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 436/2000 [00:06<00:22, 69.03it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 445/2000 [00:06<00:21, 73.90it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 454/2000 [00:06<00:19, 77.97it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 463/2000 [00:06<00:20, 75.05it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 472/2000 [00:06<00:21, 72.67it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 480/2000 [00:06<00:20, 72.51it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 489/2000 [00:07<00:19, 76.49it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 498/2000 [00:07<00:18, 79.71it/s, train_loss=1.4908, val_loss=1.5246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 498/2000 [00:07<00:18, 79.71it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 507/2000 [00:07<00:31, 47.22it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 517/2000 [00:07<00:26, 55.93it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 526/2000 [00:07<00:23, 62.89it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 535/2000 [00:07<00:21, 68.58it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 545/2000 [00:07<00:19, 74.19it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 554/2000 [00:08<00:19, 74.13it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 563/2000 [00:08<00:19, 73.85it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:08<00:19, 74.55it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 580/2000 [00:08<00:18, 76.67it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 589/2000 [00:08<00:17, 78.43it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:08<00:17, 80.79it/s, train_loss=1.0801, val_loss=1.0813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:08<00:17, 80.79it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 607/2000 [00:08<00:29, 47.54it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 616/2000 [00:09<00:25, 54.30it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 624/2000 [00:09<00:23, 58.93it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 632/2000 [00:09<00:21, 63.33it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 640/2000 [00:09<00:20, 66.59it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 648/2000 [00:09<00:19, 69.24it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:09<00:18, 73.38it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:09<00:17, 76.88it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 675/2000 [00:09<00:16, 79.69it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 684/2000 [00:09<00:16, 79.59it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:10<00:16, 80.14it/s, train_loss=0.7971, val_loss=0.7816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:10<00:16, 80.14it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 702/2000 [00:10<00:29, 43.38it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 710/2000 [00:10<00:26, 49.55it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 718/2000 [00:10<00:23, 55.45it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 726/2000 [00:10<00:21, 60.57it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:10<00:19, 65.16it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:10<00:18, 68.37it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 751/2000 [00:11<00:17, 72.11it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 760/2000 [00:11<00:16, 75.63it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 769/2000 [00:11<00:15, 78.22it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 778/2000 [00:11<00:15, 78.07it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 787/2000 [00:11<00:16, 74.48it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:11<00:15, 75.44it/s, train_loss=0.6729, val_loss=0.6813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:12<00:15, 75.44it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 803/2000 [00:12<00:29, 40.53it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 812/2000 [00:12<00:24, 48.04it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:12<00:21, 55.47it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 830/2000 [00:12<00:18, 61.87it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 839/2000 [00:12<00:17, 67.19it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 848/2000 [00:12<00:16, 71.34it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 857/2000 [00:12<00:15, 74.85it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 866/2000 [00:12<00:14, 77.81it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:12<00:13, 80.45it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 884/2000 [00:13<00:13, 81.72it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 893/2000 [00:13<00:13, 82.86it/s, train_loss=0.5952, val_loss=0.6036]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 893/2000 [00:13<00:13, 82.86it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 902/2000 [00:13<00:25, 43.36it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:13<00:21, 50.37it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 920/2000 [00:13<00:19, 56.81it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 929/2000 [00:13<00:17, 62.60it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 938/2000 [00:14<00:15, 67.77it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 947/2000 [00:14<00:14, 72.44it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 956/2000 [00:14<00:13, 76.13it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 965/2000 [00:14<00:13, 79.00it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 974/2000 [00:14<00:12, 80.34it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 983/2000 [00:14<00:12, 79.34it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 992/2000 [00:14<00:12, 79.48it/s, train_loss=0.5728, val_loss=0.5657]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 992/2000 [00:15<00:12, 79.48it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1001/2000 [00:15<00:24, 41.03it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1009/2000 [00:15<00:20, 47.37it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1018/2000 [00:15<00:17, 55.04it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1027/2000 [00:15<00:15, 62.00it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1036/2000 [00:15<00:14, 67.62it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1045/2000 [00:15<00:13, 71.77it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:15<00:12, 75.74it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1063/2000 [00:15<00:12, 77.37it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1072/2000 [00:15<00:11, 78.27it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:16<00:11, 78.67it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1090/2000 [00:16<00:11, 78.83it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1099/2000 [00:16<00:11, 78.74it/s, train_loss=0.5510, val_loss=0.5564]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1099/2000 [00:16<00:11, 78.74it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1107/2000 [00:16<00:20, 42.74it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1115/2000 [00:16<00:18, 48.86it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1123/2000 [00:16<00:16, 54.30it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1132/2000 [00:17<00:14, 60.69it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1141/2000 [00:17<00:13, 65.85it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▊    | 1150/2000 [00:17<00:12, 70.27it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1159/2000 [00:17<00:11, 73.96it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:17<00:10, 76.73it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1177/2000 [00:17<00:10, 78.10it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1186/2000 [00:17<00:10, 80.11it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:17<00:09, 81.78it/s, train_loss=0.5208, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:18<00:09, 81.78it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1204/2000 [00:18<00:17, 45.53it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1213/2000 [00:18<00:14, 52.87it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1222/2000 [00:18<00:13, 59.52it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:18<00:11, 65.38it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:18<00:10, 69.78it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1249/2000 [00:18<00:10, 73.12it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:18<00:09, 75.71it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:18<00:09, 78.27it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1276/2000 [00:19<00:09, 79.43it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1285/2000 [00:19<00:08, 80.11it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1294/2000 [00:19<00:08, 80.04it/s, train_loss=0.4956, val_loss=0.4973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1294/2000 [00:19<00:08, 80.04it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1303/2000 [00:19<00:15, 44.15it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1311/2000 [00:19<00:13, 49.53it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:19<00:12, 54.46it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:20<00:10, 61.11it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1337/2000 [00:20<00:10, 66.18it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1345/2000 [00:20<00:09, 69.23it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1354/2000 [00:20<00:08, 73.37it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1363/2000 [00:20<00:08, 76.54it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1372/2000 [00:20<00:07, 79.02it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1381/2000 [00:20<00:07, 80.45it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1390/2000 [00:20<00:07, 81.72it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:20<00:07, 82.89it/s, train_loss=0.4940, val_loss=0.4946]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:21<00:07, 82.89it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1408/2000 [00:21<00:12, 46.91it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:21<00:10, 54.10it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1425/2000 [00:21<00:10, 57.01it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1432/2000 [00:21<00:09, 58.88it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1439/2000 [00:21<00:09, 60.82it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1448/2000 [00:21<00:08, 66.56it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:21<00:07, 71.48it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1466/2000 [00:22<00:07, 75.52it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1475/2000 [00:22<00:06, 78.64it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1484/2000 [00:22<00:06, 80.55it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1493/2000 [00:22<00:06, 80.79it/s, train_loss=0.4892, val_loss=0.4959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1493/2000 [00:22<00:06, 80.79it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1502/2000 [00:22<00:11, 44.50it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1511/2000 [00:22<00:09, 52.06it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1520/2000 [00:22<00:08, 59.38it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1529/2000 [00:23<00:07, 65.23it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1538/2000 [00:23<00:06, 70.14it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1547/2000 [00:23<00:06, 74.21it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1556/2000 [00:23<00:05, 77.23it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1565/2000 [00:23<00:05, 79.63it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1574/2000 [00:23<00:05, 80.72it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1583/2000 [00:23<00:05, 82.32it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:23<00:04, 83.19it/s, train_loss=0.4918, val_loss=0.4948]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:24<00:04, 83.19it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1601/2000 [00:24<00:08, 45.11it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1610/2000 [00:24<00:07, 52.63it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:24<00:06, 59.79it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1628/2000 [00:24<00:05, 65.87it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1637/2000 [00:24<00:05, 71.21it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1646/2000 [00:24<00:04, 75.14it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1655/2000 [00:24<00:04, 77.80it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:24<00:04, 79.76it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:25<00:04, 81.60it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1682/2000 [00:25<00:03, 82.93it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:25<00:03, 83.65it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1700/2000 [00:25<00:03, 84.63it/s, train_loss=0.4734, val_loss=0.4758]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1700/2000 [00:25<00:03, 84.63it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1709/2000 [00:25<00:06, 47.29it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:25<00:05, 54.89it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1727/2000 [00:25<00:04, 61.82it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:26<00:03, 67.63it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1745/2000 [00:26<00:03, 72.37it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:26<00:03, 75.95it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1763/2000 [00:26<00:03, 78.65it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:26<00:02, 80.57it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1781/2000 [00:26<00:02, 81.73it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1790/2000 [00:26<00:02, 83.22it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:26<00:02, 84.22it/s, train_loss=0.4853, val_loss=0.4869]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:27<00:02, 84.22it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1808/2000 [00:27<00:04, 47.38it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1817/2000 [00:27<00:03, 54.70it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1826/2000 [00:27<00:02, 61.09it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1835/2000 [00:27<00:02, 67.07it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1844/2000 [00:27<00:02, 71.82it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1853/2000 [00:27<00:01, 75.83it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1862/2000 [00:27<00:01, 79.14it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1871/2000 [00:27<00:01, 81.38it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:28<00:01, 82.95it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:28<00:01, 83.52it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:28<00:01, 84.38it/s, train_loss=0.4634, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:28<00:01, 84.38it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:28<00:01, 47.62it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1916/2000 [00:28<00:01, 55.27it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1925/2000 [00:28<00:01, 61.88it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1934/2000 [00:28<00:00, 67.59it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1943/2000 [00:29<00:00, 71.73it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1952/2000 [00:29<00:00, 75.28it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1961/2000 [00:29<00:00, 78.61it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1970/2000 [00:29<00:00, 80.93it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1979/2000 [00:29<00:00, 82.75it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1988/2000 [00:29<00:00, 83.35it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:29<00:00, 67.31it/s, train_loss=0.4613, val_loss=0.4700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:15,  3.25it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:06, 30.13it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:41, 47.84it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 28/2000 [00:00<00:32, 59.76it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 37/2000 [00:00<00:28, 67.91it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 46/2000 [00:00<00:26, 73.41it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:25, 76.57it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 64/2000 [00:01<00:24, 78.12it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 73/2000 [00:01<00:24, 79.88it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 82/2000 [00:01<00:23, 81.77it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:22, 83.25it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:22, 83.88it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:22, 83.88it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 109/2000 [00:01<00:40, 47.21it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 118/2000 [00:01<00:34, 54.81it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 127/2000 [00:02<00:30, 61.62it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 136/2000 [00:02<00:27, 67.15it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:02<00:25, 71.80it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:24, 75.73it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 163/2000 [00:02<00:23, 78.76it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 172/2000 [00:02<00:22, 81.00it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:22, 81.67it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 190/2000 [00:02<00:21, 82.47it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 199/2000 [00:02<00:21, 83.62it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 199/2000 [00:03<00:21, 83.62it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 208/2000 [00:03<00:38, 47.15it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 217/2000 [00:03<00:32, 54.75it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 226/2000 [00:03<00:28, 61.54it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 235/2000 [00:03<00:26, 67.41it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 244/2000 [00:03<00:24, 72.19it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 253/2000 [00:03<00:23, 75.35it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 262/2000 [00:03<00:22, 78.32it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:04<00:21, 80.36it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 280/2000 [00:04<00:21, 81.64it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 289/2000 [00:04<00:20, 82.53it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:20, 83.46it/s, train_loss=2.2631, val_loss=2.2710]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:20, 83.46it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 307/2000 [00:04<00:35, 47.08it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 316/2000 [00:04<00:30, 54.55it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:27, 61.35it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 334/2000 [00:05<00:25, 66.51it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 343/2000 [00:05<00:23, 70.75it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:05<00:22, 72.66it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 360/2000 [00:05<00:21, 76.09it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 369/2000 [00:05<00:20, 78.54it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 378/2000 [00:05<00:20, 79.99it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 387/2000 [00:05<00:19, 81.52it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:21, 76.35it/s, train_loss=1.8674, val_loss=1.8609]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:06<00:21, 76.35it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 404/2000 [00:06<00:39, 40.72it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 413/2000 [00:06<00:32, 48.61it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 422/2000 [00:06<00:28, 55.96it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:06<00:24, 62.84it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 440/2000 [00:06<00:22, 68.16it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 449/2000 [00:06<00:21, 72.54it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 458/2000 [00:06<00:20, 76.10it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 467/2000 [00:06<00:19, 79.16it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 476/2000 [00:07<00:18, 80.67it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 485/2000 [00:07<00:18, 81.84it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 494/2000 [00:07<00:18, 82.83it/s, train_loss=1.3184, val_loss=1.2986]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 494/2000 [00:07<00:18, 82.83it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 503/2000 [00:07<00:32, 46.41it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 512/2000 [00:07<00:27, 53.95it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:07<00:24, 60.65it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 530/2000 [00:08<00:22, 66.57it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 539/2000 [00:08<00:20, 71.52it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 548/2000 [00:08<00:19, 75.27it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 557/2000 [00:08<00:18, 77.90it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 566/2000 [00:08<00:18, 78.36it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 575/2000 [00:08<00:17, 80.29it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 584/2000 [00:08<00:17, 81.60it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:08<00:16, 83.00it/s, train_loss=0.9190, val_loss=0.9208]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:09<00:16, 83.00it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 602/2000 [00:09<00:30, 45.54it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:09<00:26, 51.58it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 619/2000 [00:09<00:23, 58.72it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 628/2000 [00:09<00:21, 65.02it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 637/2000 [00:09<00:19, 70.39it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:09<00:18, 73.89it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 655/2000 [00:09<00:17, 76.99it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 664/2000 [00:09<00:16, 79.19it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 673/2000 [00:09<00:16, 81.08it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 682/2000 [00:10<00:16, 82.04it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:10<00:16, 81.36it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:10<00:15, 82.57it/s, train_loss=0.7180, val_loss=0.7221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:10<00:15, 82.57it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 709/2000 [00:10<00:27, 46.94it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 718/2000 [00:10<00:23, 54.34it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 727/2000 [00:10<00:20, 61.19it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 736/2000 [00:11<00:18, 66.69it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 745/2000 [00:11<00:17, 71.74it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:11<00:16, 75.44it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 763/2000 [00:11<00:15, 78.22it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:11<00:15, 80.36it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:11<00:14, 81.81it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:11<00:14, 83.20it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:11<00:14, 84.05it/s, train_loss=0.6397, val_loss=0.6383]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:12<00:14, 84.05it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 808/2000 [00:12<00:25, 46.99it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 817/2000 [00:12<00:21, 54.45it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 826/2000 [00:12<00:19, 61.35it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 835/2000 [00:12<00:17, 67.22it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 844/2000 [00:12<00:16, 71.38it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:12<00:15, 74.79it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:12<00:14, 77.29it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 871/2000 [00:12<00:14, 79.72it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 880/2000 [00:12<00:13, 81.58it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 889/2000 [00:13<00:13, 82.50it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 898/2000 [00:13<00:13, 82.78it/s, train_loss=0.5858, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 898/2000 [00:13<00:13, 82.78it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 907/2000 [00:13<00:23, 46.34it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 916/2000 [00:13<00:20, 53.86it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 925/2000 [00:13<00:17, 60.87it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 934/2000 [00:13<00:16, 66.47it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 943/2000 [00:14<00:14, 71.35it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 952/2000 [00:14<00:13, 74.90it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 961/2000 [00:14<00:13, 77.89it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 970/2000 [00:14<00:12, 80.21it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:14<00:12, 82.54it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 988/2000 [00:14<00:12, 83.65it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:14<00:11, 83.66it/s, train_loss=0.5546, val_loss=0.5621]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:14<00:11, 83.66it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1006/2000 [00:15<00:20, 47.74it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1015/2000 [00:15<00:17, 55.39it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1024/2000 [00:15<00:15, 61.95it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1033/2000 [00:15<00:14, 67.78it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1042/2000 [00:15<00:13, 73.03it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1051/2000 [00:15<00:12, 76.66it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1060/2000 [00:15<00:11, 79.52it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1069/2000 [00:15<00:11, 81.45it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:15<00:11, 82.63it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1087/2000 [00:15<00:10, 83.99it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:16<00:10, 84.97it/s, train_loss=0.5465, val_loss=0.5479]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:16<00:10, 84.97it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1105/2000 [00:16<00:18, 47.64it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1114/2000 [00:16<00:16, 55.33it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1123/2000 [00:16<00:14, 62.23it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1132/2000 [00:16<00:12, 68.24it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1141/2000 [00:16<00:11, 73.09it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▊    | 1150/2000 [00:16<00:11, 76.49it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1159/2000 [00:17<00:10, 79.57it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:17<00:10, 81.38it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1177/2000 [00:17<00:09, 82.83it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1186/2000 [00:17<00:09, 84.18it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:17<00:09, 84.95it/s, train_loss=0.5320, val_loss=0.5334]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:17<00:09, 84.95it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1204/2000 [00:17<00:16, 47.63it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1213/2000 [00:17<00:14, 55.26it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1222/2000 [00:18<00:12, 62.10it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:18<00:11, 68.28it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:18<00:10, 72.97it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1249/2000 [00:18<00:09, 76.67it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:18<00:09, 79.63it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:18<00:09, 81.23it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1276/2000 [00:18<00:08, 82.58it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1285/2000 [00:18<00:08, 83.85it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1294/2000 [00:18<00:08, 84.92it/s, train_loss=0.5157, val_loss=0.5245]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1294/2000 [00:19<00:08, 84.92it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1303/2000 [00:19<00:17, 39.54it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1312/2000 [00:19<00:14, 47.16it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:19<00:12, 54.50it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1330/2000 [00:19<00:10, 61.26it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1339/2000 [00:19<00:09, 67.38it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1348/2000 [00:19<00:09, 71.96it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:20<00:08, 75.72it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1366/2000 [00:20<00:08, 78.61it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1375/2000 [00:20<00:07, 79.80it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1384/2000 [00:20<00:07, 81.65it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:20<00:07, 82.90it/s, train_loss=0.4968, val_loss=0.5058]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:20<00:07, 82.90it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:20<00:12, 46.64it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:20<00:10, 54.32it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1420/2000 [00:21<00:09, 61.04it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:21<00:08, 66.95it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1438/2000 [00:21<00:07, 71.80it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:21<00:07, 75.97it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1456/2000 [00:21<00:06, 78.76it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1465/2000 [00:21<00:06, 80.82it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1474/2000 [00:21<00:06, 82.00it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1483/2000 [00:21<00:06, 83.36it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:21<00:06, 84.21it/s, train_loss=0.4780, val_loss=0.4926]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:22<00:06, 84.21it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1501/2000 [00:22<00:10, 47.26it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1510/2000 [00:22<00:08, 55.01it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1521/2000 [00:22<00:07, 65.28it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1532/2000 [00:22<00:06, 73.82it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1543/2000 [00:22<00:05, 80.86it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1553/2000 [00:22<00:05, 85.63it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:22<00:04, 89.95it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1575/2000 [00:22<00:04, 93.11it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1586/2000 [00:23<00:04, 95.51it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:23<00:04, 97.15it/s, train_loss=0.4822, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:23<00:04, 97.15it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1607/2000 [00:23<00:07, 55.39it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:23<00:05, 64.30it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:23<00:05, 72.30it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:23<00:04, 79.18it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1651/2000 [00:24<00:04, 84.77it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1662/2000 [00:24<00:03, 89.17it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1672/2000 [00:24<00:03, 90.16it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1682/2000 [00:24<00:03, 92.18it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:24<00:03, 94.58it/s, train_loss=0.4718, val_loss=0.4687]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:24<00:03, 94.58it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1703/2000 [00:24<00:05, 54.24it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:24<00:04, 63.29it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:25<00:03, 71.71it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:25<00:03, 78.81it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:25<00:03, 83.05it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1757/2000 [00:25<00:02, 87.82it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1768/2000 [00:25<00:02, 91.52it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1779/2000 [00:25<00:02, 94.06it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1790/2000 [00:25<00:02, 96.34it/s, train_loss=0.4783, val_loss=0.4901]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1790/2000 [00:26<00:02, 96.34it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1801/2000 [00:26<00:03, 56.42it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1812/2000 [00:26<00:02, 65.18it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1823/2000 [00:26<00:02, 73.03it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:26<00:02, 79.72it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1845/2000 [00:26<00:01, 85.30it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:26<00:01, 89.61it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1867/2000 [00:26<00:01, 93.18it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1878/2000 [00:26<00:01, 95.68it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:26<00:01, 97.15it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:27<00:01, 98.67it/s, train_loss=0.4752, val_loss=0.4771]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:27<00:01, 98.67it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:27<00:01, 56.97it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:27<00:01, 65.73it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:27<00:00, 73.58it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1943/2000 [00:27<00:00, 79.44it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1954/2000 [00:27<00:00, 84.96it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1965/2000 [00:27<00:00, 89.34it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1976/2000 [00:28<00:00, 92.81it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:28<00:00, 95.20it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 70.63it/s, train_loss=0.4840, val_loss=0.4671]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:00,  3.70it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:53, 36.96it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 59.45it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:26, 73.54it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:23, 82.67it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:21, 88.79it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:00<00:20, 93.07it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 77/2000 [00:01<00:20, 95.66it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 88/2000 [00:01<00:19, 97.74it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:19, 98.71it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:19, 98.71it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 110/2000 [00:01<00:33, 56.34it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:28, 65.35it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 132/2000 [00:01<00:25, 73.25it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:01<00:23, 79.82it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:21, 85.16it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:20, 89.46it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 176/2000 [00:02<00:19, 92.87it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 187/2000 [00:02<00:19, 95.33it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:18, 96.58it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:18, 96.58it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:02<00:32, 55.39it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 218/2000 [00:02<00:27, 64.58it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 229/2000 [00:03<00:24, 72.65it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 240/2000 [00:03<00:22, 79.42it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:20, 85.04it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 262/2000 [00:03<00:19, 89.55it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 273/2000 [00:03<00:18, 92.83it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 284/2000 [00:03<00:17, 95.47it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:03<00:17, 97.34it/s, train_loss=2.2324, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:04<00:17, 97.34it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:04<00:29, 56.78it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 317/2000 [00:04<00:25, 65.55it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 328/2000 [00:04<00:22, 73.47it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 339/2000 [00:04<00:20, 80.07it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 349/2000 [00:04<00:19, 84.62it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 360/2000 [00:04<00:18, 89.08it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:04<00:17, 92.68it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 382/2000 [00:04<00:17, 95.13it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:04<00:16, 97.11it/s, train_loss=1.7772, val_loss=1.7740]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:05<00:16, 97.11it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 404/2000 [00:05<00:28, 56.53it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 415/2000 [00:05<00:24, 65.26it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 426/2000 [00:05<00:21, 73.19it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 437/2000 [00:05<00:19, 80.02it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 448/2000 [00:05<00:18, 85.47it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 459/2000 [00:05<00:17, 89.74it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 470/2000 [00:05<00:16, 92.88it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:06<00:15, 95.21it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:06<00:15, 97.21it/s, train_loss=1.2370, val_loss=1.2500]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 492/2000 [00:06<00:15, 97.21it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 503/2000 [00:06<00:26, 56.52it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 514/2000 [00:06<00:22, 65.14it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 525/2000 [00:06<00:20, 72.95it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:06<00:18, 79.62it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 547/2000 [00:07<00:17, 85.16it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 558/2000 [00:07<00:16, 89.49it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 569/2000 [00:07<00:15, 92.89it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 580/2000 [00:07<00:14, 95.11it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 590/2000 [00:07<00:14, 96.31it/s, train_loss=0.8899, val_loss=0.9047]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 590/2000 [00:07<00:14, 96.31it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:07<00:25, 55.71it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 611/2000 [00:07<00:21, 63.70it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 622/2000 [00:08<00:19, 71.90it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 632/2000 [00:08<00:17, 78.04it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 642/2000 [00:08<00:16, 83.34it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 653/2000 [00:08<00:15, 88.29it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 664/2000 [00:08<00:14, 91.99it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 674/2000 [00:08<00:14, 94.01it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 685/2000 [00:08<00:13, 96.23it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:08<00:13, 97.59it/s, train_loss=0.7134, val_loss=0.7015]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:09<00:13, 97.59it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:09<00:23, 55.25it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 716/2000 [00:09<00:20, 63.34it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 727/2000 [00:09<00:17, 71.58it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 737/2000 [00:09<00:16, 77.63it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 748/2000 [00:09<00:14, 83.56it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 759/2000 [00:09<00:14, 88.28it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 770/2000 [00:09<00:13, 91.82it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:09<00:12, 94.49it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:10<00:12, 96.71it/s, train_loss=0.6316, val_loss=0.6251]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:10<00:12, 96.71it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 802/2000 [00:10<00:21, 55.97it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 812/2000 [00:10<00:18, 64.02it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 823/2000 [00:10<00:16, 72.23it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 834/2000 [00:10<00:14, 79.19it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 845/2000 [00:10<00:13, 85.03it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 856/2000 [00:10<00:12, 89.55it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 867/2000 [00:11<00:12, 92.71it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 878/2000 [00:11<00:11, 95.11it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 889/2000 [00:11<00:11, 97.18it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 900/2000 [00:11<00:11, 98.54it/s, train_loss=0.5868, val_loss=0.5820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 900/2000 [00:11<00:11, 98.54it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:11<00:19, 57.22it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 922/2000 [00:11<00:16, 65.96it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 932/2000 [00:11<00:14, 72.79it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 943/2000 [00:12<00:13, 79.85it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 954/2000 [00:12<00:12, 85.49it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 965/2000 [00:12<00:11, 89.89it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 976/2000 [00:12<00:11, 93.03it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 987/2000 [00:12<00:10, 95.37it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:12<00:10, 97.23it/s, train_loss=0.5595, val_loss=0.5545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:12<00:10, 97.23it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1009/2000 [00:12<00:17, 56.59it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:13<00:14, 65.36it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1031/2000 [00:13<00:13, 73.18it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1042/2000 [00:13<00:11, 79.94it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1053/2000 [00:13<00:11, 85.52it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1064/2000 [00:13<00:10, 89.78it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1075/2000 [00:13<00:09, 93.24it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1086/2000 [00:13<00:09, 95.54it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:13<00:09, 97.40it/s, train_loss=0.5332, val_loss=0.5388]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:14<00:09, 97.40it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1108/2000 [00:14<00:15, 56.97it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1119/2000 [00:14<00:13, 65.74it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1130/2000 [00:14<00:11, 73.63it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1140/2000 [00:14<00:10, 79.18it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1151/2000 [00:14<00:10, 84.90it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:14<00:09, 88.44it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1172/2000 [00:14<00:08, 92.06it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1183/2000 [00:14<00:08, 94.59it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:15<00:08, 96.43it/s, train_loss=0.5105, val_loss=0.5236]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:15<00:08, 96.43it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1204/2000 [00:15<00:14, 55.33it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1215/2000 [00:15<00:12, 64.19it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1226/2000 [00:15<00:10, 72.26it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:15<00:09, 79.13it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:15<00:08, 84.65it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1259/2000 [00:15<00:08, 89.00it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1270/2000 [00:16<00:07, 92.59it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1281/2000 [00:16<00:07, 94.90it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:16<00:07, 96.75it/s, train_loss=0.5108, val_loss=0.5187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:16<00:07, 96.75it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1302/2000 [00:16<00:12, 55.55it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1313/2000 [00:16<00:10, 64.46it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1324/2000 [00:16<00:09, 72.55it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1335/2000 [00:17<00:08, 79.42it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1346/2000 [00:17<00:07, 85.06it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:17<00:07, 89.13it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1368/2000 [00:17<00:06, 92.55it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:17<00:06, 95.02it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1390/2000 [00:17<00:06, 96.79it/s, train_loss=0.4940, val_loss=0.5154]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1390/2000 [00:17<00:06, 96.79it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1401/2000 [00:17<00:10, 56.47it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1412/2000 [00:18<00:09, 65.13it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1423/2000 [00:18<00:07, 73.06it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1434/2000 [00:18<00:07, 79.69it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1445/2000 [00:18<00:06, 85.01it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1456/2000 [00:18<00:06, 89.65it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:18<00:05, 92.61it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1478/2000 [00:18<00:05, 95.25it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1489/2000 [00:18<00:05, 96.98it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1500/2000 [00:18<00:05, 98.13it/s, train_loss=0.4840, val_loss=0.4956]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1500/2000 [00:19<00:05, 98.13it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1511/2000 [00:19<00:08, 56.72it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1522/2000 [00:19<00:07, 65.28it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1532/2000 [00:19<00:06, 72.16it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1542/2000 [00:19<00:05, 78.22it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1553/2000 [00:19<00:05, 84.04it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:19<00:04, 88.60it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1575/2000 [00:19<00:04, 92.23it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1586/2000 [00:20<00:04, 94.57it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:20<00:04, 96.51it/s, train_loss=0.4860, val_loss=0.4884]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:20<00:04, 96.51it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1607/2000 [00:20<00:07, 55.87it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:20<00:05, 64.71it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:20<00:05, 72.80it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:20<00:04, 79.52it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1651/2000 [00:20<00:04, 85.11it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1662/2000 [00:21<00:03, 89.34it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:21<00:03, 92.57it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1684/2000 [00:21<00:03, 94.92it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:21<00:03, 95.38it/s, train_loss=0.4802, val_loss=0.4744]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:21<00:03, 95.38it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1704/2000 [00:21<00:05, 55.15it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1715/2000 [00:21<00:04, 64.32it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1726/2000 [00:21<00:03, 72.55it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:22<00:03, 79.45it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1748/2000 [00:22<00:02, 84.93it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1759/2000 [00:22<00:02, 89.31it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1770/2000 [00:22<00:02, 92.50it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1781/2000 [00:22<00:02, 94.87it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:22<00:02, 96.59it/s, train_loss=0.4759, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:22<00:02, 96.59it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1802/2000 [00:23<00:03, 55.97it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1813/2000 [00:23<00:02, 64.86it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1824/2000 [00:23<00:02, 72.83it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1835/2000 [00:23<00:02, 79.59it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1845/2000 [00:23<00:01, 84.46it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:23<00:01, 88.81it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1866/2000 [00:23<00:01, 91.58it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1877/2000 [00:23<00:01, 94.09it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1887/2000 [00:23<00:01, 95.70it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:23<00:01, 96.56it/s, train_loss=0.4712, val_loss=0.4833]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:24<00:01, 96.56it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:24<00:01, 54.61it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1918/2000 [00:24<00:01, 63.72it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1928/2000 [00:24<00:01, 70.86it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1938/2000 [00:24<00:00, 77.45it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1949/2000 [00:24<00:00, 83.48it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1960/2000 [00:24<00:00, 88.17it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1970/2000 [00:24<00:00, 90.82it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1981/2000 [00:25<00:00, 93.82it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 79.17it/s, train_loss=0.4666, val_loss=0.4819]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:55<00:00, 87.04s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR:  67%|██████▋   | 2/3 [05:38<02:50, 170.30s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0005, beta2=0.999, avg_val_loss=0.4730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:08,  3.65it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:51, 38.88it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 58.22it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:27, 72.56it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 43/2000 [00:00<00:24, 80.75it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 54/2000 [00:00<00:22, 87.34it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 65/2000 [00:00<00:21, 91.50it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 76/2000 [00:01<00:20, 94.48it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 86/2000 [00:01<00:19, 95.71it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:19, 96.56it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:19, 96.56it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 106/2000 [00:01<00:35, 53.86it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 117/2000 [00:01<00:29, 63.29it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 128/2000 [00:01<00:26, 71.61it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 139/2000 [00:01<00:23, 78.61it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 150/2000 [00:02<00:21, 84.25it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:20, 88.95it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:19, 91.78it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:19, 93.71it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:18, 95.93it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:18, 95.93it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 202/2000 [00:02<00:32, 54.82it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 213/2000 [00:02<00:27, 63.94it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 223/2000 [00:03<00:24, 71.35it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 234/2000 [00:03<00:22, 78.48it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 245/2000 [00:03<00:20, 84.27it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:19, 88.23it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 266/2000 [00:03<00:18, 92.14it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 277/2000 [00:03<00:18, 94.58it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 288/2000 [00:03<00:17, 96.79it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:03<00:17, 97.68it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:17, 97.68it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:30, 55.19it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:04<00:26, 63.27it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 328/2000 [00:04<00:23, 70.83it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 339/2000 [00:04<00:21, 78.13it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 350/2000 [00:04<00:19, 83.87it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 88.34it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:04<00:17, 91.78it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 383/2000 [00:04<00:17, 94.69it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:16, 96.37it/s, train_loss=1.4734, val_loss=1.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:16, 96.37it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 404/2000 [00:05<00:28, 55.71it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 415/2000 [00:05<00:24, 64.54it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 426/2000 [00:05<00:21, 72.47it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 436/2000 [00:05<00:19, 78.53it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 447/2000 [00:05<00:18, 84.38it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 457/2000 [00:05<00:17, 88.18it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 468/2000 [00:06<00:16, 91.66it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 478/2000 [00:06<00:16, 93.65it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 489/2000 [00:06<00:15, 95.58it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:06<00:15, 96.47it/s, train_loss=1.1638, val_loss=1.1911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:06<00:15, 96.47it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 509/2000 [00:06<00:27, 54.80it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 520/2000 [00:06<00:23, 64.07it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:06<00:20, 72.37it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 542/2000 [00:07<00:18, 79.39it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 553/2000 [00:07<00:17, 84.83it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 564/2000 [00:07<00:16, 89.42it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 575/2000 [00:07<00:15, 93.10it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 586/2000 [00:07<00:14, 95.57it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 597/2000 [00:07<00:14, 97.09it/s, train_loss=0.8387, val_loss=0.8454]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 597/2000 [00:07<00:14, 97.09it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 608/2000 [00:07<00:24, 56.55it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 619/2000 [00:08<00:21, 65.21it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 630/2000 [00:08<00:18, 72.96it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 641/2000 [00:08<00:17, 79.85it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 652/2000 [00:08<00:15, 85.20it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 663/2000 [00:08<00:14, 89.37it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 674/2000 [00:08<00:14, 92.37it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 685/2000 [00:08<00:13, 95.03it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:08<00:13, 96.74it/s, train_loss=0.6576, val_loss=0.6419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:09<00:13, 96.74it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:09<00:23, 55.31it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 716/2000 [00:09<00:20, 62.99it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 726/2000 [00:09<00:18, 70.21it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 736/2000 [00:09<00:16, 76.72it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 747/2000 [00:09<00:15, 82.97it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 757/2000 [00:09<00:14, 87.03it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 768/2000 [00:09<00:13, 90.84it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 779/2000 [00:09<00:13, 93.67it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:10<00:12, 95.49it/s, train_loss=0.5730, val_loss=0.5704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:10<00:12, 95.49it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:21, 55.28it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 812/2000 [00:10<00:18, 63.98it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 822/2000 [00:10<00:16, 71.16it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 833/2000 [00:10<00:14, 78.15it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 843/2000 [00:10<00:13, 83.26it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:10<00:13, 87.23it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:11<00:12, 91.31it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:11<00:11, 94.09it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 885/2000 [00:11<00:11, 95.14it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:11, 96.28it/s, train_loss=0.5372, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:11, 96.28it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:11<00:20, 54.00it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 915/2000 [00:11<00:17, 62.45it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 925/2000 [00:11<00:15, 70.24it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 935/2000 [00:12<00:13, 76.79it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 946/2000 [00:12<00:12, 83.19it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:12<00:11, 87.90it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 968/2000 [00:12<00:11, 91.54it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:12<00:10, 94.52it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 990/2000 [00:12<00:10, 96.37it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:12<00:10, 95.13it/s, train_loss=0.5334, val_loss=0.5307]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:13<00:10, 95.13it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1010/2000 [00:13<00:18, 53.86it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:13<00:15, 63.10it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1032/2000 [00:13<00:13, 71.38it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1043/2000 [00:13<00:12, 78.36it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:13<00:11, 84.15it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1065/2000 [00:13<00:10, 88.52it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1076/2000 [00:13<00:10, 91.82it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1087/2000 [00:13<00:09, 94.42it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:13<00:09, 95.90it/s, train_loss=0.5155, val_loss=0.5185]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:14<00:09, 95.90it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1107/2000 [00:14<00:16, 54.39it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1117/2000 [00:14<00:14, 62.67it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1128/2000 [00:14<00:12, 71.01it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1139/2000 [00:14<00:11, 78.11it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▊    | 1150/2000 [00:14<00:10, 83.91it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:14<00:09, 88.16it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1171/2000 [00:15<00:09, 89.87it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1181/2000 [00:15<00:08, 92.33it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:15<00:08, 94.85it/s, train_loss=0.4977, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:15<00:08, 94.85it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1202/2000 [00:15<00:14, 54.28it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1213/2000 [00:15<00:12, 63.42it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1224/2000 [00:15<00:10, 71.72it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1235/2000 [00:15<00:09, 78.62it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1245/2000 [00:16<00:09, 83.65it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1256/2000 [00:16<00:08, 88.35it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:16<00:07, 91.94it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1278/2000 [00:16<00:07, 94.41it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:16<00:07, 96.22it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 97.55it/s, train_loss=0.4825, val_loss=0.4883]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 97.55it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:16<00:12, 55.95it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:17<00:10, 64.83it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1332/2000 [00:17<00:09, 73.03it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1343/2000 [00:17<00:08, 79.92it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1354/2000 [00:17<00:07, 85.18it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1364/2000 [00:17<00:07, 88.86it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1375/2000 [00:17<00:06, 92.11it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1386/2000 [00:17<00:06, 94.85it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:17<00:06, 96.58it/s, train_loss=0.4902, val_loss=0.4876]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:18<00:06, 96.58it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:18<00:10, 55.57it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1418/2000 [00:18<00:09, 64.60it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1428/2000 [00:18<00:07, 71.77it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1438/2000 [00:18<00:07, 78.09it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1449/2000 [00:18<00:06, 84.18it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1460/2000 [00:18<00:06, 88.73it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1471/2000 [00:18<00:05, 92.26it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1482/2000 [00:18<00:05, 94.82it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:19<00:05, 96.03it/s, train_loss=0.4835, val_loss=0.4843]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:19<00:05, 96.03it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1502/2000 [00:19<00:09, 54.13it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1512/2000 [00:19<00:07, 62.17it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1522/2000 [00:19<00:06, 69.94it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1532/2000 [00:19<00:06, 76.57it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1542/2000 [00:19<00:05, 81.89it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1553/2000 [00:19<00:05, 86.94it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1563/2000 [00:20<00:04, 90.34it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1573/2000 [00:20<00:04, 92.79it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1583/2000 [00:20<00:04, 94.79it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.42it/s, train_loss=0.4841, val_loss=0.4863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.42it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:20<00:07, 54.07it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1614/2000 [00:20<00:06, 62.42it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1624/2000 [00:20<00:05, 70.21it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:21<00:04, 77.67it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:21<00:04, 82.81it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1655/2000 [00:21<00:03, 87.22it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1665/2000 [00:21<00:03, 90.19it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1676/2000 [00:21<00:03, 93.35it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1686/2000 [00:21<00:03, 94.73it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:21<00:03, 95.93it/s, train_loss=0.4798, val_loss=0.4813]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:21<00:03, 95.93it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1706/2000 [00:22<00:05, 53.85it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1717/2000 [00:22<00:04, 63.21it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1727/2000 [00:22<00:03, 70.78it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:22<00:03, 77.36it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1748/2000 [00:22<00:03, 83.53it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1759/2000 [00:22<00:02, 88.28it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1770/2000 [00:22<00:02, 91.86it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1781/2000 [00:22<00:02, 94.41it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:22<00:02, 95.92it/s, train_loss=0.4669, val_loss=0.4690]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:23<00:02, 95.92it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1801/2000 [00:23<00:03, 54.07it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1812/2000 [00:23<00:02, 63.22it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1823/2000 [00:23<00:02, 71.68it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:23<00:02, 78.52it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1844/2000 [00:23<00:01, 83.36it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1855/2000 [00:23<00:01, 88.39it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1866/2000 [00:23<00:01, 91.79it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1876/2000 [00:23<00:01, 93.97it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1887/2000 [00:24<00:01, 96.17it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:24<00:01, 97.50it/s, train_loss=0.4588, val_loss=0.4755]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:24<00:01, 97.50it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1908/2000 [00:24<00:01, 55.02it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:24<00:01, 64.12it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1930/2000 [00:24<00:00, 72.29it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1941/2000 [00:24<00:00, 79.07it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1952/2000 [00:25<00:00, 84.67it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1963/2000 [00:25<00:00, 88.92it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1974/2000 [00:25<00:00, 92.09it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1985/2000 [00:25<00:00, 94.73it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.45it/s, train_loss=0.4678, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:13,  3.61it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:51, 38.64it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 58.30it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:27, 72.13it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:23, 81.61it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:22, 87.73it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 65/2000 [00:00<00:21, 91.00it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 75/2000 [00:01<00:20, 92.89it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 85/2000 [00:01<00:20, 94.91it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:19, 96.83it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:19, 96.83it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 106/2000 [00:01<00:35, 53.73it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 116/2000 [00:01<00:30, 62.20it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 127/2000 [00:01<00:26, 70.86it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 137/2000 [00:01<00:24, 77.47it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 148/2000 [00:02<00:22, 83.33it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 158/2000 [00:02<00:21, 87.03it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 169/2000 [00:02<00:20, 90.88it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:19, 93.81it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:18, 95.85it/s, train_loss=2.3625, val_loss=2.3668]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:18, 95.85it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:02<00:33, 54.38it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 212/2000 [00:02<00:28, 63.44it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 223/2000 [00:03<00:24, 71.63it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 234/2000 [00:03<00:22, 78.43it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 245/2000 [00:03<00:20, 84.17it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 256/2000 [00:03<00:19, 88.57it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 266/2000 [00:03<00:18, 91.44it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 276/2000 [00:03<00:18, 93.70it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 287/2000 [00:03<00:17, 95.74it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:03<00:17, 97.10it/s, train_loss=1.8797, val_loss=1.8850]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:17, 97.10it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:30, 54.59it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:26, 63.56it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 329/2000 [00:04<00:23, 70.83it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 339/2000 [00:04<00:21, 77.03it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 350/2000 [00:04<00:19, 82.91it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 87.70it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:04<00:17, 91.25it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 382/2000 [00:04<00:17, 93.38it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:16, 94.84it/s, train_loss=1.4027, val_loss=1.3965]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:16, 94.84it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 402/2000 [00:05<00:29, 53.33it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 413/2000 [00:05<00:25, 62.72it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 423/2000 [00:05<00:22, 70.18it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 434/2000 [00:05<00:20, 77.65it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 444/2000 [00:05<00:18, 82.83it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 455/2000 [00:05<00:17, 87.34it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 466/2000 [00:06<00:16, 91.34it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 477/2000 [00:06<00:16, 94.24it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 488/2000 [00:06<00:15, 96.13it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:06<00:15, 97.90it/s, train_loss=0.9080, val_loss=0.8890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 499/2000 [00:06<00:15, 97.90it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:06<00:26, 55.65it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 520/2000 [00:06<00:23, 63.48it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:06<00:20, 71.59it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 542/2000 [00:07<00:18, 78.57it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 553/2000 [00:07<00:17, 84.20it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 564/2000 [00:07<00:16, 88.73it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 574/2000 [00:07<00:15, 91.30it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:07<00:15, 94.05it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:07<00:14, 95.37it/s, train_loss=0.6799, val_loss=0.6818]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:07<00:14, 95.37it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 605/2000 [00:08<00:25, 53.89it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 615/2000 [00:08<00:22, 62.15it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 626/2000 [00:08<00:19, 70.76it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 637/2000 [00:08<00:17, 77.91it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 648/2000 [00:08<00:16, 83.90it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 659/2000 [00:08<00:15, 88.55it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 670/2000 [00:08<00:14, 92.00it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 681/2000 [00:08<00:13, 94.33it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:08<00:13, 95.74it/s, train_loss=0.6036, val_loss=0.6057]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:09<00:13, 95.74it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 701/2000 [00:09<00:23, 54.32it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 712/2000 [00:09<00:20, 63.46it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 723/2000 [00:09<00:17, 71.78it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:09<00:16, 78.68it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 744/2000 [00:09<00:15, 83.57it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 755/2000 [00:09<00:14, 88.32it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 766/2000 [00:09<00:13, 91.73it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:10<00:12, 94.56it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 788/2000 [00:10<00:12, 96.37it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:12, 97.70it/s, train_loss=0.5431, val_loss=0.5419]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:12, 97.70it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 809/2000 [00:10<00:21, 55.29it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 819/2000 [00:10<00:18, 63.13it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 830/2000 [00:10<00:16, 71.50it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 841/2000 [00:10<00:14, 78.66it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 852/2000 [00:11<00:13, 84.51it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 863/2000 [00:11<00:12, 88.87it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 874/2000 [00:11<00:12, 91.89it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 884/2000 [00:11<00:11, 93.96it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:11, 95.68it/s, train_loss=0.5316, val_loss=0.5282]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:11, 95.68it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:11<00:19, 54.83it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 916/2000 [00:11<00:16, 63.96it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 926/2000 [00:12<00:15, 71.17it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 936/2000 [00:12<00:13, 77.55it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 946/2000 [00:12<00:12, 82.83it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 956/2000 [00:12<00:11, 87.10it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 967/2000 [00:12<00:11, 90.86it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 978/2000 [00:12<00:10, 93.69it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 988/2000 [00:12<00:10, 95.39it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:12<00:10, 96.99it/s, train_loss=0.5171, val_loss=0.5167]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:13<00:10, 96.99it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1009/2000 [00:13<00:18, 54.99it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:13<00:15, 64.10it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:13<00:13, 71.44it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1040/2000 [00:13<00:12, 77.32it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▎    | 1050/2000 [00:13<00:11, 82.61it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1061/2000 [00:13<00:10, 87.46it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1072/2000 [00:13<00:10, 91.12it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1082/2000 [00:13<00:09, 93.04it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:13<00:09, 94.89it/s, train_loss=0.5094, val_loss=0.5053]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:14<00:09, 94.89it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1102/2000 [00:14<00:16, 53.67it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1112/2000 [00:14<00:14, 62.16it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1123/2000 [00:14<00:12, 70.76it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1134/2000 [00:14<00:11, 77.97it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:14<00:10, 83.74it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1156/2000 [00:14<00:09, 88.22it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1166/2000 [00:15<00:09, 91.26it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1177/2000 [00:15<00:08, 93.75it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1187/2000 [00:15<00:08, 95.42it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:15<00:08, 94.98it/s, train_loss=0.5047, val_loss=0.5050]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:15<00:08, 94.98it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1207/2000 [00:15<00:14, 53.52it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1217/2000 [00:15<00:12, 62.00it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1228/2000 [00:15<00:10, 70.66it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1238/2000 [00:16<00:09, 77.14it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:16<00:09, 82.31it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1259/2000 [00:16<00:08, 87.36it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1269/2000 [00:16<00:08, 90.53it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:16<00:07, 92.80it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:16<00:07, 94.08it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:16<00:07, 95.36it/s, train_loss=0.5023, val_loss=0.5100]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:16<00:07, 95.36it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:17<00:12, 53.57it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:17<00:10, 62.13it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1329/2000 [00:17<00:09, 69.79it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1339/2000 [00:17<00:08, 76.13it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1349/2000 [00:17<00:07, 81.71it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1360/2000 [00:17<00:07, 86.88it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1371/2000 [00:17<00:06, 90.91it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1381/2000 [00:17<00:06, 93.28it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:17<00:06, 95.38it/s, train_loss=0.4748, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:18<00:06, 95.38it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:18<00:11, 54.26it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1413/2000 [00:18<00:09, 63.38it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1423/2000 [00:18<00:08, 70.76it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1434/2000 [00:18<00:07, 77.96it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1444/2000 [00:18<00:07, 78.46it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1453/2000 [00:18<00:06, 78.65it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1462/2000 [00:18<00:06, 78.10it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1471/2000 [00:19<00:06, 79.78it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1480/2000 [00:19<00:06, 75.24it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1489/2000 [00:19<00:06, 77.59it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:06, 80.27it/s, train_loss=0.4679, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:06, 80.27it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:19<00:10, 46.69it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1516/2000 [00:19<00:08, 54.27it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1525/2000 [00:19<00:07, 61.04it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1534/2000 [00:20<00:06, 66.74it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1543/2000 [00:20<00:06, 71.98it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1552/2000 [00:20<00:05, 76.07it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1561/2000 [00:20<00:05, 79.12it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1570/2000 [00:20<00:05, 81.69it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1579/2000 [00:20<00:05, 83.53it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1588/2000 [00:20<00:04, 84.48it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:20<00:04, 85.64it/s, train_loss=0.4715, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:21<00:04, 85.64it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1606/2000 [00:21<00:08, 48.77it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1615/2000 [00:21<00:06, 56.26it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1624/2000 [00:21<00:05, 63.13it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1633/2000 [00:21<00:05, 68.99it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1642/2000 [00:21<00:04, 74.03it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1651/2000 [00:21<00:04, 77.88it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1660/2000 [00:21<00:04, 80.68it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:21<00:04, 82.75it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1678/2000 [00:21<00:03, 84.40it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1687/2000 [00:22<00:03, 85.88it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:22<00:03, 85.93it/s, train_loss=0.4660, val_loss=0.4670]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:22<00:03, 85.93it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1705/2000 [00:22<00:06, 47.36it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:22<00:05, 54.86it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1723/2000 [00:22<00:04, 61.60it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1732/2000 [00:22<00:03, 67.67it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1741/2000 [00:22<00:03, 68.05it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1750/2000 [00:23<00:03, 72.34it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1759/2000 [00:23<00:03, 76.12it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1768/2000 [00:23<00:03, 76.23it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1777/2000 [00:23<00:02, 78.84it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1786/2000 [00:23<00:02, 80.42it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:23<00:02, 82.97it/s, train_loss=0.4658, val_loss=0.4798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:23<00:02, 82.97it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:24<00:04, 48.10it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1814/2000 [00:24<00:03, 56.66it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1823/2000 [00:24<00:02, 63.41it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1832/2000 [00:24<00:02, 69.39it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1841/2000 [00:24<00:02, 74.12it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▎| 1850/2000 [00:24<00:01, 78.20it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1859/2000 [00:24<00:01, 81.17it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1868/2000 [00:24<00:01, 83.41it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1877/2000 [00:24<00:01, 84.52it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1886/2000 [00:24<00:01, 85.75it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1895/2000 [00:25<00:01, 85.99it/s, train_loss=0.4745, val_loss=0.4775]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1895/2000 [00:25<00:01, 85.99it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1904/2000 [00:25<00:01, 48.02it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1913/2000 [00:25<00:01, 55.77it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1923/2000 [00:25<00:01, 63.53it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1932/2000 [00:25<00:00, 69.40it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1941/2000 [00:25<00:00, 74.15it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1950/2000 [00:25<00:00, 78.17it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1959/2000 [00:26<00:00, 80.37it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1968/2000 [00:26<00:00, 82.76it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1977/2000 [00:26<00:00, 84.09it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1986/2000 [00:26<00:00, 85.32it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 75.50it/s, train_loss=0.4838, val_loss=0.4704]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:30,  3.51it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:02, 31.99it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:39, 50.42it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 29/2000 [00:00<00:30, 63.81it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 38/2000 [00:00<00:27, 71.62it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 47/2000 [00:00<00:25, 76.80it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:24, 78.03it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 65/2000 [00:01<00:25, 75.50it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 73/2000 [00:01<00:25, 74.65it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:25, 76.14it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 90/2000 [00:01<00:24, 79.07it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:23, 81.48it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:23, 81.48it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 108/2000 [00:01<00:39, 47.60it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 117/2000 [00:01<00:33, 55.61it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 126/2000 [00:02<00:29, 62.87it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 135/2000 [00:02<00:26, 69.16it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 144/2000 [00:02<00:25, 74.19it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:23, 78.26it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 162/2000 [00:02<00:22, 80.82it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:22, 83.04it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:21, 84.85it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 189/2000 [00:02<00:21, 86.01it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:20, 86.68it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:20, 86.68it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:36, 48.97it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:31, 56.58it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:27, 63.61it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 234/2000 [00:03<00:25, 69.68it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:23, 74.61it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 252/2000 [00:03<00:22, 78.41it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:21, 81.13it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 270/2000 [00:03<00:20, 82.97it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 280/2000 [00:04<00:20, 85.30it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 289/2000 [00:04<00:19, 85.62it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:19, 86.72it/s, train_loss=1.8443, val_loss=1.8502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:19, 86.72it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 307/2000 [00:04<00:34, 48.98it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 316/2000 [00:04<00:29, 56.49it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:26, 63.35it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 334/2000 [00:04<00:24, 69.10it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 343/2000 [00:05<00:22, 73.37it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:05<00:21, 77.19it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:05<00:20, 80.14it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 370/2000 [00:05<00:19, 82.06it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 379/2000 [00:05<00:19, 83.19it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 388/2000 [00:05<00:19, 84.46it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 397/2000 [00:05<00:18, 85.74it/s, train_loss=1.3920, val_loss=1.3842]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 397/2000 [00:05<00:18, 85.74it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 406/2000 [00:06<00:32, 48.73it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 415/2000 [00:06<00:28, 56.41it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 424/2000 [00:06<00:24, 63.45it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 433/2000 [00:06<00:22, 69.28it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 442/2000 [00:06<00:20, 74.29it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:06<00:19, 77.54it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 460/2000 [00:06<00:19, 80.48it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 469/2000 [00:06<00:18, 82.78it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 478/2000 [00:06<00:18, 84.33it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 487/2000 [00:06<00:17, 85.69it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:07<00:17, 86.68it/s, train_loss=0.8809, val_loss=0.8988]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:07<00:17, 86.68it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 505/2000 [00:07<00:30, 49.03it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 514/2000 [00:07<00:26, 56.72it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 523/2000 [00:07<00:23, 63.65it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 532/2000 [00:07<00:21, 69.72it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:19, 74.39it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 550/2000 [00:07<00:18, 77.67it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 559/2000 [00:07<00:17, 80.44it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 568/2000 [00:08<00:17, 82.42it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 577/2000 [00:08<00:16, 84.40it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 586/2000 [00:08<00:16, 85.70it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:16, 86.70it/s, train_loss=0.6554, val_loss=0.6702]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:16, 86.70it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 604/2000 [00:08<00:28, 48.70it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 613/2000 [00:08<00:24, 56.20it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 622/2000 [00:08<00:21, 63.06it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 631/2000 [00:09<00:19, 68.57it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 640/2000 [00:09<00:18, 72.84it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 649/2000 [00:09<00:17, 76.90it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 658/2000 [00:09<00:16, 80.13it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:09<00:16, 82.22it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:09<00:15, 84.25it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 685/2000 [00:09<00:15, 85.71it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:09<00:15, 86.05it/s, train_loss=0.5858, val_loss=0.5751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 694/2000 [00:10<00:15, 86.05it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 703/2000 [00:10<00:26, 48.31it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 712/2000 [00:10<00:23, 55.95it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 721/2000 [00:10<00:20, 62.57it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 730/2000 [00:10<00:18, 68.85it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 739/2000 [00:10<00:17, 73.31it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 748/2000 [00:10<00:16, 77.44it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 757/2000 [00:10<00:15, 80.82it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 766/2000 [00:10<00:14, 83.18it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 776/2000 [00:11<00:14, 85.40it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 786/2000 [00:11<00:14, 86.67it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:11<00:13, 86.77it/s, train_loss=0.5491, val_loss=0.5455]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 795/2000 [00:11<00:13, 86.77it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 804/2000 [00:11<00:24, 48.86it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 813/2000 [00:11<00:21, 56.40it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 823/2000 [00:11<00:18, 64.02it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:11<00:16, 69.77it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 841/2000 [00:12<00:15, 74.40it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▎     | 850/2000 [00:12<00:14, 78.09it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 859/2000 [00:12<00:14, 80.68it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 868/2000 [00:12<00:13, 82.27it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 877/2000 [00:12<00:13, 83.71it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:12<00:13, 85.48it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:12<00:12, 86.12it/s, train_loss=0.5298, val_loss=0.5258]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:12<00:12, 86.12it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 904/2000 [00:13<00:24, 45.55it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 912/2000 [00:13<00:21, 51.16it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:13<00:18, 57.85it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 930/2000 [00:13<00:16, 64.73it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 939/2000 [00:13<00:15, 70.62it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:13<00:14, 74.96it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:13<00:13, 78.73it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:13<00:12, 81.68it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:13<00:12, 83.39it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:13<00:12, 82.68it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:14<00:12, 82.58it/s, train_loss=0.5063, val_loss=0.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 993/2000 [00:14<00:12, 82.58it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1002/2000 [00:14<00:21, 46.09it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1011/2000 [00:14<00:18, 53.51it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:14<00:16, 60.72it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1029/2000 [00:14<00:14, 67.05it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:14<00:13, 71.55it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1047/2000 [00:15<00:17, 53.78it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1055/2000 [00:15<00:16, 57.80it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1064/2000 [00:15<00:14, 63.92it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1073/2000 [00:15<00:13, 68.89it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1082/2000 [00:15<00:12, 73.29it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:15<00:11, 76.99it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:15<00:11, 79.85it/s, train_loss=0.5080, val_loss=0.5123]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:16<00:11, 79.85it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1109/2000 [00:16<00:20, 42.43it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1118/2000 [00:16<00:17, 49.95it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1127/2000 [00:16<00:15, 56.87it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1136/2000 [00:16<00:13, 63.02it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:16<00:12, 68.92it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1154/2000 [00:16<00:11, 73.39it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1163/2000 [00:16<00:10, 77.36it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1172/2000 [00:16<00:10, 80.31it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1181/2000 [00:17<00:09, 82.38it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1190/2000 [00:17<00:09, 83.35it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1199/2000 [00:17<00:09, 84.08it/s, train_loss=0.4912, val_loss=0.5072]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1199/2000 [00:17<00:09, 84.08it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:17<00:16, 47.58it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1217/2000 [00:17<00:14, 55.06it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1226/2000 [00:17<00:12, 62.11it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1235/2000 [00:17<00:11, 68.20it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1244/2000 [00:18<00:10, 73.39it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1253/2000 [00:18<00:09, 77.50it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1262/2000 [00:18<00:09, 80.55it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1271/2000 [00:18<00:08, 82.92it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1280/2000 [00:18<00:08, 84.20it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:18<00:08, 85.52it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:18<00:08, 86.11it/s, train_loss=0.4933, val_loss=0.5043]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:18<00:08, 86.11it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:19<00:14, 48.72it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1316/2000 [00:19<00:12, 56.39it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1325/2000 [00:19<00:10, 63.37it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1334/2000 [00:19<00:09, 69.25it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1343/2000 [00:19<00:08, 74.09it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:19<00:08, 78.05it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1361/2000 [00:19<00:07, 80.94it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1370/2000 [00:19<00:07, 83.18it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:19<00:07, 84.78it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1388/2000 [00:19<00:07, 85.55it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:20<00:06, 86.75it/s, train_loss=0.4671, val_loss=0.4862]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:20<00:06, 86.75it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1406/2000 [00:20<00:12, 48.75it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1415/2000 [00:20<00:10, 56.48it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1424/2000 [00:20<00:09, 63.23it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1433/2000 [00:20<00:08, 68.25it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1442/2000 [00:20<00:07, 72.70it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1451/2000 [00:20<00:07, 76.91it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1460/2000 [00:21<00:06, 80.29it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1469/2000 [00:21<00:06, 82.62it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1478/2000 [00:21<00:06, 83.93it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1487/2000 [00:21<00:06, 85.31it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:21<00:05, 85.57it/s, train_loss=0.4871, val_loss=0.4974]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:21<00:05, 85.57it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1505/2000 [00:21<00:10, 48.32it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1514/2000 [00:21<00:08, 55.94it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1523/2000 [00:22<00:07, 62.90it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1532/2000 [00:22<00:06, 68.88it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1541/2000 [00:22<00:06, 73.74it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1550/2000 [00:22<00:05, 77.51it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1559/2000 [00:22<00:05, 80.25it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1568/2000 [00:22<00:05, 82.47it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1577/2000 [00:22<00:05, 84.13it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1586/2000 [00:22<00:04, 85.10it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:22<00:04, 86.37it/s, train_loss=0.4823, val_loss=0.4896]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:23<00:04, 86.37it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:23<00:08, 48.47it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1613/2000 [00:23<00:06, 56.12it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1622/2000 [00:23<00:06, 62.97it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1631/2000 [00:23<00:05, 68.65it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:23<00:04, 73.58it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:23<00:04, 77.00it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:23<00:04, 79.92it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1667/2000 [00:23<00:04, 82.47it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1676/2000 [00:24<00:03, 84.29it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1685/2000 [00:24<00:03, 85.15it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:24<00:03, 86.19it/s, train_loss=0.4725, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:24<00:03, 86.19it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1703/2000 [00:24<00:06, 45.96it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1711/2000 [00:24<00:05, 50.94it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1719/2000 [00:24<00:04, 56.28it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:24<00:04, 63.38it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:25<00:03, 69.28it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:25<00:03, 73.96it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1755/2000 [00:25<00:03, 77.68it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:25<00:02, 80.82it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1773/2000 [00:25<00:02, 83.29it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1782/2000 [00:25<00:02, 84.85it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:25<00:02, 86.46it/s, train_loss=0.4758, val_loss=0.4906]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:26<00:02, 86.46it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1801/2000 [00:26<00:04, 48.53it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1810/2000 [00:26<00:03, 55.83it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1819/2000 [00:26<00:02, 62.76it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1828/2000 [00:26<00:02, 68.79it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1837/2000 [00:26<00:02, 72.86it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:26<00:02, 76.92it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1855/2000 [00:26<00:01, 80.12it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1864/2000 [00:26<00:01, 82.60it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1873/2000 [00:26<00:01, 84.38it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1882/2000 [00:27<00:01, 84.97it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:27<00:01, 85.91it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:27<00:01, 86.79it/s, train_loss=0.4676, val_loss=0.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:27<00:01, 86.79it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1909/2000 [00:27<00:01, 48.85it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:27<00:01, 57.25it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1928/2000 [00:27<00:01, 63.76it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1937/2000 [00:27<00:00, 69.70it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1946/2000 [00:28<00:00, 74.43it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1955/2000 [00:28<00:00, 77.50it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1964/2000 [00:28<00:00, 80.24it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1973/2000 [00:28<00:00, 82.15it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1982/2000 [00:28<00:00, 83.97it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|█████████▉| 1991/2000 [00:28<00:00, 85.46it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 69.88it/s, train_loss=0.4619, val_loss=0.4751]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:20<01:20, 80.64s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.001, beta2=0.98, avg_val_loss=0.4721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:10,  3.27it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:59, 33.18it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:37, 52.47it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:30, 65.54it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:26, 74.18it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:24, 79.72it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:00<00:23, 83.74it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:22, 86.66it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:21, 89.36it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 91.12it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 91.12it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:37, 50.45it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:32, 58.63it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:28, 66.33it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:01<00:25, 72.90it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:23, 78.46it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:22, 82.60it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:21, 86.15it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:20, 89.04it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:19, 91.21it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 92.16it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 92.16it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:03<00:35, 51.28it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 211/2000 [00:03<00:30, 59.20it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 221/2000 [00:03<00:26, 66.88it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:24, 73.59it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 241/2000 [00:03<00:22, 78.47it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:21, 82.45it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:20, 85.93it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:03<00:19, 88.87it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:03<00:19, 89.90it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:03<00:18, 92.13it/s, train_loss=1.8066, val_loss=1.8156]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:18, 92.13it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 301/2000 [00:04<00:32, 51.83it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:28, 59.81it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 321/2000 [00:04<00:25, 66.82it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 331/2000 [00:04<00:22, 73.32it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:21, 78.33it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:04<00:20, 81.87it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:19, 84.97it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:05<00:18, 87.38it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:18, 89.70it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 91.88it/s, train_loss=1.3977, val_loss=1.4035]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 91.88it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:30, 52.51it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:05<00:26, 60.46it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:23, 68.14it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:06<00:20, 74.79it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:06<00:19, 80.08it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:06<00:18, 84.52it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 461/2000 [00:06<00:17, 87.91it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 471/2000 [00:06<00:16, 90.41it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:06<00:16, 92.34it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:16, 93.60it/s, train_loss=0.9072, val_loss=0.9205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:07<00:16, 93.60it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:07<00:29, 51.45it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:07<00:24, 59.61it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:07<00:22, 67.12it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:07<00:19, 73.61it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:18, 78.68it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:07<00:17, 82.72it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 561/2000 [00:07<00:16, 86.39it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:07<00:16, 89.15it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 581/2000 [00:07<00:15, 90.18it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:07<00:15, 92.06it/s, train_loss=0.6744, val_loss=0.6743]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:15, 92.06it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:08<00:26, 51.82it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 611/2000 [00:08<00:22, 60.46it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 622/2000 [00:08<00:19, 69.34it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 633/2000 [00:08<00:17, 76.98it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 644/2000 [00:08<00:16, 83.02it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 655/2000 [00:08<00:15, 87.87it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 665/2000 [00:08<00:14, 90.98it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:09<00:14, 93.69it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 687/2000 [00:09<00:13, 95.73it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 97.26it/s, train_loss=0.5960, val_loss=0.5863]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 97.26it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 708/2000 [00:09<00:23, 55.68it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 718/2000 [00:09<00:20, 63.83it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 728/2000 [00:09<00:17, 71.32it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 739/2000 [00:10<00:16, 78.46it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 749/2000 [00:10<00:14, 83.67it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 760/2000 [00:10<00:14, 88.31it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 771/2000 [00:10<00:13, 91.85it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:10<00:12, 94.01it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 95.66it/s, train_loss=0.5291, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 95.66it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:22, 53.02it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:11<00:19, 61.38it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 822/2000 [00:11<00:16, 70.00it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 833/2000 [00:11<00:15, 77.28it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 844/2000 [00:11<00:13, 83.41it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 854/2000 [00:11<00:13, 87.34it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 865/2000 [00:11<00:12, 91.28it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:11<00:12, 93.32it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:11<00:11, 95.78it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:11<00:11, 97.30it/s, train_loss=0.5027, val_loss=0.5090]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:12<00:11, 97.30it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 907/2000 [00:12<00:19, 55.57it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 918/2000 [00:12<00:16, 64.52it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 929/2000 [00:12<00:14, 72.45it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 940/2000 [00:12<00:13, 79.46it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 951/2000 [00:12<00:12, 84.88it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 962/2000 [00:12<00:11, 89.00it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 973/2000 [00:12<00:11, 92.36it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:13<00:10, 94.87it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:13<00:10, 96.78it/s, train_loss=0.5145, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:13<00:10, 96.78it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:13<00:17, 55.72it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1016/2000 [00:13<00:15, 64.65it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1027/2000 [00:13<00:13, 72.69it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:13<00:12, 79.39it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1049/2000 [00:13<00:11, 84.84it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1060/2000 [00:14<00:10, 89.33it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1071/2000 [00:14<00:10, 92.72it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1082/2000 [00:14<00:09, 95.01it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:14<00:09, 96.76it/s, train_loss=0.5011, val_loss=0.5039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:14<00:09, 96.76it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1103/2000 [00:14<00:16, 54.86it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1113/2000 [00:14<00:14, 62.84it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1124/2000 [00:14<00:12, 71.29it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1135/2000 [00:15<00:11, 78.46it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1146/2000 [00:15<00:10, 84.43it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1157/2000 [00:15<00:09, 88.99it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:15<00:08, 92.53it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1179/2000 [00:15<00:08, 95.16it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1190/2000 [00:15<00:08, 96.83it/s, train_loss=0.4793, val_loss=0.4880]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1190/2000 [00:15<00:08, 96.83it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:15<00:14, 55.68it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1211/2000 [00:16<00:12, 63.44it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1222/2000 [00:16<00:10, 71.67it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1233/2000 [00:16<00:09, 78.83it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1244/2000 [00:16<00:08, 84.52it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:16<00:08, 89.00it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1266/2000 [00:16<00:07, 92.45it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1277/2000 [00:16<00:07, 94.96it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1288/2000 [00:16<00:07, 97.03it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:16<00:07, 98.46it/s, train_loss=0.4776, val_loss=0.4851]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:17<00:07, 98.46it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:17<00:12, 57.33it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:17<00:10, 65.84it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1332/2000 [00:17<00:09, 73.57it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1343/2000 [00:17<00:08, 80.44it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1354/2000 [00:17<00:07, 85.64it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1365/2000 [00:17<00:07, 89.99it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1376/2000 [00:17<00:06, 93.39it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1387/2000 [00:18<00:06, 95.71it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:18<00:06, 97.17it/s, train_loss=0.4737, val_loss=0.4721]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:18<00:06, 97.17it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:18<00:10, 56.99it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1420/2000 [00:18<00:08, 65.72it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1431/2000 [00:18<00:07, 73.50it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1442/2000 [00:18<00:06, 80.08it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1453/2000 [00:19<00:06, 85.25it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1464/2000 [00:19<00:05, 89.43it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1474/2000 [00:19<00:05, 91.92it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1485/2000 [00:19<00:05, 94.50it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1495/2000 [00:19<00:05, 95.84it/s, train_loss=0.4722, val_loss=0.4752]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1495/2000 [00:19<00:05, 95.84it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1505/2000 [00:19<00:09, 54.09it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1515/2000 [00:19<00:07, 62.29it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1526/2000 [00:20<00:06, 70.72it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1537/2000 [00:20<00:05, 77.79it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1548/2000 [00:20<00:05, 83.73it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1558/2000 [00:20<00:05, 87.66it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1569/2000 [00:20<00:04, 91.33it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1579/2000 [00:20<00:04, 93.42it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1590/2000 [00:20<00:04, 95.63it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:20<00:04, 96.81it/s, train_loss=0.4826, val_loss=0.4823]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:21<00:04, 96.81it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1610/2000 [00:21<00:07, 54.53it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1621/2000 [00:21<00:05, 63.73it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1632/2000 [00:21<00:05, 71.86it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1643/2000 [00:21<00:04, 78.81it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1653/2000 [00:21<00:04, 83.66it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:21<00:03, 88.23it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1675/2000 [00:21<00:03, 91.90it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1686/2000 [00:21<00:03, 94.45it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:22<00:03, 96.18it/s, train_loss=0.4648, val_loss=0.4627]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:22<00:03, 96.18it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1707/2000 [00:22<00:05, 55.40it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:22<00:04, 64.31it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:22<00:03, 71.58it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1739/2000 [00:22<00:03, 78.60it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1749/2000 [00:22<00:03, 83.61it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1759/2000 [00:22<00:02, 87.71it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1769/2000 [00:23<00:02, 90.89it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1780/2000 [00:23<00:02, 93.73it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:23<00:02, 95.73it/s, train_loss=0.4616, val_loss=0.4631]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:23<00:02, 95.73it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1801/2000 [00:23<00:03, 53.57it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1811/2000 [00:23<00:03, 61.68it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1822/2000 [00:23<00:02, 70.23it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1832/2000 [00:23<00:02, 76.77it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1843/2000 [00:24<00:01, 82.94it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1854/2000 [00:24<00:01, 87.73it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1865/2000 [00:24<00:01, 91.39it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1876/2000 [00:24<00:01, 94.26it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1886/2000 [00:24<00:01, 93.99it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:24<00:01, 95.89it/s, train_loss=0.4580, val_loss=0.4727]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:24<00:01, 95.89it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:24<00:01, 55.52it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1918/2000 [00:25<00:01, 64.55it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1929/2000 [00:25<00:00, 72.63it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1940/2000 [00:25<00:00, 79.55it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:25<00:00, 84.96it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:25<00:00, 89.56it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1973/2000 [00:25<00:00, 92.63it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1984/2000 [00:25<00:00, 94.99it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 77.29it/s, train_loss=0.4627, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<08:52,  3.76it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.69it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 61.06it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 74.58it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:23, 83.43it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:21, 88.94it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 67/2000 [00:00<00:20, 92.78it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 78/2000 [00:01<00:20, 95.37it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:19, 97.39it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.55it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.55it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:33, 56.71it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 122/2000 [00:01<00:28, 65.58it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 133/2000 [00:01<00:25, 73.38it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 144/2000 [00:01<00:23, 80.24it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 155/2000 [00:02<00:21, 85.53it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 166/2000 [00:02<00:20, 89.63it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 177/2000 [00:02<00:19, 93.03it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 188/2000 [00:02<00:19, 95.34it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 199/2000 [00:02<00:18, 96.82it/s, train_loss=2.3564, val_loss=2.3601]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 199/2000 [00:02<00:18, 96.82it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 209/2000 [00:02<00:31, 56.05it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 220/2000 [00:02<00:27, 65.00it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 230/2000 [00:03<00:24, 71.98it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 240/2000 [00:03<00:22, 78.04it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:20, 83.91it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 262/2000 [00:03<00:19, 88.37it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 273/2000 [00:03<00:18, 91.66it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 283/2000 [00:03<00:18, 93.78it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 294/2000 [00:03<00:17, 95.67it/s, train_loss=1.8502, val_loss=1.8608]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 294/2000 [00:04<00:17, 95.67it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 304/2000 [00:04<00:31, 54.29it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 314/2000 [00:04<00:27, 62.41it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 324/2000 [00:04<00:23, 70.12it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 334/2000 [00:04<00:21, 76.88it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 344/2000 [00:04<00:20, 82.36it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 355/2000 [00:04<00:18, 87.49it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 366/2000 [00:04<00:17, 91.25it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 377/2000 [00:04<00:17, 94.21it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 387/2000 [00:04<00:16, 95.77it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 398/2000 [00:05<00:16, 97.41it/s, train_loss=1.1217, val_loss=1.1205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 398/2000 [00:05<00:16, 97.41it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 408/2000 [00:05<00:28, 55.38it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 419/2000 [00:05<00:24, 64.41it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 430/2000 [00:05<00:21, 72.54it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:05<00:19, 79.51it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 452/2000 [00:05<00:18, 85.17it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 463/2000 [00:05<00:17, 89.48it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 474/2000 [00:06<00:16, 92.45it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 485/2000 [00:06<00:15, 94.93it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:06<00:15, 96.78it/s, train_loss=0.7508, val_loss=0.7392]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:06<00:15, 96.78it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:06<00:26, 55.69it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 517/2000 [00:06<00:22, 64.69it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 528/2000 [00:06<00:20, 72.56it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 539/2000 [00:06<00:18, 79.40it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 550/2000 [00:07<00:17, 85.03it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 561/2000 [00:07<00:16, 89.21it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:07<00:15, 91.79it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 582/2000 [00:07<00:15, 94.35it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:07<00:14, 95.88it/s, train_loss=0.6167, val_loss=0.6201]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 592/2000 [00:07<00:14, 95.88it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 602/2000 [00:07<00:26, 53.56it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 612/2000 [00:07<00:22, 61.64it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:08<00:19, 70.31it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 633/2000 [00:08<00:17, 76.84it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 644/2000 [00:08<00:16, 83.25it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 655/2000 [00:08<00:15, 88.16it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:08<00:14, 92.09it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 677/2000 [00:08<00:13, 94.70it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:08<00:13, 96.52it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:08<00:13, 97.29it/s, train_loss=0.5584, val_loss=0.5590]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 97.29it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 708/2000 [00:09<00:23, 55.63it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 719/2000 [00:09<00:19, 64.89it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 730/2000 [00:09<00:17, 73.03it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 741/2000 [00:09<00:15, 80.00it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:09<00:14, 85.60it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 762/2000 [00:09<00:13, 89.08it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:09<00:13, 91.67it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:09<00:12, 94.60it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 96.62it/s, train_loss=0.5373, val_loss=0.5364]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 96.62it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 804/2000 [00:10<00:21, 55.10it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:10<00:18, 63.21it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 825/2000 [00:10<00:16, 71.59it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 836/2000 [00:10<00:14, 78.78it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 847/2000 [00:10<00:13, 84.47it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 858/2000 [00:10<00:12, 89.17it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 869/2000 [00:11<00:12, 92.52it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 880/2000 [00:11<00:11, 94.94it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 96.88it/s, train_loss=0.5207, val_loss=0.5203]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 96.88it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 902/2000 [00:11<00:19, 55.95it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 912/2000 [00:11<00:17, 63.62it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 923/2000 [00:11<00:15, 71.80it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 934/2000 [00:12<00:13, 78.79it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 945/2000 [00:12<00:12, 84.39it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 956/2000 [00:12<00:11, 88.93it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 967/2000 [00:12<00:11, 92.41it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 977/2000 [00:12<00:10, 94.18it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 988/2000 [00:12<00:10, 96.11it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:12<00:10, 97.72it/s, train_loss=0.5045, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:12<00:10, 97.72it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1009/2000 [00:13<00:17, 56.11it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:13<00:15, 64.91it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:13<00:13, 72.09it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:13<00:12, 78.98it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:13<00:11, 84.59it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1062/2000 [00:13<00:10, 88.46it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1073/2000 [00:13<00:10, 91.78it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:13<00:09, 93.91it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:13<00:09, 95.57it/s, train_loss=0.4962, val_loss=0.4971]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:14<00:09, 95.57it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1103/2000 [00:14<00:16, 53.29it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1113/2000 [00:14<00:14, 61.54it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1123/2000 [00:14<00:12, 69.33it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1133/2000 [00:14<00:11, 76.28it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1143/2000 [00:14<00:10, 82.07it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1153/2000 [00:14<00:09, 86.48it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1164/2000 [00:14<00:09, 90.54it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1174/2000 [00:14<00:08, 93.07it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1184/2000 [00:15<00:08, 94.69it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:15<00:08, 96.10it/s, train_loss=0.4949, val_loss=0.4972]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:15<00:08, 96.10it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1204/2000 [00:15<00:14, 53.51it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1214/2000 [00:15<00:12, 61.74it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1224/2000 [00:15<00:11, 69.48it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1234/2000 [00:15<00:10, 76.30it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1244/2000 [00:15<00:09, 82.02it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:16<00:08, 87.14it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1265/2000 [00:16<00:08, 90.43it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1275/2000 [00:16<00:07, 92.94it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1286/2000 [00:16<00:07, 95.37it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 97.14it/s, train_loss=0.4859, val_loss=0.4932]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 97.14it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:16<00:12, 55.10it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:16<00:10, 64.19it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:17<00:09, 71.23it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1339/2000 [00:17<00:08, 78.45it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1349/2000 [00:17<00:07, 83.45it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1359/2000 [00:17<00:07, 87.45it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:17<00:06, 90.79it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:17<00:06, 93.00it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1390/2000 [00:17<00:06, 95.44it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1400/2000 [00:17<00:06, 96.17it/s, train_loss=0.4720, val_loss=0.4839]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1400/2000 [00:18<00:06, 96.17it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1410/2000 [00:18<00:10, 54.13it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1420/2000 [00:18<00:09, 62.27it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1430/2000 [00:18<00:08, 69.88it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1440/2000 [00:18<00:07, 76.68it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▎  | 1450/2000 [00:18<00:06, 82.25it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1460/2000 [00:18<00:06, 86.76it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1470/2000 [00:18<00:05, 90.17it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1481/2000 [00:18<00:05, 93.26it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:18<00:05, 95.48it/s, train_loss=0.4577, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:19<00:05, 95.48it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1502/2000 [00:19<00:09, 54.09it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1512/2000 [00:19<00:07, 61.99it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1522/2000 [00:19<00:06, 69.70it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1533/2000 [00:19<00:06, 77.28it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1543/2000 [00:19<00:05, 82.61it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:19<00:05, 87.50it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:20<00:04, 90.39it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1575/2000 [00:20<00:04, 93.46it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1585/2000 [00:20<00:04, 95.07it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:20<00:04, 96.71it/s, train_loss=0.4704, val_loss=0.4698]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:20<00:04, 96.71it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1606/2000 [00:20<00:07, 55.18it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1617/2000 [00:20<00:05, 64.24it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1628/2000 [00:20<00:05, 72.49it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1639/2000 [00:21<00:04, 79.33it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▎ | 1650/2000 [00:21<00:04, 84.82it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1661/2000 [00:21<00:03, 89.15it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1672/2000 [00:21<00:03, 92.60it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1683/2000 [00:21<00:03, 94.88it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:21<00:03, 96.94it/s, train_loss=0.4603, val_loss=0.4585]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:21<00:03, 96.94it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1704/2000 [00:21<00:05, 55.82it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:22<00:04, 63.81it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:22<00:03, 72.05it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:22<00:03, 79.06it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1747/2000 [00:22<00:02, 84.49it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1758/2000 [00:22<00:02, 88.85it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1769/2000 [00:22<00:02, 92.45it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1780/2000 [00:22<00:02, 95.02it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:22<00:02, 96.77it/s, train_loss=0.4623, val_loss=0.4765]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:23<00:02, 96.77it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1801/2000 [00:23<00:03, 54.08it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1811/2000 [00:23<00:03, 61.97it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1822/2000 [00:23<00:02, 70.35it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1832/2000 [00:23<00:02, 76.89it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1842/2000 [00:23<00:01, 82.09it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1852/2000 [00:23<00:01, 85.92it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1862/2000 [00:23<00:01, 89.29it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1872/2000 [00:23<00:01, 91.96it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1882/2000 [00:24<00:01, 94.21it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1893/2000 [00:24<00:01, 96.41it/s, train_loss=0.4784, val_loss=0.4820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1893/2000 [00:24<00:01, 96.41it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1903/2000 [00:24<00:01, 53.48it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1913/2000 [00:24<00:01, 61.65it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1923/2000 [00:24<00:01, 69.41it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1934/2000 [00:24<00:00, 77.11it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1944/2000 [00:24<00:00, 82.39it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1954/2000 [00:25<00:00, 86.87it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1965/2000 [00:25<00:00, 90.99it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1975/2000 [00:25<00:00, 92.59it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1986/2000 [00:25<00:00, 94.86it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.47it/s, train_loss=0.4707, val_loss=0.4556]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<08:54,  3.74it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.44it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.59it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:27, 72.82it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 43/2000 [00:00<00:24, 80.91it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 53/2000 [00:00<00:23, 82.90it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 63/2000 [00:00<00:22, 87.58it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:20, 92.05it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 85/2000 [00:01<00:20, 94.54it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:19, 96.40it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 96/2000 [00:01<00:19, 96.40it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 106/2000 [00:01<00:34, 54.60it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 117/2000 [00:01<00:29, 63.83it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 128/2000 [00:01<00:26, 71.94it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 139/2000 [00:01<00:23, 78.96it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 150/2000 [00:02<00:21, 84.68it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:20, 89.18it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 172/2000 [00:02<00:19, 92.47it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 183/2000 [00:02<00:19, 94.98it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 194/2000 [00:02<00:18, 96.89it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 194/2000 [00:02<00:18, 96.89it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 204/2000 [00:02<00:32, 55.23it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 214/2000 [00:02<00:28, 63.02it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 224/2000 [00:03<00:25, 70.50it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 234/2000 [00:03<00:23, 76.06it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 245/2000 [00:03<00:21, 82.28it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:20, 86.69it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 266/2000 [00:03<00:19, 90.82it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 277/2000 [00:03<00:18, 93.91it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 287/2000 [00:03<00:17, 95.51it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:03<00:17, 97.04it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:17, 97.04it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:30, 55.44it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:04<00:26, 63.69it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 329/2000 [00:04<00:23, 72.04it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 340/2000 [00:04<00:21, 78.94it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:04<00:19, 84.76it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 88.48it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:04<00:17, 91.97it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 383/2000 [00:04<00:17, 94.31it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:16, 96.15it/s, train_loss=1.1531, val_loss=1.1434]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:16, 96.15it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 404/2000 [00:05<00:29, 55.02it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 414/2000 [00:05<00:25, 63.07it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 425/2000 [00:05<00:22, 71.35it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 436/2000 [00:05<00:19, 78.35it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 447/2000 [00:05<00:18, 84.31it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 458/2000 [00:05<00:17, 88.96it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 469/2000 [00:06<00:16, 92.37it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 480/2000 [00:06<00:16, 94.76it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:15, 96.85it/s, train_loss=0.7501, val_loss=0.7647]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:15, 96.85it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 502/2000 [00:06<00:27, 55.38it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 512/2000 [00:06<00:23, 63.01it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 522/2000 [00:06<00:20, 70.46it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 532/2000 [00:06<00:19, 76.96it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 543/2000 [00:07<00:17, 83.16it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 554/2000 [00:07<00:16, 87.71it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 565/2000 [00:07<00:15, 91.49it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 576/2000 [00:07<00:15, 94.24it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 587/2000 [00:07<00:14, 96.06it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:07<00:14, 97.88it/s, train_loss=0.6105, val_loss=0.6286]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:07<00:14, 97.88it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 609/2000 [00:08<00:24, 56.52it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 620/2000 [00:08<00:21, 65.19it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 631/2000 [00:08<00:18, 73.05it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 642/2000 [00:08<00:17, 79.63it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 652/2000 [00:08<00:15, 84.32it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 663/2000 [00:08<00:15, 88.70it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 674/2000 [00:08<00:14, 92.13it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 684/2000 [00:08<00:13, 94.00it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 695/2000 [00:08<00:13, 96.21it/s, train_loss=0.5612, val_loss=0.5476]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 695/2000 [00:09<00:13, 96.21it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 705/2000 [00:09<00:23, 54.83it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 715/2000 [00:09<00:20, 62.75it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 726/2000 [00:09<00:17, 71.29it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 737/2000 [00:09<00:16, 78.50it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 748/2000 [00:09<00:14, 84.29it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 759/2000 [00:09<00:14, 88.62it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 769/2000 [00:09<00:13, 91.46it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 780/2000 [00:09<00:12, 94.16it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:10<00:12, 95.75it/s, train_loss=0.5414, val_loss=0.5375]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 790/2000 [00:10<00:12, 95.75it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:22, 54.16it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:10<00:19, 62.26it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 822/2000 [00:10<00:16, 70.69it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:10<00:15, 77.09it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 843/2000 [00:10<00:13, 83.19it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:11<00:13, 87.30it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 863/2000 [00:11<00:12, 90.41it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 874/2000 [00:11<00:12, 93.58it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 885/2000 [00:11<00:11, 96.03it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:11<00:11, 97.30it/s, train_loss=0.5117, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:11<00:11, 97.30it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 906/2000 [00:11<00:19, 55.29it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 917/2000 [00:11<00:16, 64.36it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 928/2000 [00:12<00:14, 72.29it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 938/2000 [00:12<00:13, 78.45it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:12<00:12, 83.55it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 959/2000 [00:12<00:11, 88.35it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 969/2000 [00:12<00:11, 91.31it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 980/2000 [00:12<00:10, 93.97it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 990/2000 [00:12<00:10, 95.51it/s, train_loss=0.5143, val_loss=0.5117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 990/2000 [00:13<00:10, 95.51it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1001/2000 [00:13<00:18, 54.62it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1011/2000 [00:13<00:15, 62.42it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:13<00:13, 70.02it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1031/2000 [00:13<00:12, 76.43it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:13<00:11, 81.96it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:13<00:10, 86.98it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1062/2000 [00:13<00:10, 90.41it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1073/2000 [00:13<00:09, 93.54it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:13<00:09, 95.08it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:13<00:09, 96.29it/s, train_loss=0.5165, val_loss=0.5216]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:14<00:09, 96.29it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1103/2000 [00:14<00:16, 53.69it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1113/2000 [00:14<00:14, 61.67it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1123/2000 [00:14<00:12, 69.52it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1134/2000 [00:14<00:11, 77.11it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:14<00:10, 83.29it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1156/2000 [00:14<00:09, 87.95it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1167/2000 [00:14<00:09, 91.76it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1178/2000 [00:15<00:08, 94.20it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1189/2000 [00:15<00:08, 95.79it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:15<00:08, 97.28it/s, train_loss=0.4757, val_loss=0.4905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:15<00:08, 97.28it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:15<00:14, 55.90it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1221/2000 [00:15<00:12, 64.71it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1232/2000 [00:15<00:10, 72.55it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1243/2000 [00:16<00:09, 79.27it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1254/2000 [00:16<00:08, 84.81it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1264/2000 [00:16<00:08, 88.49it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1274/2000 [00:16<00:07, 90.92it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1285/2000 [00:16<00:07, 94.12it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1295/2000 [00:16<00:07, 95.21it/s, train_loss=0.4723, val_loss=0.4799]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1295/2000 [00:16<00:07, 95.21it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1305/2000 [00:16<00:12, 54.59it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1315/2000 [00:17<00:10, 62.86it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1325/2000 [00:17<00:09, 70.60it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1335/2000 [00:17<00:08, 77.12it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1346/2000 [00:17<00:07, 83.32it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:17<00:07, 88.37it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1368/2000 [00:17<00:06, 91.63it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:17<00:06, 94.43it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:17<00:06, 95.25it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:17<00:06, 95.91it/s, train_loss=0.4581, val_loss=0.4792]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:18<00:06, 95.91it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:18<00:10, 54.44it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1420/2000 [00:18<00:09, 63.58it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1431/2000 [00:18<00:07, 71.96it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1441/2000 [00:18<00:07, 77.69it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1451/2000 [00:18<00:06, 82.94it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1462/2000 [00:18<00:06, 88.19it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1473/2000 [00:18<00:05, 91.66it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1483/2000 [00:18<00:05, 93.71it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1493/2000 [00:19<00:05, 95.38it/s, train_loss=0.4772, val_loss=0.4895]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1493/2000 [00:19<00:05, 95.38it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1503/2000 [00:19<00:09, 53.50it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1513/2000 [00:19<00:07, 61.81it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1524/2000 [00:19<00:06, 70.51it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1535/2000 [00:19<00:05, 77.82it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1545/2000 [00:19<00:05, 83.12it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1556/2000 [00:19<00:05, 87.72it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1567/2000 [00:20<00:04, 91.32it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1578/2000 [00:20<00:04, 93.63it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:20<00:04, 95.69it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:20<00:04, 97.03it/s, train_loss=0.4781, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:20<00:04, 97.03it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1610/2000 [00:20<00:07, 55.33it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1620/2000 [00:20<00:05, 63.39it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1630/2000 [00:21<00:05, 70.72it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:21<00:04, 77.30it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▎ | 1650/2000 [00:21<00:04, 82.60it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1660/2000 [00:21<00:03, 87.06it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1670/2000 [00:21<00:03, 90.39it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1680/2000 [00:21<00:03, 93.03it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:21<00:03, 95.25it/s, train_loss=0.4783, val_loss=0.4726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:22<00:03, 95.25it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:22<00:05, 52.90it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1711/2000 [00:22<00:04, 61.08it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1721/2000 [00:22<00:04, 68.76it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1731/2000 [00:22<00:03, 75.58it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1742/2000 [00:22<00:03, 82.03it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1752/2000 [00:22<00:02, 86.41it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:22<00:02, 89.64it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:22<00:02, 92.45it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1782/2000 [00:22<00:02, 94.24it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:22<00:02, 96.33it/s, train_loss=0.4609, val_loss=0.4732]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1793/2000 [00:23<00:02, 96.33it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1803/2000 [00:23<00:03, 54.19it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1813/2000 [00:23<00:02, 62.44it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1823/2000 [00:23<00:02, 70.10it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1833/2000 [00:23<00:02, 76.92it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1843/2000 [00:23<00:01, 82.50it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1853/2000 [00:23<00:01, 86.89it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1863/2000 [00:23<00:01, 90.22it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1874/2000 [00:24<00:01, 93.34it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1885/2000 [00:24<00:01, 95.71it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1895/2000 [00:24<00:01, 96.37it/s, train_loss=0.4662, val_loss=0.4781]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1895/2000 [00:24<00:01, 96.37it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1905/2000 [00:24<00:01, 54.66it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1915/2000 [00:24<00:01, 63.07it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1926/2000 [00:24<00:01, 71.64it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1936/2000 [00:24<00:00, 78.05it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1946/2000 [00:25<00:00, 82.85it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1956/2000 [00:25<00:00, 87.11it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1967/2000 [00:25<00:00, 90.90it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1977/2000 [00:25<00:00, 93.29it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:25<00:00, 95.17it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.20it/s, train_loss=0.4569, val_loss=0.4738]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:37<00:00, 78.48s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR: 100%|██████████| 3/3 [08:15<00:00, 164.51s/it]\u001B[A\n",
      "Constant LR:  67%|██████▋   | 2/3 [18:27<09:12, 552.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.001, beta2=0.999, avg_val_loss=0.4658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<09:01,  3.69it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   1%|          | 20/2000 [00:00<00:29, 66.95it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   2%|▏         | 39/2000 [00:00<00:18, 105.65it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   3%|▎         | 58/2000 [00:00<00:14, 130.11it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   4%|▍         | 76/2000 [00:00<00:13, 145.33it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   5%|▍         | 95/2000 [00:00<00:12, 156.48it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   5%|▍         | 95/2000 [00:01<00:12, 156.48it/s, train_loss=2.8932, val_loss=2.9002]\u001B[A\n",
      "Training (constant):   6%|▌         | 113/2000 [00:01<00:21, 89.36it/s, train_loss=2.8932, val_loss=2.9002]\u001B[A\n",
      "Training (constant):   7%|▋         | 131/2000 [00:01<00:17, 106.55it/s, train_loss=2.8932, val_loss=2.9002]\u001B[A\n",
      "Training (constant):   8%|▊         | 150/2000 [00:01<00:15, 123.10it/s, train_loss=2.8932, val_loss=2.9002]\u001B[A\n",
      "Training (constant):   8%|▊         | 169/2000 [00:01<00:13, 136.96it/s, train_loss=2.8932, val_loss=2.9002]\u001B[A\n",
      "Training (constant):   9%|▉         | 188/2000 [00:01<00:12, 148.47it/s, train_loss=2.8932, val_loss=2.9002]\u001B[A\n",
      "Training (constant):   9%|▉         | 188/2000 [00:01<00:12, 148.47it/s, train_loss=2.6226, val_loss=2.6212]\u001B[A\n",
      "Training (constant):  10%|█         | 205/2000 [00:01<00:19, 89.86it/s, train_loss=2.6226, val_loss=2.6212] \u001B[A\n",
      "Training (constant):  11%|█         | 223/2000 [00:02<00:16, 105.67it/s, train_loss=2.6226, val_loss=2.6212]\u001B[A\n",
      "Training (constant):  12%|█▏        | 241/2000 [00:02<00:14, 120.41it/s, train_loss=2.6226, val_loss=2.6212]\u001B[A\n",
      "Training (constant):  13%|█▎        | 260/2000 [00:02<00:12, 134.73it/s, train_loss=2.6226, val_loss=2.6212]\u001B[A\n",
      "Training (constant):  14%|█▍        | 278/2000 [00:02<00:11, 145.44it/s, train_loss=2.6226, val_loss=2.6212]\u001B[A\n",
      "Training (constant):  15%|█▍        | 297/2000 [00:02<00:11, 154.66it/s, train_loss=2.6226, val_loss=2.6212]\u001B[A\n",
      "Training (constant):  15%|█▍        | 297/2000 [00:02<00:11, 154.66it/s, train_loss=2.4271, val_loss=2.4377]\u001B[A\n",
      "Training (constant):  16%|█▌        | 315/2000 [00:02<00:18, 93.16it/s, train_loss=2.4271, val_loss=2.4377] \u001B[A\n",
      "Training (constant):  17%|█▋        | 333/2000 [00:02<00:15, 108.48it/s, train_loss=2.4271, val_loss=2.4377]\u001B[A\n",
      "Training (constant):  18%|█▊        | 352/2000 [00:03<00:13, 123.96it/s, train_loss=2.4271, val_loss=2.4377]\u001B[A\n",
      "Training (constant):  18%|█▊        | 370/2000 [00:03<00:11, 136.46it/s, train_loss=2.4271, val_loss=2.4377]\u001B[A\n",
      "Training (constant):  19%|█▉        | 388/2000 [00:03<00:10, 146.68it/s, train_loss=2.4271, val_loss=2.4377]\u001B[A\n",
      "Training (constant):  19%|█▉        | 388/2000 [00:03<00:10, 146.68it/s, train_loss=2.2975, val_loss=2.3126]\u001B[A\n",
      "Training (constant):  20%|██        | 405/2000 [00:03<00:18, 88.37it/s, train_loss=2.2975, val_loss=2.3126] \u001B[A\n",
      "Training (constant):  21%|██        | 423/2000 [00:03<00:15, 104.14it/s, train_loss=2.2975, val_loss=2.3126]\u001B[A\n",
      "Training (constant):  22%|██▏       | 441/2000 [00:03<00:13, 119.01it/s, train_loss=2.2975, val_loss=2.3126]\u001B[A\n",
      "Training (constant):  23%|██▎       | 459/2000 [00:03<00:11, 132.39it/s, train_loss=2.2975, val_loss=2.3126]\u001B[A\n",
      "Training (constant):  24%|██▍       | 477/2000 [00:04<00:10, 143.19it/s, train_loss=2.2975, val_loss=2.3126]\u001B[A\n",
      "Training (constant):  25%|██▍       | 495/2000 [00:04<00:09, 152.25it/s, train_loss=2.2975, val_loss=2.3126]\u001B[A\n",
      "Training (constant):  25%|██▍       | 495/2000 [00:04<00:09, 152.25it/s, train_loss=2.2610, val_loss=2.2585]\u001B[A\n",
      "Training (constant):  26%|██▌       | 513/2000 [00:04<00:16, 91.55it/s, train_loss=2.2610, val_loss=2.2585] \u001B[A\n",
      "Training (constant):  27%|██▋       | 531/2000 [00:04<00:13, 107.23it/s, train_loss=2.2610, val_loss=2.2585]\u001B[A\n",
      "Training (constant):  27%|██▋       | 549/2000 [00:04<00:11, 121.92it/s, train_loss=2.2610, val_loss=2.2585]\u001B[A\n",
      "Training (constant):  28%|██▊       | 567/2000 [00:04<00:10, 134.25it/s, train_loss=2.2610, val_loss=2.2585]\u001B[A\n",
      "Training (constant):  29%|██▉       | 585/2000 [00:04<00:09, 145.30it/s, train_loss=2.2610, val_loss=2.2585]\u001B[A\n",
      "Training (constant):  29%|██▉       | 585/2000 [00:05<00:09, 145.30it/s, train_loss=2.1511, val_loss=2.1481]\u001B[A\n",
      "Training (constant):  30%|███       | 602/2000 [00:05<00:15, 88.08it/s, train_loss=2.1511, val_loss=2.1481] \u001B[A\n",
      "Training (constant):  31%|███       | 620/2000 [00:05<00:13, 103.56it/s, train_loss=2.1511, val_loss=2.1481]\u001B[A\n",
      "Training (constant):  32%|███▏      | 638/2000 [00:05<00:11, 118.70it/s, train_loss=2.1511, val_loss=2.1481]\u001B[A\n",
      "Training (constant):  33%|███▎      | 656/2000 [00:05<00:10, 132.18it/s, train_loss=2.1511, val_loss=2.1481]\u001B[A\n",
      "Training (constant):  34%|███▍      | 675/2000 [00:05<00:09, 144.51it/s, train_loss=2.1511, val_loss=2.1481]\u001B[A\n",
      "Training (constant):  35%|███▍      | 694/2000 [00:05<00:08, 154.09it/s, train_loss=2.1511, val_loss=2.1481]\u001B[A\n",
      "Training (constant):  35%|███▍      | 694/2000 [00:06<00:08, 154.09it/s, train_loss=2.1064, val_loss=2.1224]\u001B[A\n",
      "Training (constant):  36%|███▌      | 712/2000 [00:06<00:13, 92.62it/s, train_loss=2.1064, val_loss=2.1224] \u001B[A\n",
      "Training (constant):  36%|███▋      | 730/2000 [00:06<00:11, 108.17it/s, train_loss=2.1064, val_loss=2.1224]\u001B[A\n",
      "Training (constant):  37%|███▋      | 749/2000 [00:06<00:10, 123.68it/s, train_loss=2.1064, val_loss=2.1224]\u001B[A\n",
      "Training (constant):  38%|███▊      | 767/2000 [00:06<00:09, 136.12it/s, train_loss=2.1064, val_loss=2.1224]\u001B[A\n",
      "Training (constant):  39%|███▉      | 786/2000 [00:06<00:08, 147.50it/s, train_loss=2.1064, val_loss=2.1224]\u001B[A\n",
      "Training (constant):  39%|███▉      | 786/2000 [00:06<00:08, 147.50it/s, train_loss=2.0926, val_loss=2.1057]\u001B[A\n",
      "Training (constant):  40%|████      | 803/2000 [00:06<00:13, 89.54it/s, train_loss=2.0926, val_loss=2.1057] \u001B[A\n",
      "Training (constant):  41%|████      | 821/2000 [00:07<00:11, 104.73it/s, train_loss=2.0926, val_loss=2.1057]\u001B[A\n",
      "Training (constant):  42%|████▏     | 839/2000 [00:07<00:09, 119.29it/s, train_loss=2.0926, val_loss=2.1057]\u001B[A\n",
      "Training (constant):  43%|████▎     | 858/2000 [00:07<00:08, 133.35it/s, train_loss=2.0926, val_loss=2.1057]\u001B[A\n",
      "Training (constant):  44%|████▍     | 876/2000 [00:07<00:07, 143.93it/s, train_loss=2.0926, val_loss=2.1057]\u001B[A\n",
      "Training (constant):  45%|████▍     | 895/2000 [00:07<00:07, 153.52it/s, train_loss=2.0926, val_loss=2.1057]\u001B[A\n",
      "Training (constant):  45%|████▍     | 895/2000 [00:07<00:07, 153.52it/s, train_loss=1.9866, val_loss=1.9865]\u001B[A\n",
      "Training (constant):  46%|████▌     | 913/2000 [00:07<00:11, 92.41it/s, train_loss=1.9866, val_loss=1.9865] \u001B[A\n",
      "Training (constant):  47%|████▋     | 932/2000 [00:07<00:09, 108.74it/s, train_loss=1.9866, val_loss=1.9865]\u001B[A\n",
      "Training (constant):  48%|████▊     | 951/2000 [00:08<00:08, 123.99it/s, train_loss=1.9866, val_loss=1.9865]\u001B[A\n",
      "Training (constant):  48%|████▊     | 969/2000 [00:08<00:07, 136.37it/s, train_loss=1.9866, val_loss=1.9865]\u001B[A\n",
      "Training (constant):  49%|████▉     | 988/2000 [00:08<00:06, 147.63it/s, train_loss=1.9866, val_loss=1.9865]\u001B[A\n",
      "Training (constant):  49%|████▉     | 988/2000 [00:08<00:06, 147.63it/s, train_loss=1.8938, val_loss=1.8931]\u001B[A\n",
      "Training (constant):  50%|█████     | 1005/2000 [00:08<00:11, 90.00it/s, train_loss=1.8938, val_loss=1.8931]\u001B[A\n",
      "Training (constant):  51%|█████     | 1023/2000 [00:08<00:09, 105.37it/s, train_loss=1.8938, val_loss=1.8931]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1041/2000 [00:08<00:07, 120.01it/s, train_loss=1.8938, val_loss=1.8931]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1060/2000 [00:08<00:07, 134.10it/s, train_loss=1.8938, val_loss=1.8931]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1079/2000 [00:09<00:06, 146.22it/s, train_loss=1.8938, val_loss=1.8931]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1098/2000 [00:09<00:05, 155.12it/s, train_loss=1.8938, val_loss=1.8931]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1098/2000 [00:09<00:05, 155.12it/s, train_loss=1.8852, val_loss=1.8883]\u001B[A\n",
      "Training (constant):  56%|█████▌    | 1116/2000 [00:09<00:09, 94.17it/s, train_loss=1.8852, val_loss=1.8883] \u001B[A\n",
      "Training (constant):  57%|█████▋    | 1135/2000 [00:09<00:07, 110.39it/s, train_loss=1.8852, val_loss=1.8883]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1153/2000 [00:09<00:06, 124.25it/s, train_loss=1.8852, val_loss=1.8883]\u001B[A\n",
      "Training (constant):  59%|█████▊    | 1171/2000 [00:09<00:06, 136.38it/s, train_loss=1.8852, val_loss=1.8883]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1190/2000 [00:09<00:05, 147.84it/s, train_loss=1.8852, val_loss=1.8883]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1190/2000 [00:10<00:05, 147.84it/s, train_loss=1.9469, val_loss=1.9550]\u001B[A\n",
      "Training (constant):  60%|██████    | 1207/2000 [00:10<00:08, 90.61it/s, train_loss=1.9469, val_loss=1.9550] \u001B[A\n",
      "Training (constant):  61%|██████▏   | 1225/2000 [00:10<00:07, 106.24it/s, train_loss=1.9469, val_loss=1.9550]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1243/2000 [00:10<00:06, 120.82it/s, train_loss=1.9469, val_loss=1.9550]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1262/2000 [00:10<00:05, 134.98it/s, train_loss=1.9469, val_loss=1.9550]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1281/2000 [00:10<00:04, 146.79it/s, train_loss=1.9469, val_loss=1.9550]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1299/2000 [00:10<00:04, 154.50it/s, train_loss=1.9469, val_loss=1.9550]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1299/2000 [00:11<00:04, 154.50it/s, train_loss=1.7715, val_loss=1.7691]\u001B[A\n",
      "Training (constant):  66%|██████▌   | 1317/2000 [00:11<00:07, 93.41it/s, train_loss=1.7715, val_loss=1.7691] \u001B[A\n",
      "Training (constant):  67%|██████▋   | 1336/2000 [00:11<00:06, 109.79it/s, train_loss=1.7715, val_loss=1.7691]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1355/2000 [00:11<00:05, 124.99it/s, train_loss=1.7715, val_loss=1.7691]\u001B[A\n",
      "Training (constant):  69%|██████▊   | 1373/2000 [00:11<00:04, 136.60it/s, train_loss=1.7715, val_loss=1.7691]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1392/2000 [00:11<00:04, 148.05it/s, train_loss=1.7715, val_loss=1.7691]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1392/2000 [00:11<00:04, 148.05it/s, train_loss=1.6251, val_loss=1.6329]\u001B[A\n",
      "Training (constant):  70%|███████   | 1409/2000 [00:12<00:06, 89.62it/s, train_loss=1.6251, val_loss=1.6329] \u001B[A\n",
      "Training (constant):  71%|███████▏  | 1427/2000 [00:12<00:05, 104.42it/s, train_loss=1.6251, val_loss=1.6329]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1445/2000 [00:12<00:04, 119.29it/s, train_loss=1.6251, val_loss=1.6329]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1464/2000 [00:12<00:04, 133.35it/s, train_loss=1.6251, val_loss=1.6329]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1483/2000 [00:12<00:03, 145.12it/s, train_loss=1.6251, val_loss=1.6329]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1483/2000 [00:12<00:03, 145.12it/s, train_loss=1.6510, val_loss=1.6412]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1501/2000 [00:12<00:05, 90.84it/s, train_loss=1.6510, val_loss=1.6412] \u001B[A\n",
      "Training (constant):  76%|███████▌  | 1519/2000 [00:12<00:04, 106.13it/s, train_loss=1.6510, val_loss=1.6412]\u001B[A\n",
      "Training (constant):  77%|███████▋  | 1538/2000 [00:13<00:03, 121.89it/s, train_loss=1.6510, val_loss=1.6412]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1557/2000 [00:13<00:03, 135.81it/s, train_loss=1.6510, val_loss=1.6412]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1576/2000 [00:13<00:02, 147.22it/s, train_loss=1.6510, val_loss=1.6412]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1595/2000 [00:13<00:02, 156.84it/s, train_loss=1.6510, val_loss=1.6412]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1595/2000 [00:13<00:02, 156.84it/s, train_loss=1.5384, val_loss=1.5344]\u001B[A\n",
      "Training (constant):  81%|████████  | 1613/2000 [00:13<00:04, 95.07it/s, train_loss=1.5384, val_loss=1.5344] \u001B[A\n",
      "Training (constant):  82%|████████▏ | 1632/2000 [00:13<00:03, 111.53it/s, train_loss=1.5384, val_loss=1.5344]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1651/2000 [00:13<00:02, 126.23it/s, train_loss=1.5384, val_loss=1.5344]\u001B[A\n",
      "Training (constant):  84%|████████▎ | 1670/2000 [00:14<00:02, 139.76it/s, train_loss=1.5384, val_loss=1.5344]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1689/2000 [00:14<00:02, 150.09it/s, train_loss=1.5384, val_loss=1.5344]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1689/2000 [00:14<00:02, 150.09it/s, train_loss=1.5840, val_loss=1.5938]\u001B[A\n",
      "Training (constant):  85%|████████▌ | 1707/2000 [00:14<00:03, 92.20it/s, train_loss=1.5840, val_loss=1.5938] \u001B[A\n",
      "Training (constant):  86%|████████▋ | 1726/2000 [00:14<00:02, 108.51it/s, train_loss=1.5840, val_loss=1.5938]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1745/2000 [00:14<00:02, 123.44it/s, train_loss=1.5840, val_loss=1.5938]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1764/2000 [00:14<00:01, 136.84it/s, train_loss=1.5840, val_loss=1.5938]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1783/2000 [00:14<00:01, 147.66it/s, train_loss=1.5840, val_loss=1.5938]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1783/2000 [00:15<00:01, 147.66it/s, train_loss=1.4908, val_loss=1.4998]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1801/2000 [00:15<00:02, 90.65it/s, train_loss=1.4908, val_loss=1.4998] \u001B[A\n",
      "Training (constant):  91%|█████████ | 1819/2000 [00:15<00:01, 105.95it/s, train_loss=1.4908, val_loss=1.4998]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1837/2000 [00:15<00:01, 120.57it/s, train_loss=1.4908, val_loss=1.4998]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1856/2000 [00:15<00:01, 134.29it/s, train_loss=1.4908, val_loss=1.4998]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1875/2000 [00:15<00:00, 146.05it/s, train_loss=1.4908, val_loss=1.4998]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1894/2000 [00:15<00:00, 155.15it/s, train_loss=1.4908, val_loss=1.4998]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1894/2000 [00:16<00:00, 155.15it/s, train_loss=1.4581, val_loss=1.4805]\u001B[A\n",
      "Training (constant):  96%|█████████▌| 1912/2000 [00:16<00:00, 94.25it/s, train_loss=1.4581, val_loss=1.4805] \u001B[A\n",
      "Training (constant):  96%|█████████▋| 1930/2000 [00:16<00:00, 109.57it/s, train_loss=1.4581, val_loss=1.4805]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1949/2000 [00:16<00:00, 124.89it/s, train_loss=1.4581, val_loss=1.4805]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1968/2000 [00:16<00:00, 138.22it/s, train_loss=1.4581, val_loss=1.4805]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:16<00:00, 119.85it/s, train_loss=1.4581, val_loss=1.4805]\u001B[A\n",
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<08:40,  3.84it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   1%|          | 19/2000 [00:00<00:29, 66.11it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   2%|▏         | 38/2000 [00:00<00:18, 106.13it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   3%|▎         | 57/2000 [00:00<00:14, 131.14it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   4%|▍         | 76/2000 [00:00<00:13, 147.99it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   5%|▍         | 95/2000 [00:00<00:11, 159.37it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   5%|▍         | 95/2000 [00:01<00:11, 159.37it/s, train_loss=2.8720, val_loss=2.8773]\u001B[A\n",
      "Training (constant):   6%|▌         | 113/2000 [00:01<00:20, 91.20it/s, train_loss=2.8720, val_loss=2.8773]\u001B[A\n",
      "Training (constant):   7%|▋         | 132/2000 [00:01<00:17, 109.27it/s, train_loss=2.8720, val_loss=2.8773]\u001B[A\n",
      "Training (constant):   8%|▊         | 151/2000 [00:01<00:14, 125.67it/s, train_loss=2.8720, val_loss=2.8773]\u001B[A\n",
      "Training (constant):   8%|▊         | 170/2000 [00:01<00:13, 139.54it/s, train_loss=2.8720, val_loss=2.8773]\u001B[A\n",
      "Training (constant):   9%|▉         | 189/2000 [00:01<00:12, 150.79it/s, train_loss=2.8720, val_loss=2.8773]\u001B[A\n",
      "Training (constant):   9%|▉         | 189/2000 [00:01<00:12, 150.79it/s, train_loss=2.6187, val_loss=2.6303]\u001B[A\n",
      "Training (constant):  10%|█         | 207/2000 [00:01<00:19, 92.50it/s, train_loss=2.6187, val_loss=2.6303] \u001B[A\n",
      "Training (constant):  11%|█▏        | 226/2000 [00:02<00:16, 108.93it/s, train_loss=2.6187, val_loss=2.6303]\u001B[A\n",
      "Training (constant):  12%|█▏        | 242/2000 [00:02<00:14, 118.46it/s, train_loss=2.6187, val_loss=2.6303]\u001B[A\n",
      "Training (constant):  13%|█▎        | 261/2000 [00:02<00:13, 133.04it/s, train_loss=2.6187, val_loss=2.6303]\u001B[A\n",
      "Training (constant):  14%|█▍        | 280/2000 [00:02<00:11, 145.28it/s, train_loss=2.6187, val_loss=2.6303]\u001B[A\n",
      "Training (constant):  15%|█▍        | 299/2000 [00:02<00:10, 155.25it/s, train_loss=2.6187, val_loss=2.6303]\u001B[A\n",
      "Training (constant):  15%|█▍        | 299/2000 [00:02<00:10, 155.25it/s, train_loss=2.4626, val_loss=2.4607]\u001B[A\n",
      "Training (constant):  16%|█▌        | 317/2000 [00:02<00:17, 94.85it/s, train_loss=2.4626, val_loss=2.4607] \u001B[A\n",
      "Training (constant):  17%|█▋        | 336/2000 [00:02<00:14, 111.44it/s, train_loss=2.4626, val_loss=2.4607]\u001B[A\n",
      "Training (constant):  18%|█▊        | 355/2000 [00:03<00:13, 126.49it/s, train_loss=2.4626, val_loss=2.4607]\u001B[A\n",
      "Training (constant):  19%|█▊        | 374/2000 [00:03<00:11, 139.28it/s, train_loss=2.4626, val_loss=2.4607]\u001B[A\n",
      "Training (constant):  20%|█▉        | 393/2000 [00:03<00:10, 150.47it/s, train_loss=2.4626, val_loss=2.4607]\u001B[A\n",
      "Training (constant):  20%|█▉        | 393/2000 [00:03<00:10, 150.47it/s, train_loss=2.3571, val_loss=2.3498]\u001B[A\n",
      "Training (constant):  21%|██        | 411/2000 [00:03<00:16, 94.15it/s, train_loss=2.3571, val_loss=2.3498] \u001B[A\n",
      "Training (constant):  22%|██▏       | 430/2000 [00:03<00:14, 110.50it/s, train_loss=2.3571, val_loss=2.3498]\u001B[A\n",
      "Training (constant):  22%|██▏       | 449/2000 [00:03<00:12, 125.56it/s, train_loss=2.3571, val_loss=2.3498]\u001B[A\n",
      "Training (constant):  23%|██▎       | 468/2000 [00:03<00:11, 138.98it/s, train_loss=2.3571, val_loss=2.3498]\u001B[A\n",
      "Training (constant):  24%|██▍       | 487/2000 [00:04<00:10, 149.76it/s, train_loss=2.3571, val_loss=2.3498]\u001B[A\n",
      "Training (constant):  24%|██▍       | 487/2000 [00:04<00:10, 149.76it/s, train_loss=2.2473, val_loss=2.2401]\u001B[A\n",
      "Training (constant):  25%|██▌       | 505/2000 [00:04<00:16, 93.00it/s, train_loss=2.2473, val_loss=2.2401] \u001B[A\n",
      "Training (constant):  26%|██▌       | 523/2000 [00:04<00:13, 108.29it/s, train_loss=2.2473, val_loss=2.2401]\u001B[A\n",
      "Training (constant):  27%|██▋       | 542/2000 [00:04<00:11, 123.74it/s, train_loss=2.2473, val_loss=2.2401]\u001B[A\n",
      "Training (constant):  28%|██▊       | 561/2000 [00:04<00:10, 137.57it/s, train_loss=2.2473, val_loss=2.2401]\u001B[A\n",
      "Training (constant):  29%|██▉       | 580/2000 [00:04<00:09, 149.06it/s, train_loss=2.2473, val_loss=2.2401]\u001B[A\n",
      "Training (constant):  30%|██▉       | 599/2000 [00:04<00:08, 157.89it/s, train_loss=2.2473, val_loss=2.2401]\u001B[A\n",
      "Training (constant):  30%|██▉       | 599/2000 [00:05<00:08, 157.89it/s, train_loss=2.1655, val_loss=2.1573]\u001B[A\n",
      "Training (constant):  31%|███       | 617/2000 [00:05<00:14, 95.97it/s, train_loss=2.1655, val_loss=2.1573] \u001B[A\n",
      "Training (constant):  32%|███▏      | 636/2000 [00:05<00:12, 112.36it/s, train_loss=2.1655, val_loss=2.1573]\u001B[A\n",
      "Training (constant):  33%|███▎      | 655/2000 [00:05<00:10, 127.37it/s, train_loss=2.1655, val_loss=2.1573]\u001B[A\n",
      "Training (constant):  34%|███▎      | 674/2000 [00:05<00:09, 140.41it/s, train_loss=2.1655, val_loss=2.1573]\u001B[A\n",
      "Training (constant):  35%|███▍      | 692/2000 [00:05<00:08, 149.83it/s, train_loss=2.1655, val_loss=2.1573]\u001B[A\n",
      "Training (constant):  35%|███▍      | 692/2000 [00:05<00:08, 149.83it/s, train_loss=2.1654, val_loss=2.1557]\u001B[A\n",
      "Training (constant):  36%|███▌      | 710/2000 [00:06<00:13, 92.59it/s, train_loss=2.1654, val_loss=2.1557] \u001B[A\n",
      "Training (constant):  36%|███▋      | 728/2000 [00:06<00:11, 108.07it/s, train_loss=2.1654, val_loss=2.1557]\u001B[A\n",
      "Training (constant):  37%|███▋      | 747/2000 [00:06<00:10, 123.32it/s, train_loss=2.1654, val_loss=2.1557]\u001B[A\n",
      "Training (constant):  38%|███▊      | 766/2000 [00:06<00:09, 136.71it/s, train_loss=2.1654, val_loss=2.1557]\u001B[A\n",
      "Training (constant):  39%|███▉      | 785/2000 [00:06<00:08, 148.20it/s, train_loss=2.1654, val_loss=2.1557]\u001B[A\n",
      "Training (constant):  39%|███▉      | 785/2000 [00:06<00:08, 148.20it/s, train_loss=2.0364, val_loss=2.0422]\u001B[A\n",
      "Training (constant):  40%|████      | 803/2000 [00:06<00:13, 91.66it/s, train_loss=2.0364, val_loss=2.0422] \u001B[A\n",
      "Training (constant):  41%|████      | 820/2000 [00:06<00:11, 105.34it/s, train_loss=2.0364, val_loss=2.0422]\u001B[A\n",
      "Training (constant):  42%|████▏     | 838/2000 [00:07<00:09, 119.75it/s, train_loss=2.0364, val_loss=2.0422]\u001B[A\n",
      "Training (constant):  43%|████▎     | 857/2000 [00:07<00:08, 133.78it/s, train_loss=2.0364, val_loss=2.0422]\u001B[A\n",
      "Training (constant):  44%|████▍     | 875/2000 [00:07<00:07, 144.65it/s, train_loss=2.0364, val_loss=2.0422]\u001B[A\n",
      "Training (constant):  45%|████▍     | 894/2000 [00:07<00:07, 154.57it/s, train_loss=2.0364, val_loss=2.0422]\u001B[A\n",
      "Training (constant):  45%|████▍     | 894/2000 [00:07<00:07, 154.57it/s, train_loss=2.0077, val_loss=2.0052]\u001B[A\n",
      "Training (constant):  46%|████▌     | 912/2000 [00:07<00:11, 93.64it/s, train_loss=2.0077, val_loss=2.0052] \u001B[A\n",
      "Training (constant):  47%|████▋     | 931/2000 [00:07<00:09, 110.15it/s, train_loss=2.0077, val_loss=2.0052]\u001B[A\n",
      "Training (constant):  48%|████▊     | 950/2000 [00:07<00:08, 125.35it/s, train_loss=2.0077, val_loss=2.0052]\u001B[A\n",
      "Training (constant):  48%|████▊     | 969/2000 [00:08<00:07, 138.19it/s, train_loss=2.0077, val_loss=2.0052]\u001B[A\n",
      "Training (constant):  49%|████▉     | 988/2000 [00:08<00:06, 148.99it/s, train_loss=2.0077, val_loss=2.0052]\u001B[A\n",
      "Training (constant):  49%|████▉     | 988/2000 [00:08<00:06, 148.99it/s, train_loss=1.9194, val_loss=1.9127]\u001B[A\n",
      "Training (constant):  50%|█████     | 1006/2000 [00:08<00:10, 90.98it/s, train_loss=1.9194, val_loss=1.9127]\u001B[A\n",
      "Training (constant):  51%|█████▏    | 1025/2000 [00:08<00:09, 107.22it/s, train_loss=1.9194, val_loss=1.9127]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1044/2000 [00:08<00:07, 122.53it/s, train_loss=1.9194, val_loss=1.9127]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1061/2000 [00:08<00:07, 131.76it/s, train_loss=1.9194, val_loss=1.9127]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1077/2000 [00:08<00:06, 136.38it/s, train_loss=1.9194, val_loss=1.9127]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1094/2000 [00:09<00:06, 144.60it/s, train_loss=1.9194, val_loss=1.9127]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1094/2000 [00:09<00:06, 144.60it/s, train_loss=1.9234, val_loss=1.9378]\u001B[A\n",
      "Training (constant):  56%|█████▌    | 1111/2000 [00:09<00:10, 88.48it/s, train_loss=1.9234, val_loss=1.9378] \u001B[A\n",
      "Training (constant):  56%|█████▋    | 1129/2000 [00:09<00:08, 104.60it/s, train_loss=1.9234, val_loss=1.9378]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1147/2000 [00:09<00:07, 119.78it/s, train_loss=1.9234, val_loss=1.9378]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1165/2000 [00:09<00:06, 133.29it/s, train_loss=1.9234, val_loss=1.9378]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1184/2000 [00:09<00:05, 145.48it/s, train_loss=1.9234, val_loss=1.9378]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1184/2000 [00:10<00:05, 145.48it/s, train_loss=1.9027, val_loss=1.9192]\u001B[A\n",
      "Training (constant):  60%|██████    | 1201/2000 [00:10<00:09, 88.48it/s, train_loss=1.9027, val_loss=1.9192] \u001B[A\n",
      "Training (constant):  61%|██████    | 1219/2000 [00:10<00:07, 104.31it/s, train_loss=1.9027, val_loss=1.9192]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1238/2000 [00:10<00:06, 120.57it/s, train_loss=1.9027, val_loss=1.9192]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1256/2000 [00:10<00:05, 133.70it/s, train_loss=1.9027, val_loss=1.9192]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1275/2000 [00:10<00:04, 145.31it/s, train_loss=1.9027, val_loss=1.9192]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1294/2000 [00:10<00:04, 155.28it/s, train_loss=1.9027, val_loss=1.9192]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1294/2000 [00:11<00:04, 155.28it/s, train_loss=1.7760, val_loss=1.7940]\u001B[A\n",
      "Training (constant):  66%|██████▌   | 1312/2000 [00:11<00:07, 94.53it/s, train_loss=1.7760, val_loss=1.7940] \u001B[A\n",
      "Training (constant):  67%|██████▋   | 1331/2000 [00:11<00:06, 110.85it/s, train_loss=1.7760, val_loss=1.7940]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1350/2000 [00:11<00:05, 125.63it/s, train_loss=1.7760, val_loss=1.7940]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1369/2000 [00:11<00:04, 138.58it/s, train_loss=1.7760, val_loss=1.7940]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1388/2000 [00:11<00:04, 149.20it/s, train_loss=1.7760, val_loss=1.7940]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1388/2000 [00:11<00:04, 149.20it/s, train_loss=1.7272, val_loss=1.7372]\u001B[A\n",
      "Training (constant):  70%|███████   | 1406/2000 [00:11<00:06, 91.72it/s, train_loss=1.7272, val_loss=1.7372] \u001B[A\n",
      "Training (constant):  71%|███████   | 1424/2000 [00:11<00:05, 107.06it/s, train_loss=1.7272, val_loss=1.7372]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1443/2000 [00:12<00:04, 122.68it/s, train_loss=1.7272, val_loss=1.7372]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1462/2000 [00:12<00:03, 136.51it/s, train_loss=1.7272, val_loss=1.7372]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1481/2000 [00:12<00:03, 147.77it/s, train_loss=1.7272, val_loss=1.7372]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1500/2000 [00:12<00:03, 156.97it/s, train_loss=1.7272, val_loss=1.7372]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1500/2000 [00:12<00:03, 156.97it/s, train_loss=1.6452, val_loss=1.6527]\u001B[A\n",
      "Training (constant):  76%|███████▌  | 1518/2000 [00:12<00:05, 95.60it/s, train_loss=1.6452, val_loss=1.6527] \u001B[A\n",
      "Training (constant):  77%|███████▋  | 1537/2000 [00:12<00:04, 111.84it/s, train_loss=1.6452, val_loss=1.6527]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1555/2000 [00:12<00:03, 125.66it/s, train_loss=1.6452, val_loss=1.6527]\u001B[A\n",
      "Training (constant):  79%|███████▊  | 1574/2000 [00:13<00:03, 138.74it/s, train_loss=1.6452, val_loss=1.6527]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1593/2000 [00:13<00:02, 149.69it/s, train_loss=1.6452, val_loss=1.6527]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1593/2000 [00:13<00:02, 149.69it/s, train_loss=1.5327, val_loss=1.5463]\u001B[A\n",
      "Training (constant):  81%|████████  | 1611/2000 [00:13<00:04, 93.31it/s, train_loss=1.5327, val_loss=1.5463] \u001B[A\n",
      "Training (constant):  82%|████████▏ | 1630/2000 [00:13<00:03, 109.65it/s, train_loss=1.5327, val_loss=1.5463]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1649/2000 [00:13<00:02, 124.72it/s, train_loss=1.5327, val_loss=1.5463]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1668/2000 [00:13<00:02, 138.05it/s, train_loss=1.5327, val_loss=1.5463]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1687/2000 [00:13<00:02, 148.57it/s, train_loss=1.5327, val_loss=1.5463]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1687/2000 [00:14<00:02, 148.57it/s, train_loss=1.5811, val_loss=1.6062]\u001B[A\n",
      "Training (constant):  85%|████████▌ | 1705/2000 [00:14<00:03, 92.36it/s, train_loss=1.5811, val_loss=1.6062] \u001B[A\n",
      "Training (constant):  86%|████████▌ | 1724/2000 [00:14<00:02, 108.73it/s, train_loss=1.5811, val_loss=1.6062]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1743/2000 [00:14<00:02, 123.84it/s, train_loss=1.5811, val_loss=1.6062]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1762/2000 [00:14<00:01, 137.24it/s, train_loss=1.5811, val_loss=1.6062]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1781/2000 [00:14<00:01, 148.33it/s, train_loss=1.5811, val_loss=1.6062]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1800/2000 [00:14<00:01, 157.30it/s, train_loss=1.5811, val_loss=1.6062]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1800/2000 [00:15<00:01, 157.30it/s, train_loss=1.5265, val_loss=1.5339]\u001B[A\n",
      "Training (constant):  91%|█████████ | 1818/2000 [00:15<00:01, 95.99it/s, train_loss=1.5265, val_loss=1.5339] \u001B[A\n",
      "Training (constant):  92%|█████████▏| 1837/2000 [00:15<00:01, 112.30it/s, train_loss=1.5265, val_loss=1.5339]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1856/2000 [00:15<00:01, 127.05it/s, train_loss=1.5265, val_loss=1.5339]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1875/2000 [00:15<00:00, 139.87it/s, train_loss=1.5265, val_loss=1.5339]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1894/2000 [00:15<00:00, 150.49it/s, train_loss=1.5265, val_loss=1.5339]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1894/2000 [00:15<00:00, 150.49it/s, train_loss=1.4478, val_loss=1.4351]\u001B[A\n",
      "Training (constant):  96%|█████████▌| 1912/2000 [00:16<00:00, 93.75it/s, train_loss=1.4478, val_loss=1.4351] \u001B[A\n",
      "Training (constant):  97%|█████████▋| 1931/2000 [00:16<00:00, 110.17it/s, train_loss=1.4478, val_loss=1.4351]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1950/2000 [00:16<00:00, 125.53it/s, train_loss=1.4478, val_loss=1.4351]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1969/2000 [00:16<00:00, 138.63it/s, train_loss=1.4478, val_loss=1.4351]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:16<00:00, 121.14it/s, train_loss=1.4478, val_loss=1.4351]\u001B[A\n",
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<08:38,  3.86it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   1%|          | 20/2000 [00:00<00:28, 69.01it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   2%|▏         | 39/2000 [00:00<00:18, 108.29it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   3%|▎         | 58/2000 [00:00<00:14, 133.40it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   4%|▍         | 77/2000 [00:00<00:12, 149.13it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   5%|▍         | 96/2000 [00:00<00:11, 160.56it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   5%|▍         | 96/2000 [00:01<00:11, 160.56it/s, train_loss=2.9000, val_loss=2.9033]\u001B[A\n",
      "Training (constant):   6%|▌         | 114/2000 [00:01<00:20, 91.91it/s, train_loss=2.9000, val_loss=2.9033]\u001B[A\n",
      "Training (constant):   7%|▋         | 133/2000 [00:01<00:16, 109.87it/s, train_loss=2.9000, val_loss=2.9033]\u001B[A\n",
      "Training (constant):   8%|▊         | 152/2000 [00:01<00:14, 125.91it/s, train_loss=2.9000, val_loss=2.9033]\u001B[A\n",
      "Training (constant):   9%|▊         | 171/2000 [00:01<00:13, 139.70it/s, train_loss=2.9000, val_loss=2.9033]\u001B[A\n",
      "Training (constant):   9%|▉         | 189/2000 [00:01<00:12, 149.68it/s, train_loss=2.9000, val_loss=2.9033]\u001B[A\n",
      "Training (constant):   9%|▉         | 189/2000 [00:01<00:12, 149.68it/s, train_loss=2.5864, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  10%|█         | 207/2000 [00:01<00:19, 91.26it/s, train_loss=2.5864, val_loss=2.5797] \u001B[A\n",
      "Training (constant):  11%|█▏        | 225/2000 [00:02<00:16, 107.02it/s, train_loss=2.5864, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  12%|█▏        | 244/2000 [00:02<00:14, 122.84it/s, train_loss=2.5864, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  13%|█▎        | 263/2000 [00:02<00:12, 136.34it/s, train_loss=2.5864, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  14%|█▍        | 282/2000 [00:02<00:11, 147.68it/s, train_loss=2.5864, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  15%|█▌        | 300/2000 [00:02<00:10, 155.27it/s, train_loss=2.5864, val_loss=2.5797]\u001B[A\n",
      "Training (constant):  15%|█▌        | 300/2000 [00:02<00:10, 155.27it/s, train_loss=2.4230, val_loss=2.4256]\u001B[A\n",
      "Training (constant):  16%|█▌        | 318/2000 [00:02<00:17, 94.04it/s, train_loss=2.4230, val_loss=2.4256] \u001B[A\n",
      "Training (constant):  17%|█▋        | 336/2000 [00:02<00:15, 109.40it/s, train_loss=2.4230, val_loss=2.4256]\u001B[A\n",
      "Training (constant):  18%|█▊        | 354/2000 [00:03<00:13, 123.72it/s, train_loss=2.4230, val_loss=2.4256]\u001B[A\n",
      "Training (constant):  19%|█▊        | 373/2000 [00:03<00:11, 137.37it/s, train_loss=2.4230, val_loss=2.4256]\u001B[A\n",
      "Training (constant):  20%|█▉        | 392/2000 [00:03<00:10, 149.02it/s, train_loss=2.4230, val_loss=2.4256]\u001B[A\n",
      "Training (constant):  20%|█▉        | 392/2000 [00:03<00:10, 149.02it/s, train_loss=2.3126, val_loss=2.3168]\u001B[A\n",
      "Training (constant):  20%|██        | 410/2000 [00:03<00:17, 91.02it/s, train_loss=2.3126, val_loss=2.3168] \u001B[A\n",
      "Training (constant):  21%|██▏       | 428/2000 [00:03<00:14, 106.17it/s, train_loss=2.3126, val_loss=2.3168]\u001B[A\n",
      "Training (constant):  22%|██▏       | 447/2000 [00:03<00:12, 121.61it/s, train_loss=2.3126, val_loss=2.3168]\u001B[A\n",
      "Training (constant):  23%|██▎       | 465/2000 [00:03<00:11, 134.37it/s, train_loss=2.3126, val_loss=2.3168]\u001B[A\n",
      "Training (constant):  24%|██▍       | 484/2000 [00:04<00:10, 145.83it/s, train_loss=2.3126, val_loss=2.3168]\u001B[A\n",
      "Training (constant):  24%|██▍       | 484/2000 [00:04<00:10, 145.83it/s, train_loss=2.2235, val_loss=2.2322]\u001B[A\n",
      "Training (constant):  25%|██▌       | 501/2000 [00:04<00:17, 87.59it/s, train_loss=2.2235, val_loss=2.2322] \u001B[A\n",
      "Training (constant):  26%|██▌       | 518/2000 [00:04<00:14, 101.64it/s, train_loss=2.2235, val_loss=2.2322]\u001B[A\n",
      "Training (constant):  27%|██▋       | 537/2000 [00:04<00:12, 118.08it/s, train_loss=2.2235, val_loss=2.2322]\u001B[A\n",
      "Training (constant):  28%|██▊       | 556/2000 [00:04<00:10, 132.27it/s, train_loss=2.2235, val_loss=2.2322]\u001B[A\n",
      "Training (constant):  29%|██▉       | 575/2000 [00:04<00:09, 144.19it/s, train_loss=2.2235, val_loss=2.2322]\u001B[A\n",
      "Training (constant):  30%|██▉       | 594/2000 [00:04<00:09, 153.63it/s, train_loss=2.2235, val_loss=2.2322]\u001B[A\n",
      "Training (constant):  30%|██▉       | 594/2000 [00:05<00:09, 153.63it/s, train_loss=2.1846, val_loss=2.1830]\u001B[A\n",
      "Training (constant):  31%|███       | 612/2000 [00:05<00:14, 94.05it/s, train_loss=2.1846, val_loss=2.1830] \u001B[A\n",
      "Training (constant):  32%|███▏      | 631/2000 [00:05<00:12, 110.65it/s, train_loss=2.1846, val_loss=2.1830]\u001B[A\n",
      "Training (constant):  32%|███▎      | 650/2000 [00:05<00:10, 125.70it/s, train_loss=2.1846, val_loss=2.1830]\u001B[A\n",
      "Training (constant):  33%|███▎      | 669/2000 [00:05<00:09, 138.76it/s, train_loss=2.1846, val_loss=2.1830]\u001B[A\n",
      "Training (constant):  34%|███▍      | 687/2000 [00:05<00:08, 148.60it/s, train_loss=2.1846, val_loss=2.1830]\u001B[A\n",
      "Training (constant):  34%|███▍      | 687/2000 [00:06<00:08, 148.60it/s, train_loss=2.1191, val_loss=2.1265]\u001B[A\n",
      "Training (constant):  35%|███▌      | 705/2000 [00:06<00:14, 90.67it/s, train_loss=2.1191, val_loss=2.1265] \u001B[A\n",
      "Training (constant):  36%|███▌      | 723/2000 [00:06<00:12, 106.10it/s, train_loss=2.1191, val_loss=2.1265]\u001B[A\n",
      "Training (constant):  37%|███▋      | 741/2000 [00:06<00:10, 120.81it/s, train_loss=2.1191, val_loss=2.1265]\u001B[A\n",
      "Training (constant):  38%|███▊      | 760/2000 [00:06<00:09, 134.57it/s, train_loss=2.1191, val_loss=2.1265]\u001B[A\n",
      "Training (constant):  39%|███▉      | 779/2000 [00:06<00:08, 146.17it/s, train_loss=2.1191, val_loss=2.1265]\u001B[A\n",
      "Training (constant):  40%|███▉      | 798/2000 [00:06<00:07, 155.74it/s, train_loss=2.1191, val_loss=2.1265]\u001B[A\n",
      "Training (constant):  40%|███▉      | 798/2000 [00:06<00:07, 155.74it/s, train_loss=2.0597, val_loss=2.0534]\u001B[A\n",
      "Training (constant):  41%|████      | 816/2000 [00:06<00:12, 94.81it/s, train_loss=2.0597, val_loss=2.0534] \u001B[A\n",
      "Training (constant):  42%|████▏     | 835/2000 [00:07<00:10, 111.42it/s, train_loss=2.0597, val_loss=2.0534]\u001B[A\n",
      "Training (constant):  43%|████▎     | 854/2000 [00:07<00:09, 126.55it/s, train_loss=2.0597, val_loss=2.0534]\u001B[A\n",
      "Training (constant):  44%|████▎     | 873/2000 [00:07<00:08, 139.48it/s, train_loss=2.0597, val_loss=2.0534]\u001B[A\n",
      "Training (constant):  45%|████▍     | 892/2000 [00:07<00:07, 150.25it/s, train_loss=2.0597, val_loss=2.0534]\u001B[A\n",
      "Training (constant):  45%|████▍     | 892/2000 [00:07<00:07, 150.25it/s, train_loss=2.0476, val_loss=2.0432]\u001B[A\n",
      "Training (constant):  46%|████▌     | 910/2000 [00:07<00:11, 93.53it/s, train_loss=2.0476, val_loss=2.0432] \u001B[A\n",
      "Training (constant):  46%|████▋     | 928/2000 [00:07<00:09, 108.69it/s, train_loss=2.0476, val_loss=2.0432]\u001B[A\n",
      "Training (constant):  47%|████▋     | 946/2000 [00:07<00:08, 123.07it/s, train_loss=2.0476, val_loss=2.0432]\u001B[A\n",
      "Training (constant):  48%|████▊     | 964/2000 [00:08<00:07, 135.60it/s, train_loss=2.0476, val_loss=2.0432]\u001B[A\n",
      "Training (constant):  49%|████▉     | 983/2000 [00:08<00:06, 146.94it/s, train_loss=2.0476, val_loss=2.0432]\u001B[A\n",
      "Training (constant):  49%|████▉     | 983/2000 [00:08<00:06, 146.94it/s, train_loss=1.9494, val_loss=1.9514]\u001B[A\n",
      "Training (constant):  50%|█████     | 1001/2000 [00:08<00:10, 90.89it/s, train_loss=1.9494, val_loss=1.9514]\u001B[A\n",
      "Training (constant):  51%|█████     | 1019/2000 [00:08<00:09, 106.41it/s, train_loss=1.9494, val_loss=1.9514]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1037/2000 [00:08<00:07, 121.06it/s, train_loss=1.9494, val_loss=1.9514]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1056/2000 [00:08<00:06, 135.20it/s, train_loss=1.9494, val_loss=1.9514]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1075/2000 [00:08<00:06, 146.83it/s, train_loss=1.9494, val_loss=1.9514]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1094/2000 [00:09<00:05, 155.86it/s, train_loss=1.9494, val_loss=1.9514]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1094/2000 [00:09<00:05, 155.86it/s, train_loss=1.8665, val_loss=1.8748]\u001B[A\n",
      "Training (constant):  56%|█████▌    | 1112/2000 [00:09<00:09, 94.78it/s, train_loss=1.8665, val_loss=1.8748] \u001B[A\n",
      "Training (constant):  57%|█████▋    | 1131/2000 [00:09<00:07, 111.05it/s, train_loss=1.8665, val_loss=1.8748]\u001B[A\n",
      "Training (constant):  57%|█████▊    | 1150/2000 [00:09<00:06, 126.37it/s, train_loss=1.8665, val_loss=1.8748]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1169/2000 [00:09<00:05, 139.46it/s, train_loss=1.8665, val_loss=1.8748]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1188/2000 [00:09<00:05, 150.05it/s, train_loss=1.8665, val_loss=1.8748]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1188/2000 [00:10<00:05, 150.05it/s, train_loss=1.8212, val_loss=1.8405]\u001B[A\n",
      "Training (constant):  60%|██████    | 1206/2000 [00:10<00:08, 92.72it/s, train_loss=1.8212, val_loss=1.8405] \u001B[A\n",
      "Training (constant):  61%|██████    | 1224/2000 [00:10<00:07, 108.02it/s, train_loss=1.8212, val_loss=1.8405]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1243/2000 [00:10<00:06, 123.27it/s, train_loss=1.8212, val_loss=1.8405]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1262/2000 [00:10<00:05, 136.70it/s, train_loss=1.8212, val_loss=1.8405]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1281/2000 [00:10<00:04, 147.80it/s, train_loss=1.8212, val_loss=1.8405]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1300/2000 [00:10<00:04, 156.55it/s, train_loss=1.8212, val_loss=1.8405]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1300/2000 [00:11<00:04, 156.55it/s, train_loss=2.0335, val_loss=2.0558]\u001B[A\n",
      "Training (constant):  66%|██████▌   | 1318/2000 [00:11<00:07, 95.41it/s, train_loss=2.0335, val_loss=2.0558] \u001B[A\n",
      "Training (constant):  67%|██████▋   | 1337/2000 [00:11<00:05, 111.55it/s, train_loss=2.0335, val_loss=2.0558]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1356/2000 [00:11<00:05, 126.62it/s, train_loss=2.0335, val_loss=2.0558]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1375/2000 [00:11<00:04, 139.45it/s, train_loss=2.0335, val_loss=2.0558]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1393/2000 [00:11<00:04, 148.99it/s, train_loss=2.0335, val_loss=2.0558]\u001B[A\n",
      "Training (constant):  70%|██████▉   | 1393/2000 [00:11<00:04, 148.99it/s, train_loss=1.7307, val_loss=1.7450]\u001B[A\n",
      "Training (constant):  71%|███████   | 1411/2000 [00:11<00:06, 92.45it/s, train_loss=1.7307, val_loss=1.7450] \u001B[A\n",
      "Training (constant):  71%|███████▏  | 1429/2000 [00:12<00:05, 107.72it/s, train_loss=1.7307, val_loss=1.7450]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1448/2000 [00:12<00:04, 123.13it/s, train_loss=1.7307, val_loss=1.7450]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1467/2000 [00:12<00:03, 136.49it/s, train_loss=1.7307, val_loss=1.7450]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1486/2000 [00:12<00:03, 147.68it/s, train_loss=1.7307, val_loss=1.7450]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1486/2000 [00:12<00:03, 147.68it/s, train_loss=1.8421, val_loss=1.8651]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1503/2000 [00:12<00:05, 90.73it/s, train_loss=1.8421, val_loss=1.8651] \u001B[A\n",
      "Training (constant):  76%|███████▌  | 1521/2000 [00:12<00:04, 106.03it/s, train_loss=1.8421, val_loss=1.8651]\u001B[A\n",
      "Training (constant):  77%|███████▋  | 1540/2000 [00:12<00:03, 121.64it/s, train_loss=1.8421, val_loss=1.8651]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1558/2000 [00:13<00:03, 134.38it/s, train_loss=1.8421, val_loss=1.8651]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1576/2000 [00:13<00:02, 144.96it/s, train_loss=1.8421, val_loss=1.8651]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1595/2000 [00:13<00:02, 154.36it/s, train_loss=1.8421, val_loss=1.8651]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1595/2000 [00:13<00:02, 154.36it/s, train_loss=1.6080, val_loss=1.6120]\u001B[A\n",
      "Training (constant):  81%|████████  | 1613/2000 [00:13<00:04, 93.03it/s, train_loss=1.6080, val_loss=1.6120] \u001B[A\n",
      "Training (constant):  82%|████████▏ | 1632/2000 [00:13<00:03, 109.43it/s, train_loss=1.6080, val_loss=1.6120]\u001B[A\n",
      "Training (constant):  82%|████████▎ | 1650/2000 [00:13<00:02, 123.49it/s, train_loss=1.6080, val_loss=1.6120]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1668/2000 [00:13<00:02, 136.12it/s, train_loss=1.6080, val_loss=1.6120]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1687/2000 [00:13<00:02, 147.56it/s, train_loss=1.6080, val_loss=1.6120]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1687/2000 [00:14<00:02, 147.56it/s, train_loss=1.6296, val_loss=1.6452]\u001B[A\n",
      "Training (constant):  85%|████████▌ | 1704/2000 [00:14<00:03, 89.91it/s, train_loss=1.6296, val_loss=1.6452] \u001B[A\n",
      "Training (constant):  86%|████████▌ | 1721/2000 [00:14<00:02, 103.42it/s, train_loss=1.6296, val_loss=1.6452]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1740/2000 [00:14<00:02, 119.49it/s, train_loss=1.6296, val_loss=1.6452]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1759/2000 [00:14<00:01, 133.70it/s, train_loss=1.6296, val_loss=1.6452]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1778/2000 [00:14<00:01, 145.61it/s, train_loss=1.6296, val_loss=1.6452]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1796/2000 [00:14<00:01, 154.15it/s, train_loss=1.6296, val_loss=1.6452]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1796/2000 [00:15<00:01, 154.15it/s, train_loss=1.4390, val_loss=1.4570]\u001B[A\n",
      "Training (constant):  91%|█████████ | 1814/2000 [00:15<00:01, 93.24it/s, train_loss=1.4390, val_loss=1.4570] \u001B[A\n",
      "Training (constant):  92%|█████████▏| 1833/2000 [00:15<00:01, 109.74it/s, train_loss=1.4390, val_loss=1.4570]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1852/2000 [00:15<00:01, 124.78it/s, train_loss=1.4390, val_loss=1.4570]\u001B[A\n",
      "Training (constant):  94%|█████████▎| 1871/2000 [00:15<00:00, 137.80it/s, train_loss=1.4390, val_loss=1.4570]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1890/2000 [00:15<00:00, 149.27it/s, train_loss=1.4390, val_loss=1.4570]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1890/2000 [00:16<00:00, 149.27it/s, train_loss=1.4881, val_loss=1.4978]\u001B[A\n",
      "Training (constant):  95%|█████████▌| 1908/2000 [00:16<00:00, 92.15it/s, train_loss=1.4881, val_loss=1.4978] \u001B[A\n",
      "Training (constant):  96%|█████████▋| 1926/2000 [00:16<00:00, 107.25it/s, train_loss=1.4881, val_loss=1.4978]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1944/2000 [00:16<00:00, 121.58it/s, train_loss=1.4881, val_loss=1.4978]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1962/2000 [00:16<00:00, 134.45it/s, train_loss=1.4881, val_loss=1.4978]\u001B[A\n",
      "Training (constant):  99%|█████████▉| 1980/2000 [00:16<00:00, 145.04it/s, train_loss=1.4881, val_loss=1.4978]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:16<00:00, 120.65it/s, train_loss=1.4881, val_loss=1.4978]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Constant: lr=0.02, avg_val_loss=1.4711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Adam LR:   0%|          | 0/3 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:00,  3.70it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.29it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.43it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 74.04it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:23, 82.59it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:22, 88.28it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 67/2000 [00:00<00:20, 92.18it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 78/2000 [00:01<00:20, 94.92it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:19, 96.68it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 97.98it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 97.98it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 110/2000 [00:01<00:33, 55.66it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:29, 64.61it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 132/2000 [00:01<00:25, 72.54it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:01<00:23, 79.50it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:21, 85.08it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 164/2000 [00:02<00:20, 88.81it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 175/2000 [00:02<00:19, 92.14it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 185/2000 [00:02<00:19, 94.22it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 196/2000 [00:02<00:18, 96.28it/s, train_loss=3.3298, val_loss=3.3317]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 196/2000 [00:02<00:18, 96.28it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 206/2000 [00:02<00:32, 55.28it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 217/2000 [00:02<00:27, 64.34it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 228/2000 [00:03<00:24, 72.37it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 239/2000 [00:03<00:22, 79.18it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▎        | 250/2000 [00:03<00:20, 84.90it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:19, 89.34it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 272/2000 [00:03<00:18, 92.28it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 283/2000 [00:03<00:18, 94.87it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 294/2000 [00:03<00:17, 96.89it/s, train_loss=3.1089, val_loss=3.1073]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 294/2000 [00:04<00:17, 96.89it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 304/2000 [00:04<00:30, 55.09it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 314/2000 [00:04<00:26, 63.19it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:23, 71.48it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 335/2000 [00:04<00:21, 77.85it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 345/2000 [00:04<00:19, 82.97it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 356/2000 [00:04<00:18, 87.77it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 367/2000 [00:04<00:17, 91.65it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 378/2000 [00:04<00:17, 94.38it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 389/2000 [00:04<00:16, 96.07it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 400/2000 [00:05<00:16, 97.39it/s, train_loss=2.9362, val_loss=2.9381]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 400/2000 [00:05<00:16, 97.39it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 410/2000 [00:05<00:28, 55.58it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:24, 64.52it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:05<00:21, 71.67it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:05<00:19, 78.07it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 452/2000 [00:05<00:18, 84.03it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 462/2000 [00:05<00:17, 87.61it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:16, 91.59it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 484/2000 [00:06<00:16, 94.21it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 494/2000 [00:06<00:15, 95.46it/s, train_loss=2.7887, val_loss=2.7959]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 494/2000 [00:06<00:15, 95.46it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 504/2000 [00:06<00:27, 53.78it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 514/2000 [00:06<00:24, 61.91it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 524/2000 [00:06<00:21, 69.26it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 535/2000 [00:06<00:19, 76.84it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 545/2000 [00:07<00:17, 82.30it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 556/2000 [00:07<00:16, 87.47it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 566/2000 [00:07<00:15, 90.41it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 576/2000 [00:07<00:15, 92.24it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 587/2000 [00:07<00:14, 94.69it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:07<00:14, 96.61it/s, train_loss=2.6612, val_loss=2.6635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:07<00:14, 96.61it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 608/2000 [00:07<00:25, 55.00it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 618/2000 [00:08<00:21, 63.25it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 629/2000 [00:08<00:19, 71.58it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 640/2000 [00:08<00:17, 78.61it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 651/2000 [00:08<00:16, 84.31it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 662/2000 [00:08<00:15, 88.64it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 672/2000 [00:08<00:14, 91.00it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 683/2000 [00:08<00:14, 93.90it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:08<00:13, 95.53it/s, train_loss=2.5454, val_loss=2.5416]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:09<00:13, 95.53it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 703/2000 [00:09<00:24, 53.73it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 713/2000 [00:09<00:20, 62.02it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 723/2000 [00:09<00:18, 69.79it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:09<00:16, 77.43it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 745/2000 [00:09<00:15, 83.44it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 756/2000 [00:09<00:14, 88.19it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 767/2000 [00:09<00:13, 91.65it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:09<00:13, 93.79it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 788/2000 [00:10<00:12, 96.14it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:12, 97.90it/s, train_loss=2.4269, val_loss=2.4430]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:12, 97.90it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 810/2000 [00:10<00:21, 56.52it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:10<00:18, 65.30it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:10<00:16, 72.98it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 843/2000 [00:10<00:14, 79.61it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 854/2000 [00:10<00:13, 85.00it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:11<00:12, 88.41it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 874/2000 [00:11<00:12, 91.27it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 884/2000 [00:11<00:11, 93.63it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:11, 95.76it/s, train_loss=2.3300, val_loss=2.3380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:11, 95.76it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:11<00:20, 54.69it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 915/2000 [00:11<00:17, 62.82it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 926/2000 [00:11<00:15, 71.23it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 936/2000 [00:12<00:13, 77.63it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 946/2000 [00:12<00:12, 83.07it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 956/2000 [00:12<00:11, 87.43it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:12<00:11, 90.68it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 977/2000 [00:12<00:10, 93.43it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 988/2000 [00:12<00:10, 95.62it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:12<00:10, 96.72it/s, train_loss=2.2400, val_loss=2.2458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:12<00:10, 96.72it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1008/2000 [00:13<00:18, 54.97it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1018/2000 [00:13<00:15, 63.17it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1028/2000 [00:13<00:13, 70.70it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:13<00:12, 77.30it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:13<00:11, 82.53it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1058/2000 [00:13<00:10, 86.99it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1068/2000 [00:13<00:10, 90.06it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1079/2000 [00:13<00:09, 92.96it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1089/2000 [00:13<00:09, 94.66it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:14<00:09, 96.66it/s, train_loss=2.1473, val_loss=2.1489]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:14<00:09, 96.66it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:14<00:16, 53.40it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:14<00:14, 60.87it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1130/2000 [00:14<00:12, 68.21it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1139/2000 [00:14<00:13, 65.34it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1148/2000 [00:14<00:12, 70.62it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:14<00:11, 76.15it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:15<00:10, 81.11it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1178/2000 [00:15<00:09, 84.47it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1188/2000 [00:15<00:09, 87.22it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:09, 88.31it/s, train_loss=2.0555, val_loss=2.0617]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:09, 88.31it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:15<00:15, 52.45it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1217/2000 [00:15<00:13, 58.32it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1225/2000 [00:15<00:12, 62.76it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1233/2000 [00:16<00:11, 65.14it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1241/2000 [00:16<00:11, 68.68it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▎   | 1250/2000 [00:16<00:10, 74.01it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1259/2000 [00:16<00:09, 77.30it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1269/2000 [00:16<00:09, 81.13it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:16<00:08, 84.05it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1288/2000 [00:16<00:08, 85.54it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:16<00:08, 87.32it/s, train_loss=1.9666, val_loss=1.9722]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:17<00:08, 87.32it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:17<00:14, 48.48it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:17<00:11, 57.38it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1327/2000 [00:17<00:10, 65.06it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1337/2000 [00:17<00:09, 71.05it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1346/2000 [00:17<00:08, 75.16it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1355/2000 [00:17<00:08, 78.66it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1365/2000 [00:17<00:07, 82.15it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1374/2000 [00:17<00:07, 82.05it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1383/2000 [00:18<00:07, 84.01it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:18<00:07, 85.30it/s, train_loss=1.9069, val_loss=1.8991]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:18<00:07, 85.30it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1401/2000 [00:18<00:12, 48.31it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:18<00:10, 57.22it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1421/2000 [00:18<00:08, 65.44it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1431/2000 [00:18<00:07, 72.55it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1441/2000 [00:18<00:07, 77.71it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1451/2000 [00:19<00:06, 81.67it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1461/2000 [00:19<00:06, 84.17it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1470/2000 [00:19<00:06, 84.50it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1479/2000 [00:19<00:06, 84.48it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1488/2000 [00:19<00:06, 83.73it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:06, 83.18it/s, train_loss=1.8191, val_loss=1.8271]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:06, 83.18it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1506/2000 [00:20<00:10, 47.23it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1515/2000 [00:20<00:08, 53.96it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1523/2000 [00:20<00:08, 59.09it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1532/2000 [00:20<00:07, 64.93it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1541/2000 [00:20<00:06, 69.33it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1550/2000 [00:20<00:06, 72.67it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1559/2000 [00:20<00:05, 75.22it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1568/2000 [00:20<00:05, 77.39it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1577/2000 [00:20<00:05, 79.02it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1586/2000 [00:20<00:05, 80.71it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:21<00:04, 82.23it/s, train_loss=1.7635, val_loss=1.7515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:21<00:04, 82.23it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:21<00:08, 47.02it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1613/2000 [00:21<00:07, 54.16it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1621/2000 [00:21<00:06, 59.16it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1630/2000 [00:21<00:05, 65.41it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1639/2000 [00:21<00:05, 70.84it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1648/2000 [00:21<00:04, 75.26it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1657/2000 [00:22<00:04, 78.39it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1666/2000 [00:22<00:04, 80.70it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1675/2000 [00:22<00:04, 79.90it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1684/2000 [00:22<00:03, 80.75it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:22<00:03, 80.82it/s, train_loss=1.6836, val_loss=1.6860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1693/2000 [00:22<00:03, 80.82it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1702/2000 [00:22<00:06, 45.04it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1711/2000 [00:23<00:05, 52.48it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1720/2000 [00:23<00:04, 59.68it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1729/2000 [00:23<00:04, 66.00it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1738/2000 [00:23<00:03, 71.40it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1747/2000 [00:23<00:03, 75.62it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1756/2000 [00:23<00:03, 78.81it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1765/2000 [00:23<00:02, 81.50it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1774/2000 [00:23<00:02, 83.49it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1783/2000 [00:23<00:02, 84.66it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:23<00:02, 84.97it/s, train_loss=1.6160, val_loss=1.6095]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1792/2000 [00:24<00:02, 84.97it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1801/2000 [00:24<00:04, 47.18it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1810/2000 [00:24<00:03, 54.43it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1819/2000 [00:24<00:02, 61.28it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1828/2000 [00:24<00:02, 67.51it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1837/2000 [00:24<00:02, 72.21it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:24<00:02, 75.92it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1855/2000 [00:24<00:01, 76.39it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1864/2000 [00:25<00:01, 77.83it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1873/2000 [00:25<00:01, 79.95it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1882/2000 [00:25<00:01, 81.84it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:25<00:01, 83.58it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:25<00:01, 77.75it/s, train_loss=1.5447, val_loss=1.5458]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:25<00:01, 77.75it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1908/2000 [00:25<00:02, 44.78it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1917/2000 [00:26<00:01, 52.57it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1926/2000 [00:26<00:01, 60.02it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1935/2000 [00:26<00:00, 66.36it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1944/2000 [00:26<00:00, 71.70it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1953/2000 [00:26<00:00, 75.77it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:26<00:00, 79.03it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1971/2000 [00:26<00:00, 81.86it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1980/2000 [00:26<00:00, 83.43it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1989/2000 [00:26<00:00, 84.69it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 74.15it/s, train_loss=1.4869, val_loss=1.5041]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:54,  3.36it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:03, 31.28it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:40, 49.43it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 28/2000 [00:00<00:32, 61.30it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 37/2000 [00:00<00:28, 69.41it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 46/2000 [00:00<00:26, 74.78it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:24, 77.80it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 64/2000 [00:01<00:24, 80.62it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 73/2000 [00:01<00:23, 82.43it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 82/2000 [00:01<00:22, 83.55it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:24, 79.39it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:25, 75.38it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:25, 75.38it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 108/2000 [00:01<00:43, 43.39it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 117/2000 [00:01<00:36, 51.46it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 126/2000 [00:02<00:31, 59.08it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 135/2000 [00:02<00:28, 65.72it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 144/2000 [00:02<00:26, 71.03it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:24, 75.31it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 162/2000 [00:02<00:23, 78.19it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:22, 80.71it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:22, 82.43it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 189/2000 [00:02<00:21, 83.93it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:21, 85.25it/s, train_loss=3.3073, val_loss=3.3099]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:21, 85.25it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:37, 47.81it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:32, 55.44it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:28, 62.32it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 234/2000 [00:03<00:25, 68.35it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:24, 73.10it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 252/2000 [00:03<00:22, 77.15it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:21, 80.47it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 270/2000 [00:03<00:20, 82.44it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 279/2000 [00:04<00:20, 84.08it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 288/2000 [00:04<00:20, 85.39it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:19, 85.93it/s, train_loss=3.0911, val_loss=3.0939]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:19, 85.93it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:04<00:35, 48.19it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:04<00:30, 55.87it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 324/2000 [00:04<00:26, 62.37it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 333/2000 [00:04<00:24, 68.22it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 342/2000 [00:05<00:22, 73.15it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:05<00:21, 76.77it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 360/2000 [00:05<00:20, 79.62it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 369/2000 [00:05<00:19, 81.95it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 378/2000 [00:05<00:19, 82.91it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 387/2000 [00:05<00:19, 84.04it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:18, 84.75it/s, train_loss=2.9239, val_loss=2.9239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:06<00:18, 84.75it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 405/2000 [00:06<00:33, 48.24it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 414/2000 [00:06<00:28, 55.83it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 423/2000 [00:06<00:25, 62.49it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 432/2000 [00:06<00:22, 68.47it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:06<00:21, 73.55it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▎       | 450/2000 [00:06<00:20, 77.00it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 459/2000 [00:06<00:19, 79.70it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 468/2000 [00:06<00:18, 81.87it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 477/2000 [00:06<00:18, 82.28it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 486/2000 [00:07<00:18, 84.09it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:07<00:17, 85.12it/s, train_loss=2.7859, val_loss=2.7820]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:07<00:17, 85.12it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 504/2000 [00:07<00:30, 48.33it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 513/2000 [00:07<00:26, 55.87it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 522/2000 [00:07<00:23, 62.59it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:07<00:21, 68.48it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 540/2000 [00:07<00:19, 73.15it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 549/2000 [00:08<00:18, 76.68it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 558/2000 [00:08<00:18, 79.27it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 567/2000 [00:08<00:17, 81.18it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 576/2000 [00:08<00:17, 83.09it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:08<00:16, 84.39it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 594/2000 [00:08<00:16, 84.41it/s, train_loss=2.6618, val_loss=2.6584]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 594/2000 [00:08<00:16, 84.41it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:08<00:29, 47.45it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 612/2000 [00:09<00:25, 54.78it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 621/2000 [00:09<00:22, 61.77it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 630/2000 [00:09<00:20, 67.62it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 639/2000 [00:09<00:18, 72.67it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 648/2000 [00:09<00:17, 76.49it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:09<00:16, 79.62it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:09<00:16, 81.96it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 675/2000 [00:09<00:15, 83.49it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 684/2000 [00:09<00:15, 84.86it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:09<00:15, 84.51it/s, train_loss=2.5461, val_loss=2.5474]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:10<00:15, 84.51it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 702/2000 [00:10<00:27, 48.05it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 711/2000 [00:10<00:23, 55.72it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 720/2000 [00:10<00:20, 62.80it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 729/2000 [00:10<00:18, 68.83it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 738/2000 [00:10<00:17, 73.57it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 747/2000 [00:10<00:16, 77.60it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 756/2000 [00:10<00:15, 80.23it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 765/2000 [00:11<00:14, 82.64it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 774/2000 [00:11<00:14, 84.10it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:11<00:14, 85.04it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:11<00:14, 86.16it/s, train_loss=2.4506, val_loss=2.4463]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:11<00:14, 86.16it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:11<00:25, 47.00it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 810/2000 [00:11<00:21, 54.34it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 819/2000 [00:11<00:19, 61.57it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 828/2000 [00:12<00:17, 67.81it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 837/2000 [00:12<00:15, 72.96it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 846/2000 [00:12<00:14, 76.93it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 855/2000 [00:12<00:14, 79.99it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:12<00:13, 81.86it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 873/2000 [00:12<00:13, 83.58it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 882/2000 [00:12<00:13, 84.92it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:12<00:13, 85.12it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 900/2000 [00:12<00:12, 86.00it/s, train_loss=2.3454, val_loss=2.3431]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 900/2000 [00:13<00:12, 86.00it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 909/2000 [00:13<00:22, 48.25it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 918/2000 [00:13<00:19, 55.59it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 927/2000 [00:13<00:17, 62.44it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 936/2000 [00:13<00:15, 68.03it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 945/2000 [00:13<00:14, 72.71it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 954/2000 [00:13<00:13, 76.36it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 963/2000 [00:13<00:13, 79.54it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 972/2000 [00:13<00:12, 79.84it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 981/2000 [00:14<00:13, 75.16it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:14<00:13, 73.41it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:14<00:13, 75.02it/s, train_loss=2.2535, val_loss=2.2526]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:14<00:13, 75.02it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:14<00:22, 44.19it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1014/2000 [00:14<00:18, 52.52it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1023/2000 [00:14<00:16, 59.98it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1032/2000 [00:15<00:14, 66.46it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:15<00:13, 71.87it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▎    | 1050/2000 [00:15<00:12, 76.24it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1059/2000 [00:15<00:11, 79.75it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1068/2000 [00:15<00:11, 82.13it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1077/2000 [00:15<00:10, 83.96it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1086/2000 [00:15<00:10, 85.18it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1095/2000 [00:15<00:10, 85.69it/s, train_loss=2.1773, val_loss=2.1746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1095/2000 [00:16<00:10, 85.69it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1104/2000 [00:16<00:18, 48.35it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1113/2000 [00:16<00:15, 55.83it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1122/2000 [00:16<00:13, 62.79it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1131/2000 [00:16<00:12, 68.59it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1140/2000 [00:16<00:11, 73.27it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1149/2000 [00:16<00:11, 76.88it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:16<00:10, 79.57it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1167/2000 [00:16<00:10, 81.70it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1176/2000 [00:16<00:09, 83.44it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1185/2000 [00:17<00:09, 85.08it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:17<00:09, 85.64it/s, train_loss=2.0763, val_loss=2.0812]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:17<00:09, 85.64it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1203/2000 [00:17<00:16, 48.07it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1212/2000 [00:17<00:14, 55.81it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1221/2000 [00:17<00:12, 62.83it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:17<00:11, 68.80it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1239/2000 [00:17<00:10, 73.59it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:18<00:09, 77.41it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:18<00:09, 80.16it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1266/2000 [00:18<00:08, 82.33it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1275/2000 [00:18<00:08, 82.55it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1284/2000 [00:18<00:08, 79.97it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1293/2000 [00:18<00:08, 81.70it/s, train_loss=1.9983, val_loss=2.0093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1293/2000 [00:18<00:08, 81.70it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1302/2000 [00:18<00:15, 46.40it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1311/2000 [00:19<00:12, 54.03it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1320/2000 [00:19<00:11, 61.02it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1329/2000 [00:19<00:09, 67.34it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1338/2000 [00:19<00:09, 72.48it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1347/2000 [00:19<00:08, 76.11it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1356/2000 [00:19<00:08, 79.19it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1365/2000 [00:19<00:07, 81.13it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1374/2000 [00:19<00:07, 82.96it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1383/2000 [00:19<00:07, 84.14it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:19<00:07, 84.99it/s, train_loss=1.9105, val_loss=1.9196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:20<00:07, 84.99it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1401/2000 [00:20<00:12, 47.02it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1410/2000 [00:20<00:10, 54.47it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1419/2000 [00:20<00:09, 61.36it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1428/2000 [00:20<00:08, 67.55it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:20<00:07, 72.22it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1446/2000 [00:20<00:07, 75.98it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1455/2000 [00:20<00:07, 76.85it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1464/2000 [00:21<00:06, 79.19it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1473/2000 [00:21<00:06, 80.92it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1482/2000 [00:21<00:06, 82.32it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1491/2000 [00:21<00:06, 83.61it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1500/2000 [00:21<00:05, 84.97it/s, train_loss=1.8497, val_loss=1.8575]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1500/2000 [00:21<00:05, 84.97it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1509/2000 [00:21<00:10, 47.62it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1518/2000 [00:22<00:08, 55.03it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1527/2000 [00:22<00:07, 61.65it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1536/2000 [00:22<00:06, 67.54it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1545/2000 [00:22<00:06, 72.17it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:22<00:05, 76.49it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1563/2000 [00:22<00:05, 79.39it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1572/2000 [00:22<00:05, 81.39it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1581/2000 [00:22<00:05, 82.85it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1590/2000 [00:22<00:04, 84.39it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:22<00:04, 85.14it/s, train_loss=1.7616, val_loss=1.7663]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:23<00:04, 85.14it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1608/2000 [00:23<00:08, 47.63it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1617/2000 [00:23<00:06, 55.31it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1626/2000 [00:23<00:06, 61.98it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:23<00:05, 67.98it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1644/2000 [00:23<00:04, 72.49it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1653/2000 [00:23<00:04, 76.27it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1662/2000 [00:23<00:04, 79.06it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1671/2000 [00:24<00:04, 81.08it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1680/2000 [00:24<00:03, 83.30it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1689/2000 [00:24<00:03, 84.55it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:24<00:03, 85.53it/s, train_loss=1.6764, val_loss=1.6925]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:24<00:03, 85.53it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1707/2000 [00:24<00:06, 47.95it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1716/2000 [00:24<00:05, 55.43it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:24<00:04, 62.36it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1734/2000 [00:25<00:03, 68.17it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1743/2000 [00:25<00:03, 73.03it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1752/2000 [00:25<00:03, 76.58it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1761/2000 [00:25<00:03, 79.44it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1770/2000 [00:25<00:02, 81.19it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1779/2000 [00:25<00:02, 83.26it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1788/2000 [00:25<00:02, 84.32it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:25<00:02, 85.39it/s, train_loss=1.6227, val_loss=1.6348]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:26<00:02, 85.39it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1806/2000 [00:26<00:04, 44.65it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1813/2000 [00:26<00:03, 48.98it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1822/2000 [00:26<00:03, 56.35it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1831/2000 [00:26<00:02, 63.53it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1840/2000 [00:26<00:02, 69.59it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1849/2000 [00:26<00:02, 74.36it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1858/2000 [00:26<00:01, 78.13it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1867/2000 [00:26<00:01, 81.13it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1876/2000 [00:27<00:01, 82.86it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1885/2000 [00:27<00:01, 84.47it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:27<00:01, 85.52it/s, train_loss=1.5342, val_loss=1.5290]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1894/2000 [00:27<00:01, 85.52it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1903/2000 [00:27<00:02, 48.04it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1912/2000 [00:27<00:01, 55.65it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1921/2000 [00:27<00:01, 62.55it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1930/2000 [00:27<00:01, 68.51it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1939/2000 [00:27<00:00, 73.50it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1948/2000 [00:28<00:00, 77.40it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1957/2000 [00:28<00:00, 79.98it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1966/2000 [00:28<00:00, 82.46it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1975/2000 [00:28<00:00, 83.96it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1984/2000 [00:28<00:00, 84.77it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 69.70it/s, train_loss=1.4479, val_loss=1.4425]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:58,  3.34it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:04, 31.04it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 19/2000 [00:00<00:40, 49.15it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|▏         | 28/2000 [00:00<00:32, 61.35it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 37/2000 [00:00<00:28, 69.59it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 46/2000 [00:00<00:26, 75.09it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:24, 78.79it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 64/2000 [00:01<00:23, 81.56it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 73/2000 [00:01<00:23, 82.52it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 82/2000 [00:01<00:22, 83.61it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:22, 84.46it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:22, 85.52it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:22, 85.52it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 109/2000 [00:01<00:39, 47.67it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 118/2000 [00:01<00:34, 55.25it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 127/2000 [00:02<00:29, 62.51it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 136/2000 [00:02<00:27, 68.63it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:02<00:25, 73.49it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:23, 77.03it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 163/2000 [00:02<00:23, 78.60it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 172/2000 [00:02<00:22, 80.72it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:22, 82.67it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 190/2000 [00:02<00:21, 84.25it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 199/2000 [00:02<00:21, 84.86it/s, train_loss=3.3388, val_loss=3.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 199/2000 [00:03<00:21, 84.86it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 208/2000 [00:03<00:37, 47.96it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 217/2000 [00:03<00:32, 55.53it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 226/2000 [00:03<00:28, 62.61it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 235/2000 [00:03<00:25, 68.76it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 244/2000 [00:03<00:23, 73.68it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 253/2000 [00:03<00:22, 77.55it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 262/2000 [00:03<00:21, 79.95it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:03<00:21, 82.27it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 280/2000 [00:04<00:20, 83.84it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 289/2000 [00:04<00:20, 85.32it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:19, 86.23it/s, train_loss=3.1170, val_loss=3.1127]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:19, 86.23it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 307/2000 [00:04<00:34, 48.44it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 316/2000 [00:04<00:30, 56.08it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:26, 62.91it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 334/2000 [00:04<00:24, 68.52it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 343/2000 [00:05<00:22, 73.43it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:05<00:21, 77.02it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:05<00:20, 79.99it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 370/2000 [00:05<00:19, 81.95it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 379/2000 [00:05<00:19, 83.69it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 388/2000 [00:05<00:18, 84.98it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 398/2000 [00:05<00:18, 87.12it/s, train_loss=2.9474, val_loss=2.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 398/2000 [00:05<00:18, 87.12it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 407/2000 [00:06<00:32, 49.15it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 418/2000 [00:06<00:26, 59.53it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 428/2000 [00:06<00:23, 67.78it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 438/2000 [00:06<00:20, 75.12it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 449/2000 [00:06<00:18, 81.84it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 460/2000 [00:06<00:17, 87.06it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 471/2000 [00:06<00:16, 91.01it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 482/2000 [00:06<00:16, 93.76it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:15, 96.04it/s, train_loss=2.8028, val_loss=2.8012]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:07<00:15, 96.04it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 503/2000 [00:07<00:26, 55.58it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 514/2000 [00:07<00:23, 64.48it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 525/2000 [00:07<00:20, 72.50it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:07<00:18, 79.40it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 547/2000 [00:07<00:17, 84.89it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 558/2000 [00:07<00:16, 89.48it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 569/2000 [00:07<00:15, 92.88it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 580/2000 [00:08<00:14, 95.34it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:14, 96.90it/s, train_loss=2.6693, val_loss=2.6780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:14, 96.90it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:08<00:24, 56.07it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 612/2000 [00:08<00:21, 64.99it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:08<00:18, 73.15it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:08<00:17, 80.00it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 645/2000 [00:08<00:15, 85.60it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 656/2000 [00:09<00:14, 89.85it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:09<00:14, 93.04it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 678/2000 [00:09<00:13, 95.35it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 689/2000 [00:09<00:13, 96.96it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:09<00:13, 98.34it/s, train_loss=2.5534, val_loss=2.5521]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:09<00:13, 98.34it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 711/2000 [00:09<00:22, 56.77it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 722/2000 [00:09<00:19, 65.40it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 733/2000 [00:10<00:17, 73.15it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 744/2000 [00:10<00:15, 79.72it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 755/2000 [00:10<00:14, 85.30it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 766/2000 [00:10<00:13, 89.54it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:10<00:13, 92.92it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 788/2000 [00:10<00:12, 95.23it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 798/2000 [00:10<00:12, 96.38it/s, train_loss=2.4379, val_loss=2.4421]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 798/2000 [00:11<00:12, 96.38it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 808/2000 [00:11<00:21, 55.14it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 818/2000 [00:11<00:19, 62.18it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 828/2000 [00:11<00:17, 68.91it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 838/2000 [00:11<00:15, 75.65it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 849/2000 [00:11<00:13, 82.22it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 859/2000 [00:11<00:13, 86.66it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 870/2000 [00:11<00:12, 90.58it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 881/2000 [00:11<00:11, 93.75it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:11<00:11, 96.07it/s, train_loss=2.3391, val_loss=2.3341]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:12<00:11, 96.07it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 902/2000 [00:12<00:19, 55.09it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 913/2000 [00:12<00:16, 64.03it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 923/2000 [00:12<00:15, 71.36it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 934/2000 [00:12<00:13, 78.41it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 945/2000 [00:12<00:12, 84.23it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 956/2000 [00:12<00:11, 88.59it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 967/2000 [00:12<00:11, 92.14it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 978/2000 [00:13<00:10, 94.40it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:13<00:10, 96.21it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:13<00:10, 97.50it/s, train_loss=2.2386, val_loss=2.2411]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:13<00:10, 97.50it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1010/2000 [00:13<00:17, 55.85it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:13<00:15, 64.85it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1031/2000 [00:13<00:13, 72.01it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1042/2000 [00:13<00:12, 79.22it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:14<00:11, 84.22it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1063/2000 [00:14<00:10, 88.90it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:14<00:10, 92.46it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1085/2000 [00:14<00:09, 95.26it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:14<00:09, 97.15it/s, train_loss=2.1366, val_loss=2.1424]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:14<00:09, 97.15it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1107/2000 [00:14<00:15, 56.78it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1118/2000 [00:15<00:13, 65.51it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1129/2000 [00:15<00:11, 73.44it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1139/2000 [00:15<00:10, 79.34it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▊    | 1150/2000 [00:15<00:09, 85.17it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1161/2000 [00:15<00:09, 89.77it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1172/2000 [00:15<00:08, 93.20it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1183/2000 [00:15<00:08, 95.67it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:15<00:08, 97.48it/s, train_loss=2.0463, val_loss=2.0513]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1194/2000 [00:16<00:08, 97.48it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1205/2000 [00:16<00:13, 57.05it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1215/2000 [00:16<00:12, 64.63it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1226/2000 [00:16<00:10, 72.66it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:16<00:09, 79.44it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:16<00:08, 84.99it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1259/2000 [00:16<00:08, 89.26it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1270/2000 [00:16<00:07, 92.90it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1281/2000 [00:16<00:07, 95.45it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:16<00:07, 97.22it/s, train_loss=1.9612, val_loss=1.9718]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:17<00:07, 97.22it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1303/2000 [00:17<00:12, 56.91it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1314/2000 [00:17<00:10, 65.63it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1325/2000 [00:17<00:09, 73.40it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1336/2000 [00:17<00:08, 80.30it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1347/2000 [00:17<00:07, 85.60it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1358/2000 [00:17<00:07, 89.97it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:18<00:06, 93.18it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1380/2000 [00:18<00:06, 95.52it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:18<00:06, 97.05it/s, train_loss=1.8705, val_loss=1.8829]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:18<00:06, 97.05it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:18<00:10, 56.87it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1413/2000 [00:18<00:08, 65.61it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1424/2000 [00:18<00:07, 73.53it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1435/2000 [00:18<00:07, 80.33it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1446/2000 [00:19<00:06, 85.75it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:19<00:06, 89.96it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1468/2000 [00:19<00:05, 93.26it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1479/2000 [00:19<00:05, 95.72it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1490/2000 [00:19<00:05, 97.65it/s, train_loss=1.7873, val_loss=1.8010]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1490/2000 [00:19<00:05, 97.65it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1501/2000 [00:19<00:08, 56.64it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1512/2000 [00:19<00:07, 65.21it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1523/2000 [00:20<00:06, 73.09it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1534/2000 [00:20<00:05, 79.99it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1545/2000 [00:20<00:05, 85.40it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1556/2000 [00:20<00:04, 89.56it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1567/2000 [00:20<00:04, 92.71it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1578/2000 [00:20<00:04, 95.34it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:20<00:04, 97.06it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:20<00:04, 97.88it/s, train_loss=1.6946, val_loss=1.7061]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:21<00:04, 97.88it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1611/2000 [00:21<00:06, 56.47it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1621/2000 [00:21<00:05, 64.19it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1632/2000 [00:21<00:05, 72.28it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1642/2000 [00:21<00:04, 78.15it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1653/2000 [00:21<00:04, 84.16it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:21<00:03, 88.79it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1675/2000 [00:21<00:03, 92.36it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1686/2000 [00:21<00:03, 94.81it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:22<00:03, 96.40it/s, train_loss=1.6134, val_loss=1.6196]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:22<00:03, 96.40it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1707/2000 [00:22<00:05, 55.20it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:22<00:04, 64.19it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:22<00:03, 71.45it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1739/2000 [00:22<00:03, 78.53it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1750/2000 [00:22<00:02, 84.18it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1761/2000 [00:22<00:02, 88.45it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:23<00:02, 91.61it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1783/2000 [00:23<00:02, 94.47it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:23<00:02, 96.58it/s, train_loss=1.5243, val_loss=1.5357]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:23<00:02, 96.58it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:23<00:03, 55.30it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1815/2000 [00:23<00:02, 64.14it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1826/2000 [00:23<00:02, 72.28it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1836/2000 [00:24<00:02, 78.42it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1847/2000 [00:24<00:01, 84.22it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1858/2000 [00:24<00:01, 88.85it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1868/2000 [00:24<00:01, 91.66it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:24<00:01, 94.24it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:24<00:01, 95.71it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:24<00:01, 97.52it/s, train_loss=1.4315, val_loss=1.4452]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1900/2000 [00:24<00:01, 97.52it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1910/2000 [00:25<00:01, 55.47it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1921/2000 [00:25<00:01, 64.49it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1932/2000 [00:25<00:00, 72.54it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1943/2000 [00:25<00:00, 79.54it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1953/2000 [00:25<00:00, 84.27it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1964/2000 [00:25<00:00, 88.88it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1975/2000 [00:25<00:00, 92.06it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1986/2000 [00:25<00:00, 94.67it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 77.16it/s, train_loss=1.3485, val_loss=1.3586]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:21<01:21, 81.63s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0001, beta2=0.98, avg_val_loss=1.4351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:03,  3.68it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.09it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.24it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 73.76it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:23, 82.22it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:22, 88.18it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 67/2000 [00:00<00:21, 92.04it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 78/2000 [00:01<00:20, 94.78it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:19, 96.52it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 97.86it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 97.86it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 110/2000 [00:01<00:34, 55.21it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:29, 64.41it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 132/2000 [00:01<00:25, 72.64it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:01<00:23, 79.30it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:21, 84.79it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:20, 89.26it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 176/2000 [00:02<00:19, 92.30it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 187/2000 [00:02<00:19, 95.01it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:18, 97.02it/s, train_loss=3.3265, val_loss=3.3285]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:18, 97.02it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 209/2000 [00:02<00:31, 56.80it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 220/2000 [00:02<00:27, 65.49it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:24, 73.41it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 242/2000 [00:03<00:21, 80.21it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 253/2000 [00:03<00:20, 85.64it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 264/2000 [00:03<00:19, 89.81it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 275/2000 [00:03<00:18, 93.16it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 286/2000 [00:03<00:17, 95.40it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:03<00:17, 97.09it/s, train_loss=3.0930, val_loss=3.0911]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:17, 97.09it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:29, 56.45it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:25, 65.14it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 330/2000 [00:04<00:22, 73.08it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:20, 79.75it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:04<00:19, 84.97it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:04<00:18, 89.35it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:04<00:17, 92.38it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:04<00:17, 94.74it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:04<00:16, 96.71it/s, train_loss=2.9143, val_loss=2.9160]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:16, 96.71it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 406/2000 [00:05<00:28, 55.52it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 416/2000 [00:05<00:24, 63.60it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 426/2000 [00:05<00:22, 70.99it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 437/2000 [00:05<00:19, 78.40it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 448/2000 [00:05<00:18, 84.04it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 458/2000 [00:05<00:17, 87.88it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 469/2000 [00:06<00:16, 91.38it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 479/2000 [00:06<00:16, 93.64it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:06<00:15, 95.80it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:06<00:15, 96.79it/s, train_loss=2.7641, val_loss=2.7713]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:06<00:15, 96.79it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 510/2000 [00:06<00:27, 54.72it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:06<00:23, 63.92it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:06<00:20, 71.37it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 542/2000 [00:07<00:18, 78.55it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 552/2000 [00:07<00:17, 83.59it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 562/2000 [00:07<00:16, 87.61it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 573/2000 [00:07<00:15, 91.33it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 583/2000 [00:07<00:15, 93.65it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 95.24it/s, train_loss=2.6284, val_loss=2.6316]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 95.24it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:07<00:25, 54.12it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 614/2000 [00:08<00:21, 63.43it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 624/2000 [00:08<00:19, 70.97it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 635/2000 [00:08<00:17, 78.34it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:08<00:16, 84.11it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:08<00:15, 88.49it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 668/2000 [00:08<00:14, 91.91it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 679/2000 [00:08<00:13, 94.39it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 690/2000 [00:08<00:13, 96.34it/s, train_loss=2.5037, val_loss=2.4997]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 690/2000 [00:09<00:13, 96.34it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 701/2000 [00:09<00:23, 56.37it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 712/2000 [00:09<00:19, 64.96it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 723/2000 [00:09<00:17, 72.76it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 734/2000 [00:09<00:15, 79.52it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 745/2000 [00:09<00:14, 85.03it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 756/2000 [00:09<00:13, 89.33it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 766/2000 [00:09<00:13, 91.32it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:09<00:12, 94.36it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 788/2000 [00:10<00:12, 96.24it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:12, 97.72it/s, train_loss=2.3700, val_loss=2.3840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 799/2000 [00:10<00:12, 97.72it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 809/2000 [00:10<00:21, 55.84it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 820/2000 [00:10<00:18, 64.72it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 831/2000 [00:10<00:16, 72.91it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 842/2000 [00:10<00:14, 79.87it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:10<00:13, 85.20it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:11<00:12, 89.48it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:11<00:12, 92.99it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:11<00:11, 95.45it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:11<00:11, 97.44it/s, train_loss=2.2466, val_loss=2.2541]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:11<00:11, 97.44it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 908/2000 [00:11<00:19, 56.73it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 919/2000 [00:11<00:16, 65.53it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 930/2000 [00:11<00:14, 73.36it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 941/2000 [00:12<00:13, 79.82it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 952/2000 [00:12<00:12, 85.10it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 963/2000 [00:12<00:11, 89.55it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 974/2000 [00:12<00:11, 92.54it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 985/2000 [00:12<00:10, 95.10it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:12<00:10, 96.36it/s, train_loss=2.1311, val_loss=2.1345]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:12<00:10, 96.36it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:12<00:17, 55.28it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1016/2000 [00:13<00:15, 64.42it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1027/2000 [00:13<00:13, 72.74it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:13<00:12, 79.58it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1049/2000 [00:13<00:11, 85.04it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1060/2000 [00:13<00:10, 89.24it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1071/2000 [00:13<00:10, 92.47it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:13<00:09, 94.30it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:13<00:09, 95.71it/s, train_loss=2.0064, val_loss=2.0133]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:14<00:09, 95.71it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:14<00:16, 54.76it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1112/2000 [00:14<00:13, 63.88it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1122/2000 [00:14<00:12, 71.33it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1132/2000 [00:14<00:11, 77.76it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1142/2000 [00:14<00:10, 83.07it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1153/2000 [00:14<00:09, 87.84it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1164/2000 [00:14<00:09, 91.60it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1174/2000 [00:14<00:08, 93.86it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1185/2000 [00:15<00:08, 95.88it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:15<00:08, 96.79it/s, train_loss=1.8864, val_loss=1.8945]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:15<00:08, 96.79it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1205/2000 [00:15<00:14, 54.14it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1215/2000 [00:15<00:12, 62.34it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1226/2000 [00:15<00:10, 71.08it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:15<00:09, 78.48it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1247/2000 [00:15<00:09, 83.65it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:16<00:08, 87.74it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1268/2000 [00:16<00:07, 91.81it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:16<00:07, 94.42it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:16<00:07, 95.83it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 97.27it/s, train_loss=1.7707, val_loss=1.7712]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 97.27it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:16<00:12, 54.70it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1320/2000 [00:16<00:10, 62.82it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1331/2000 [00:17<00:09, 71.22it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1342/2000 [00:17<00:08, 78.23it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1353/2000 [00:17<00:07, 84.01it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1364/2000 [00:17<00:07, 88.48it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1375/2000 [00:17<00:06, 92.00it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1386/2000 [00:17<00:06, 94.57it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:17<00:06, 96.40it/s, train_loss=1.6596, val_loss=1.6519]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:18<00:06, 96.40it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:18<00:10, 55.12it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1418/2000 [00:18<00:09, 63.91it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:18<00:07, 71.90it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1440/2000 [00:18<00:07, 78.67it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▎  | 1450/2000 [00:18<00:06, 83.69it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1461/2000 [00:18<00:06, 88.12it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1472/2000 [00:18<00:05, 91.58it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1482/2000 [00:18<00:05, 93.48it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1493/2000 [00:18<00:05, 95.93it/s, train_loss=1.5093, val_loss=1.5221]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1493/2000 [00:19<00:05, 95.93it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1503/2000 [00:19<00:09, 54.98it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1513/2000 [00:19<00:07, 63.03it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1524/2000 [00:19<00:06, 71.41it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1535/2000 [00:19<00:05, 78.51it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1546/2000 [00:19<00:05, 84.21it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1556/2000 [00:19<00:05, 87.84it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1567/2000 [00:19<00:04, 91.60it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1577/2000 [00:20<00:04, 93.83it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1587/2000 [00:20<00:04, 95.53it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1598/2000 [00:20<00:04, 97.15it/s, train_loss=1.4071, val_loss=1.3966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1598/2000 [00:20<00:04, 97.15it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1608/2000 [00:20<00:07, 54.98it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:20<00:05, 64.13it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1630/2000 [00:20<00:05, 72.39it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1641/2000 [00:20<00:04, 79.15it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1652/2000 [00:21<00:04, 84.72it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1663/2000 [00:21<00:03, 89.11it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:21<00:03, 92.69it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1685/2000 [00:21<00:03, 95.28it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:21<00:03, 97.16it/s, train_loss=1.2686, val_loss=1.2703]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:21<00:03, 97.16it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1707/2000 [00:21<00:05, 56.34it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:22<00:04, 65.03it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1729/2000 [00:22<00:03, 72.90it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1740/2000 [00:22<00:03, 79.45it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1751/2000 [00:22<00:02, 84.98it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:22<00:02, 89.01it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1773/2000 [00:22<00:02, 92.30it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1784/2000 [00:22<00:02, 94.58it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:22<00:02, 96.46it/s, train_loss=1.1487, val_loss=1.1446]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:23<00:02, 96.46it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1805/2000 [00:23<00:03, 55.74it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1816/2000 [00:23<00:02, 64.70it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1827/2000 [00:23<00:02, 72.67it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:23<00:02, 79.52it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1849/2000 [00:23<00:01, 84.99it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1860/2000 [00:23<00:01, 89.37it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1870/2000 [00:23<00:01, 91.78it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:23<00:01, 93.88it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:23<00:01, 95.90it/s, train_loss=1.0453, val_loss=1.0497]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:24<00:01, 95.90it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:24<00:01, 54.67it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:24<00:01, 62.62it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:24<00:01, 71.02it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1932/2000 [00:24<00:00, 77.50it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1942/2000 [00:24<00:00, 82.77it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1953/2000 [00:24<00:00, 87.88it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1964/2000 [00:25<00:00, 91.61it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1974/2000 [00:25<00:00, 93.34it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1984/2000 [00:25<00:00, 95.05it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.83it/s, train_loss=0.9587, val_loss=0.9673]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:07,  3.65it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:54, 36.38it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 58.64it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:27, 72.44it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 43/2000 [00:00<00:24, 80.01it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 53/2000 [00:00<00:22, 85.93it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 63/2000 [00:00<00:21, 90.02it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 73/2000 [00:00<00:20, 92.92it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 83/2000 [00:01<00:20, 92.73it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:20, 93.94it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:20, 93.94it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 103/2000 [00:01<00:36, 52.08it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 113/2000 [00:01<00:31, 60.86it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 123/2000 [00:01<00:27, 69.05it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 133/2000 [00:01<00:24, 76.11it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:01<00:22, 81.90it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:21, 86.07it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 163/2000 [00:02<00:20, 89.60it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 174/2000 [00:02<00:19, 92.97it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 184/2000 [00:02<00:19, 94.37it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 194/2000 [00:02<00:18, 95.59it/s, train_loss=3.3044, val_loss=3.3071]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 194/2000 [00:02<00:18, 95.59it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 204/2000 [00:02<00:33, 53.02it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 214/2000 [00:03<00:28, 61.64it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 224/2000 [00:03<00:25, 69.54it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 235/2000 [00:03<00:22, 77.20it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 245/2000 [00:03<00:21, 82.40it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:20, 86.78it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 265/2000 [00:03<00:19, 89.66it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 275/2000 [00:03<00:18, 91.96it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 285/2000 [00:03<00:18, 93.59it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:03<00:17, 95.04it/s, train_loss=3.0788, val_loss=3.0816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:04<00:17, 95.04it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 305/2000 [00:04<00:31, 53.57it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:04<00:27, 62.02it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:24, 69.74it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 335/2000 [00:04<00:21, 76.41it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 345/2000 [00:04<00:20, 81.86it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 355/2000 [00:04<00:19, 86.52it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 365/2000 [00:04<00:18, 89.99it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 375/2000 [00:04<00:17, 92.10it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:05<00:17, 94.32it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 395/2000 [00:05<00:16, 95.61it/s, train_loss=2.9058, val_loss=2.9060]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 395/2000 [00:05<00:16, 95.61it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 405/2000 [00:05<00:30, 52.58it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 415/2000 [00:05<00:25, 61.20it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 425/2000 [00:05<00:22, 68.96it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 435/2000 [00:05<00:20, 75.90it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 445/2000 [00:05<00:19, 81.41it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 455/2000 [00:06<00:17, 86.16it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 465/2000 [00:06<00:17, 89.66it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 475/2000 [00:06<00:16, 92.20it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 485/2000 [00:06<00:16, 94.17it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:06<00:15, 96.25it/s, train_loss=2.7659, val_loss=2.7615]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:06<00:15, 96.25it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:06<00:27, 54.58it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 516/2000 [00:06<00:23, 62.80it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 526/2000 [00:06<00:20, 70.45it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:07<00:19, 77.05it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 546/2000 [00:07<00:17, 82.45it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 557/2000 [00:07<00:16, 87.50it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 568/2000 [00:07<00:15, 91.13it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 579/2000 [00:07<00:15, 93.95it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 589/2000 [00:07<00:14, 95.59it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 599/2000 [00:07<00:14, 96.52it/s, train_loss=2.6414, val_loss=2.6380]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 599/2000 [00:08<00:14, 96.52it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 609/2000 [00:08<00:25, 54.49it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 619/2000 [00:08<00:21, 62.93it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 630/2000 [00:08<00:19, 71.39it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 641/2000 [00:08<00:17, 78.59it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 651/2000 [00:08<00:16, 83.20it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 661/2000 [00:08<00:15, 86.97it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 671/2000 [00:08<00:14, 90.35it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 682/2000 [00:08<00:14, 93.14it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 692/2000 [00:08<00:13, 94.59it/s, train_loss=2.5155, val_loss=2.5194]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 692/2000 [00:09<00:13, 94.59it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 702/2000 [00:09<00:24, 53.35it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 712/2000 [00:09<00:20, 61.75it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 722/2000 [00:09<00:18, 69.65it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 732/2000 [00:09<00:16, 76.25it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:09<00:15, 81.76it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:09<00:14, 86.39it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 762/2000 [00:09<00:13, 89.78it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:10<00:13, 92.07it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:10<00:12, 94.53it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 793/2000 [00:10<00:12, 96.02it/s, train_loss=2.4061, val_loss=2.4011]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 793/2000 [00:10<00:12, 96.02it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 803/2000 [00:10<00:22, 53.66it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:10<00:18, 63.15it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 825/2000 [00:10<00:16, 71.59it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 836/2000 [00:10<00:14, 78.72it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 847/2000 [00:11<00:13, 84.54it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 857/2000 [00:11<00:12, 88.35it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 868/2000 [00:11<00:12, 92.02it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 879/2000 [00:11<00:11, 94.47it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 890/2000 [00:11<00:11, 96.22it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 900/2000 [00:11<00:11, 97.22it/s, train_loss=2.2731, val_loss=2.2701]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 900/2000 [00:11<00:11, 97.22it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 910/2000 [00:11<00:19, 54.82it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:12<00:16, 63.95it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 932/2000 [00:12<00:14, 72.15it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 942/2000 [00:12<00:13, 78.28it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 953/2000 [00:12<00:12, 84.36it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 963/2000 [00:12<00:11, 88.08it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 974/2000 [00:12<00:11, 91.63it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:12<00:10, 93.82it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:12<00:10, 95.45it/s, train_loss=2.1574, val_loss=2.1563]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:13<00:10, 95.45it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1004/2000 [00:13<00:18, 53.96it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1014/2000 [00:13<00:15, 62.26it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1024/2000 [00:13<00:13, 69.78it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1034/2000 [00:13<00:12, 76.40it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1044/2000 [00:13<00:11, 81.98it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:13<00:10, 86.59it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1064/2000 [00:13<00:10, 90.11it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:13<00:09, 92.81it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1084/2000 [00:13<00:09, 94.80it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1094/2000 [00:14<00:09, 96.18it/s, train_loss=2.0482, val_loss=2.0423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1094/2000 [00:14<00:09, 96.18it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1104/2000 [00:14<00:16, 53.78it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1115/2000 [00:14<00:13, 63.24it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1125/2000 [00:14<00:12, 70.84it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1136/2000 [00:14<00:11, 78.27it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1146/2000 [00:14<00:10, 83.45it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1157/2000 [00:14<00:09, 88.16it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:15<00:09, 91.84it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1179/2000 [00:15<00:08, 94.31it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1189/2000 [00:15<00:08, 95.76it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:15<00:08, 97.21it/s, train_loss=1.8960, val_loss=1.8987]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:15<00:08, 97.21it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:15<00:14, 55.48it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1221/2000 [00:15<00:12, 64.59it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:16<00:10, 71.86it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1241/2000 [00:16<00:09, 78.20it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1251/2000 [00:16<00:08, 83.35it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1261/2000 [00:16<00:08, 87.52it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1271/2000 [00:16<00:08, 90.82it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1281/2000 [00:16<00:07, 93.35it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:16<00:07, 95.60it/s, train_loss=1.7599, val_loss=1.7649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:16<00:07, 95.60it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1302/2000 [00:16<00:12, 54.37it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1313/2000 [00:17<00:10, 63.63it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1323/2000 [00:17<00:09, 71.05it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1334/2000 [00:17<00:08, 78.34it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1345/2000 [00:17<00:07, 84.13it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1356/2000 [00:17<00:07, 88.85it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1367/2000 [00:17<00:06, 92.10it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1378/2000 [00:17<00:06, 95.00it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:17<00:06, 96.51it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:17<00:06, 97.38it/s, train_loss=1.6078, val_loss=1.6136]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:18<00:06, 97.38it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:18<00:10, 55.57it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1419/2000 [00:18<00:09, 63.52it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:18<00:08, 70.61it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1439/2000 [00:18<00:07, 77.26it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▎  | 1450/2000 [00:18<00:06, 83.39it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1461/2000 [00:18<00:06, 88.31it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1471/2000 [00:18<00:05, 91.16it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1481/2000 [00:19<00:05, 93.32it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:19<00:05, 95.72it/s, train_loss=1.4747, val_loss=1.4788]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:19<00:05, 95.72it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1502/2000 [00:19<00:09, 54.21it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1513/2000 [00:19<00:07, 63.37it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1523/2000 [00:19<00:06, 70.52it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1534/2000 [00:19<00:06, 77.61it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1544/2000 [00:19<00:05, 82.91it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:20<00:05, 86.67it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1565/2000 [00:20<00:04, 90.52it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1575/2000 [00:20<00:04, 92.85it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1585/2000 [00:20<00:04, 94.55it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:20<00:04, 95.98it/s, train_loss=1.3297, val_loss=1.3399]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:20<00:04, 95.98it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1605/2000 [00:20<00:07, 53.67it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1615/2000 [00:20<00:06, 62.20it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1625/2000 [00:21<00:05, 69.61it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:21<00:04, 76.52it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:21<00:04, 82.19it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1655/2000 [00:21<00:03, 86.57it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1666/2000 [00:21<00:03, 90.66it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1677/2000 [00:21<00:03, 93.66it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1687/2000 [00:21<00:03, 95.22it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:21<00:03, 96.29it/s, train_loss=1.2179, val_loss=1.2109]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:22<00:03, 96.29it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1707/2000 [00:22<00:05, 54.18it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1717/2000 [00:22<00:04, 62.60it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1727/2000 [00:22<00:03, 70.39it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:22<00:03, 77.06it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1747/2000 [00:22<00:03, 82.58it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1757/2000 [00:22<00:02, 86.52it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1767/2000 [00:22<00:02, 89.74it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1777/2000 [00:22<00:02, 92.42it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1788/2000 [00:22<00:02, 94.92it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:23<00:02, 96.34it/s, train_loss=1.1098, val_loss=1.1186]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:23<00:02, 96.34it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1808/2000 [00:23<00:03, 54.22it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:23<00:02, 62.48it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1828/2000 [00:23<00:02, 70.33it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:23<00:02, 77.08it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1848/2000 [00:23<00:01, 82.75it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1858/2000 [00:23<00:01, 87.15it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1869/2000 [00:24<00:01, 90.96it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:24<00:01, 93.08it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:24<00:01, 94.93it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:24<00:01, 96.09it/s, train_loss=1.0142, val_loss=1.0157]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:24<00:01, 96.09it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1909/2000 [00:24<00:01, 54.18it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1920/2000 [00:24<00:01, 63.48it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1930/2000 [00:24<00:00, 71.06it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1940/2000 [00:25<00:00, 77.40it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:25<00:00, 83.79it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:25<00:00, 88.29it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1972/2000 [00:25<00:00, 91.19it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1982/2000 [00:25<00:00, 93.40it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 77.95it/s, train_loss=0.9344, val_loss=0.9233]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:00,  3.70it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:53, 36.88it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 59.17it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 32/2000 [00:00<00:27, 71.66it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 42/2000 [00:00<00:24, 79.82it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 53/2000 [00:00<00:22, 86.54it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 63/2000 [00:00<00:21, 90.45it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 73/2000 [00:00<00:20, 92.93it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 83/2000 [00:01<00:20, 95.01it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:19, 96.15it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 93/2000 [00:01<00:19, 96.15it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 103/2000 [00:01<00:35, 53.44it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 114/2000 [00:01<00:29, 63.15it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 124/2000 [00:01<00:26, 70.74it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 135/2000 [00:01<00:23, 78.19it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 145/2000 [00:01<00:22, 83.40it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 155/2000 [00:02<00:21, 87.66it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 166/2000 [00:02<00:20, 91.35it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 176/2000 [00:02<00:19, 93.38it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 186/2000 [00:02<00:19, 95.17it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 196/2000 [00:02<00:18, 96.24it/s, train_loss=3.3352, val_loss=3.3363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 196/2000 [00:02<00:18, 96.24it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 206/2000 [00:02<00:33, 53.90it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:02<00:28, 62.35it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 226/2000 [00:03<00:25, 69.94it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 237/2000 [00:03<00:22, 77.25it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 247/2000 [00:03<00:21, 82.59it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 258/2000 [00:03<00:19, 87.60it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 268/2000 [00:03<00:19, 90.50it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 278/2000 [00:03<00:18, 92.80it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 289/2000 [00:03<00:17, 95.08it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:03<00:17, 96.68it/s, train_loss=3.1012, val_loss=3.0970]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 300/2000 [00:04<00:17, 96.68it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 310/2000 [00:04<00:30, 54.84it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 320/2000 [00:04<00:26, 63.00it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 330/2000 [00:04<00:23, 70.57it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 340/2000 [00:04<00:21, 76.85it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 350/2000 [00:04<00:20, 82.37it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 360/2000 [00:04<00:18, 86.80it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 370/2000 [00:04<00:18, 90.17it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 380/2000 [00:04<00:17, 92.67it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:16, 95.28it/s, train_loss=2.9252, val_loss=2.9247]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:16, 95.28it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:29, 53.91it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:05<00:25, 61.47it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:22, 68.97it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:05<00:20, 75.65it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 442/2000 [00:05<00:18, 82.30it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 452/2000 [00:05<00:17, 86.56it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 462/2000 [00:06<00:17, 89.51it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 472/2000 [00:06<00:16, 92.10it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 482/2000 [00:06<00:16, 94.06it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:15, 96.54it/s, train_loss=2.7781, val_loss=2.7774]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:15, 96.54it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 503/2000 [00:06<00:27, 54.02it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 514/2000 [00:06<00:23, 63.28it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 525/2000 [00:06<00:20, 71.53it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:07<00:18, 78.40it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 547/2000 [00:07<00:17, 84.10it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 558/2000 [00:07<00:16, 88.76it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 568/2000 [00:07<00:15, 91.27it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 578/2000 [00:07<00:15, 93.17it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 589/2000 [00:07<00:14, 95.54it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:07<00:14, 97.18it/s, train_loss=2.6381, val_loss=2.6468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 600/2000 [00:07<00:14, 97.18it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 610/2000 [00:08<00:25, 55.33it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 621/2000 [00:08<00:21, 64.42it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 632/2000 [00:08<00:18, 72.69it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 643/2000 [00:08<00:17, 79.45it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 654/2000 [00:08<00:15, 84.75it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 665/2000 [00:08<00:15, 88.89it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:08<00:14, 92.31it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 687/2000 [00:08<00:13, 94.54it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:08<00:13, 95.66it/s, train_loss=2.5112, val_loss=2.5110]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:09<00:13, 95.66it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 707/2000 [00:09<00:23, 55.06it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 717/2000 [00:09<00:20, 63.23it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 728/2000 [00:09<00:17, 71.61it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 738/2000 [00:09<00:16, 77.99it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 749/2000 [00:09<00:14, 83.82it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 760/2000 [00:09<00:14, 88.50it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 770/2000 [00:09<00:13, 90.99it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 780/2000 [00:10<00:13, 92.75it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 95.08it/s, train_loss=2.3751, val_loss=2.3793]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 95.08it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:21, 54.78it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:10<00:18, 62.83it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:10<00:16, 70.38it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:10<00:15, 77.73it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 843/2000 [00:10<00:13, 83.63it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 854/2000 [00:11<00:12, 88.37it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 865/2000 [00:11<00:12, 91.64it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:11<00:11, 93.83it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:11<00:11, 95.79it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:11<00:11, 97.04it/s, train_loss=2.2422, val_loss=2.2368]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:11<00:11, 97.04it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 907/2000 [00:11<00:19, 55.45it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 918/2000 [00:11<00:16, 64.39it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 928/2000 [00:12<00:14, 71.65it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 939/2000 [00:12<00:13, 78.65it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 949/2000 [00:12<00:12, 83.73it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 960/2000 [00:12<00:11, 88.27it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 970/2000 [00:12<00:11, 90.94it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 980/2000 [00:12<00:10, 93.25it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 990/2000 [00:12<00:10, 94.66it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:12<00:10, 95.98it/s, train_loss=2.0971, val_loss=2.0996]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:13<00:10, 95.98it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1010/2000 [00:13<00:18, 53.80it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:13<00:15, 63.16it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1031/2000 [00:13<00:13, 70.78it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:13<00:12, 77.19it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1051/2000 [00:13<00:11, 82.05it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1062/2000 [00:13<00:10, 87.09it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1073/2000 [00:13<00:10, 91.30it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:13<00:09, 93.53it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:13<00:09, 95.14it/s, train_loss=1.9408, val_loss=1.9466]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1093/2000 [00:14<00:09, 95.14it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1103/2000 [00:14<00:16, 53.44it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1113/2000 [00:14<00:14, 61.74it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1124/2000 [00:14<00:12, 70.55it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1134/2000 [00:14<00:11, 76.90it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:14<00:10, 83.20it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1155/2000 [00:14<00:09, 87.14it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1165/2000 [00:15<00:09, 90.29it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1176/2000 [00:15<00:08, 93.39it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1186/2000 [00:15<00:08, 94.99it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1196/2000 [00:15<00:08, 96.22it/s, train_loss=1.7720, val_loss=1.7864]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1196/2000 [00:15<00:08, 96.22it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1206/2000 [00:15<00:14, 54.20it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1216/2000 [00:15<00:12, 62.68it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1226/2000 [00:15<00:10, 70.46it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1236/2000 [00:15<00:09, 76.95it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1246/2000 [00:16<00:09, 82.20it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1256/2000 [00:16<00:08, 86.63it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1266/2000 [00:16<00:08, 90.16it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1276/2000 [00:16<00:07, 92.63it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1286/2000 [00:16<00:07, 93.94it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 96.26it/s, train_loss=1.6142, val_loss=1.6187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 96.26it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:16<00:12, 54.14it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:17<00:10, 62.61it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:17<00:09, 71.24it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1339/2000 [00:17<00:08, 78.50it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1349/2000 [00:17<00:07, 83.52it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1360/2000 [00:17<00:07, 88.23it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1370/2000 [00:17<00:06, 91.02it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1381/2000 [00:17<00:06, 94.12it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:17<00:06, 95.06it/s, train_loss=1.4552, val_loss=1.4603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:18<00:06, 95.06it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1401/2000 [00:18<00:11, 54.23it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:18<00:09, 62.64it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1422/2000 [00:18<00:08, 71.27it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1432/2000 [00:18<00:07, 77.66it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1442/2000 [00:18<00:06, 83.00it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1453/2000 [00:18<00:06, 88.16it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1464/2000 [00:18<00:05, 91.74it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1475/2000 [00:18<00:05, 94.31it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1486/2000 [00:19<00:05, 96.03it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 97.40it/s, train_loss=1.3124, val_loss=1.3199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 97.40it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:19<00:08, 55.52it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1518/2000 [00:19<00:07, 64.57it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1529/2000 [00:19<00:06, 72.60it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1540/2000 [00:19<00:05, 79.43it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1551/2000 [00:19<00:05, 84.94it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1562/2000 [00:20<00:04, 89.40it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1573/2000 [00:20<00:04, 92.42it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1583/2000 [00:20<00:04, 94.34it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.14it/s, train_loss=1.1813, val_loss=1.1914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.14it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:20<00:07, 55.00it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1615/2000 [00:20<00:06, 63.95it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1626/2000 [00:20<00:05, 72.00it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1636/2000 [00:21<00:04, 78.20it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1647/2000 [00:21<00:04, 84.12it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:21<00:03, 88.87it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:21<00:03, 92.41it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1680/2000 [00:21<00:03, 94.76it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1690/2000 [00:21<00:03, 96.07it/s, train_loss=1.0797, val_loss=1.0794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1690/2000 [00:21<00:03, 96.07it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:22<00:05, 55.87it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1711/2000 [00:22<00:04, 63.82it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1722/2000 [00:22<00:03, 72.11it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1733/2000 [00:22<00:03, 79.05it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1743/2000 [00:22<00:03, 84.04it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1753/2000 [00:22<00:02, 87.91it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1764/2000 [00:22<00:02, 91.73it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1775/2000 [00:22<00:02, 94.37it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1785/2000 [00:22<00:02, 95.41it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:22<00:02, 95.92it/s, train_loss=0.9875, val_loss=0.9990]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:23<00:02, 95.92it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1805/2000 [00:23<00:03, 53.83it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1816/2000 [00:23<00:02, 63.06it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1826/2000 [00:23<00:02, 70.32it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1836/2000 [00:23<00:02, 76.88it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:23<00:01, 82.50it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1857/2000 [00:23<00:01, 87.64it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1868/2000 [00:23<00:01, 91.36it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:24<00:01, 93.81it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1889/2000 [00:24<00:01, 95.48it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:24<00:01, 96.32it/s, train_loss=0.9170, val_loss=0.9328]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:24<00:01, 96.32it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1909/2000 [00:24<00:01, 54.10it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1920/2000 [00:24<00:01, 63.41it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1931/2000 [00:24<00:00, 71.74it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1942/2000 [00:24<00:00, 78.77it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1952/2000 [00:25<00:00, 83.70it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:25<00:00, 87.61it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1972/2000 [00:25<00:00, 90.76it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1983/2000 [00:25<00:00, 93.70it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.24it/s, train_loss=0.8497, val_loss=0.8598]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:38<00:00, 78.68s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR:  33%|███▎      | 1/3 [02:38<05:16, 158.25s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0001, beta2=0.999, avg_val_loss=0.9168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:13,  3.61it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:51, 38.55it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 58.21it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:27, 72.66it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:23, 81.61it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:22, 87.95it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:00<00:20, 92.26it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 77/2000 [00:01<00:20, 95.02it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 88/2000 [00:01<00:19, 97.22it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:19, 98.75it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:19, 98.75it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 110/2000 [00:01<00:33, 56.55it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:28, 65.35it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 132/2000 [00:01<00:25, 73.31it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:01<00:23, 79.79it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:21, 85.43it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:20, 89.93it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 176/2000 [00:02<00:19, 93.05it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 187/2000 [00:02<00:18, 95.60it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:18, 97.31it/s, train_loss=2.7062, val_loss=2.7085]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:18, 97.31it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 209/2000 [00:02<00:31, 57.20it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 220/2000 [00:02<00:27, 65.88it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:23, 73.75it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 242/2000 [00:03<00:21, 80.38it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 253/2000 [00:03<00:20, 85.93it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 264/2000 [00:03<00:19, 90.28it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 275/2000 [00:03<00:18, 93.40it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 286/2000 [00:03<00:17, 95.72it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:03<00:17, 97.51it/s, train_loss=2.2536, val_loss=2.2512]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:17, 97.51it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:29, 57.05it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:25, 65.74it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 330/2000 [00:04<00:22, 73.64it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:20, 80.28it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:04<00:19, 85.72it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:04<00:18, 90.13it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:04<00:17, 93.08it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:04<00:16, 95.52it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:04<00:16, 97.41it/s, train_loss=1.8927, val_loss=1.9093]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:16, 97.41it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 407/2000 [00:05<00:27, 57.10it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 418/2000 [00:05<00:24, 65.78it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 429/2000 [00:05<00:21, 73.67it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 440/2000 [00:05<00:19, 80.29it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:05<00:18, 85.72it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 462/2000 [00:05<00:17, 90.08it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:16, 93.49it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 484/2000 [00:06<00:15, 95.76it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:15, 97.63it/s, train_loss=1.6046, val_loss=1.6366]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:15, 97.63it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:06<00:26, 57.32it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 517/2000 [00:06<00:22, 65.99it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 528/2000 [00:06<00:19, 73.75it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 539/2000 [00:06<00:18, 80.31it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 549/2000 [00:07<00:17, 82.96it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 559/2000 [00:07<00:17, 84.43it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 569/2000 [00:07<00:16, 88.41it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 580/2000 [00:07<00:15, 91.84it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:07<00:14, 94.74it/s, train_loss=1.3904, val_loss=1.4000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:07<00:14, 94.74it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:07<00:25, 55.12it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 612/2000 [00:07<00:21, 64.13it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:08<00:19, 72.42it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:08<00:17, 79.47it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 645/2000 [00:08<00:15, 84.76it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 656/2000 [00:08<00:15, 89.45it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:08<00:14, 92.78it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 678/2000 [00:08<00:13, 95.61it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 689/2000 [00:08<00:13, 97.33it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:08<00:13, 98.78it/s, train_loss=1.1316, val_loss=1.1213]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 700/2000 [00:09<00:13, 98.78it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 711/2000 [00:09<00:22, 56.95it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 722/2000 [00:09<00:19, 65.66it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 733/2000 [00:09<00:17, 73.53it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 744/2000 [00:09<00:15, 80.23it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 755/2000 [00:09<00:14, 85.73it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 766/2000 [00:09<00:13, 89.94it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:09<00:13, 93.23it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 787/2000 [00:09<00:12, 94.49it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:10<00:12, 94.98it/s, train_loss=0.8761, val_loss=0.8794]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:10<00:12, 94.98it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 807/2000 [00:10<00:21, 54.23it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 817/2000 [00:10<00:18, 62.52it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 828/2000 [00:10<00:16, 71.14it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 839/2000 [00:10<00:14, 78.42it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▎     | 850/2000 [00:10<00:13, 84.35it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 861/2000 [00:10<00:12, 89.01it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 872/2000 [00:11<00:12, 92.49it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 883/2000 [00:11<00:11, 95.25it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:11<00:11, 97.16it/s, train_loss=0.6875, val_loss=0.6973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:11<00:11, 97.16it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:11<00:19, 56.06it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 915/2000 [00:11<00:17, 63.80it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 926/2000 [00:11<00:14, 71.95it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 937/2000 [00:11<00:13, 78.97it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:12<00:12, 84.69it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 959/2000 [00:12<00:11, 89.22it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 970/2000 [00:12<00:11, 92.61it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 981/2000 [00:12<00:10, 95.20it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 992/2000 [00:12<00:10, 97.31it/s, train_loss=0.6307, val_loss=0.6257]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 992/2000 [00:12<00:10, 97.31it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1003/2000 [00:12<00:17, 56.35it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1013/2000 [00:13<00:15, 63.99it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1024/2000 [00:13<00:13, 72.01it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1034/2000 [00:13<00:12, 78.24it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1045/2000 [00:13<00:11, 84.21it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1056/2000 [00:13<00:10, 88.87it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1067/2000 [00:13<00:10, 92.22it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1078/2000 [00:13<00:09, 94.88it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1089/2000 [00:13<00:09, 96.63it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:13<00:09, 97.63it/s, train_loss=0.6022, val_loss=0.6042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:14<00:09, 97.63it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:14<00:15, 56.29it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1121/2000 [00:14<00:13, 65.18it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1132/2000 [00:14<00:11, 73.19it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1143/2000 [00:14<00:10, 80.09it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1154/2000 [00:14<00:09, 85.82it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1165/2000 [00:14<00:09, 89.80it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1176/2000 [00:14<00:08, 93.16it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1187/2000 [00:14<00:08, 95.34it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 97.04it/s, train_loss=0.5480, val_loss=0.5568]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 97.04it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1209/2000 [00:15<00:13, 56.95it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1220/2000 [00:15<00:11, 65.56it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:15<00:10, 73.41it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1242/2000 [00:15<00:09, 80.07it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1253/2000 [00:15<00:08, 85.53it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1264/2000 [00:16<00:08, 90.08it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1275/2000 [00:16<00:07, 93.36it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1286/2000 [00:16<00:07, 95.44it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 97.26it/s, train_loss=0.5159, val_loss=0.5220]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 97.26it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1308/2000 [00:16<00:12, 57.09it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:16<00:10, 65.75it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1330/2000 [00:16<00:09, 73.66it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1341/2000 [00:17<00:08, 80.29it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:17<00:07, 85.70it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1363/2000 [00:17<00:07, 89.76it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1374/2000 [00:17<00:06, 93.04it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1384/2000 [00:17<00:06, 94.86it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1395/2000 [00:17<00:06, 96.55it/s, train_loss=0.5293, val_loss=0.5254]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1395/2000 [00:17<00:06, 96.55it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1405/2000 [00:17<00:10, 55.34it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1415/2000 [00:18<00:09, 63.36it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1426/2000 [00:18<00:07, 71.81it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:18<00:07, 78.78it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1448/2000 [00:18<00:06, 84.55it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1459/2000 [00:18<00:06, 88.81it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1470/2000 [00:18<00:05, 92.10it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1481/2000 [00:18<00:05, 94.59it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1491/2000 [00:18<00:05, 95.98it/s, train_loss=0.5025, val_loss=0.5088]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1491/2000 [00:19<00:05, 95.98it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1501/2000 [00:19<00:09, 54.15it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1511/2000 [00:19<00:07, 62.09it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1522/2000 [00:19<00:06, 70.73it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1533/2000 [00:19<00:05, 78.01it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1544/2000 [00:19<00:05, 83.68it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1555/2000 [00:19<00:05, 88.26it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1565/2000 [00:19<00:04, 91.17it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1576/2000 [00:19<00:04, 93.90it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1587/2000 [00:20<00:04, 96.02it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:20<00:04, 96.98it/s, train_loss=0.5081, val_loss=0.5103]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:20<00:04, 96.98it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1607/2000 [00:20<00:07, 55.15it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:20<00:05, 64.23it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:20<00:05, 72.36it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:20<00:04, 79.08it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1651/2000 [00:20<00:04, 84.80it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1662/2000 [00:21<00:03, 89.12it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:21<00:03, 92.24it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1684/2000 [00:21<00:03, 94.31it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:21<00:03, 96.30it/s, train_loss=0.4835, val_loss=0.4860]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:21<00:03, 96.30it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1705/2000 [00:21<00:05, 55.91it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1716/2000 [00:21<00:04, 64.84it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1726/2000 [00:21<00:03, 71.79it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1737/2000 [00:22<00:03, 78.59it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1748/2000 [00:22<00:02, 84.27it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1758/2000 [00:22<00:02, 88.19it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1769/2000 [00:22<00:02, 92.06it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1780/2000 [00:22<00:02, 94.70it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:22<00:02, 96.55it/s, train_loss=0.4796, val_loss=0.4782]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:23<00:02, 96.55it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1801/2000 [00:23<00:03, 54.91it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1811/2000 [00:23<00:03, 62.90it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1822/2000 [00:23<00:02, 71.24it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1833/2000 [00:23<00:02, 78.38it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1844/2000 [00:23<00:01, 84.11it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1855/2000 [00:23<00:01, 88.87it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1866/2000 [00:23<00:01, 92.43it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1877/2000 [00:23<00:01, 94.80it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1887/2000 [00:23<00:01, 96.18it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:23<00:01, 97.22it/s, train_loss=0.4719, val_loss=0.4866]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:24<00:01, 97.22it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:24<00:01, 55.41it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1918/2000 [00:24<00:01, 64.59it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1929/2000 [00:24<00:00, 72.58it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1940/2000 [00:24<00:00, 79.55it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:24<00:00, 85.10it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:24<00:00, 89.66it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1973/2000 [00:24<00:00, 92.66it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1984/2000 [00:25<00:00, 94.95it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 79.17it/s, train_loss=0.4784, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<08:48,  3.78it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:49, 39.80it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 61.03it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 74.12it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:23, 82.77it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:21, 88.94it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 67/2000 [00:00<00:20, 93.01it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 78/2000 [00:01<00:20, 95.46it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:19, 97.38it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.29it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.29it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 110/2000 [00:01<00:33, 55.72it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:29, 64.79it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 132/2000 [00:01<00:25, 72.78it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:01<00:23, 79.47it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:21, 85.16it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 164/2000 [00:02<00:20, 88.78it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 175/2000 [00:02<00:19, 92.27it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 185/2000 [00:02<00:19, 94.11it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 196/2000 [00:02<00:18, 96.35it/s, train_loss=2.7083, val_loss=2.7134]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 196/2000 [00:02<00:18, 96.35it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 206/2000 [00:02<00:32, 55.01it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 217/2000 [00:02<00:27, 64.06it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 227/2000 [00:03<00:24, 71.40it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 238/2000 [00:03<00:22, 78.62it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 249/2000 [00:03<00:20, 84.38it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 260/2000 [00:03<00:19, 88.96it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:03<00:18, 92.49it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 282/2000 [00:03<00:18, 94.95it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 293/2000 [00:03<00:17, 96.74it/s, train_loss=2.2897, val_loss=2.2973]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 293/2000 [00:04<00:17, 96.74it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 303/2000 [00:04<00:31, 54.51it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 313/2000 [00:04<00:26, 62.48it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 324/2000 [00:04<00:23, 70.98it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 335/2000 [00:04<00:21, 78.02it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 346/2000 [00:04<00:19, 83.80it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 357/2000 [00:04<00:18, 88.52it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 367/2000 [00:04<00:17, 91.40it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 378/2000 [00:04<00:17, 94.19it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 389/2000 [00:04<00:16, 95.99it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 400/2000 [00:05<00:16, 97.56it/s, train_loss=1.9629, val_loss=1.9545]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 400/2000 [00:05<00:16, 97.56it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 410/2000 [00:05<00:28, 55.82it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:24, 64.72it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 432/2000 [00:05<00:21, 72.63it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 443/2000 [00:05<00:19, 79.42it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 454/2000 [00:05<00:18, 85.12it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 465/2000 [00:05<00:17, 89.58it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 476/2000 [00:06<00:16, 92.69it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 487/2000 [00:06<00:15, 95.19it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:06<00:15, 96.47it/s, train_loss=1.6491, val_loss=1.6275]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:06<00:15, 96.47it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 507/2000 [00:06<00:27, 55.28it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 517/2000 [00:06<00:23, 63.51it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 528/2000 [00:06<00:20, 71.84it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 539/2000 [00:06<00:18, 78.90it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 549/2000 [00:07<00:17, 83.64it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 560/2000 [00:07<00:16, 88.43it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:07<00:15, 91.91it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 582/2000 [00:07<00:14, 94.73it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 96.59it/s, train_loss=1.2470, val_loss=1.2423]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 96.59it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:07<00:25, 55.59it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 613/2000 [00:07<00:21, 63.61it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 624/2000 [00:08<00:19, 71.95it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 635/2000 [00:08<00:17, 78.94it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:08<00:16, 84.61it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:08<00:15, 89.11it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:08<00:14, 91.81it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 677/2000 [00:08<00:14, 93.94it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:08<00:13, 96.35it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:08<00:13, 97.70it/s, train_loss=0.9215, val_loss=0.9170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:09<00:13, 97.70it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 709/2000 [00:09<00:23, 56.03it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 720/2000 [00:09<00:19, 65.14it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 731/2000 [00:09<00:17, 73.09it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:09<00:15, 79.82it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 753/2000 [00:09<00:14, 85.26it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 764/2000 [00:09<00:13, 89.61it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 775/2000 [00:09<00:13, 92.82it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 786/2000 [00:09<00:12, 95.17it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:10<00:12, 96.94it/s, train_loss=0.7245, val_loss=0.7215]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:10<00:12, 96.94it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 807/2000 [00:10<00:21, 56.11it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 818/2000 [00:10<00:18, 65.12it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 829/2000 [00:10<00:16, 73.10it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 840/2000 [00:10<00:14, 80.05it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 851/2000 [00:10<00:13, 85.66it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:10<00:12, 89.91it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 873/2000 [00:11<00:12, 92.95it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 884/2000 [00:11<00:11, 95.57it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:11, 97.12it/s, train_loss=0.6196, val_loss=0.6199]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:11, 97.12it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 906/2000 [00:11<00:19, 56.78it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 917/2000 [00:11<00:16, 65.27it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 927/2000 [00:11<00:14, 71.73it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 937/2000 [00:12<00:13, 77.82it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 947/2000 [00:12<00:12, 83.05it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:12<00:11, 87.38it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 968/2000 [00:12<00:11, 91.15it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:12<00:10, 93.80it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:12<00:10, 95.38it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:12<00:10, 96.48it/s, train_loss=0.5833, val_loss=0.5905]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:12<00:10, 96.48it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1009/2000 [00:13<00:18, 54.65it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:13<00:15, 64.00it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1031/2000 [00:13<00:13, 72.32it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1042/2000 [00:13<00:12, 79.40it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:13<00:11, 84.23it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1062/2000 [00:13<00:10, 87.96it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1072/2000 [00:13<00:10, 91.13it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1082/2000 [00:13<00:09, 93.44it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:13<00:09, 95.14it/s, train_loss=0.5600, val_loss=0.5551]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:14<00:09, 95.14it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1102/2000 [00:14<00:16, 53.18it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1112/2000 [00:14<00:14, 61.54it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1123/2000 [00:14<00:12, 70.26it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1134/2000 [00:14<00:11, 77.57it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:14<00:10, 83.48it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1155/2000 [00:14<00:09, 86.37it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1165/2000 [00:14<00:09, 89.64it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1175/2000 [00:14<00:08, 92.13it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1185/2000 [00:15<00:08, 94.14it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1196/2000 [00:15<00:08, 96.18it/s, train_loss=0.5374, val_loss=0.5313]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1196/2000 [00:15<00:08, 96.18it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1206/2000 [00:15<00:14, 54.53it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1217/2000 [00:15<00:12, 63.81it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1227/2000 [00:15<00:10, 71.16it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1238/2000 [00:15<00:09, 78.32it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:15<00:09, 83.47it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:16<00:08, 87.60it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1269/2000 [00:16<00:07, 91.54it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1279/2000 [00:16<00:07, 93.83it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:16<00:07, 95.26it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 96.77it/s, train_loss=0.5286, val_loss=0.5361]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 96.77it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:16<00:12, 54.61it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:16<00:10, 63.74it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1332/2000 [00:17<00:09, 71.94it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1342/2000 [00:17<00:08, 78.10it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1353/2000 [00:17<00:07, 84.13it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1364/2000 [00:17<00:07, 88.56it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1374/2000 [00:17<00:06, 91.51it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1385/2000 [00:17<00:06, 94.33it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:17<00:06, 96.60it/s, train_loss=0.4956, val_loss=0.5065]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:18<00:06, 96.60it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1406/2000 [00:18<00:10, 55.80it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1416/2000 [00:18<00:09, 63.83it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1426/2000 [00:18<00:08, 71.32it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:18<00:07, 78.45it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:18<00:06, 83.56it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:18<00:06, 87.70it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:18<00:05, 90.50it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:18<00:05, 92.78it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1488/2000 [00:18<00:05, 94.98it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 96.30it/s, train_loss=0.4760, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 96.30it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:19<00:08, 54.82it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:19<00:07, 64.21it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1530/2000 [00:19<00:06, 72.47it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1541/2000 [00:19<00:05, 79.27it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1552/2000 [00:19<00:05, 84.94it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1562/2000 [00:19<00:04, 88.71it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1572/2000 [00:20<00:04, 91.56it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1583/2000 [00:20<00:04, 94.38it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.44it/s, train_loss=0.4961, val_loss=0.4942]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.44it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:20<00:07, 55.21it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1614/2000 [00:20<00:06, 63.10it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1624/2000 [00:20<00:05, 70.51it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1635/2000 [00:20<00:04, 78.00it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:21<00:04, 83.19it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1655/2000 [00:21<00:03, 87.37it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1666/2000 [00:21<00:03, 91.12it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1676/2000 [00:21<00:03, 93.40it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1687/2000 [00:21<00:03, 95.33it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:21<00:03, 96.90it/s, train_loss=0.4752, val_loss=0.4729]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:21<00:03, 96.90it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1708/2000 [00:21<00:05, 55.30it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1719/2000 [00:22<00:04, 64.37it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1730/2000 [00:22<00:03, 72.56it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1741/2000 [00:22<00:03, 79.38it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1751/2000 [00:22<00:02, 84.29it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:22<00:02, 88.70it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:22<00:02, 91.63it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1783/2000 [00:22<00:02, 94.28it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:22<00:02, 96.40it/s, train_loss=0.4729, val_loss=0.4888]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:23<00:02, 96.40it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:23<00:03, 54.23it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1814/2000 [00:23<00:02, 62.30it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1825/2000 [00:23<00:02, 70.70it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1836/2000 [00:23<00:02, 77.99it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:23<00:01, 83.19it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1857/2000 [00:23<00:01, 87.94it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1868/2000 [00:23<00:01, 91.32it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:23<00:01, 93.93it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1890/2000 [00:24<00:01, 96.16it/s, train_loss=0.4758, val_loss=0.4777]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1890/2000 [00:24<00:01, 96.16it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:24<00:01, 54.98it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:24<00:01, 62.84it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:24<00:01, 70.97it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:24<00:00, 77.96it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1944/2000 [00:24<00:00, 83.99it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1954/2000 [00:24<00:00, 87.79it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1964/2000 [00:25<00:00, 90.54it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1975/2000 [00:25<00:00, 93.57it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1985/2000 [00:25<00:00, 95.25it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.71it/s, train_loss=0.4843, val_loss=0.4694]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<08:57,  3.72it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.36it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 59.03it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 33/2000 [00:00<00:26, 72.91it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:23, 82.16it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 55/2000 [00:00<00:22, 88.31it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 65/2000 [00:00<00:21, 91.50it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 75/2000 [00:01<00:20, 93.88it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 86/2000 [00:01<00:19, 96.14it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 97/2000 [00:01<00:19, 97.74it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 97/2000 [00:01<00:19, 97.74it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 107/2000 [00:01<00:34, 54.74it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 118/2000 [00:01<00:29, 64.02it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 128/2000 [00:01<00:26, 71.34it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 139/2000 [00:01<00:23, 78.56it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 149/2000 [00:02<00:22, 83.39it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 160/2000 [00:02<00:20, 88.17it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:19, 91.69it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 182/2000 [00:02<00:19, 94.18it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:18, 95.64it/s, train_loss=2.7278, val_loss=2.7303]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:18, 95.64it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 202/2000 [00:02<00:33, 54.20it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 212/2000 [00:02<00:28, 62.38it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 223/2000 [00:03<00:25, 70.84it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:03<00:22, 77.18it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:21, 82.44it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 254/2000 [00:03<00:19, 87.44it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 265/2000 [00:03<00:19, 91.25it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 275/2000 [00:03<00:18, 93.60it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 285/2000 [00:03<00:18, 95.12it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:03<00:17, 96.79it/s, train_loss=2.2635, val_loss=2.2649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:04<00:17, 96.79it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:04<00:30, 55.22it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 317/2000 [00:04<00:26, 64.37it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 328/2000 [00:04<00:23, 72.52it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 339/2000 [00:04<00:20, 79.22it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 349/2000 [00:04<00:19, 84.17it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 360/2000 [00:04<00:18, 88.83it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:04<00:17, 92.38it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 382/2000 [00:04<00:17, 94.70it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:05<00:16, 96.38it/s, train_loss=1.8969, val_loss=1.8940]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:05<00:16, 96.38it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 403/2000 [00:05<00:28, 55.12it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 413/2000 [00:05<00:25, 63.16it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 424/2000 [00:05<00:22, 71.43it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 435/2000 [00:05<00:19, 78.32it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 446/2000 [00:05<00:18, 84.07it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 457/2000 [00:05<00:17, 88.43it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 468/2000 [00:06<00:16, 91.80it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 479/2000 [00:06<00:16, 94.59it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:06<00:15, 96.46it/s, train_loss=1.5714, val_loss=1.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:06<00:15, 96.46it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:06<00:27, 55.06it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:06<00:23, 62.79it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 522/2000 [00:06<00:20, 70.85it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 532/2000 [00:06<00:19, 77.18it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 543/2000 [00:07<00:17, 83.02it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 553/2000 [00:07<00:16, 87.17it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 563/2000 [00:07<00:15, 90.43it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 574/2000 [00:07<00:15, 93.32it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:07<00:14, 95.23it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:07<00:14, 96.53it/s, train_loss=1.1817, val_loss=1.1949]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:07<00:14, 96.53it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 605/2000 [00:07<00:25, 54.37it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 615/2000 [00:08<00:22, 62.36it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 625/2000 [00:08<00:19, 69.70it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 636/2000 [00:08<00:17, 77.19it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:08<00:16, 82.51it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 656/2000 [00:08<00:15, 86.98it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:08<00:14, 90.46it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 677/2000 [00:08<00:14, 93.76it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 687/2000 [00:08<00:13, 95.15it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:08<00:13, 95.68it/s, train_loss=0.8908, val_loss=0.8779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:09<00:13, 95.68it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 707/2000 [00:09<00:23, 53.91it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 715/2000 [00:09<00:23, 55.64it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 723/2000 [00:09<00:21, 60.40it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 733/2000 [00:09<00:18, 67.77it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 742/2000 [00:09<00:17, 72.89it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:09<00:15, 78.75it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 761/2000 [00:09<00:16, 75.19it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 771/2000 [00:10<00:15, 80.21it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:10<00:14, 84.66it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:13, 87.81it/s, train_loss=0.7101, val_loss=0.7064]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:13, 87.81it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:24, 49.50it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:10<00:20, 57.44it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 820/2000 [00:10<00:18, 63.71it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 829/2000 [00:10<00:17, 68.53it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 839/2000 [00:11<00:15, 74.40it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 849/2000 [00:11<00:14, 79.52it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 859/2000 [00:11<00:13, 83.08it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 868/2000 [00:11<00:13, 83.24it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 877/2000 [00:11<00:13, 83.70it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:11<00:13, 84.44it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:11<00:12, 85.27it/s, train_loss=0.6231, val_loss=0.6166]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:12<00:12, 85.27it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 904/2000 [00:12<00:22, 47.82it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 913/2000 [00:12<00:19, 55.14it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 922/2000 [00:12<00:17, 61.51it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 931/2000 [00:12<00:15, 67.41it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 940/2000 [00:12<00:14, 72.85it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 949/2000 [00:12<00:13, 76.88it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 958/2000 [00:12<00:13, 79.80it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 967/2000 [00:12<00:12, 81.06it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 976/2000 [00:12<00:12, 82.38it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 985/2000 [00:13<00:12, 83.79it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:13<00:11, 85.15it/s, train_loss=0.5840, val_loss=0.5798]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 994/2000 [00:13<00:11, 85.15it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1003/2000 [00:13<00:20, 48.54it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1012/2000 [00:13<00:17, 55.62it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:13<00:15, 61.98it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:13<00:14, 67.06it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1038/2000 [00:13<00:13, 70.01it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1046/2000 [00:14<00:13, 72.29it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:14<00:12, 73.14it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1062/2000 [00:14<00:12, 74.35it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1071/2000 [00:14<00:12, 76.40it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1079/2000 [00:14<00:11, 77.20it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1087/2000 [00:14<00:11, 77.55it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:14<00:11, 78.27it/s, train_loss=0.5399, val_loss=0.5502]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:15<00:11, 78.27it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1104/2000 [00:15<00:20, 44.02it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1113/2000 [00:15<00:17, 51.38it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1121/2000 [00:15<00:15, 57.09it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1129/2000 [00:15<00:14, 62.00it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:15<00:12, 66.90it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1147/2000 [00:15<00:12, 70.67it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1156/2000 [00:15<00:11, 73.91it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1165/2000 [00:15<00:10, 76.79it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1174/2000 [00:15<00:10, 78.72it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1183/2000 [00:16<00:10, 80.26it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:16<00:09, 81.64it/s, train_loss=0.5257, val_loss=0.5410]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1192/2000 [00:16<00:09, 81.64it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:16<00:17, 46.72it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:16<00:14, 54.38it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1219/2000 [00:16<00:12, 61.57it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1228/2000 [00:16<00:11, 67.75it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:16<00:10, 73.02it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1246/2000 [00:17<00:09, 76.25it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:17<00:09, 78.95it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1264/2000 [00:17<00:09, 81.01it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1273/2000 [00:17<00:08, 82.79it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1282/2000 [00:17<00:08, 84.50it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1291/2000 [00:17<00:08, 85.63it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:17<00:08, 86.30it/s, train_loss=0.5215, val_loss=0.5326]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:17<00:08, 86.30it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:18<00:14, 48.24it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:18<00:12, 55.85it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1327/2000 [00:18<00:10, 62.90it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1335/2000 [00:18<00:10, 66.43it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1344/2000 [00:18<00:09, 70.58it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1353/2000 [00:18<00:08, 73.86it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1362/2000 [00:18<00:08, 77.11it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1371/2000 [00:18<00:07, 79.90it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1380/2000 [00:18<00:08, 74.07it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1388/2000 [00:19<00:09, 65.18it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1395/2000 [00:19<00:10, 58.43it/s, train_loss=0.5028, val_loss=0.5269]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1395/2000 [00:19<00:10, 58.43it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:19<00:16, 35.63it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:19<00:13, 44.25it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1420/2000 [00:19<00:10, 52.77it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1429/2000 [00:19<00:09, 60.38it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1438/2000 [00:20<00:08, 66.73it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:20<00:07, 71.79it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1456/2000 [00:20<00:07, 76.12it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1465/2000 [00:20<00:06, 79.62it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1474/2000 [00:20<00:06, 81.91it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1483/2000 [00:20<00:06, 83.52it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:20<00:06, 84.65it/s, train_loss=0.4963, val_loss=0.5074]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:21<00:06, 84.65it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1501/2000 [00:21<00:11, 42.96it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1510/2000 [00:21<00:09, 50.32it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:21<00:08, 57.95it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1528/2000 [00:21<00:07, 64.55it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1537/2000 [00:21<00:06, 70.01it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1546/2000 [00:21<00:06, 74.66it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1555/2000 [00:21<00:05, 78.12it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1564/2000 [00:21<00:05, 80.77it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1573/2000 [00:21<00:05, 82.62it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1582/2000 [00:22<00:04, 84.52it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:22<00:04, 85.46it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:22<00:04, 86.17it/s, train_loss=0.4918, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:22<00:04, 86.17it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:22<00:08, 47.84it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:22<00:06, 55.51it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1627/2000 [00:22<00:05, 62.38it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1636/2000 [00:22<00:05, 68.45it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:23<00:04, 73.44it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:23<00:04, 77.36it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1663/2000 [00:23<00:04, 80.41it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1672/2000 [00:23<00:04, 78.51it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1681/2000 [00:23<00:03, 80.03it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1690/2000 [00:23<00:03, 82.31it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:23<00:03, 83.89it/s, train_loss=0.4833, val_loss=0.4797]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:23<00:03, 83.89it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1708/2000 [00:24<00:06, 48.06it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1717/2000 [00:24<00:05, 55.79it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1726/2000 [00:24<00:04, 62.00it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1735/2000 [00:24<00:03, 67.75it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:24<00:03, 72.66it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1753/2000 [00:24<00:03, 76.67it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:24<00:02, 79.38it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1771/2000 [00:24<00:02, 81.34it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1780/2000 [00:24<00:02, 82.92it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1789/2000 [00:24<00:02, 84.54it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:25<00:02, 85.49it/s, train_loss=0.4847, val_loss=0.4992]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:25<00:02, 85.49it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1807/2000 [00:25<00:03, 48.53it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1816/2000 [00:25<00:03, 56.10it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1825/2000 [00:25<00:02, 63.13it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:25<00:02, 69.04it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1843/2000 [00:25<00:02, 73.85it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1852/2000 [00:25<00:01, 77.73it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1861/2000 [00:26<00:01, 79.57it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1870/2000 [00:26<00:01, 81.84it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:26<00:01, 83.48it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:26<00:01, 85.12it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:26<00:01, 86.09it/s, train_loss=0.4730, val_loss=0.4824]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:26<00:01, 86.09it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1906/2000 [00:26<00:01, 48.52it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1915/2000 [00:26<00:01, 56.12it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1924/2000 [00:27<00:01, 62.98it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:27<00:00, 68.86it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1942/2000 [00:27<00:00, 73.87it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:27<00:00, 77.37it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1960/2000 [00:27<00:00, 80.13it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1969/2000 [00:27<00:00, 82.28it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1978/2000 [00:27<00:00, 83.86it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:27<00:00, 84.55it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:27<00:00, 71.64it/s, train_loss=0.4733, val_loss=0.4877]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:18<01:18, 78.62s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0005, beta2=0.98, avg_val_loss=0.4808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:06,  3.30it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:59, 33.64it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:37, 53.37it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:29, 66.94it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 75.70it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:23, 81.63it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:00<00:22, 86.24it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:21, 88.41it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:21, 91.05it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:21, 90.85it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:21, 90.85it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:36, 51.35it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:31, 59.85it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:27, 67.26it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:01<00:25, 73.68it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:23, 79.22it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 150/2000 [00:02<00:23, 79.99it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 159/2000 [00:02<00:23, 78.67it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 168/2000 [00:02<00:23, 79.34it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 178/2000 [00:02<00:21, 83.21it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 188/2000 [00:02<00:20, 86.77it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:20, 89.23it/s, train_loss=2.6944, val_loss=2.6968]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:20, 89.23it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 208/2000 [00:03<00:33, 52.87it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 218/2000 [00:03<00:29, 61.06it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 228/2000 [00:03<00:25, 68.71it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 238/2000 [00:03<00:23, 74.89it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 248/2000 [00:03<00:21, 80.75it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 258/2000 [00:03<00:20, 85.24it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 268/2000 [00:03<00:19, 88.01it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 278/2000 [00:03<00:19, 89.99it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 288/2000 [00:03<00:18, 91.88it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:18, 93.37it/s, train_loss=2.2391, val_loss=2.2353]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:18, 93.37it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:31, 54.44it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:04<00:26, 62.88it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 328/2000 [00:04<00:23, 70.31it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 338/2000 [00:04<00:21, 76.80it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 348/2000 [00:04<00:20, 81.87it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 358/2000 [00:04<00:19, 86.11it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 368/2000 [00:04<00:18, 89.34it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 378/2000 [00:05<00:17, 91.73it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 388/2000 [00:05<00:17, 93.51it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 398/2000 [00:05<00:16, 94.58it/s, train_loss=1.8327, val_loss=1.8468]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 398/2000 [00:05<00:16, 94.58it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 408/2000 [00:05<00:29, 54.31it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 418/2000 [00:05<00:25, 62.59it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 428/2000 [00:05<00:22, 70.08it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 438/2000 [00:05<00:20, 75.88it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 448/2000 [00:06<00:19, 81.32it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 458/2000 [00:06<00:18, 85.15it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 468/2000 [00:06<00:17, 87.87it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 478/2000 [00:06<00:16, 90.41it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 488/2000 [00:06<00:16, 92.50it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 498/2000 [00:06<00:15, 94.10it/s, train_loss=1.4914, val_loss=1.5260]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 498/2000 [00:06<00:15, 94.10it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 508/2000 [00:06<00:27, 54.50it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 518/2000 [00:07<00:23, 62.97it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 528/2000 [00:07<00:20, 70.42it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 538/2000 [00:07<00:18, 76.96it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 548/2000 [00:07<00:17, 82.41it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 558/2000 [00:07<00:16, 86.57it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 568/2000 [00:07<00:16, 89.16it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 578/2000 [00:07<00:15, 91.41it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 588/2000 [00:07<00:15, 93.50it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:07<00:14, 94.67it/s, train_loss=1.0789, val_loss=1.0811]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 598/2000 [00:08<00:14, 94.67it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 608/2000 [00:08<00:25, 54.67it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 618/2000 [00:08<00:22, 62.64it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 627/2000 [00:08<00:20, 68.41it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 637/2000 [00:08<00:18, 74.69it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 647/2000 [00:08<00:16, 80.04it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:08<00:15, 84.33it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:08<00:15, 87.61it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 677/2000 [00:08<00:14, 90.42it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 687/2000 [00:09<00:14, 91.50it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:09<00:14, 92.65it/s, train_loss=0.7975, val_loss=0.7808]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 697/2000 [00:09<00:14, 92.65it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 707/2000 [00:09<00:23, 54.13it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 717/2000 [00:09<00:20, 62.18it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 727/2000 [00:09<00:18, 69.61it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 737/2000 [00:09<00:16, 75.49it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 747/2000 [00:09<00:15, 79.97it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 757/2000 [00:10<00:14, 84.27it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 767/2000 [00:10<00:14, 87.73it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:10<00:13, 90.04it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 787/2000 [00:10<00:13, 91.61it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:10<00:13, 92.35it/s, train_loss=0.6640, val_loss=0.6708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 797/2000 [00:10<00:13, 92.35it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 807/2000 [00:10<00:22, 53.47it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 817/2000 [00:10<00:19, 61.62it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 827/2000 [00:11<00:17, 68.53it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 837/2000 [00:11<00:15, 74.99it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 847/2000 [00:11<00:14, 79.66it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 857/2000 [00:11<00:13, 84.48it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 867/2000 [00:11<00:12, 88.17it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 877/2000 [00:11<00:12, 90.73it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 887/2000 [00:11<00:12, 91.71it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:11<00:11, 92.22it/s, train_loss=0.5888, val_loss=0.5975]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:12<00:11, 92.22it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 907/2000 [00:12<00:20, 53.63it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 917/2000 [00:12<00:17, 62.03it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 927/2000 [00:12<00:15, 69.65it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 937/2000 [00:12<00:14, 75.74it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 947/2000 [00:12<00:12, 81.05it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:12<00:12, 85.12it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 967/2000 [00:12<00:11, 87.89it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 977/2000 [00:12<00:11, 90.12it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 987/2000 [00:13<00:11, 91.72it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:13<00:10, 92.83it/s, train_loss=0.5655, val_loss=0.5595]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:13<00:10, 92.83it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1007/2000 [00:13<00:18, 54.18it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1017/2000 [00:13<00:15, 62.56it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1027/2000 [00:13<00:13, 70.28it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1037/2000 [00:13<00:12, 76.20it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1047/2000 [00:13<00:11, 80.95it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1057/2000 [00:13<00:11, 84.42it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1067/2000 [00:14<00:10, 87.51it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1077/2000 [00:14<00:10, 90.14it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1087/2000 [00:14<00:09, 92.13it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:14<00:09, 93.80it/s, train_loss=0.5449, val_loss=0.5475]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1097/2000 [00:14<00:09, 93.80it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1107/2000 [00:14<00:16, 53.74it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1117/2000 [00:14<00:14, 62.02it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1127/2000 [00:14<00:12, 69.28it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1137/2000 [00:15<00:11, 75.63it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1147/2000 [00:15<00:10, 80.72it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1157/2000 [00:15<00:09, 84.71it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1167/2000 [00:15<00:09, 87.85it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1177/2000 [00:15<00:09, 90.29it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1187/2000 [00:15<00:08, 92.19it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:15<00:08, 93.79it/s, train_loss=0.5151, val_loss=0.5232]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:16<00:08, 93.79it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1207/2000 [00:16<00:14, 54.29it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1217/2000 [00:16<00:12, 62.52it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1227/2000 [00:16<00:11, 70.08it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:16<00:10, 76.25it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1247/2000 [00:16<00:09, 81.09it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:16<00:08, 85.48it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:16<00:08, 88.13it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1277/2000 [00:16<00:07, 90.45it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1287/2000 [00:16<00:07, 92.49it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:17<00:07, 93.38it/s, train_loss=0.4945, val_loss=0.4952]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:17<00:07, 93.38it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:17<00:12, 54.08it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:17<00:10, 62.43it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1327/2000 [00:17<00:09, 70.00it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1337/2000 [00:17<00:08, 76.46it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1347/2000 [00:17<00:07, 82.13it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:17<00:07, 85.81it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1367/2000 [00:17<00:07, 89.06it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1377/2000 [00:18<00:06, 90.81it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1387/2000 [00:18<00:06, 92.90it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:18<00:06, 94.16it/s, train_loss=0.4981, val_loss=0.4978]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1397/2000 [00:18<00:06, 94.16it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1407/2000 [00:18<00:10, 54.58it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:18<00:09, 63.10it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1427/2000 [00:18<00:08, 70.15it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1437/2000 [00:18<00:07, 76.71it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1447/2000 [00:19<00:06, 82.09it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:19<00:06, 85.32it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:19<00:06, 88.51it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:19<00:05, 90.96it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1487/2000 [00:19<00:05, 92.66it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 93.57it/s, train_loss=0.4897, val_loss=0.4960]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 93.57it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:19<00:09, 54.13it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1517/2000 [00:20<00:07, 62.55it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1527/2000 [00:20<00:06, 69.90it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1537/2000 [00:20<00:06, 76.48it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1547/2000 [00:20<00:05, 81.79it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1557/2000 [00:20<00:05, 86.06it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1567/2000 [00:20<00:04, 88.80it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1577/2000 [00:20<00:04, 90.62it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1587/2000 [00:20<00:04, 92.06it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:20<00:04, 91.06it/s, train_loss=0.4992, val_loss=0.4980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1597/2000 [00:21<00:04, 91.06it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1607/2000 [00:21<00:08, 48.77it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1617/2000 [00:21<00:06, 57.16it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1626/2000 [00:21<00:05, 63.58it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1636/2000 [00:21<00:05, 70.73it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1646/2000 [00:21<00:04, 76.75it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1656/2000 [00:21<00:04, 81.25it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1666/2000 [00:21<00:03, 85.10it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1676/2000 [00:22<00:03, 87.97it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1686/2000 [00:22<00:03, 90.23it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:22<00:03, 91.72it/s, train_loss=0.4736, val_loss=0.4759]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1696/2000 [00:22<00:03, 91.72it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1706/2000 [00:22<00:05, 53.29it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1716/2000 [00:22<00:04, 61.51it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1726/2000 [00:22<00:03, 69.17it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:22<00:03, 75.76it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:23<00:03, 80.82it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1756/2000 [00:23<00:02, 84.93it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1766/2000 [00:23<00:02, 87.66it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1776/2000 [00:23<00:02, 89.25it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1786/2000 [00:23<00:02, 90.93it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1796/2000 [00:23<00:02, 92.31it/s, train_loss=0.4816, val_loss=0.4857]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1796/2000 [00:23<00:02, 92.31it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1806/2000 [00:23<00:03, 53.80it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1816/2000 [00:24<00:02, 62.26it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1826/2000 [00:24<00:02, 69.35it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1836/2000 [00:24<00:02, 75.50it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:24<00:01, 81.05it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:24<00:01, 85.30it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1866/2000 [00:24<00:01, 88.65it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1876/2000 [00:24<00:01, 90.59it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1886/2000 [00:24<00:01, 92.02it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1896/2000 [00:24<00:01, 93.44it/s, train_loss=0.4612, val_loss=0.4728]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1896/2000 [00:25<00:01, 93.44it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1906/2000 [00:25<00:01, 54.27it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1916/2000 [00:25<00:01, 61.84it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1926/2000 [00:25<00:01, 69.14it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1936/2000 [00:25<00:00, 75.18it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1946/2000 [00:25<00:00, 79.64it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1956/2000 [00:25<00:00, 84.09it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1966/2000 [00:25<00:00, 86.44it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1976/2000 [00:25<00:00, 89.22it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1986/2000 [00:26<00:00, 91.03it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 76.23it/s, train_loss=0.4702, val_loss=0.4779]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:16,  3.59it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:55, 35.96it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:35, 56.04it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:28, 68.79it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 77.04it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 51/2000 [00:00<00:23, 82.49it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 61/2000 [00:00<00:22, 86.23it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 71/2000 [00:01<00:21, 89.06it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:21, 91.28it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 92.25it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 91/2000 [00:01<00:20, 92.25it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 101/2000 [00:01<00:36, 51.70it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:31, 59.90it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:27, 67.61it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 131/2000 [00:01<00:25, 74.40it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 141/2000 [00:02<00:23, 79.91it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 151/2000 [00:02<00:21, 84.23it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 161/2000 [00:02<00:21, 87.26it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:20, 89.90it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 181/2000 [00:02<00:19, 91.30it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 93.09it/s, train_loss=2.6988, val_loss=2.7039]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 191/2000 [00:02<00:19, 93.09it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:02<00:34, 52.37it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 211/2000 [00:03<00:29, 60.66it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 221/2000 [00:03<00:25, 68.47it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:23, 75.07it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 241/2000 [00:03<00:21, 80.36it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 251/2000 [00:03<00:20, 84.61it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:03<00:19, 88.00it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 271/2000 [00:03<00:19, 90.74it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:03<00:18, 91.86it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:03<00:18, 92.24it/s, train_loss=2.2632, val_loss=2.2715]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:18, 92.24it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 301/2000 [00:04<00:33, 50.75it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:28, 59.02it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 321/2000 [00:04<00:25, 66.77it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 331/2000 [00:04<00:22, 73.92it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:20, 79.88it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:04<00:19, 84.23it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 87.87it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 371/2000 [00:04<00:18, 90.37it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 381/2000 [00:05<00:17, 92.02it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 93.05it/s, train_loss=1.8721, val_loss=1.8636]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:17, 93.05it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:29, 53.31it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:05<00:25, 61.49it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:22, 69.05it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 431/2000 [00:05<00:20, 75.55it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 441/2000 [00:05<00:19, 80.33it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:06<00:18, 84.53it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 461/2000 [00:06<00:17, 88.08it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 471/2000 [00:06<00:16, 90.63it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 481/2000 [00:06<00:16, 92.52it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:16, 93.63it/s, train_loss=1.3377, val_loss=1.3170]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:16, 93.63it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:06<00:28, 52.90it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:06<00:24, 60.93it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:07<00:21, 68.52it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:07<00:19, 74.84it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:18, 80.09it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:07<00:17, 84.60it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 561/2000 [00:07<00:16, 88.24it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:07<00:15, 91.03it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 581/2000 [00:07<00:15, 92.87it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:07<00:14, 93.98it/s, train_loss=0.9298, val_loss=0.9289]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:08<00:14, 93.98it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 601/2000 [00:08<00:26, 52.30it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 611/2000 [00:08<00:22, 60.95it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 621/2000 [00:08<00:20, 68.95it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 632/2000 [00:08<00:17, 76.87it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 643/2000 [00:08<00:16, 83.04it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 654/2000 [00:08<00:15, 87.85it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 665/2000 [00:08<00:14, 91.51it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:08<00:14, 94.37it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 687/2000 [00:09<00:13, 96.06it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 97.57it/s, train_loss=0.7203, val_loss=0.7242]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 97.57it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 708/2000 [00:09<00:22, 56.43it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 719/2000 [00:09<00:19, 65.35it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 730/2000 [00:09<00:17, 73.14it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 741/2000 [00:09<00:15, 79.83it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:09<00:14, 85.12it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 763/2000 [00:10<00:13, 89.48it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 774/2000 [00:10<00:13, 92.75it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 785/2000 [00:10<00:12, 95.09it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:10<00:12, 97.06it/s, train_loss=0.6389, val_loss=0.6427]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:10<00:12, 97.06it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 807/2000 [00:10<00:20, 56.96it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 818/2000 [00:10<00:18, 65.65it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 828/2000 [00:10<00:16, 72.58it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 839/2000 [00:11<00:14, 79.27it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▎     | 850/2000 [00:11<00:13, 84.80it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 861/2000 [00:11<00:12, 89.26it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 872/2000 [00:11<00:12, 92.53it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 883/2000 [00:11<00:11, 95.14it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:11<00:11, 96.81it/s, train_loss=0.5918, val_loss=0.5885]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 894/2000 [00:11<00:11, 96.81it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 904/2000 [00:12<00:19, 55.99it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 914/2000 [00:12<00:16, 64.01it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 925/2000 [00:12<00:14, 72.11it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 936/2000 [00:12<00:13, 79.05it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 947/2000 [00:12<00:12, 84.59it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 958/2000 [00:12<00:11, 88.93it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 969/2000 [00:12<00:11, 92.31it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 980/2000 [00:12<00:10, 94.83it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 991/2000 [00:12<00:10, 96.60it/s, train_loss=0.5641, val_loss=0.5700]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 991/2000 [00:13<00:10, 96.60it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1001/2000 [00:13<00:18, 54.45it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1011/2000 [00:13<00:15, 62.42it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:13<00:13, 70.05it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1032/2000 [00:13<00:12, 77.59it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1043/2000 [00:13<00:11, 83.44it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:13<00:10, 88.08it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1064/2000 [00:13<00:10, 91.12it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:14<00:09, 93.39it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1085/2000 [00:14<00:09, 95.50it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:14<00:09, 97.15it/s, train_loss=0.5416, val_loss=0.5372]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:14<00:09, 97.15it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1106/2000 [00:14<00:16, 55.50it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1116/2000 [00:14<00:13, 63.70it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1127/2000 [00:14<00:12, 72.09it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:14<00:10, 78.99it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1149/2000 [00:15<00:10, 84.52it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1159/2000 [00:15<00:09, 88.30it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1170/2000 [00:15<00:09, 91.74it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1180/2000 [00:15<00:08, 93.74it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1190/2000 [00:15<00:08, 95.35it/s, train_loss=0.5268, val_loss=0.5239]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1190/2000 [00:15<00:08, 95.35it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:15<00:14, 53.44it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1211/2000 [00:15<00:12, 61.55it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1222/2000 [00:16<00:11, 70.25it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1233/2000 [00:16<00:09, 77.56it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1244/2000 [00:16<00:09, 83.46it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1255/2000 [00:16<00:08, 88.20it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1266/2000 [00:16<00:08, 91.57it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1277/2000 [00:16<00:07, 94.66it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1288/2000 [00:16<00:07, 96.67it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:16<00:07, 98.06it/s, train_loss=0.5139, val_loss=0.5205]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1299/2000 [00:17<00:07, 98.06it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:17<00:12, 56.91it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:17<00:10, 65.59it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1332/2000 [00:17<00:09, 73.50it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1343/2000 [00:17<00:08, 80.02it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1354/2000 [00:17<00:07, 85.69it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1365/2000 [00:17<00:07, 89.79it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1376/2000 [00:17<00:06, 93.02it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1387/2000 [00:17<00:06, 95.68it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:18<00:06, 97.51it/s, train_loss=0.4918, val_loss=0.5037]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1398/2000 [00:18<00:06, 97.51it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:18<00:10, 57.45it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1420/2000 [00:18<00:08, 66.11it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1431/2000 [00:18<00:07, 74.02it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1442/2000 [00:18<00:06, 80.70it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1453/2000 [00:18<00:06, 85.99it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1464/2000 [00:18<00:05, 90.31it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1475/2000 [00:19<00:05, 93.22it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1486/2000 [00:19<00:05, 95.80it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 97.64it/s, train_loss=0.4820, val_loss=0.4955]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 97.64it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:19<00:08, 57.36it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:19<00:07, 66.10it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1530/2000 [00:19<00:06, 73.88it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1541/2000 [00:19<00:05, 80.65it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1552/2000 [00:20<00:05, 86.15it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1563/2000 [00:20<00:04, 90.17it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1574/2000 [00:20<00:04, 93.55it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1585/2000 [00:20<00:04, 95.99it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:20<00:04, 97.61it/s, train_loss=0.4865, val_loss=0.4856]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1596/2000 [00:20<00:04, 97.61it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1607/2000 [00:20<00:06, 57.37it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:21<00:05, 66.00it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:21<00:05, 73.74it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:21<00:04, 80.63it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1651/2000 [00:21<00:04, 86.06it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1662/2000 [00:21<00:03, 90.25it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:21<00:03, 93.28it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1684/2000 [00:21<00:03, 95.89it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:21<00:03, 97.83it/s, train_loss=0.4691, val_loss=0.4665]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1695/2000 [00:22<00:03, 97.83it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1706/2000 [00:22<00:05, 57.22it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1715/2000 [00:22<00:04, 63.14it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:22<00:03, 70.64it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:22<00:03, 78.26it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1747/2000 [00:22<00:03, 84.21it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1758/2000 [00:22<00:02, 88.80it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1769/2000 [00:22<00:02, 92.46it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1780/2000 [00:22<00:02, 95.19it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:22<00:02, 96.98it/s, train_loss=0.4743, val_loss=0.4875]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1791/2000 [00:23<00:02, 96.98it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1802/2000 [00:23<00:03, 55.66it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1812/2000 [00:23<00:02, 63.50it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1823/2000 [00:23<00:02, 71.82it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:23<00:02, 78.79it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1845/2000 [00:23<00:01, 84.76it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:23<00:01, 89.33it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1867/2000 [00:24<00:01, 92.50it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1878/2000 [00:24<00:01, 94.98it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:24<00:01, 96.32it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:24<00:01, 97.84it/s, train_loss=0.4715, val_loss=0.4750]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1899/2000 [00:24<00:01, 97.84it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1910/2000 [00:24<00:01, 56.51it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1921/2000 [00:24<00:01, 65.33it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1932/2000 [00:24<00:00, 73.37it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1943/2000 [00:25<00:00, 80.17it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1954/2000 [00:25<00:00, 85.84it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1965/2000 [00:25<00:00, 89.92it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1975/2000 [00:25<00:00, 90.99it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1986/2000 [00:25<00:00, 93.79it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.06it/s, train_loss=0.4776, val_loss=0.4635]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<08:56,  3.72it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.34it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.67it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 74.12it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:23, 82.81it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:21, 88.98it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 67/2000 [00:00<00:20, 92.74it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 78/2000 [00:01<00:20, 95.20it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:19, 97.01it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.15it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.15it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 110/2000 [00:01<00:34, 55.57it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 121/2000 [00:01<00:29, 64.66it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 132/2000 [00:01<00:25, 72.76it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 143/2000 [00:01<00:23, 79.69it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 154/2000 [00:02<00:21, 85.15it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:20, 89.49it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 176/2000 [00:02<00:19, 93.09it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 187/2000 [00:02<00:19, 95.37it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:18, 97.01it/s, train_loss=2.7161, val_loss=2.7187]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 198/2000 [00:02<00:18, 97.01it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 209/2000 [00:02<00:31, 56.94it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 220/2000 [00:02<00:27, 65.67it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 231/2000 [00:03<00:24, 73.40it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 242/2000 [00:03<00:21, 80.32it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 253/2000 [00:03<00:20, 85.73it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 264/2000 [00:03<00:19, 89.80it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 275/2000 [00:03<00:18, 93.22it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 286/2000 [00:03<00:17, 95.67it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:03<00:17, 97.65it/s, train_loss=2.2323, val_loss=2.2339]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:17, 97.65it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:29, 57.29it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:25, 65.97it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 330/2000 [00:04<00:22, 73.78it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:20, 80.46it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:04<00:19, 86.08it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:04<00:18, 90.05it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:04<00:17, 93.30it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:04<00:16, 95.51it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:04<00:16, 97.32it/s, train_loss=1.7759, val_loss=1.7736]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:16, 97.32it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 407/2000 [00:05<00:27, 57.24it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 418/2000 [00:05<00:23, 65.93it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 429/2000 [00:05<00:21, 73.78it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 440/2000 [00:05<00:19, 80.46it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:05<00:18, 85.91it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 462/2000 [00:05<00:17, 90.14it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:05<00:16, 93.16it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 484/2000 [00:06<00:15, 95.59it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:15, 97.49it/s, train_loss=1.2497, val_loss=1.2640]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:15, 97.49it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:06<00:26, 57.11it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 517/2000 [00:06<00:22, 65.78it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 528/2000 [00:06<00:19, 73.64it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 539/2000 [00:06<00:18, 80.37it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 550/2000 [00:07<00:16, 85.58it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 561/2000 [00:07<00:15, 90.05it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 572/2000 [00:07<00:15, 93.29it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 583/2000 [00:07<00:14, 95.65it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 96.79it/s, train_loss=0.8856, val_loss=0.8980]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 96.79it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:07<00:25, 55.08it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 613/2000 [00:07<00:21, 63.07it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 624/2000 [00:08<00:19, 71.46it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:08<00:17, 77.81it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 645/2000 [00:08<00:16, 83.92it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 656/2000 [00:08<00:15, 88.71it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:08<00:14, 91.65it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:08<00:14, 93.67it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 687/2000 [00:08<00:13, 95.75it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:08<00:13, 97.45it/s, train_loss=0.7126, val_loss=0.6985]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 97.45it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 708/2000 [00:09<00:23, 55.55it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 718/2000 [00:09<00:20, 63.71it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 728/2000 [00:09<00:17, 71.20it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 739/2000 [00:09<00:16, 78.58it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 750/2000 [00:09<00:14, 84.34it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 761/2000 [00:09<00:13, 88.82it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:09<00:13, 92.25it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:09<00:12, 94.80it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:09<00:12, 96.68it/s, train_loss=0.6325, val_loss=0.6229]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 96.68it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 804/2000 [00:10<00:21, 55.35it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:10<00:18, 63.22it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 824/2000 [00:10<00:16, 70.72it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 835/2000 [00:10<00:14, 78.25it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 846/2000 [00:10<00:13, 84.42it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 857/2000 [00:10<00:12, 88.87it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 868/2000 [00:11<00:12, 92.42it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 878/2000 [00:11<00:11, 94.32it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 888/2000 [00:11<00:11, 95.73it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 899/2000 [00:11<00:11, 97.18it/s, train_loss=0.5803, val_loss=0.5726]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 899/2000 [00:11<00:11, 97.18it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 909/2000 [00:11<00:19, 55.32it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 919/2000 [00:11<00:17, 63.42it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 930/2000 [00:11<00:14, 71.77it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 941/2000 [00:12<00:13, 78.72it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 952/2000 [00:12<00:12, 84.32it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 962/2000 [00:12<00:11, 87.97it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▊     | 973/2000 [00:12<00:11, 91.61it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 984/2000 [00:12<00:10, 94.12it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:12<00:10, 96.29it/s, train_loss=0.5554, val_loss=0.5487]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:12<00:10, 96.29it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:12<00:17, 55.87it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1016/2000 [00:13<00:15, 64.71it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1026/2000 [00:13<00:13, 71.91it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1037/2000 [00:13<00:12, 79.16it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:13<00:11, 84.79it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1058/2000 [00:13<00:10, 88.61it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1069/2000 [00:13<00:10, 92.05it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1080/2000 [00:13<00:09, 94.44it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:13<00:09, 96.29it/s, train_loss=0.5319, val_loss=0.5373]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:14<00:09, 96.29it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:14<00:16, 53.99it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:14<00:14, 62.06it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1121/2000 [00:14<00:12, 69.73it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1132/2000 [00:14<00:11, 77.23it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1143/2000 [00:14<00:10, 83.14it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1154/2000 [00:14<00:09, 87.88it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1165/2000 [00:14<00:09, 91.52it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1176/2000 [00:14<00:08, 94.24it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1187/2000 [00:15<00:08, 96.20it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 97.49it/s, train_loss=0.5061, val_loss=0.5198]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 97.49it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:15<00:14, 56.25it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1219/2000 [00:15<00:12, 65.00it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1230/2000 [00:15<00:10, 72.97it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1241/2000 [00:15<00:09, 79.60it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1252/2000 [00:15<00:08, 84.98it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1263/2000 [00:16<00:08, 89.34it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1274/2000 [00:16<00:07, 92.51it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1285/2000 [00:16<00:07, 94.93it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1296/2000 [00:16<00:07, 96.84it/s, train_loss=0.5136, val_loss=0.5243]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1296/2000 [00:16<00:07, 96.84it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1306/2000 [00:16<00:12, 56.30it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:16<00:10, 65.20it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1328/2000 [00:16<00:09, 72.96it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1338/2000 [00:17<00:08, 78.96it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1349/2000 [00:17<00:07, 84.73it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1359/2000 [00:17<00:07, 88.57it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:17<00:06, 91.55it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1379/2000 [00:17<00:06, 93.86it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:17<00:06, 95.51it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1400/2000 [00:17<00:06, 97.08it/s, train_loss=0.4810, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1400/2000 [00:17<00:06, 97.08it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1410/2000 [00:18<00:10, 54.75it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1421/2000 [00:18<00:09, 63.86it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1432/2000 [00:18<00:07, 72.22it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1443/2000 [00:18<00:07, 78.97it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1454/2000 [00:18<00:06, 84.51it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1465/2000 [00:18<00:06, 88.91it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1475/2000 [00:18<00:05, 91.47it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1486/2000 [00:18<00:05, 94.37it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:18<00:05, 95.46it/s, train_loss=0.4860, val_loss=0.4964]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:19<00:05, 95.46it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1506/2000 [00:19<00:09, 54.59it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1517/2000 [00:19<00:07, 63.82it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1528/2000 [00:19<00:06, 72.12it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1538/2000 [00:19<00:05, 78.34it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1549/2000 [00:19<00:05, 84.03it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1559/2000 [00:19<00:05, 87.94it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1570/2000 [00:19<00:04, 91.53it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1581/2000 [00:20<00:04, 93.99it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:20<00:04, 95.82it/s, train_loss=0.4884, val_loss=0.4922]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1592/2000 [00:20<00:04, 95.82it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1602/2000 [00:20<00:07, 54.13it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1612/2000 [00:20<00:06, 62.19it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1622/2000 [00:20<00:05, 69.89it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1632/2000 [00:20<00:04, 76.57it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1643/2000 [00:20<00:04, 82.92it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:21<00:03, 87.92it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:21<00:03, 91.01it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1675/2000 [00:21<00:03, 94.16it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1686/2000 [00:21<00:03, 95.80it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:21<00:03, 97.37it/s, train_loss=0.4740, val_loss=0.4708]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1697/2000 [00:21<00:03, 97.37it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1707/2000 [00:21<00:05, 55.60it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1718/2000 [00:21<00:04, 64.67it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1729/2000 [00:22<00:03, 72.75it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1740/2000 [00:22<00:03, 79.67it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1750/2000 [00:22<00:02, 84.52it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1761/2000 [00:22<00:02, 88.91it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:22<00:02, 92.35it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1783/2000 [00:22<00:02, 94.85it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:22<00:02, 96.74it/s, train_loss=0.4698, val_loss=0.4846]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:23<00:02, 96.74it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1804/2000 [00:23<00:03, 53.93it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1814/2000 [00:23<00:03, 61.92it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1824/2000 [00:23<00:02, 69.49it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1835/2000 [00:23<00:02, 77.31it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:23<00:01, 83.31it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1857/2000 [00:23<00:01, 88.05it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1868/2000 [00:23<00:01, 91.88it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1879/2000 [00:23<00:01, 94.55it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1890/2000 [00:23<00:01, 96.28it/s, train_loss=0.4736, val_loss=0.4840]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1890/2000 [00:24<00:01, 96.28it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:24<00:01, 55.68it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:24<00:01, 63.22it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:24<00:01, 71.41it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:24<00:00, 78.49it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1944/2000 [00:24<00:00, 84.18it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1955/2000 [00:24<00:00, 88.71it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1966/2000 [00:24<00:00, 92.02it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1977/2000 [00:25<00:00, 94.83it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1988/2000 [00:25<00:00, 96.37it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.95it/s, train_loss=0.4683, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:35<00:00, 77.80s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR:  67%|██████▋   | 2/3 [05:14<02:36, 156.84s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.0005, beta2=0.999, avg_val_loss=0.4735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Beta2:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:05,  3.66it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.06it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.35it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 73.77it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:23, 82.75it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:21, 88.80it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 67/2000 [00:00<00:20, 92.68it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 78/2000 [00:01<00:20, 95.40it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:19, 97.18it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.38it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.38it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:33, 56.52it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 122/2000 [00:01<00:28, 65.42it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 133/2000 [00:01<00:25, 73.24it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 144/2000 [00:01<00:23, 79.98it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 155/2000 [00:02<00:21, 85.31it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 165/2000 [00:02<00:20, 88.89it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 176/2000 [00:02<00:19, 92.45it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 187/2000 [00:02<00:19, 94.97it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:18, 96.11it/s, train_loss=2.3310, val_loss=2.3331]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:18, 96.11it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 207/2000 [00:02<00:32, 55.04it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 218/2000 [00:02<00:27, 64.21it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 229/2000 [00:03<00:24, 72.38it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 240/2000 [00:03<00:22, 79.31it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▎        | 250/2000 [00:03<00:20, 84.22it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 260/2000 [00:03<00:19, 88.20it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 270/2000 [00:03<00:18, 91.19it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 281/2000 [00:03<00:18, 94.00it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:03<00:17, 95.52it/s, train_loss=1.8187, val_loss=1.8273]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 291/2000 [00:04<00:17, 95.52it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 301/2000 [00:04<00:31, 53.15it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 311/2000 [00:04<00:27, 61.50it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 322/2000 [00:04<00:23, 70.01it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 332/2000 [00:04<00:22, 75.50it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 342/2000 [00:04<00:20, 80.97it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:04<00:19, 85.74it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 362/2000 [00:04<00:18, 89.36it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:04<00:17, 92.29it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 383/2000 [00:04<00:17, 94.80it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:16, 96.52it/s, train_loss=1.4719, val_loss=1.4848]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 394/2000 [00:05<00:16, 96.52it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 404/2000 [00:05<00:29, 54.59it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 414/2000 [00:05<00:25, 62.60it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 424/2000 [00:05<00:22, 70.27it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 435/2000 [00:05<00:20, 77.70it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 445/2000 [00:05<00:18, 82.78it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 456/2000 [00:05<00:17, 87.47it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 466/2000 [00:06<00:16, 90.60it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 476/2000 [00:06<00:16, 92.84it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 487/2000 [00:06<00:15, 95.15it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:06<00:15, 96.20it/s, train_loss=1.1650, val_loss=1.1919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:06<00:15, 96.20it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 507/2000 [00:06<00:27, 54.70it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 517/2000 [00:06<00:23, 63.09it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 528/2000 [00:06<00:20, 71.58it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 539/2000 [00:07<00:18, 78.73it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 549/2000 [00:07<00:17, 83.74it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 560/2000 [00:07<00:16, 88.54it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 571/2000 [00:07<00:15, 92.06it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 582/2000 [00:07<00:15, 94.52it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 96.41it/s, train_loss=0.8324, val_loss=0.8400]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 96.41it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:07<00:25, 54.91it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 613/2000 [00:08<00:22, 62.77it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 624/2000 [00:08<00:19, 71.07it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 635/2000 [00:08<00:17, 78.17it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:08<00:16, 83.82it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 657/2000 [00:08<00:15, 88.18it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 668/2000 [00:08<00:14, 91.54it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 678/2000 [00:08<00:14, 93.36it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 689/2000 [00:08<00:13, 95.44it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:08<00:13, 96.67it/s, train_loss=0.6557, val_loss=0.6440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:09<00:13, 96.67it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 709/2000 [00:09<00:23, 54.79it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 720/2000 [00:09<00:20, 63.87it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 730/2000 [00:09<00:17, 71.31it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 741/2000 [00:09<00:16, 78.56it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 752/2000 [00:09<00:14, 84.24it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 763/2000 [00:09<00:13, 88.59it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 773/2000 [00:09<00:13, 91.55it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 784/2000 [00:10<00:12, 94.39it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 95.78it/s, train_loss=0.5646, val_loss=0.5618]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 95.78it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 804/2000 [00:10<00:21, 54.87it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:10<00:18, 62.97it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 825/2000 [00:10<00:16, 71.41it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 836/2000 [00:10<00:14, 78.46it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 847/2000 [00:10<00:13, 84.47it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 858/2000 [00:11<00:12, 88.78it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 869/2000 [00:11<00:12, 92.21it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 880/2000 [00:11<00:11, 94.78it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 890/2000 [00:11<00:11, 96.15it/s, train_loss=0.5357, val_loss=0.5426]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 890/2000 [00:11<00:11, 96.15it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 901/2000 [00:11<00:20, 54.72it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:11<00:17, 62.64it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 922/2000 [00:11<00:15, 70.98it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 933/2000 [00:12<00:13, 77.93it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 944/2000 [00:12<00:12, 83.77it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 954/2000 [00:12<00:11, 87.54it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 964/2000 [00:12<00:11, 90.53it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:12<00:10, 93.84it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 985/2000 [00:12<00:10, 95.42it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:12<00:10, 96.62it/s, train_loss=0.5238, val_loss=0.5173]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:13<00:10, 96.62it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:13<00:18, 54.49it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1016/2000 [00:13<00:15, 63.76it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1027/2000 [00:13<00:13, 72.09it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1037/2000 [00:13<00:12, 78.23it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:13<00:11, 84.22it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1059/2000 [00:13<00:10, 88.59it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1069/2000 [00:13<00:10, 91.31it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1079/2000 [00:13<00:09, 93.38it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1090/2000 [00:13<00:09, 95.76it/s, train_loss=0.5269, val_loss=0.5291]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1090/2000 [00:14<00:09, 95.76it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:14<00:16, 53.89it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:14<00:14, 61.61it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1121/2000 [00:14<00:12, 69.25it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1132/2000 [00:14<00:11, 76.77it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1143/2000 [00:14<00:10, 82.81it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1153/2000 [00:14<00:09, 86.92it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1164/2000 [00:14<00:09, 90.83it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▊    | 1174/2000 [00:15<00:08, 92.99it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1184/2000 [00:15<00:08, 94.70it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:15<00:08, 96.35it/s, train_loss=0.4929, val_loss=0.4982]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:15<00:08, 96.35it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1205/2000 [00:15<00:14, 54.28it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1215/2000 [00:15<00:12, 62.46it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1225/2000 [00:15<00:11, 70.22it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1236/2000 [00:15<00:09, 77.63it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1246/2000 [00:16<00:09, 82.85it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:16<00:08, 87.76it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1268/2000 [00:16<00:07, 91.66it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1278/2000 [00:16<00:07, 93.67it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1288/2000 [00:16<00:07, 95.14it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:16<00:07, 96.17it/s, train_loss=0.4945, val_loss=0.4977]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1298/2000 [00:16<00:07, 96.17it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1308/2000 [00:16<00:12, 54.02it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:17<00:10, 62.49it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1329/2000 [00:17<00:09, 71.11it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1340/2000 [00:17<00:08, 78.25it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1350/2000 [00:17<00:07, 83.05it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1360/2000 [00:17<00:07, 87.29it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1371/2000 [00:17<00:06, 91.22it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1382/2000 [00:17<00:06, 93.85it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:17<00:06, 95.48it/s, train_loss=0.4934, val_loss=0.4933]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:18<00:06, 95.48it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:18<00:11, 53.81it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1412/2000 [00:18<00:09, 62.05it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1423/2000 [00:18<00:08, 70.56it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1434/2000 [00:18<00:07, 77.75it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1444/2000 [00:18<00:06, 82.93it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1455/2000 [00:18<00:06, 87.67it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1466/2000 [00:18<00:05, 91.37it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:18<00:05, 94.17it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1488/2000 [00:19<00:05, 96.00it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:19<00:05, 97.24it/s, train_loss=0.4896, val_loss=0.4909]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:19<00:05, 97.24it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1509/2000 [00:19<00:08, 55.95it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:19<00:07, 64.03it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1530/2000 [00:19<00:06, 72.17it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1540/2000 [00:19<00:05, 78.35it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1550/2000 [00:19<00:05, 83.53it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1560/2000 [00:20<00:05, 87.20it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1570/2000 [00:20<00:04, 90.46it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1580/2000 [00:20<00:04, 92.98it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1590/2000 [00:20<00:04, 94.83it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:20<00:04, 96.25it/s, train_loss=0.4875, val_loss=0.4904]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1600/2000 [00:20<00:04, 96.25it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1610/2000 [00:20<00:07, 54.24it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1620/2000 [00:20<00:06, 62.84it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1630/2000 [00:21<00:05, 70.68it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:21<00:04, 77.35it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▎ | 1650/2000 [00:21<00:04, 82.84it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1661/2000 [00:21<00:03, 87.81it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1671/2000 [00:21<00:03, 91.05it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1681/2000 [00:21<00:03, 93.32it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:21<00:03, 95.41it/s, train_loss=0.4804, val_loss=0.4832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:21<00:03, 95.41it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1702/2000 [00:22<00:05, 54.08it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1712/2000 [00:22<00:04, 62.27it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1722/2000 [00:22<00:03, 69.97it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1732/2000 [00:22<00:03, 76.37it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1742/2000 [00:22<00:03, 81.95it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1753/2000 [00:22<00:02, 87.28it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1763/2000 [00:22<00:02, 90.36it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▊ | 1774/2000 [00:22<00:02, 93.31it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1785/2000 [00:22<00:02, 95.50it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:22<00:02, 96.44it/s, train_loss=0.4859, val_loss=0.4890]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1795/2000 [00:23<00:02, 96.44it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1805/2000 [00:23<00:03, 54.42it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1815/2000 [00:23<00:02, 62.52it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1825/2000 [00:23<00:02, 70.09it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1835/2000 [00:23<00:02, 76.91it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:23<00:01, 83.17it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1856/2000 [00:23<00:01, 87.26it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1867/2000 [00:23<00:01, 91.27it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1878/2000 [00:24<00:01, 94.09it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:24<00:01, 94.81it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:24<00:01, 96.16it/s, train_loss=0.4623, val_loss=0.4790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1898/2000 [00:24<00:01, 96.16it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1908/2000 [00:24<00:01, 54.29it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1919/2000 [00:24<00:01, 63.53it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1929/2000 [00:24<00:00, 71.04it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1940/2000 [00:24<00:00, 78.16it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:25<00:00, 83.90it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1961/2000 [00:25<00:00, 87.84it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1972/2000 [00:25<00:00, 91.48it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1983/2000 [00:25<00:00, 94.08it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.24it/s, train_loss=0.4645, val_loss=0.4688]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:02,  3.68it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.09it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 22/2000 [00:00<00:33, 58.61it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 32/2000 [00:00<00:27, 71.28it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 42/2000 [00:00<00:24, 79.27it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 52/2000 [00:00<00:22, 84.99it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 63/2000 [00:00<00:21, 89.87it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▎         | 74/2000 [00:01<00:20, 93.03it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 84/2000 [00:01<00:20, 94.98it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 94/2000 [00:01<00:19, 96.43it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 94/2000 [00:01<00:19, 96.43it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 104/2000 [00:01<00:36, 51.25it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 114/2000 [00:01<00:31, 59.77it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 125/2000 [00:01<00:27, 68.91it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 136/2000 [00:01<00:24, 76.75it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 146/2000 [00:02<00:22, 82.16it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 157/2000 [00:02<00:21, 87.19it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 168/2000 [00:02<00:20, 91.49it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 178/2000 [00:02<00:19, 93.71it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 189/2000 [00:02<00:18, 96.00it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 200/2000 [00:02<00:18, 97.80it/s, train_loss=2.3623, val_loss=2.3666]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 200/2000 [00:02<00:18, 97.80it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 211/2000 [00:02<00:31, 56.55it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 222/2000 [00:03<00:27, 65.22it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:03<00:24, 73.06it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 244/2000 [00:03<00:22, 79.73it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 255/2000 [00:03<00:20, 85.41it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 266/2000 [00:03<00:19, 89.83it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 277/2000 [00:03<00:18, 92.69it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 287/2000 [00:03<00:18, 94.55it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:03<00:17, 96.63it/s, train_loss=1.8777, val_loss=1.8832]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:17, 96.63it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:30, 55.73it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:25, 64.77it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 330/2000 [00:04<00:22, 72.84it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:20, 79.56it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:04<00:19, 84.89it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:04<00:18, 89.16it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:04<00:17, 92.60it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:04<00:16, 95.33it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:16, 96.79it/s, train_loss=1.4078, val_loss=1.4030]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:16, 96.79it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 406/2000 [00:05<00:28, 55.82it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 416/2000 [00:05<00:24, 63.80it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 427/2000 [00:05<00:21, 72.02it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 438/2000 [00:05<00:19, 78.86it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 448/2000 [00:05<00:18, 83.86it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 458/2000 [00:05<00:17, 87.69it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 469/2000 [00:06<00:16, 91.57it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 480/2000 [00:06<00:16, 94.19it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:06<00:15, 95.58it/s, train_loss=0.8912, val_loss=0.8762]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 490/2000 [00:06<00:15, 95.58it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:06<00:27, 54.19it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:06<00:24, 61.78it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 521/2000 [00:06<00:21, 69.10it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 531/2000 [00:06<00:19, 75.76it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 541/2000 [00:07<00:17, 81.55it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 552/2000 [00:07<00:16, 86.62it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 562/2000 [00:07<00:16, 89.36it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 572/2000 [00:07<00:15, 91.75it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 582/2000 [00:07<00:15, 94.02it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 95.99it/s, train_loss=0.6864, val_loss=0.6855]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 95.99it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:07<00:26, 52.79it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 613/2000 [00:08<00:22, 60.82it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:08<00:20, 68.72it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 633/2000 [00:08<00:18, 75.66it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 643/2000 [00:08<00:16, 81.36it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 654/2000 [00:08<00:15, 86.72it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 665/2000 [00:08<00:14, 90.61it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:08<00:14, 93.63it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 686/2000 [00:08<00:13, 94.86it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:08<00:13, 94.51it/s, train_loss=0.5903, val_loss=0.5927]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:09<00:13, 94.51it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:09<00:24, 53.82it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 717/2000 [00:09<00:20, 63.15it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 728/2000 [00:09<00:17, 71.47it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 739/2000 [00:09<00:16, 78.65it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 749/2000 [00:09<00:14, 83.59it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 760/2000 [00:09<00:14, 88.47it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 770/2000 [00:09<00:13, 91.32it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:10<00:12, 94.19it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 95.65it/s, train_loss=0.5524, val_loss=0.5496]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 95.65it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:22, 53.88it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:10<00:19, 62.19it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 822/2000 [00:10<00:16, 70.77it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 832/2000 [00:10<00:15, 77.32it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 843/2000 [00:10<00:13, 83.37it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:11<00:13, 87.21it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:11<00:12, 91.03it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:11<00:11, 93.86it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 886/2000 [00:11<00:11, 95.85it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:11<00:11, 96.84it/s, train_loss=0.5237, val_loss=0.5228]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:11<00:11, 96.84it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 906/2000 [00:11<00:19, 55.07it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 916/2000 [00:11<00:17, 63.26it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 926/2000 [00:12<00:15, 70.88it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 936/2000 [00:12<00:13, 77.24it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 947/2000 [00:12<00:12, 83.23it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 958/2000 [00:12<00:11, 88.01it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 969/2000 [00:12<00:11, 91.88it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:12<00:10, 93.80it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:12<00:10, 95.43it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:12<00:10, 97.11it/s, train_loss=0.5250, val_loss=0.5264]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:13<00:10, 97.11it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1010/2000 [00:13<00:17, 55.22it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:13<00:15, 64.27it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1032/2000 [00:13<00:13, 72.41it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1043/2000 [00:13<00:12, 79.46it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1053/2000 [00:13<00:11, 84.19it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1064/2000 [00:13<00:10, 88.65it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1074/2000 [00:13<00:10, 91.55it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1084/2000 [00:13<00:09, 93.58it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1095/2000 [00:14<00:09, 95.73it/s, train_loss=0.5073, val_loss=0.5081]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1095/2000 [00:14<00:09, 95.73it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1105/2000 [00:14<00:16, 55.06it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1116/2000 [00:14<00:13, 64.29it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1127/2000 [00:14<00:12, 72.34it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:14<00:10, 79.02it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1148/2000 [00:14<00:10, 83.97it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:14<00:09, 88.03it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1169/2000 [00:15<00:09, 91.71it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1180/2000 [00:15<00:08, 94.80it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:15<00:08, 96.88it/s, train_loss=0.5034, val_loss=0.5017]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1191/2000 [00:15<00:08, 96.88it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1201/2000 [00:15<00:14, 54.04it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1212/2000 [00:15<00:12, 63.02it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1223/2000 [00:15<00:10, 71.28it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1234/2000 [00:15<00:09, 78.24it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1245/2000 [00:16<00:08, 84.13it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1256/2000 [00:16<00:08, 88.44it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1267/2000 [00:16<00:07, 91.94it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1278/2000 [00:16<00:07, 94.40it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1289/2000 [00:16<00:07, 96.31it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 97.86it/s, train_loss=0.4937, val_loss=0.5031]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:16<00:07, 97.86it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1311/2000 [00:17<00:12, 56.60it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:17<00:10, 64.04it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1331/2000 [00:17<00:09, 71.29it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1341/2000 [00:17<00:08, 77.76it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1351/2000 [00:17<00:07, 82.44it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1361/2000 [00:17<00:07, 86.34it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1371/2000 [00:17<00:07, 89.66it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1381/2000 [00:17<00:06, 92.06it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:17<00:06, 94.71it/s, train_loss=0.4879, val_loss=0.4966]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1392/2000 [00:18<00:06, 94.71it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:18<00:11, 53.38it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1412/2000 [00:18<00:09, 61.52it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1422/2000 [00:18<00:08, 69.05it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1432/2000 [00:18<00:07, 75.79it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1442/2000 [00:18<00:06, 81.44it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1452/2000 [00:18<00:06, 85.97it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1463/2000 [00:18<00:05, 90.12it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1474/2000 [00:18<00:05, 93.11it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1485/2000 [00:19<00:05, 95.42it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:19<00:05, 96.99it/s, train_loss=0.4715, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1496/2000 [00:19<00:05, 96.99it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1506/2000 [00:19<00:08, 54.89it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1516/2000 [00:19<00:07, 63.03it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1526/2000 [00:19<00:06, 70.46it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1536/2000 [00:19<00:06, 77.12it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1546/2000 [00:19<00:05, 82.39it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1557/2000 [00:20<00:05, 87.52it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1567/2000 [00:20<00:04, 90.38it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1578/2000 [00:20<00:04, 93.52it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:20<00:04, 95.53it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:20<00:04, 96.41it/s, train_loss=0.4912, val_loss=0.4924]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:20<00:04, 96.41it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:20<00:07, 54.74it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:20<00:06, 63.11it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:21<00:05, 70.47it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1639/2000 [00:21<00:04, 76.97it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▎ | 1650/2000 [00:21<00:04, 83.44it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1661/2000 [00:21<00:03, 88.00it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1671/2000 [00:21<00:03, 89.93it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1681/2000 [00:21<00:03, 92.11it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:21<00:03, 94.80it/s, train_loss=0.4655, val_loss=0.4649]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:22<00:03, 94.80it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1702/2000 [00:22<00:05, 53.62it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1712/2000 [00:22<00:04, 61.58it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1722/2000 [00:22<00:04, 69.36it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1733/2000 [00:22<00:03, 76.96it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:22<00:03, 83.01it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:22<00:02, 87.11it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1765/2000 [00:22<00:02, 91.10it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1775/2000 [00:22<00:02, 93.26it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1786/2000 [00:22<00:02, 95.48it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:23<00:02, 96.83it/s, train_loss=0.4665, val_loss=0.4806]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:23<00:02, 96.83it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1807/2000 [00:23<00:03, 55.31it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:23<00:02, 64.38it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1828/2000 [00:23<00:02, 71.50it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1838/2000 [00:23<00:02, 77.82it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1848/2000 [00:23<00:01, 83.17it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1859/2000 [00:23<00:01, 87.78it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1869/2000 [00:24<00:01, 90.74it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:24<00:01, 93.55it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:24<00:01, 95.98it/s, train_loss=0.4769, val_loss=0.4816]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:24<00:01, 95.98it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:24<00:01, 53.21it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:24<00:01, 61.57it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:24<00:01, 70.10it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:24<00:00, 77.73it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1944/2000 [00:25<00:00, 83.48it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1955/2000 [00:25<00:00, 88.08it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1966/2000 [00:25<00:00, 91.70it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1977/2000 [00:25<00:00, 94.48it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1988/2000 [00:25<00:00, 96.32it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.06it/s, train_loss=0.5019, val_loss=0.4872]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<08:59,  3.71it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.22it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.61it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 73.81it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:23, 82.63it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:21, 88.42it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 66/2000 [00:00<00:21, 91.64it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 77/2000 [00:01<00:20, 94.94it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 87/2000 [00:01<00:19, 95.87it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 98/2000 [00:01<00:19, 97.34it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 98/2000 [00:01<00:19, 97.34it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 108/2000 [00:01<00:34, 54.12it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 119/2000 [00:01<00:29, 63.37it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 129/2000 [00:01<00:26, 70.94it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 139/2000 [00:01<00:23, 77.55it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 149/2000 [00:02<00:22, 82.68it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 160/2000 [00:02<00:20, 87.70it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:20, 91.17it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 182/2000 [00:02<00:19, 93.77it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:19, 95.14it/s, train_loss=2.3420, val_loss=2.3440]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 192/2000 [00:02<00:19, 95.14it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 202/2000 [00:02<00:33, 53.73it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 212/2000 [00:02<00:29, 61.65it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 222/2000 [00:03<00:25, 69.03it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:03<00:23, 76.58it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:21, 80.98it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 253/2000 [00:03<00:20, 85.73it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 264/2000 [00:03<00:19, 90.03it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▎        | 274/2000 [00:03<00:18, 92.43it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 284/2000 [00:03<00:18, 94.38it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:03<00:17, 96.02it/s, train_loss=1.8458, val_loss=1.8515]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 295/2000 [00:04<00:17, 96.02it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 305/2000 [00:04<00:31, 53.94it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:04<00:27, 62.08it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 325/2000 [00:04<00:23, 69.91it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 335/2000 [00:04<00:21, 76.14it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 345/2000 [00:04<00:20, 81.53it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 355/2000 [00:04<00:19, 86.10it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 365/2000 [00:04<00:18, 89.30it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 376/2000 [00:04<00:17, 92.52it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 386/2000 [00:04<00:17, 93.69it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 397/2000 [00:05<00:16, 95.80it/s, train_loss=1.3960, val_loss=1.3891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 397/2000 [00:05<00:16, 95.80it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 407/2000 [00:05<00:29, 54.20it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 417/2000 [00:05<00:25, 62.53it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 428/2000 [00:05<00:22, 71.14it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 438/2000 [00:05<00:20, 77.35it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 449/2000 [00:05<00:18, 83.61it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 459/2000 [00:06<00:17, 87.56it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 469/2000 [00:06<00:17, 90.05it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 480/2000 [00:06<00:16, 93.11it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:15, 95.32it/s, train_loss=0.8855, val_loss=0.9042]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:06<00:15, 95.32it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 501/2000 [00:06<00:28, 53.43it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 511/2000 [00:06<00:24, 61.48it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 522/2000 [00:06<00:21, 70.02it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 532/2000 [00:07<00:19, 76.42it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 542/2000 [00:07<00:17, 81.63it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 553/2000 [00:07<00:16, 86.89it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 564/2000 [00:07<00:15, 91.12it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 574/2000 [00:07<00:15, 92.99it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 584/2000 [00:07<00:14, 94.87it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:07<00:14, 96.47it/s, train_loss=0.6527, val_loss=0.6674]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:07<00:14, 96.47it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 605/2000 [00:08<00:25, 55.34it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 616/2000 [00:08<00:21, 64.49it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 627/2000 [00:08<00:18, 72.77it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 638/2000 [00:08<00:17, 79.87it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 649/2000 [00:08<00:15, 85.62it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 660/2000 [00:08<00:14, 89.86it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▎      | 671/2000 [00:08<00:14, 93.24it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 682/2000 [00:08<00:13, 95.96it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:08<00:13, 97.31it/s, train_loss=0.5889, val_loss=0.5769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 693/2000 [00:09<00:13, 97.31it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 704/2000 [00:09<00:22, 56.41it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 714/2000 [00:09<00:20, 64.02it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 725/2000 [00:09<00:17, 72.21it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 736/2000 [00:09<00:15, 79.45it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 747/2000 [00:09<00:14, 85.23it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 758/2000 [00:09<00:13, 89.83it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 769/2000 [00:09<00:13, 93.18it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 780/2000 [00:10<00:12, 95.65it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 97.36it/s, train_loss=0.5466, val_loss=0.5408]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 97.36it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 802/2000 [00:10<00:21, 56.47it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 812/2000 [00:10<00:18, 64.13it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 823/2000 [00:10<00:16, 72.28it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 834/2000 [00:10<00:14, 79.35it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 845/2000 [00:10<00:13, 84.97it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 856/2000 [00:11<00:12, 89.41it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 867/2000 [00:11<00:12, 92.74it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 878/2000 [00:11<00:11, 95.31it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 889/2000 [00:11<00:11, 97.00it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 900/2000 [00:11<00:11, 98.39it/s, train_loss=0.5350, val_loss=0.5318]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 900/2000 [00:11<00:11, 98.39it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:11<00:19, 57.09it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:11<00:16, 64.71it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 932/2000 [00:12<00:14, 72.77it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 943/2000 [00:12<00:13, 79.57it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 954/2000 [00:12<00:12, 85.16it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 965/2000 [00:12<00:11, 89.35it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:12<00:11, 92.02it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 986/2000 [00:12<00:10, 94.70it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:12<00:10, 96.30it/s, train_loss=0.5244, val_loss=0.5209]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:13<00:10, 96.30it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1007/2000 [00:13<00:17, 55.18it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1017/2000 [00:13<00:15, 63.35it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████▏    | 1027/2000 [00:13<00:13, 70.16it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1037/2000 [00:13<00:12, 76.83it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1048/2000 [00:13<00:11, 83.03it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1059/2000 [00:13<00:10, 87.79it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1070/2000 [00:13<00:10, 91.43it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:13<00:09, 94.03it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:13<00:09, 95.88it/s, train_loss=0.5216, val_loss=0.5276]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1092/2000 [00:14<00:09, 95.88it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1102/2000 [00:14<00:16, 54.23it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1112/2000 [00:14<00:14, 62.20it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1123/2000 [00:14<00:12, 70.58it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1134/2000 [00:14<00:11, 77.66it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1144/2000 [00:14<00:10, 82.93it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1155/2000 [00:14<00:09, 87.86it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1166/2000 [00:14<00:09, 91.40it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1177/2000 [00:15<00:08, 94.18it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1187/2000 [00:15<00:08, 95.58it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 97.63it/s, train_loss=0.4919, val_loss=0.5075]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 97.63it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1209/2000 [00:15<00:14, 56.45it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1220/2000 [00:15<00:11, 65.21it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:15<00:10, 73.22it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1242/2000 [00:16<00:09, 79.88it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1253/2000 [00:16<00:08, 85.13it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1264/2000 [00:16<00:08, 89.43it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1275/2000 [00:16<00:07, 92.83it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1286/2000 [00:16<00:07, 95.37it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 97.28it/s, train_loss=0.4854, val_loss=0.4935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 97.28it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1308/2000 [00:16<00:12, 57.24it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1319/2000 [00:17<00:10, 65.85it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1330/2000 [00:17<00:09, 73.63it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1341/2000 [00:17<00:08, 80.25it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:17<00:07, 85.55it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1363/2000 [00:17<00:07, 89.80it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1374/2000 [00:17<00:06, 92.92it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1385/2000 [00:17<00:06, 95.09it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:17<00:06, 96.87it/s, train_loss=0.4763, val_loss=0.4951]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:18<00:06, 96.87it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1406/2000 [00:18<00:10, 55.81it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:18<00:09, 64.58it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1428/2000 [00:18<00:07, 72.64it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1439/2000 [00:18<00:07, 79.44it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▎  | 1450/2000 [00:18<00:06, 85.05it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1461/2000 [00:18<00:06, 89.35it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1472/2000 [00:18<00:05, 92.72it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1483/2000 [00:18<00:05, 95.15it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1494/2000 [00:19<00:05, 96.89it/s, train_loss=0.4883, val_loss=0.5009]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1494/2000 [00:19<00:05, 96.89it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1504/2000 [00:19<00:08, 55.98it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1514/2000 [00:19<00:07, 63.87it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1525/2000 [00:19<00:06, 72.15it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1536/2000 [00:19<00:05, 79.07it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1547/2000 [00:19<00:05, 84.58it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1558/2000 [00:19<00:04, 88.82it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1569/2000 [00:20<00:04, 92.47it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1580/2000 [00:20<00:04, 95.07it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:20<00:04, 96.98it/s, train_loss=0.4777, val_loss=0.4847]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:20<00:04, 96.98it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1602/2000 [00:20<00:07, 55.81it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1612/2000 [00:20<00:06, 63.44it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1623/2000 [00:20<00:05, 71.50it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1634/2000 [00:20<00:04, 78.56it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1645/2000 [00:21<00:04, 84.39it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1656/2000 [00:21<00:03, 88.70it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1667/2000 [00:21<00:03, 92.09it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1678/2000 [00:21<00:03, 94.67it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1689/2000 [00:21<00:03, 96.60it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1700/2000 [00:21<00:03, 98.09it/s, train_loss=0.4776, val_loss=0.4709]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1700/2000 [00:21<00:03, 98.09it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1711/2000 [00:22<00:05, 56.66it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1722/2000 [00:22<00:04, 65.23it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1733/2000 [00:22<00:03, 73.05it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1744/2000 [00:22<00:03, 79.55it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1755/2000 [00:22<00:02, 85.18it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1766/2000 [00:22<00:02, 89.42it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1777/2000 [00:22<00:02, 92.56it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1788/2000 [00:22<00:02, 94.90it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:22<00:02, 96.69it/s, train_loss=0.4754, val_loss=0.4914]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:23<00:02, 96.69it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1809/2000 [00:23<00:03, 55.96it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1820/2000 [00:23<00:02, 64.78it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1831/2000 [00:23<00:02, 72.66it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1842/2000 [00:23<00:01, 79.44it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1853/2000 [00:23<00:01, 85.11it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1864/2000 [00:23<00:01, 89.39it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1875/2000 [00:23<00:01, 92.55it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1886/2000 [00:24<00:01, 94.72it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:24<00:01, 96.78it/s, train_loss=0.4758, val_loss=0.4852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:24<00:01, 96.78it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1907/2000 [00:24<00:01, 55.68it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1918/2000 [00:24<00:01, 64.54it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1929/2000 [00:24<00:00, 72.59it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1940/2000 [00:24<00:00, 79.31it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:24<00:00, 84.75it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:25<00:00, 89.03it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1973/2000 [00:25<00:00, 92.49it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1984/2000 [00:25<00:00, 94.69it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.66it/s, train_loss=0.4665, val_loss=0.4746]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2:  50%|█████     | 1/2 [01:16<01:16, 76.64s/it]\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.001, beta2=0.98, avg_val_loss=0.4769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:06,  3.65it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:51, 38.86it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.10it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 73.50it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 45/2000 [00:00<00:23, 82.30it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 56/2000 [00:00<00:22, 88.09it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 67/2000 [00:00<00:20, 92.33it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 78/2000 [00:01<00:20, 94.99it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:19, 97.20it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.27it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:19, 98.27it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 111/2000 [00:01<00:33, 56.42it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 122/2000 [00:01<00:28, 65.16it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 133/2000 [00:01<00:25, 73.06it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 144/2000 [00:01<00:23, 79.88it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 155/2000 [00:02<00:21, 85.52it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 166/2000 [00:02<00:20, 89.79it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 177/2000 [00:02<00:19, 92.88it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 188/2000 [00:02<00:19, 95.33it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 199/2000 [00:02<00:18, 97.03it/s, train_loss=2.3257, val_loss=2.3284]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 199/2000 [00:02<00:18, 97.03it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 210/2000 [00:02<00:31, 57.06it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 221/2000 [00:02<00:27, 65.66it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 232/2000 [00:03<00:24, 73.33it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:22, 79.77it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 254/2000 [00:03<00:20, 85.14it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 265/2000 [00:03<00:19, 89.48it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 276/2000 [00:03<00:18, 92.82it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 287/2000 [00:03<00:17, 95.19it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:03<00:17, 96.77it/s, train_loss=1.8065, val_loss=1.8159]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 298/2000 [00:04<00:17, 96.77it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 308/2000 [00:04<00:30, 56.28it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 319/2000 [00:04<00:25, 65.27it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 330/2000 [00:04<00:22, 73.34it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 341/2000 [00:04<00:20, 80.21it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 352/2000 [00:04<00:19, 85.43it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 363/2000 [00:04<00:18, 89.80it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▊        | 374/2000 [00:04<00:17, 93.17it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:04<00:16, 95.43it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:04<00:16, 97.24it/s, train_loss=1.4061, val_loss=1.4117]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:16, 97.24it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 407/2000 [00:05<00:27, 56.98it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 418/2000 [00:05<00:24, 65.56it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 429/2000 [00:05<00:21, 73.35it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 440/2000 [00:05<00:19, 80.09it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 451/2000 [00:05<00:18, 85.49it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 462/2000 [00:05<00:17, 89.70it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:16, 92.79it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 484/2000 [00:06<00:15, 95.37it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:15, 97.11it/s, train_loss=0.9108, val_loss=0.9246]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 495/2000 [00:06<00:15, 97.11it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:06<00:26, 56.49it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 516/2000 [00:06<00:23, 64.26it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 526/2000 [00:06<00:20, 71.56it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 537/2000 [00:06<00:18, 78.90it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 547/2000 [00:07<00:17, 83.88it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 558/2000 [00:07<00:16, 88.56it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 569/2000 [00:07<00:15, 92.36it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 580/2000 [00:07<00:14, 95.05it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:07<00:14, 96.92it/s, train_loss=0.6799, val_loss=0.6790]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 591/2000 [00:07<00:14, 96.92it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 602/2000 [00:07<00:25, 55.48it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 612/2000 [00:07<00:21, 63.34it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:08<00:19, 71.42it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:08<00:17, 78.36it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 644/2000 [00:08<00:16, 83.48it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 655/2000 [00:08<00:15, 88.20it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:08<00:14, 91.93it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:08<00:14, 94.04it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 687/2000 [00:08<00:13, 96.04it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:08<00:13, 97.46it/s, train_loss=0.5799, val_loss=0.5692]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 698/2000 [00:09<00:13, 97.46it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 708/2000 [00:09<00:23, 55.59it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 719/2000 [00:09<00:19, 64.61it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 730/2000 [00:09<00:17, 72.66it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 741/2000 [00:09<00:15, 79.57it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 751/2000 [00:09<00:14, 84.17it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 762/2000 [00:09<00:13, 88.82it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:09<00:13, 91.57it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:09<00:12, 94.36it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 96.24it/s, train_loss=0.5282, val_loss=0.5335]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 96.24it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 804/2000 [00:10<00:22, 54.33it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 814/2000 [00:10<00:19, 61.75it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████▏     | 825/2000 [00:10<00:16, 70.28it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 836/2000 [00:10<00:14, 77.60it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 847/2000 [00:10<00:13, 83.37it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 858/2000 [00:10<00:12, 88.15it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 869/2000 [00:11<00:12, 92.04it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 880/2000 [00:11<00:11, 94.38it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 96.08it/s, train_loss=0.5066, val_loss=0.5114]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 891/2000 [00:11<00:11, 96.08it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 901/2000 [00:11<00:19, 55.37it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:11<00:17, 63.16it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 922/2000 [00:11<00:15, 71.72it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 933/2000 [00:11<00:13, 78.66it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 944/2000 [00:12<00:12, 84.35it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 955/2000 [00:12<00:11, 88.94it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:12<00:11, 92.17it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 977/2000 [00:12<00:10, 95.08it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 987/2000 [00:12<00:10, 96.16it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:12<00:10, 97.56it/s, train_loss=0.5217, val_loss=0.5177]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 998/2000 [00:12<00:10, 97.56it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1008/2000 [00:13<00:17, 55.81it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1019/2000 [00:13<00:15, 64.79it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:13<00:13, 72.87it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:13<00:12, 79.51it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1052/2000 [00:13<00:11, 84.94it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1063/2000 [00:13<00:10, 89.28it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1073/2000 [00:13<00:10, 91.98it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1083/2000 [00:13<00:09, 93.60it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1094/2000 [00:13<00:09, 96.20it/s, train_loss=0.4971, val_loss=0.5000]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1094/2000 [00:14<00:09, 96.20it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1104/2000 [00:14<00:16, 55.61it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1114/2000 [00:14<00:13, 63.69it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1125/2000 [00:14<00:12, 72.10it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1135/2000 [00:14<00:11, 78.38it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:14<00:10, 80.82it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1156/2000 [00:14<00:09, 86.17it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1167/2000 [00:14<00:09, 90.44it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1178/2000 [00:14<00:08, 93.49it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1189/2000 [00:15<00:08, 96.26it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:15<00:08, 97.44it/s, train_loss=0.4802, val_loss=0.4891]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1200/2000 [00:15<00:08, 97.44it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1210/2000 [00:15<00:14, 56.23it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1221/2000 [00:15<00:11, 65.24it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1232/2000 [00:15<00:10, 73.16it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1243/2000 [00:15<00:09, 79.92it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1254/2000 [00:15<00:08, 85.37it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1265/2000 [00:16<00:08, 89.45it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1275/2000 [00:16<00:07, 91.98it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1286/2000 [00:16<00:07, 95.00it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 96.74it/s, train_loss=0.4717, val_loss=0.4770]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1297/2000 [00:16<00:07, 96.74it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1307/2000 [00:16<00:12, 55.72it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1318/2000 [00:16<00:10, 64.85it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1329/2000 [00:17<00:09, 73.03it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1340/2000 [00:17<00:08, 79.69it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1351/2000 [00:17<00:07, 85.40it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1362/2000 [00:17<00:07, 89.93it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▊   | 1373/2000 [00:17<00:06, 93.15it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1384/2000 [00:17<00:06, 95.26it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1394/2000 [00:17<00:06, 96.23it/s, train_loss=0.4889, val_loss=0.4919]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1394/2000 [00:17<00:06, 96.23it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1404/2000 [00:18<00:10, 54.20it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1414/2000 [00:18<00:09, 62.33it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████▏  | 1425/2000 [00:18<00:08, 70.75it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1436/2000 [00:18<00:07, 77.77it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1446/2000 [00:18<00:06, 82.95it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:18<00:06, 87.62it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:18<00:05, 90.44it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:18<00:05, 92.87it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1487/2000 [00:18<00:05, 94.54it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:18<00:05, 96.45it/s, train_loss=0.4748, val_loss=0.4776]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:19<00:05, 96.45it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1508/2000 [00:19<00:08, 54.91it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1519/2000 [00:19<00:07, 64.19it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1530/2000 [00:19<00:06, 72.33it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1541/2000 [00:19<00:05, 79.10it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1552/2000 [00:19<00:05, 84.77it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1562/2000 [00:19<00:04, 88.51it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▊  | 1573/2000 [00:19<00:04, 92.37it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1583/2000 [00:20<00:04, 94.42it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.48it/s, train_loss=0.4751, val_loss=0.4769]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:20<00:04, 96.48it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1604/2000 [00:20<00:07, 54.79it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1614/2000 [00:20<00:06, 62.72it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1624/2000 [00:20<00:05, 69.95it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1634/2000 [00:20<00:04, 76.32it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1644/2000 [00:20<00:04, 81.88it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1654/2000 [00:21<00:04, 86.01it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1664/2000 [00:21<00:03, 89.31it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:21<00:03, 91.55it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1684/2000 [00:21<00:03, 93.35it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:21<00:03, 94.31it/s, train_loss=0.4600, val_loss=0.4583]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:21<00:03, 94.31it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1704/2000 [00:21<00:05, 53.54it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:21<00:04, 62.14it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1725/2000 [00:22<00:03, 70.89it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:22<00:03, 78.40it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1747/2000 [00:22<00:03, 84.18it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1758/2000 [00:22<00:02, 88.77it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1768/2000 [00:22<00:02, 91.69it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1778/2000 [00:22<00:02, 93.94it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1789/2000 [00:22<00:02, 95.90it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:22<00:02, 97.14it/s, train_loss=0.4572, val_loss=0.4600]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1800/2000 [00:23<00:02, 97.14it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1810/2000 [00:23<00:03, 55.42it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1821/2000 [00:23<00:02, 64.35it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1832/2000 [00:23<00:02, 72.31it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1843/2000 [00:23<00:01, 79.22it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1853/2000 [00:23<00:01, 83.70it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1863/2000 [00:23<00:01, 87.76it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1874/2000 [00:23<00:01, 91.65it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1884/2000 [00:23<00:01, 93.65it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1895/2000 [00:24<00:01, 95.86it/s, train_loss=0.4632, val_loss=0.4763]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1895/2000 [00:24<00:01, 95.86it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1905/2000 [00:24<00:01, 54.80it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1915/2000 [00:24<00:01, 63.03it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▋| 1926/2000 [00:24<00:01, 71.33it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1937/2000 [00:24<00:00, 78.49it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1948/2000 [00:24<00:00, 84.38it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1959/2000 [00:24<00:00, 88.88it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1970/2000 [00:25<00:00, 92.18it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1981/2000 [00:25<00:00, 94.91it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 78.83it/s, train_loss=0.4570, val_loss=0.4616]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<08:51,  3.76it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 12/2000 [00:00<00:50, 39.65it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 23/2000 [00:00<00:32, 60.81it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 34/2000 [00:00<00:26, 73.88it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:23, 81.68it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 54/2000 [00:00<00:22, 87.13it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 64/2000 [00:00<00:21, 90.90it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 75/2000 [00:01<00:20, 93.98it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 85/2000 [00:01<00:20, 95.31it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:19, 96.31it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 95/2000 [00:01<00:19, 96.31it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 105/2000 [00:01<00:35, 53.91it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 116/2000 [00:01<00:29, 63.49it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 127/2000 [00:01<00:26, 71.79it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 138/2000 [00:01<00:23, 78.80it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 148/2000 [00:02<00:22, 83.70it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 159/2000 [00:02<00:20, 88.39it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 169/2000 [00:02<00:20, 91.42it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:19, 94.05it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 190/2000 [00:02<00:18, 95.35it/s, train_loss=2.3566, val_loss=2.3603]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 190/2000 [00:02<00:18, 95.35it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 201/2000 [00:02<00:33, 54.30it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 211/2000 [00:02<00:28, 62.31it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 222/2000 [00:03<00:25, 70.70it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 233/2000 [00:03<00:22, 77.81it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:21, 82.90it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 254/2000 [00:03<00:19, 87.82it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 265/2000 [00:03<00:18, 91.67it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 276/2000 [00:03<00:18, 94.34it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 287/2000 [00:03<00:17, 96.13it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:03<00:17, 97.00it/s, train_loss=1.8493, val_loss=1.8597]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:17, 97.00it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 307/2000 [00:04<00:30, 55.01it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 317/2000 [00:04<00:26, 63.28it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 327/2000 [00:04<00:23, 70.73it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 338/2000 [00:04<00:21, 78.09it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 348/2000 [00:04<00:19, 83.31it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 359/2000 [00:04<00:18, 88.12it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 369/2000 [00:04<00:17, 91.04it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 380/2000 [00:04<00:17, 93.82it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:04<00:16, 95.56it/s, train_loss=1.1306, val_loss=1.1259]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 391/2000 [00:05<00:16, 95.56it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 401/2000 [00:05<00:29, 54.08it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 411/2000 [00:05<00:25, 62.19it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 421/2000 [00:05<00:22, 69.62it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 432/2000 [00:05<00:20, 77.30it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 442/2000 [00:05<00:18, 82.32it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 453/2000 [00:05<00:17, 87.31it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 464/2000 [00:06<00:16, 91.11it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 475/2000 [00:06<00:16, 93.85it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 486/2000 [00:06<00:15, 95.69it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:06<00:15, 97.52it/s, train_loss=0.7434, val_loss=0.7362]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 497/2000 [00:06<00:15, 97.52it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 507/2000 [00:06<00:26, 56.03it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 518/2000 [00:06<00:22, 64.86it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 529/2000 [00:06<00:20, 72.90it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 540/2000 [00:07<00:18, 79.64it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 551/2000 [00:07<00:17, 84.97it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 561/2000 [00:07<00:16, 88.66it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▊       | 572/2000 [00:07<00:15, 91.98it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 583/2000 [00:07<00:14, 94.64it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 96.08it/s, train_loss=0.6322, val_loss=0.6356]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 593/2000 [00:07<00:14, 96.08it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 603/2000 [00:07<00:25, 54.61it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 613/2000 [00:08<00:22, 62.85it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 623/2000 [00:08<00:19, 70.48it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 634/2000 [00:08<00:17, 77.88it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 645/2000 [00:08<00:16, 83.90it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 656/2000 [00:08<00:15, 88.36it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 667/2000 [00:08<00:14, 91.58it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 677/2000 [00:08<00:14, 93.54it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:08<00:13, 95.72it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:08<00:13, 97.41it/s, train_loss=0.5821, val_loss=0.5852]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 699/2000 [00:09<00:13, 97.41it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 709/2000 [00:09<00:23, 55.61it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 719/2000 [00:09<00:20, 63.79it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 730/2000 [00:09<00:17, 72.12it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 740/2000 [00:09<00:16, 78.29it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 750/2000 [00:09<00:14, 83.56it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 760/2000 [00:09<00:14, 87.80it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 770/2000 [00:09<00:13, 90.98it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 781/2000 [00:09<00:12, 94.06it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 95.68it/s, train_loss=0.5416, val_loss=0.5395]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 791/2000 [00:10<00:12, 95.68it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 801/2000 [00:10<00:22, 53.66it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 811/2000 [00:10<00:19, 61.87it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 821/2000 [00:10<00:16, 69.72it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 831/2000 [00:10<00:15, 76.45it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 842/2000 [00:10<00:13, 82.73it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:11<00:13, 87.85it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 864/2000 [00:11<00:12, 91.58it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:11<00:11, 94.19it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 885/2000 [00:11<00:11, 95.68it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:11<00:11, 97.30it/s, train_loss=0.5098, val_loss=0.5045]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 896/2000 [00:11<00:11, 97.30it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 906/2000 [00:11<00:19, 55.25it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 916/2000 [00:11<00:17, 63.39it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 927/2000 [00:12<00:14, 71.81it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 938/2000 [00:12<00:13, 78.82it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:12<00:12, 83.83it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 959/2000 [00:12<00:11, 88.37it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 969/2000 [00:12<00:11, 91.25it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:12<00:10, 93.55it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 990/2000 [00:12<00:10, 95.78it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:12<00:10, 96.80it/s, train_loss=0.4992, val_loss=0.5026]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1000/2000 [00:13<00:10, 96.80it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1010/2000 [00:13<00:18, 54.74it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:13<00:15, 63.11it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:13<00:13, 70.61it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:13<00:12, 77.83it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1051/2000 [00:13<00:11, 83.19it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1061/2000 [00:13<00:10, 87.32it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1071/2000 [00:13<00:10, 90.59it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:13<00:09, 93.17it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:13<00:09, 95.03it/s, train_loss=0.5054, val_loss=0.5005]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:14<00:09, 95.03it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:14<00:17, 50.89it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:14<00:15, 59.23it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1122/2000 [00:14<00:12, 68.20it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1133/2000 [00:14<00:11, 75.99it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1143/2000 [00:14<00:10, 81.47it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1154/2000 [00:14<00:09, 86.73it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1164/2000 [00:14<00:09, 89.74it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1175/2000 [00:15<00:08, 93.09it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1185/2000 [00:15<00:08, 94.92it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:15<00:08, 96.29it/s, train_loss=0.4896, val_loss=0.4894]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:15<00:08, 96.29it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1205/2000 [00:15<00:14, 54.15it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1215/2000 [00:15<00:12, 62.45it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████▏   | 1226/2000 [00:15<00:10, 71.09it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1237/2000 [00:15<00:09, 78.39it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:16<00:08, 84.07it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1258/2000 [00:16<00:08, 87.98it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1269/2000 [00:16<00:07, 91.83it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1280/2000 [00:16<00:07, 94.40it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1291/2000 [00:16<00:07, 96.17it/s, train_loss=0.4963, val_loss=0.5014]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▍   | 1291/2000 [00:16<00:07, 96.17it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1301/2000 [00:16<00:12, 54.76it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1311/2000 [00:17<00:10, 62.92it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1322/2000 [00:17<00:09, 71.30it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1333/2000 [00:17<00:08, 78.49it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1344/2000 [00:17<00:07, 84.47it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1355/2000 [00:17<00:07, 88.98it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1366/2000 [00:17<00:06, 92.56it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1377/2000 [00:17<00:06, 94.95it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1388/2000 [00:17<00:06, 96.83it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:17<00:06, 98.17it/s, train_loss=0.4715, val_loss=0.4821]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1399/2000 [00:18<00:06, 98.17it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1410/2000 [00:18<00:10, 57.21it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1421/2000 [00:18<00:08, 65.96it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1432/2000 [00:18<00:07, 73.65it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1443/2000 [00:18<00:06, 80.32it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1454/2000 [00:18<00:06, 85.60it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1465/2000 [00:18<00:05, 90.04it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1476/2000 [00:18<00:05, 93.45it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1487/2000 [00:19<00:05, 95.59it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 96.75it/s, train_loss=0.4582, val_loss=0.4717]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1497/2000 [00:19<00:05, 96.75it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1507/2000 [00:19<00:08, 55.37it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1518/2000 [00:19<00:07, 64.39it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1528/2000 [00:19<00:06, 70.38it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1538/2000 [00:19<00:06, 76.90it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1548/2000 [00:19<00:05, 82.37it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1559/2000 [00:20<00:05, 87.19it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1570/2000 [00:20<00:04, 91.18it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1581/2000 [00:20<00:04, 94.11it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:20<00:04, 95.39it/s, train_loss=0.4734, val_loss=0.4720]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1591/2000 [00:20<00:04, 95.39it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1601/2000 [00:20<00:07, 53.12it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1611/2000 [00:20<00:06, 61.31it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1620/2000 [00:20<00:05, 66.51it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1630/2000 [00:21<00:05, 73.56it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1640/2000 [00:21<00:04, 79.72it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1651/2000 [00:21<00:04, 85.40it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1662/2000 [00:21<00:03, 89.73it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▎ | 1673/2000 [00:21<00:03, 92.77it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1684/2000 [00:21<00:03, 95.09it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:21<00:03, 96.28it/s, train_loss=0.4789, val_loss=0.4767]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1694/2000 [00:22<00:03, 96.28it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1704/2000 [00:22<00:06, 46.14it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:22<00:05, 54.54it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1724/2000 [00:22<00:04, 62.95it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1734/2000 [00:22<00:03, 70.64it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1745/2000 [00:22<00:03, 78.14it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1756/2000 [00:22<00:02, 83.97it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1766/2000 [00:22<00:02, 87.50it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1776/2000 [00:22<00:02, 90.46it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1787/2000 [00:22<00:02, 93.41it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:23<00:02, 94.98it/s, train_loss=0.4704, val_loss=0.4830]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1797/2000 [00:23<00:02, 94.98it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1807/2000 [00:23<00:03, 53.85it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:23<00:02, 63.17it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1828/2000 [00:23<00:02, 70.66it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1839/2000 [00:23<00:02, 77.95it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1849/2000 [00:23<00:01, 82.87it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1860/2000 [00:24<00:01, 87.82it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1870/2000 [00:24<00:01, 90.80it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:24<00:01, 92.56it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:24<00:01, 95.29it/s, train_loss=0.4651, val_loss=0.4660]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:24<00:01, 95.29it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:24<00:01, 53.34it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:24<00:01, 61.38it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1921/2000 [00:24<00:01, 69.10it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1931/2000 [00:25<00:00, 76.02it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1941/2000 [00:25<00:00, 81.59it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:25<00:00, 85.90it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1961/2000 [00:25<00:00, 89.39it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1972/2000 [00:25<00:00, 92.73it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1983/2000 [00:25<00:00, 95.07it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:25<00:00, 77.79it/s, train_loss=0.4715, val_loss=0.4574]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:30,  3.51it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:56, 35.30it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:35, 55.00it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:28, 68.18it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 40/2000 [00:00<00:26, 72.68it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   2%|▏         | 49/2000 [00:00<00:27, 71.70it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 59/2000 [00:00<00:24, 78.39it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   3%|▎         | 69/2000 [00:01<00:23, 83.02it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 79/2000 [00:01<00:22, 87.15it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   4%|▍         | 89/2000 [00:01<00:21, 89.65it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:20, 90.73it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:20, 90.73it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   5%|▌         | 109/2000 [00:01<00:35, 52.95it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▌         | 119/2000 [00:01<00:30, 61.27it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   6%|▋         | 128/2000 [00:01<00:27, 67.25it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 138/2000 [00:02<00:25, 73.46it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   7%|▋         | 147/2000 [00:02<00:24, 76.71it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 157/2000 [00:02<00:22, 80.82it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   8%|▊         | 167/2000 [00:02<00:21, 84.00it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 177/2000 [00:02<00:21, 86.43it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):   9%|▉         | 187/2000 [00:02<00:20, 88.37it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:21, 85.21it/s, train_loss=2.3316, val_loss=2.3342]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|▉         | 197/2000 [00:03<00:21, 85.21it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  10%|█         | 206/2000 [00:03<00:37, 47.42it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:31, 55.91it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  11%|█▏        | 226/2000 [00:03<00:27, 63.91it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 236/2000 [00:03<00:24, 71.16it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  12%|█▏        | 246/2000 [00:03<00:22, 77.42it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 256/2000 [00:03<00:21, 82.09it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  13%|█▎        | 266/2000 [00:03<00:20, 85.03it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 276/2000 [00:03<00:19, 87.85it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  14%|█▍        | 286/2000 [00:03<00:19, 89.58it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:04<00:18, 91.04it/s, train_loss=1.8084, val_loss=1.8122]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▍        | 296/2000 [00:04<00:18, 91.04it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:04<00:31, 53.46it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▌        | 316/2000 [00:04<00:27, 61.68it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  16%|█▋        | 326/2000 [00:04<00:24, 69.13it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 336/2000 [00:04<00:22, 74.78it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  17%|█▋        | 346/2000 [00:04<00:20, 79.64it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 356/2000 [00:04<00:19, 83.30it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  18%|█▊        | 366/2000 [00:05<00:18, 86.58it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 376/2000 [00:05<00:18, 89.07it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  19%|█▉        | 386/2000 [00:05<00:17, 90.30it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:17, 91.64it/s, train_loss=1.1453, val_loss=1.1371]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:17, 91.64it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  20%|██        | 406/2000 [00:05<00:29, 54.02it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██        | 416/2000 [00:05<00:25, 62.06it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  21%|██▏       | 426/2000 [00:05<00:22, 69.31it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 436/2000 [00:06<00:20, 75.07it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  22%|██▏       | 446/2000 [00:06<00:19, 79.86it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 456/2000 [00:06<00:18, 83.45it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  23%|██▎       | 466/2000 [00:06<00:17, 86.34it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 476/2000 [00:06<00:17, 88.79it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  24%|██▍       | 486/2000 [00:06<00:16, 90.23it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:06<00:16, 91.57it/s, train_loss=0.7568, val_loss=0.7716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:07<00:16, 91.57it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:07<00:27, 53.69it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▌       | 515/2000 [00:07<00:24, 59.94it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  26%|██▋       | 525/2000 [00:07<00:21, 67.16it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 535/2000 [00:07<00:19, 73.70it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  27%|██▋       | 545/2000 [00:07<00:18, 79.05it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 555/2000 [00:07<00:17, 83.12it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  28%|██▊       | 565/2000 [00:07<00:16, 85.59it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 575/2000 [00:07<00:16, 87.92it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  29%|██▉       | 585/2000 [00:07<00:15, 89.09it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:15, 89.99it/s, train_loss=0.5985, val_loss=0.6150]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|██▉       | 595/2000 [00:08<00:15, 89.99it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  30%|███       | 605/2000 [00:08<00:26, 51.97it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███       | 615/2000 [00:08<00:23, 60.10it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  31%|███▏      | 625/2000 [00:08<00:20, 67.51it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 635/2000 [00:08<00:18, 73.56it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  32%|███▏      | 645/2000 [00:08<00:17, 78.16it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 655/2000 [00:08<00:16, 81.82it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  33%|███▎      | 665/2000 [00:09<00:15, 85.06it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 675/2000 [00:09<00:15, 87.12it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  34%|███▍      | 685/2000 [00:09<00:14, 89.21it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 695/2000 [00:09<00:14, 90.18it/s, train_loss=0.5658, val_loss=0.5537]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▍      | 695/2000 [00:09<00:14, 90.18it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  35%|███▌      | 705/2000 [00:09<00:24, 52.69it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▌      | 715/2000 [00:09<00:21, 60.35it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  36%|███▋      | 725/2000 [00:09<00:18, 67.49it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 735/2000 [00:10<00:17, 73.63it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  37%|███▋      | 744/2000 [00:10<00:16, 77.16it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:10<00:15, 81.65it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  38%|███▊      | 763/2000 [00:10<00:14, 83.76it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▊      | 773/2000 [00:10<00:14, 86.54it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:10<00:13, 88.68it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 793/2000 [00:10<00:13, 90.00it/s, train_loss=0.5435, val_loss=0.5363]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|███▉      | 793/2000 [00:11<00:13, 90.00it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  40%|████      | 803/2000 [00:11<00:23, 50.68it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 813/2000 [00:11<00:20, 58.72it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  41%|████      | 823/2000 [00:11<00:17, 66.31it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 833/2000 [00:11<00:15, 73.00it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  42%|████▏     | 843/2000 [00:11<00:14, 78.42it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 852/2000 [00:11<00:14, 80.51it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:11<00:13, 83.88it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▎     | 872/2000 [00:11<00:13, 85.99it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  44%|████▍     | 882/2000 [00:11<00:12, 87.34it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:12<00:12, 89.03it/s, train_loss=0.5248, val_loss=0.5180]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▍     | 892/2000 [00:12<00:12, 89.03it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  45%|████▌     | 902/2000 [00:12<00:21, 51.55it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 911/2000 [00:12<00:18, 58.39it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▌     | 921/2000 [00:12<00:16, 65.70it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  46%|████▋     | 930/2000 [00:12<00:15, 70.28it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 939/2000 [00:12<00:14, 72.98it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  47%|████▋     | 948/2000 [00:12<00:13, 75.94it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:13<00:13, 78.33it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  48%|████▊     | 966/2000 [00:13<00:12, 81.26it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 976/2000 [00:13<00:12, 84.03it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  49%|████▉     | 986/2000 [00:13<00:11, 86.19it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 996/2000 [00:13<00:11, 88.31it/s, train_loss=0.5089, val_loss=0.5113]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|████▉     | 996/2000 [00:13<00:11, 88.31it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:13<00:19, 50.95it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1013/2000 [00:13<00:17, 56.18it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  51%|█████     | 1021/2000 [00:14<00:16, 60.67it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:14<00:14, 65.96it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▏    | 1040/2000 [00:14<00:13, 72.75it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  52%|█████▎    | 1050/2000 [00:14<00:12, 77.91it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  53%|█████▎    | 1060/2000 [00:14<00:11, 81.96it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▎    | 1070/2000 [00:14<00:10, 84.82it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  54%|█████▍    | 1080/2000 [00:14<00:10, 87.69it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▍    | 1090/2000 [00:14<00:10, 88.62it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:14<00:10, 89.06it/s, train_loss=0.5286, val_loss=0.5330]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  55%|█████▌    | 1100/2000 [00:15<00:10, 89.06it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1110/2000 [00:15<00:16, 52.40it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:15<00:14, 60.46it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  56%|█████▋    | 1129/2000 [00:15<00:13, 66.50it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1138/2000 [00:15<00:12, 71.09it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  57%|█████▋    | 1148/2000 [00:15<00:11, 76.37it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1157/2000 [00:15<00:10, 79.10it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  58%|█████▊    | 1167/2000 [00:15<00:10, 82.71it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1177/2000 [00:16<00:09, 85.57it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  59%|█████▉    | 1186/2000 [00:16<00:09, 82.42it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:16<00:09, 83.46it/s, train_loss=0.4823, val_loss=0.4983]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|█████▉    | 1195/2000 [00:16<00:09, 83.46it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  60%|██████    | 1204/2000 [00:16<00:16, 48.27it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1213/2000 [00:16<00:14, 55.42it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  61%|██████    | 1222/2000 [00:16<00:12, 62.30it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:16<00:11, 68.40it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  62%|██████▏   | 1241/2000 [00:17<00:10, 74.49it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1251/2000 [00:17<00:09, 79.98it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  63%|██████▎   | 1261/2000 [00:17<00:08, 83.39it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▎   | 1271/2000 [00:17<00:08, 85.94it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1280/2000 [00:17<00:08, 86.93it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  64%|██████▍   | 1290/2000 [00:17<00:07, 89.40it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:17<00:07, 90.40it/s, train_loss=0.4725, val_loss=0.4780]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  65%|██████▌   | 1300/2000 [00:17<00:07, 90.40it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1310/2000 [00:18<00:13, 51.05it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▌   | 1320/2000 [00:18<00:11, 59.42it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  66%|██████▋   | 1330/2000 [00:18<00:09, 67.17it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1339/2000 [00:18<00:09, 70.40it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  67%|██████▋   | 1348/2000 [00:18<00:08, 73.65it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1357/2000 [00:18<00:08, 76.65it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  68%|██████▊   | 1366/2000 [00:18<00:08, 75.08it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1375/2000 [00:18<00:08, 77.79it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  69%|██████▉   | 1384/2000 [00:18<00:07, 80.39it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:19<00:07, 82.90it/s, train_loss=0.4601, val_loss=0.4836]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|██████▉   | 1393/2000 [00:19<00:07, 82.90it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  70%|███████   | 1402/2000 [00:19<00:12, 47.04it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1412/2000 [00:19<00:10, 55.93it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  71%|███████   | 1421/2000 [00:19<00:09, 62.56it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1430/2000 [00:19<00:08, 68.29it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1439/2000 [00:19<00:07, 70.84it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  72%|███████▏  | 1448/2000 [00:20<00:08, 63.84it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1456/2000 [00:20<00:08, 66.25it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  73%|███████▎  | 1464/2000 [00:20<00:07, 69.32it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▎  | 1473/2000 [00:20<00:07, 73.26it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  74%|███████▍  | 1482/2000 [00:20<00:06, 77.31it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:20<00:06, 81.63it/s, train_loss=0.4752, val_loss=0.4929]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▍  | 1492/2000 [00:20<00:06, 81.63it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  75%|███████▌  | 1501/2000 [00:20<00:10, 47.44it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1510/2000 [00:21<00:08, 55.02it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▌  | 1520/2000 [00:21<00:07, 63.31it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  76%|███████▋  | 1530/2000 [00:21<00:06, 70.76it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  77%|███████▋  | 1540/2000 [00:21<00:05, 76.82it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1550/2000 [00:21<00:05, 81.53it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1560/2000 [00:21<00:05, 84.29it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  78%|███████▊  | 1569/2000 [00:21<00:05, 84.13it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1579/2000 [00:21<00:04, 87.05it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  79%|███████▉  | 1589/2000 [00:21<00:04, 88.52it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:22<00:04, 90.12it/s, train_loss=0.4713, val_loss=0.4785]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|███████▉  | 1599/2000 [00:22<00:04, 90.12it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  80%|████████  | 1609/2000 [00:22<00:07, 50.96it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████  | 1619/2000 [00:22<00:06, 59.23it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:22<00:05, 66.87it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1639/2000 [00:22<00:04, 73.14it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  82%|████████▏ | 1649/2000 [00:22<00:04, 78.58it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1659/2000 [00:22<00:04, 82.21it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:23<00:03, 85.21it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1679/2000 [00:23<00:03, 87.25it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  84%|████████▍ | 1689/2000 [00:23<00:03, 88.60it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:23<00:03, 89.85it/s, train_loss=0.4699, val_loss=0.4681]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▍ | 1699/2000 [00:23<00:03, 89.85it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  85%|████████▌ | 1709/2000 [00:23<00:05, 53.31it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▌ | 1719/2000 [00:23<00:04, 61.56it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  86%|████████▋ | 1729/2000 [00:23<00:03, 68.67it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1738/2000 [00:24<00:03, 70.52it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  87%|████████▋ | 1748/2000 [00:24<00:03, 76.15it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1758/2000 [00:24<00:02, 81.17it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  88%|████████▊ | 1768/2000 [00:24<00:02, 85.21it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1778/2000 [00:24<00:02, 88.48it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  89%|████████▉ | 1788/2000 [00:24<00:02, 90.44it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:24<00:02, 92.03it/s, train_loss=0.4600, val_loss=0.4739]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|████████▉ | 1798/2000 [00:24<00:02, 92.03it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  90%|█████████ | 1808/2000 [00:25<00:03, 53.98it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:25<00:02, 62.22it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  91%|█████████▏| 1827/2000 [00:25<00:02, 66.97it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1837/2000 [00:25<00:02, 73.63it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  92%|█████████▏| 1846/2000 [00:25<00:02, 74.75it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1855/2000 [00:25<00:01, 73.69it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  93%|█████████▎| 1864/2000 [00:25<00:01, 75.87it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▎| 1873/2000 [00:25<00:01, 79.07it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  94%|█████████▍| 1883/2000 [00:25<00:01, 83.07it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1893/2000 [00:26<00:01, 86.09it/s, train_loss=0.4624, val_loss=0.4731]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▍| 1893/2000 [00:26<00:01, 86.09it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  95%|█████████▌| 1902/2000 [00:26<00:01, 49.62it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1912/2000 [00:26<00:01, 58.06it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:26<00:01, 66.14it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1932/2000 [00:26<00:00, 72.92it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  97%|█████████▋| 1942/2000 [00:26<00:00, 78.55it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1952/2000 [00:26<00:00, 82.72it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  98%|█████████▊| 1962/2000 [00:27<00:00, 81.30it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▊| 1972/2000 [00:27<00:00, 84.97it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam):  99%|█████████▉| 1982/2000 [00:27<00:00, 87.63it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:27<00:00, 72.74it/s, train_loss=0.4568, val_loss=0.4716]\u001B[A\u001B[A\u001B[A\n",
      "\n",
      "\n",
      "Beta2: 100%|██████████| 2/2 [02:35<00:00, 77.80s/it]\u001B[A\u001B[A\n",
      "\n",
      "                                                    \u001B[A\u001B[A\n",
      "Adam LR: 100%|██████████| 3/3 [07:49<00:00, 156.11s/it]\u001B[A\n",
      "Constant LR: 100%|██████████| 3/3 [27:06<00:00, 542.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW: lr=0.001, beta2=0.999, avg_val_loss=0.4635\n",
      "\n",
      "Best Parameters: {'constant': 0.005, 'adam': 0.001, 'beta2': 0.999}\n",
      "\n",
      "Grid Search Results Table:\n",
      "| optimizer    |     lr | beta2   |   avg_val_loss |\n",
      "|:-------------|-------:|:--------|---------------:|\n",
      "| SGD Constant | 0.005  | N/A     |       2.17043  |\n",
      "| AdamW        | 0.0001 | 0.98    |       1.43556  |\n",
      "| AdamW        | 0.0001 | 0.999   |       0.919496 |\n",
      "| AdamW        | 0.0005 | 0.98    |       0.477582 |\n",
      "| AdamW        | 0.0005 | 0.999   |       0.469358 |\n",
      "| AdamW        | 0.001  | 0.98    |       0.472907 |\n",
      "| AdamW        | 0.001  | 0.999   |       0.466452 |\n",
      "| SGD Constant | 0.01   | N/A     |       1.83359  |\n",
      "| AdamW        | 0.0001 | 0.98    |       1.43669  |\n",
      "| AdamW        | 0.0001 | 0.999   |       0.9183   |\n",
      "| AdamW        | 0.0005 | 0.98    |       0.478004 |\n",
      "| AdamW        | 0.0005 | 0.999   |       0.472984 |\n",
      "| AdamW        | 0.001  | 0.98    |       0.472111 |\n",
      "| AdamW        | 0.001  | 0.999   |       0.465839 |\n",
      "| SGD Constant | 0.02   | N/A     |       1.47112  |\n",
      "| AdamW        | 0.0001 | 0.98    |       1.43507  |\n",
      "| AdamW        | 0.0001 | 0.999   |       0.916789 |\n",
      "| AdamW        | 0.0005 | 0.98    |       0.480777 |\n",
      "| AdamW        | 0.0005 | 0.999   |       0.473485 |\n",
      "| AdamW        | 0.001  | 0.98    |       0.476877 |\n",
      "| AdamW        | 0.001  | 0.999   |       0.463522 |\n",
      "\n",
      "Running final training with best parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trials:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1/3:\n",
      "Training with Constant SGD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<09:34,  3.48it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   1%|          | 19/2000 [00:00<00:32, 61.59it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   2%|▏         | 37/2000 [00:00<00:20, 97.95it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   3%|▎         | 55/2000 [00:00<00:15, 123.26it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   4%|▎         | 73/2000 [00:00<00:13, 139.58it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   5%|▍         | 91/2000 [00:00<00:12, 149.95it/s, train_loss=3.6847, val_loss=3.6854]\u001B[A\n",
      "Training (constant):   5%|▍         | 91/2000 [00:01<00:12, 149.95it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   5%|▌         | 108/2000 [00:01<00:21, 87.37it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   6%|▋         | 126/2000 [00:01<00:17, 104.67it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   7%|▋         | 144/2000 [00:01<00:15, 120.60it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   8%|▊         | 162/2000 [00:01<00:13, 134.19it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):   9%|▉         | 180/2000 [00:01<00:12, 145.41it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):  10%|▉         | 198/2000 [00:01<00:11, 153.10it/s, train_loss=3.2878, val_loss=3.2905]\u001B[A\n",
      "Training (constant):  10%|▉         | 198/2000 [00:01<00:11, 153.10it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  11%|█         | 215/2000 [00:02<00:19, 92.99it/s, train_loss=3.0945, val_loss=3.0954] \u001B[A\n",
      "Training (constant):  12%|█▏        | 234/2000 [00:02<00:16, 109.99it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  13%|█▎        | 252/2000 [00:02<00:14, 123.87it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  13%|█▎        | 268/2000 [00:02<00:13, 129.16it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  14%|█▍        | 286/2000 [00:02<00:12, 140.88it/s, train_loss=3.0945, val_loss=3.0954]\u001B[A\n",
      "Training (constant):  14%|█▍        | 286/2000 [00:02<00:12, 140.88it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  15%|█▌        | 302/2000 [00:02<00:19, 87.17it/s, train_loss=2.9476, val_loss=2.9521] \u001B[A\n",
      "Training (constant):  16%|█▌        | 320/2000 [00:02<00:16, 103.80it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  17%|█▋        | 338/2000 [00:03<00:13, 118.87it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  18%|█▊        | 357/2000 [00:03<00:12, 133.15it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  19%|█▉        | 375/2000 [00:03<00:11, 143.59it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  20%|█▉        | 393/2000 [00:03<00:10, 152.22it/s, train_loss=2.9476, val_loss=2.9521]\u001B[A\n",
      "Training (constant):  20%|█▉        | 393/2000 [00:03<00:10, 152.22it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  20%|██        | 410/2000 [00:03<00:17, 92.98it/s, train_loss=2.8402, val_loss=2.8529] \u001B[A\n",
      "Training (constant):  21%|██▏       | 429/2000 [00:03<00:14, 109.71it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  22%|██▏       | 447/2000 [00:03<00:12, 123.44it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  23%|██▎       | 465/2000 [00:03<00:11, 136.22it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  24%|██▍       | 483/2000 [00:04<00:10, 146.51it/s, train_loss=2.8402, val_loss=2.8529]\u001B[A\n",
      "Training (constant):  24%|██▍       | 483/2000 [00:04<00:10, 146.51it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  25%|██▌       | 501/2000 [00:04<00:16, 90.34it/s, train_loss=2.7599, val_loss=2.7619] \u001B[A\n",
      "Training (constant):  26%|██▌       | 519/2000 [00:04<00:13, 106.14it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  27%|██▋       | 537/2000 [00:04<00:12, 120.53it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  28%|██▊       | 555/2000 [00:04<00:10, 133.32it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  29%|██▊       | 573/2000 [00:04<00:09, 144.21it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  30%|██▉       | 591/2000 [00:04<00:09, 152.96it/s, train_loss=2.7599, val_loss=2.7619]\u001B[A\n",
      "Training (constant):  30%|██▉       | 591/2000 [00:05<00:09, 152.96it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  30%|███       | 609/2000 [00:05<00:15, 91.83it/s, train_loss=2.6856, val_loss=2.6849] \u001B[A\n",
      "Training (constant):  31%|███▏      | 627/2000 [00:05<00:12, 107.47it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  32%|███▏      | 645/2000 [00:05<00:11, 122.09it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  33%|███▎      | 663/2000 [00:05<00:09, 134.76it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  34%|███▍      | 681/2000 [00:05<00:09, 144.75it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  35%|███▍      | 699/2000 [00:05<00:08, 152.64it/s, train_loss=2.6856, val_loss=2.6849]\u001B[A\n",
      "Training (constant):  35%|███▍      | 699/2000 [00:06<00:08, 152.64it/s, train_loss=2.6108, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  36%|███▌      | 716/2000 [00:06<00:13, 93.08it/s, train_loss=2.6108, val_loss=2.6284] \u001B[A\n",
      "Training (constant):  37%|███▋      | 734/2000 [00:06<00:11, 108.42it/s, train_loss=2.6108, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  38%|███▊      | 752/2000 [00:06<00:10, 122.21it/s, train_loss=2.6108, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  38%|███▊      | 770/2000 [00:06<00:09, 134.62it/s, train_loss=2.6108, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  39%|███▉      | 788/2000 [00:06<00:08, 145.41it/s, train_loss=2.6108, val_loss=2.6284]\u001B[A\n",
      "Training (constant):  39%|███▉      | 788/2000 [00:06<00:08, 145.41it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  40%|████      | 805/2000 [00:06<00:13, 90.42it/s, train_loss=2.5555, val_loss=2.5652] \u001B[A\n",
      "Training (constant):  41%|████      | 823/2000 [00:07<00:11, 106.34it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  42%|████▏     | 841/2000 [00:07<00:09, 121.33it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  43%|████▎     | 859/2000 [00:07<00:08, 133.00it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  44%|████▍     | 877/2000 [00:07<00:07, 143.18it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  45%|████▍     | 895/2000 [00:07<00:07, 152.03it/s, train_loss=2.5555, val_loss=2.5652]\u001B[A\n",
      "Training (constant):  45%|████▍     | 895/2000 [00:07<00:07, 152.03it/s, train_loss=2.5130, val_loss=2.5189]\u001B[A\n",
      "Training (constant):  46%|████▌     | 912/2000 [00:07<00:11, 94.46it/s, train_loss=2.5130, val_loss=2.5189] \u001B[A\n",
      "Training (constant):  47%|████▋     | 931/2000 [00:07<00:09, 111.32it/s, train_loss=2.5130, val_loss=2.5189]\u001B[A\n",
      "Training (constant):  48%|████▊     | 950/2000 [00:08<00:08, 126.67it/s, train_loss=2.5130, val_loss=2.5189]\u001B[A\n",
      "Training (constant):  48%|████▊     | 969/2000 [00:08<00:07, 139.58it/s, train_loss=2.5130, val_loss=2.5189]\u001B[A\n",
      "Training (constant):  49%|████▉     | 987/2000 [00:08<00:06, 149.43it/s, train_loss=2.5130, val_loss=2.5189]\u001B[A\n",
      "Training (constant):  49%|████▉     | 987/2000 [00:08<00:06, 149.43it/s, train_loss=2.4607, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  50%|█████     | 1004/2000 [00:08<00:10, 93.30it/s, train_loss=2.4607, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  51%|█████     | 1022/2000 [00:08<00:09, 108.38it/s, train_loss=2.4607, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1040/2000 [00:08<00:07, 122.78it/s, train_loss=2.4607, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1058/2000 [00:08<00:06, 135.73it/s, train_loss=2.4607, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1076/2000 [00:09<00:06, 146.30it/s, train_loss=2.4607, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1093/2000 [00:09<00:06, 145.65it/s, train_loss=2.4607, val_loss=2.4627]\u001B[A\n",
      "Training (constant):  55%|█████▍    | 1093/2000 [00:09<00:06, 145.65it/s, train_loss=2.4171, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  56%|█████▌    | 1110/2000 [00:09<00:09, 92.32it/s, train_loss=2.4171, val_loss=2.4188] \u001B[A\n",
      "Training (constant):  56%|█████▋    | 1129/2000 [00:09<00:07, 109.25it/s, train_loss=2.4171, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1148/2000 [00:09<00:06, 124.75it/s, train_loss=2.4171, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1166/2000 [00:09<00:06, 136.95it/s, train_loss=2.4171, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1185/2000 [00:09<00:05, 148.03it/s, train_loss=2.4171, val_loss=2.4188]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1185/2000 [00:10<00:05, 148.03it/s, train_loss=2.3681, val_loss=2.3786]\u001B[A\n",
      "Training (constant):  60%|██████    | 1202/2000 [00:10<00:08, 92.37it/s, train_loss=2.3681, val_loss=2.3786] \u001B[A\n",
      "Training (constant):  61%|██████    | 1220/2000 [00:10<00:07, 107.91it/s, train_loss=2.3681, val_loss=2.3786]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1239/2000 [00:10<00:06, 123.61it/s, train_loss=2.3681, val_loss=2.3786]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1257/2000 [00:10<00:05, 136.15it/s, train_loss=2.3681, val_loss=2.3786]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1276/2000 [00:10<00:04, 147.29it/s, train_loss=2.3681, val_loss=2.3786]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1295/2000 [00:10<00:04, 156.38it/s, train_loss=2.3681, val_loss=2.3786]\u001B[A\n",
      "Training (constant):  65%|██████▍   | 1295/2000 [00:11<00:04, 156.38it/s, train_loss=2.3563, val_loss=2.3527]\u001B[A\n",
      "Training (constant):  66%|██████▌   | 1313/2000 [00:11<00:07, 96.81it/s, train_loss=2.3563, val_loss=2.3527] \u001B[A\n",
      "Training (constant):  67%|██████▋   | 1332/2000 [00:11<00:05, 113.05it/s, train_loss=2.3563, val_loss=2.3527]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1350/2000 [00:11<00:05, 126.75it/s, train_loss=2.3563, val_loss=2.3527]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1369/2000 [00:11<00:04, 139.70it/s, train_loss=2.3563, val_loss=2.3527]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1387/2000 [00:11<00:04, 149.50it/s, train_loss=2.3563, val_loss=2.3527]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1387/2000 [00:11<00:04, 149.50it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  70%|███████   | 1404/2000 [00:11<00:06, 92.28it/s, train_loss=2.3103, val_loss=2.3096] \u001B[A\n",
      "Training (constant):  71%|███████   | 1422/2000 [00:12<00:05, 107.89it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1440/2000 [00:12<00:04, 122.46it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1458/2000 [00:12<00:04, 134.94it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1477/2000 [00:12<00:03, 146.52it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1495/2000 [00:12<00:03, 154.55it/s, train_loss=2.3103, val_loss=2.3096]\u001B[A\n",
      "Training (constant):  75%|███████▍  | 1495/2000 [00:12<00:03, 154.55it/s, train_loss=2.2904, val_loss=2.2843]\u001B[A\n",
      "Training (constant):  76%|███████▌  | 1513/2000 [00:12<00:05, 95.67it/s, train_loss=2.2904, val_loss=2.2843] \u001B[A\n",
      "Training (constant):  77%|███████▋  | 1531/2000 [00:12<00:04, 111.10it/s, train_loss=2.2904, val_loss=2.2843]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1550/2000 [00:12<00:03, 126.30it/s, train_loss=2.2904, val_loss=2.2843]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1566/2000 [00:13<00:03, 123.78it/s, train_loss=2.2904, val_loss=2.2843]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1581/2000 [00:13<00:03, 112.09it/s, train_loss=2.2904, val_loss=2.2843]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1596/2000 [00:13<00:03, 119.06it/s, train_loss=2.2904, val_loss=2.2843]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1596/2000 [00:13<00:03, 119.06it/s, train_loss=2.2514, val_loss=2.2519]\u001B[A\n",
      "Training (constant):  80%|████████  | 1610/2000 [00:13<00:05, 69.39it/s, train_loss=2.2514, val_loss=2.2519] \u001B[A\n",
      "Training (constant):  81%|████████  | 1624/2000 [00:13<00:04, 80.25it/s, train_loss=2.2514, val_loss=2.2519]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1636/2000 [00:14<00:04, 86.86it/s, train_loss=2.2514, val_loss=2.2519]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1652/2000 [00:14<00:03, 101.89it/s, train_loss=2.2514, val_loss=2.2519]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1669/2000 [00:14<00:02, 116.77it/s, train_loss=2.2514, val_loss=2.2519]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1686/2000 [00:14<00:02, 129.02it/s, train_loss=2.2514, val_loss=2.2519]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1686/2000 [00:14<00:02, 129.02it/s, train_loss=2.2235, val_loss=2.2091]\u001B[A\n",
      "Training (constant):  85%|████████▌ | 1701/2000 [00:14<00:03, 78.74it/s, train_loss=2.2235, val_loss=2.2091] \u001B[A\n",
      "Training (constant):  86%|████████▌ | 1719/2000 [00:14<00:02, 95.99it/s, train_loss=2.2235, val_loss=2.2091]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1737/2000 [00:14<00:02, 112.40it/s, train_loss=2.2235, val_loss=2.2091]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1754/2000 [00:15<00:01, 125.19it/s, train_loss=2.2235, val_loss=2.2091]\u001B[A\n",
      "Training (constant):  89%|████████▊ | 1772/2000 [00:15<00:01, 138.22it/s, train_loss=2.2235, val_loss=2.2091]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1790/2000 [00:15<00:01, 147.94it/s, train_loss=2.2235, val_loss=2.2091]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1790/2000 [00:15<00:01, 147.94it/s, train_loss=2.1788, val_loss=2.1828]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1807/2000 [00:15<00:02, 82.77it/s, train_loss=2.1788, val_loss=2.1828] \u001B[A\n",
      "Training (constant):  91%|█████████ | 1822/2000 [00:15<00:01, 93.07it/s, train_loss=2.1788, val_loss=2.1828]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1837/2000 [00:15<00:01, 103.76it/s, train_loss=2.1788, val_loss=2.1828]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1852/2000 [00:15<00:01, 112.99it/s, train_loss=2.1788, val_loss=2.1828]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1867/2000 [00:16<00:01, 121.01it/s, train_loss=2.1788, val_loss=2.1828]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1882/2000 [00:16<00:00, 122.51it/s, train_loss=2.1788, val_loss=2.1828]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1897/2000 [00:16<00:00, 127.09it/s, train_loss=2.1788, val_loss=2.1828]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1897/2000 [00:16<00:00, 127.09it/s, train_loss=2.1456, val_loss=2.1559]\u001B[A\n",
      "Training (constant):  96%|█████████▌| 1911/2000 [00:16<00:01, 71.65it/s, train_loss=2.1456, val_loss=2.1559] \u001B[A\n",
      "Training (constant):  96%|█████████▋| 1927/2000 [00:16<00:00, 85.94it/s, train_loss=2.1456, val_loss=2.1559]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1942/2000 [00:16<00:00, 98.02it/s, train_loss=2.1456, val_loss=2.1559]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1957/2000 [00:17<00:00, 108.84it/s, train_loss=2.1456, val_loss=2.1559]\u001B[A\n",
      "Training (constant):  99%|█████████▊| 1973/2000 [00:17<00:00, 118.85it/s, train_loss=2.1456, val_loss=2.1559]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:17<00:00, 115.59it/s, train_loss=2.1456, val_loss=2.1559]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Polyak SGD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (polyak):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (polyak):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   0%|          | 1/2000 [00:00<11:42,  2.85it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   1%|          | 13/2000 [00:00<00:54, 36.14it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   1%|▏         | 25/2000 [00:00<00:33, 59.47it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   2%|▏         | 36/2000 [00:00<00:26, 73.56it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   2%|▏         | 47/2000 [00:00<00:23, 84.09it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   3%|▎         | 59/2000 [00:00<00:20, 92.56it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   4%|▎         | 70/2000 [00:00<00:20, 96.37it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   4%|▍         | 81/2000 [00:01<00:19, 99.67it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   5%|▍         | 92/2000 [00:01<00:18, 100.51it/s, train_loss=3.6814, val_loss=3.6817]\u001B[A\n",
      "Training (polyak):   5%|▍         | 92/2000 [00:01<00:18, 100.51it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):   5%|▌         | 103/2000 [00:01<00:37, 51.08it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):   6%|▌         | 115/2000 [00:01<00:30, 62.20it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):   6%|▋         | 127/2000 [00:01<00:25, 72.76it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):   7%|▋         | 139/2000 [00:01<00:22, 81.84it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):   8%|▊         | 151/2000 [00:02<00:20, 89.39it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):   8%|▊         | 163/2000 [00:02<00:19, 95.57it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):   9%|▉         | 175/2000 [00:02<00:18, 100.24it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):   9%|▉         | 186/2000 [00:02<00:17, 102.74it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):  10%|▉         | 197/2000 [00:02<00:17, 103.60it/s, train_loss=2.3013, val_loss=2.2879]\u001B[A\n",
      "Training (polyak):  10%|▉         | 197/2000 [00:02<00:17, 103.60it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  10%|█         | 208/2000 [00:02<00:31, 56.70it/s, train_loss=2.2213, val_loss=2.2270] \u001B[A\n",
      "Training (polyak):  11%|█         | 219/2000 [00:02<00:26, 66.07it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  12%|█▏        | 231/2000 [00:03<00:23, 76.11it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  12%|█▏        | 241/2000 [00:03<00:22, 79.86it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  13%|█▎        | 252/2000 [00:03<00:20, 86.55it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  13%|█▎        | 264/2000 [00:03<00:18, 93.51it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  14%|█▍        | 275/2000 [00:03<00:17, 96.34it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  14%|█▍        | 287/2000 [00:03<00:17, 100.53it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  15%|█▍        | 298/2000 [00:03<00:16, 102.60it/s, train_loss=2.2213, val_loss=2.2270]\u001B[A\n",
      "Training (polyak):  15%|█▍        | 298/2000 [00:04<00:16, 102.60it/s, train_loss=2.0531, val_loss=2.0430]\u001B[A\n",
      "Training (polyak):  15%|█▌        | 309/2000 [00:04<00:30, 55.12it/s, train_loss=2.0531, val_loss=2.0430] \u001B[A\n",
      "Training (polyak):  16%|█▌        | 320/2000 [00:04<00:26, 64.57it/s, train_loss=2.0531, val_loss=2.0430]\u001B[A\n",
      "Training (polyak):  17%|█▋        | 331/2000 [00:04<00:22, 73.15it/s, train_loss=2.0531, val_loss=2.0430]\u001B[A\n",
      "Training (polyak):  17%|█▋        | 343/2000 [00:04<00:20, 82.18it/s, train_loss=2.0531, val_loss=2.0430]\u001B[A\n",
      "Training (polyak):  18%|█▊        | 355/2000 [00:04<00:18, 89.92it/s, train_loss=2.0531, val_loss=2.0430]\u001B[A\n",
      "Training (polyak):  18%|█▊        | 367/2000 [00:04<00:16, 96.23it/s, train_loss=2.0531, val_loss=2.0430]\u001B[A\n",
      "Training (polyak):  19%|█▉        | 379/2000 [00:04<00:16, 101.16it/s, train_loss=2.0531, val_loss=2.0430]\u001B[A\n",
      "Training (polyak):  20%|█▉        | 390/2000 [00:04<00:15, 102.78it/s, train_loss=2.0531, val_loss=2.0430]\u001B[A\n",
      "Training (polyak):  20%|█▉        | 390/2000 [00:05<00:15, 102.78it/s, train_loss=1.8976, val_loss=1.8994]\u001B[A\n",
      "Training (polyak):  20%|██        | 401/2000 [00:05<00:29, 53.72it/s, train_loss=1.8976, val_loss=1.8994] \u001B[A\n",
      "Training (polyak):  21%|██        | 413/2000 [00:05<00:24, 64.82it/s, train_loss=1.8976, val_loss=1.8994]\u001B[A\n",
      "Training (polyak):  21%|██▏       | 426/2000 [00:05<00:20, 77.21it/s, train_loss=1.8976, val_loss=1.8994]\u001B[A\n",
      "Training (polyak):  22%|██▏       | 440/2000 [00:05<00:17, 89.60it/s, train_loss=1.8976, val_loss=1.8994]\u001B[A\n",
      "Training (polyak):  23%|██▎       | 454/2000 [00:05<00:15, 99.85it/s, train_loss=1.8976, val_loss=1.8994]\u001B[A\n",
      "Training (polyak):  23%|██▎       | 468/2000 [00:05<00:14, 108.65it/s, train_loss=1.8976, val_loss=1.8994]\u001B[A\n",
      "Training (polyak):  24%|██▍       | 482/2000 [00:05<00:13, 114.83it/s, train_loss=1.8976, val_loss=1.8994]\u001B[A\n",
      "Training (polyak):  25%|██▍       | 496/2000 [00:06<00:12, 120.77it/s, train_loss=1.8976, val_loss=1.8994]\u001B[A\n",
      "Training (polyak):  25%|██▍       | 496/2000 [00:06<00:12, 120.77it/s, train_loss=1.7515, val_loss=1.7803]\u001B[A\n",
      "Training (polyak):  25%|██▌       | 509/2000 [00:06<00:20, 72.36it/s, train_loss=1.7515, val_loss=1.7803] \u001B[A\n",
      "Training (polyak):  26%|██▌       | 523/2000 [00:06<00:17, 84.58it/s, train_loss=1.7515, val_loss=1.7803]\u001B[A\n",
      "Training (polyak):  27%|██▋       | 537/2000 [00:06<00:15, 95.58it/s, train_loss=1.7515, val_loss=1.7803]\u001B[A\n",
      "Training (polyak):  28%|██▊       | 550/2000 [00:06<00:14, 102.33it/s, train_loss=1.7515, val_loss=1.7803]\u001B[A\n",
      "Training (polyak):  28%|██▊       | 564/2000 [00:06<00:13, 110.31it/s, train_loss=1.7515, val_loss=1.7803]\u001B[A\n",
      "Training (polyak):  29%|██▉       | 578/2000 [00:06<00:12, 117.54it/s, train_loss=1.7515, val_loss=1.7803]\u001B[A\n",
      "Training (polyak):  30%|██▉       | 592/2000 [00:07<00:11, 123.29it/s, train_loss=1.7515, val_loss=1.7803]\u001B[A\n",
      "Training (polyak):  30%|██▉       | 592/2000 [00:07<00:11, 123.29it/s, train_loss=1.4198, val_loss=1.4092]\u001B[A\n",
      "Training (polyak):  30%|███       | 606/2000 [00:07<00:19, 71.70it/s, train_loss=1.4198, val_loss=1.4092] \u001B[A\n",
      "Training (polyak):  31%|███       | 620/2000 [00:07<00:16, 83.52it/s, train_loss=1.4198, val_loss=1.4092]\u001B[A\n",
      "Training (polyak):  32%|███▏      | 634/2000 [00:07<00:14, 94.61it/s, train_loss=1.4198, val_loss=1.4092]\u001B[A\n",
      "Training (polyak):  32%|███▏      | 648/2000 [00:07<00:12, 104.06it/s, train_loss=1.4198, val_loss=1.4092]\u001B[A\n",
      "Training (polyak):  33%|███▎      | 662/2000 [00:07<00:11, 112.05it/s, train_loss=1.4198, val_loss=1.4092]\u001B[A\n",
      "Training (polyak):  34%|███▍      | 676/2000 [00:07<00:11, 118.33it/s, train_loss=1.4198, val_loss=1.4092]\u001B[A\n",
      "Training (polyak):  34%|███▍      | 690/2000 [00:08<00:10, 123.12it/s, train_loss=1.4198, val_loss=1.4092]\u001B[A\n",
      "Training (polyak):  34%|███▍      | 690/2000 [00:08<00:10, 123.12it/s, train_loss=0.9976, val_loss=0.9921]\u001B[A\n",
      "Training (polyak):  35%|███▌      | 704/2000 [00:08<00:17, 73.56it/s, train_loss=0.9976, val_loss=0.9921] \u001B[A\n",
      "Training (polyak):  36%|███▌      | 718/2000 [00:08<00:15, 85.34it/s, train_loss=0.9976, val_loss=0.9921]\u001B[A\n",
      "Training (polyak):  37%|███▋      | 732/2000 [00:08<00:13, 96.42it/s, train_loss=0.9976, val_loss=0.9921]\u001B[A\n",
      "Training (polyak):  37%|███▋      | 746/2000 [00:08<00:11, 105.90it/s, train_loss=0.9976, val_loss=0.9921]\u001B[A\n",
      "Training (polyak):  38%|███▊      | 760/2000 [00:08<00:10, 114.01it/s, train_loss=0.9976, val_loss=0.9921]\u001B[A\n",
      "Training (polyak):  39%|███▊      | 774/2000 [00:08<00:10, 120.31it/s, train_loss=0.9976, val_loss=0.9921]\u001B[A\n",
      "Training (polyak):  39%|███▉      | 788/2000 [00:08<00:09, 125.44it/s, train_loss=0.9976, val_loss=0.9921]\u001B[A\n",
      "Training (polyak):  39%|███▉      | 788/2000 [00:09<00:09, 125.44it/s, train_loss=0.8670, val_loss=0.8824]\u001B[A\n",
      "Training (polyak):  40%|████      | 802/2000 [00:09<00:16, 74.00it/s, train_loss=0.8670, val_loss=0.8824] \u001B[A\n",
      "Training (polyak):  41%|████      | 816/2000 [00:09<00:13, 85.69it/s, train_loss=0.8670, val_loss=0.8824]\u001B[A\n",
      "Training (polyak):  42%|████▏     | 830/2000 [00:09<00:12, 96.88it/s, train_loss=0.8670, val_loss=0.8824]\u001B[A\n",
      "Training (polyak):  42%|████▏     | 844/2000 [00:09<00:10, 105.99it/s, train_loss=0.8670, val_loss=0.8824]\u001B[A\n",
      "Training (polyak):  43%|████▎     | 858/2000 [00:09<00:10, 113.63it/s, train_loss=0.8670, val_loss=0.8824]\u001B[A\n",
      "Training (polyak):  44%|████▎     | 872/2000 [00:09<00:09, 120.30it/s, train_loss=0.8670, val_loss=0.8824]\u001B[A\n",
      "Training (polyak):  44%|████▍     | 886/2000 [00:09<00:08, 125.09it/s, train_loss=0.8670, val_loss=0.8824]\u001B[A\n",
      "Training (polyak):  45%|████▌     | 900/2000 [00:10<00:08, 128.41it/s, train_loss=0.8670, val_loss=0.8824]\u001B[A\n",
      "Training (polyak):  45%|████▌     | 900/2000 [00:10<00:08, 128.41it/s, train_loss=0.7127, val_loss=0.7064]\u001B[A\n",
      "Training (polyak):  46%|████▌     | 914/2000 [00:10<00:14, 74.71it/s, train_loss=0.7127, val_loss=0.7064] \u001B[A\n",
      "Training (polyak):  46%|████▋     | 928/2000 [00:10<00:12, 86.34it/s, train_loss=0.7127, val_loss=0.7064]\u001B[A\n",
      "Training (polyak):  47%|████▋     | 942/2000 [00:10<00:10, 96.90it/s, train_loss=0.7127, val_loss=0.7064]\u001B[A\n",
      "Training (polyak):  48%|████▊     | 956/2000 [00:10<00:09, 106.38it/s, train_loss=0.7127, val_loss=0.7064]\u001B[A\n",
      "Training (polyak):  48%|████▊     | 970/2000 [00:10<00:09, 114.02it/s, train_loss=0.7127, val_loss=0.7064]\u001B[A\n",
      "Training (polyak):  49%|████▉     | 984/2000 [00:10<00:08, 120.36it/s, train_loss=0.7127, val_loss=0.7064]\u001B[A\n",
      "Training (polyak):  50%|████▉     | 998/2000 [00:11<00:08, 125.17it/s, train_loss=0.7127, val_loss=0.7064]\u001B[A\n",
      "Training (polyak):  50%|████▉     | 998/2000 [00:11<00:08, 125.17it/s, train_loss=0.6713, val_loss=0.6736]\u001B[A\n",
      "Training (polyak):  51%|█████     | 1012/2000 [00:11<00:13, 75.29it/s, train_loss=0.6713, val_loss=0.6736]\u001B[A\n",
      "Training (polyak):  51%|█████▏    | 1026/2000 [00:11<00:11, 87.25it/s, train_loss=0.6713, val_loss=0.6736]\u001B[A\n",
      "Training (polyak):  52%|█████▏    | 1040/2000 [00:11<00:09, 98.11it/s, train_loss=0.6713, val_loss=0.6736]\u001B[A\n",
      "Training (polyak):  53%|█████▎    | 1054/2000 [00:11<00:08, 107.26it/s, train_loss=0.6713, val_loss=0.6736]\u001B[A\n",
      "Training (polyak):  53%|█████▎    | 1068/2000 [00:11<00:08, 114.26it/s, train_loss=0.6713, val_loss=0.6736]\u001B[A\n",
      "Training (polyak):  54%|█████▍    | 1082/2000 [00:11<00:07, 120.49it/s, train_loss=0.6713, val_loss=0.6736]\u001B[A\n",
      "Training (polyak):  55%|█████▍    | 1096/2000 [00:12<00:07, 125.36it/s, train_loss=0.6713, val_loss=0.6736]\u001B[A\n",
      "Training (polyak):  55%|█████▍    | 1096/2000 [00:12<00:07, 125.36it/s, train_loss=0.6750, val_loss=0.6750]\u001B[A\n",
      "Training (polyak):  56%|█████▌    | 1110/2000 [00:12<00:12, 73.46it/s, train_loss=0.6750, val_loss=0.6750] \u001B[A\n",
      "Training (polyak):  56%|█████▌    | 1121/2000 [00:12<00:11, 79.82it/s, train_loss=0.6750, val_loss=0.6750]\u001B[A\n",
      "Training (polyak):  57%|█████▋    | 1133/2000 [00:12<00:09, 87.67it/s, train_loss=0.6750, val_loss=0.6750]\u001B[A\n",
      "Training (polyak):  57%|█████▋    | 1147/2000 [00:12<00:08, 97.93it/s, train_loss=0.6750, val_loss=0.6750]\u001B[A\n",
      "Training (polyak):  58%|█████▊    | 1161/2000 [00:12<00:07, 107.22it/s, train_loss=0.6750, val_loss=0.6750]\u001B[A\n",
      "Training (polyak):  59%|█████▉    | 1175/2000 [00:12<00:07, 115.28it/s, train_loss=0.6750, val_loss=0.6750]\u001B[A\n",
      "Training (polyak):  59%|█████▉    | 1189/2000 [00:13<00:06, 121.62it/s, train_loss=0.6750, val_loss=0.6750]\u001B[A\n",
      "Training (polyak):  59%|█████▉    | 1189/2000 [00:13<00:06, 121.62it/s, train_loss=0.6643, val_loss=0.6687]\u001B[A\n",
      "Training (polyak):  60%|██████    | 1203/2000 [00:13<00:11, 72.43it/s, train_loss=0.6643, val_loss=0.6687] \u001B[A\n",
      "Training (polyak):  61%|██████    | 1217/2000 [00:13<00:09, 84.54it/s, train_loss=0.6643, val_loss=0.6687]\u001B[A\n",
      "Training (polyak):  62%|██████▏   | 1231/2000 [00:13<00:08, 95.72it/s, train_loss=0.6643, val_loss=0.6687]\u001B[A\n",
      "Training (polyak):  62%|██████▏   | 1245/2000 [00:13<00:07, 105.65it/s, train_loss=0.6643, val_loss=0.6687]\u001B[A\n",
      "Training (polyak):  63%|██████▎   | 1259/2000 [00:13<00:06, 113.64it/s, train_loss=0.6643, val_loss=0.6687]\u001B[A\n",
      "Training (polyak):  64%|██████▎   | 1273/2000 [00:13<00:06, 120.09it/s, train_loss=0.6643, val_loss=0.6687]\u001B[A\n",
      "Training (polyak):  64%|██████▍   | 1287/2000 [00:14<00:05, 125.22it/s, train_loss=0.6643, val_loss=0.6687]\u001B[A\n",
      "Training (polyak):  64%|██████▍   | 1287/2000 [00:14<00:05, 125.22it/s, train_loss=0.5533, val_loss=0.5628]\u001B[A\n",
      "Training (polyak):  65%|██████▌   | 1301/2000 [00:14<00:09, 72.86it/s, train_loss=0.5533, val_loss=0.5628] \u001B[A\n",
      "Training (polyak):  66%|██████▌   | 1315/2000 [00:14<00:08, 84.24it/s, train_loss=0.5533, val_loss=0.5628]\u001B[A\n",
      "Training (polyak):  66%|██████▋   | 1329/2000 [00:14<00:07, 94.93it/s, train_loss=0.5533, val_loss=0.5628]\u001B[A\n",
      "Training (polyak):  67%|██████▋   | 1343/2000 [00:14<00:06, 104.40it/s, train_loss=0.5533, val_loss=0.5628]\u001B[A\n",
      "Training (polyak):  68%|██████▊   | 1357/2000 [00:14<00:05, 112.45it/s, train_loss=0.5533, val_loss=0.5628]\u001B[A\n",
      "Training (polyak):  69%|██████▊   | 1371/2000 [00:14<00:05, 119.13it/s, train_loss=0.5533, val_loss=0.5628]\u001B[A\n",
      "Training (polyak):  69%|██████▉   | 1385/2000 [00:15<00:04, 124.57it/s, train_loss=0.5533, val_loss=0.5628]\u001B[A\n",
      "Training (polyak):  70%|██████▉   | 1399/2000 [00:15<00:04, 128.64it/s, train_loss=0.5533, val_loss=0.5628]\u001B[A\n",
      "Training (polyak):  70%|██████▉   | 1399/2000 [00:15<00:04, 128.64it/s, train_loss=0.6293, val_loss=0.6303]\u001B[A\n",
      "Training (polyak):  71%|███████   | 1413/2000 [00:15<00:07, 74.14it/s, train_loss=0.6293, val_loss=0.6303] \u001B[A\n",
      "Training (polyak):  71%|███████▏  | 1427/2000 [00:15<00:06, 86.12it/s, train_loss=0.6293, val_loss=0.6303]\u001B[A\n",
      "Training (polyak):  72%|███████▏  | 1441/2000 [00:15<00:05, 97.16it/s, train_loss=0.6293, val_loss=0.6303]\u001B[A\n",
      "Training (polyak):  73%|███████▎  | 1455/2000 [00:15<00:05, 106.75it/s, train_loss=0.6293, val_loss=0.6303]\u001B[A\n",
      "Training (polyak):  73%|███████▎  | 1469/2000 [00:15<00:04, 110.99it/s, train_loss=0.6293, val_loss=0.6303]\u001B[A\n",
      "Training (polyak):  74%|███████▍  | 1483/2000 [00:16<00:04, 117.17it/s, train_loss=0.6293, val_loss=0.6303]\u001B[A\n",
      "Training (polyak):  75%|███████▍  | 1497/2000 [00:16<00:04, 122.52it/s, train_loss=0.6293, val_loss=0.6303]\u001B[A\n",
      "Training (polyak):  75%|███████▍  | 1497/2000 [00:16<00:04, 122.52it/s, train_loss=0.5541, val_loss=0.5592]\u001B[A\n",
      "Training (polyak):  76%|███████▌  | 1511/2000 [00:16<00:06, 74.50it/s, train_loss=0.5541, val_loss=0.5592] \u001B[A\n",
      "Training (polyak):  76%|███████▋  | 1525/2000 [00:16<00:05, 86.33it/s, train_loss=0.5541, val_loss=0.5592]\u001B[A\n",
      "Training (polyak):  77%|███████▋  | 1539/2000 [00:16<00:04, 97.43it/s, train_loss=0.5541, val_loss=0.5592]\u001B[A\n",
      "Training (polyak):  78%|███████▊  | 1553/2000 [00:16<00:04, 106.97it/s, train_loss=0.5541, val_loss=0.5592]\u001B[A\n",
      "Training (polyak):  78%|███████▊  | 1567/2000 [00:16<00:03, 114.36it/s, train_loss=0.5541, val_loss=0.5592]\u001B[A\n",
      "Training (polyak):  79%|███████▉  | 1581/2000 [00:16<00:03, 120.33it/s, train_loss=0.5541, val_loss=0.5592]\u001B[A\n",
      "Training (polyak):  80%|███████▉  | 1595/2000 [00:17<00:03, 125.54it/s, train_loss=0.5541, val_loss=0.5592]\u001B[A\n",
      "Training (polyak):  80%|███████▉  | 1595/2000 [00:17<00:03, 125.54it/s, train_loss=0.5769, val_loss=0.5694]\u001B[A\n",
      "Training (polyak):  80%|████████  | 1609/2000 [00:17<00:05, 75.48it/s, train_loss=0.5769, val_loss=0.5694] \u001B[A\n",
      "Training (polyak):  81%|████████  | 1623/2000 [00:17<00:04, 87.38it/s, train_loss=0.5769, val_loss=0.5694]\u001B[A\n",
      "Training (polyak):  82%|████████▏ | 1637/2000 [00:17<00:03, 98.17it/s, train_loss=0.5769, val_loss=0.5694]\u001B[A\n",
      "Training (polyak):  83%|████████▎ | 1651/2000 [00:17<00:03, 107.56it/s, train_loss=0.5769, val_loss=0.5694]\u001B[A\n",
      "Training (polyak):  83%|████████▎ | 1665/2000 [00:17<00:02, 115.03it/s, train_loss=0.5769, val_loss=0.5694]\u001B[A\n",
      "Training (polyak):  84%|████████▍ | 1679/2000 [00:17<00:02, 120.09it/s, train_loss=0.5769, val_loss=0.5694]\u001B[A\n",
      "Training (polyak):  85%|████████▍ | 1693/2000 [00:18<00:02, 124.80it/s, train_loss=0.5769, val_loss=0.5694]\u001B[A\n",
      "Training (polyak):  85%|████████▍ | 1693/2000 [00:18<00:02, 124.80it/s, train_loss=0.5728, val_loss=0.5717]\u001B[A\n",
      "Training (polyak):  85%|████████▌ | 1707/2000 [00:18<00:03, 74.73it/s, train_loss=0.5728, val_loss=0.5717] \u001B[A\n",
      "Training (polyak):  86%|████████▌ | 1721/2000 [00:18<00:03, 86.64it/s, train_loss=0.5728, val_loss=0.5717]\u001B[A\n",
      "Training (polyak):  87%|████████▋ | 1735/2000 [00:18<00:02, 97.51it/s, train_loss=0.5728, val_loss=0.5717]\u001B[A\n",
      "Training (polyak):  87%|████████▋ | 1749/2000 [00:18<00:02, 106.18it/s, train_loss=0.5728, val_loss=0.5717]\u001B[A\n",
      "Training (polyak):  88%|████████▊ | 1763/2000 [00:18<00:02, 113.01it/s, train_loss=0.5728, val_loss=0.5717]\u001B[A\n",
      "Training (polyak):  89%|████████▉ | 1777/2000 [00:18<00:01, 119.40it/s, train_loss=0.5728, val_loss=0.5717]\u001B[A\n",
      "Training (polyak):  90%|████████▉ | 1791/2000 [00:19<00:01, 123.70it/s, train_loss=0.5728, val_loss=0.5717]\u001B[A\n",
      "Training (polyak):  90%|████████▉ | 1791/2000 [00:19<00:01, 123.70it/s, train_loss=0.5438, val_loss=0.5441]\u001B[A\n",
      "Training (polyak):  90%|█████████ | 1805/2000 [00:19<00:02, 74.12it/s, train_loss=0.5438, val_loss=0.5441] \u001B[A\n",
      "Training (polyak):  91%|█████████ | 1819/2000 [00:19<00:02, 85.75it/s, train_loss=0.5438, val_loss=0.5441]\u001B[A\n",
      "Training (polyak):  92%|█████████▏| 1833/2000 [00:19<00:01, 96.89it/s, train_loss=0.5438, val_loss=0.5441]\u001B[A\n",
      "Training (polyak):  92%|█████████▏| 1847/2000 [00:19<00:01, 106.41it/s, train_loss=0.5438, val_loss=0.5441]\u001B[A\n",
      "Training (polyak):  93%|█████████▎| 1861/2000 [00:19<00:01, 114.12it/s, train_loss=0.5438, val_loss=0.5441]\u001B[A\n",
      "Training (polyak):  94%|█████████▍| 1875/2000 [00:19<00:01, 119.92it/s, train_loss=0.5438, val_loss=0.5441]\u001B[A\n",
      "Training (polyak):  94%|█████████▍| 1889/2000 [00:20<00:00, 119.33it/s, train_loss=0.5438, val_loss=0.5441]\u001B[A\n",
      "Training (polyak):  94%|█████████▍| 1889/2000 [00:20<00:00, 119.33it/s, train_loss=0.5020, val_loss=0.5090]\u001B[A\n",
      "Training (polyak):  95%|█████████▌| 1902/2000 [00:20<00:01, 71.54it/s, train_loss=0.5020, val_loss=0.5090] \u001B[A\n",
      "Training (polyak):  96%|█████████▌| 1916/2000 [00:20<00:01, 83.78it/s, train_loss=0.5020, val_loss=0.5090]\u001B[A\n",
      "Training (polyak):  96%|█████████▋| 1930/2000 [00:20<00:00, 94.95it/s, train_loss=0.5020, val_loss=0.5090]\u001B[A\n",
      "Training (polyak):  97%|█████████▋| 1944/2000 [00:20<00:00, 104.64it/s, train_loss=0.5020, val_loss=0.5090]\u001B[A\n",
      "Training (polyak):  98%|█████████▊| 1958/2000 [00:20<00:00, 112.66it/s, train_loss=0.5020, val_loss=0.5090]\u001B[A\n",
      "Training (polyak):  99%|█████████▊| 1972/2000 [00:20<00:00, 118.56it/s, train_loss=0.5020, val_loss=0.5090]\u001B[A\n",
      "Training (polyak):  99%|█████████▉| 1986/2000 [00:20<00:00, 123.78it/s, train_loss=0.5020, val_loss=0.5090]\u001B[A\n",
      "Training (polyak): 100%|██████████| 2000/2000 [00:21<00:00, 94.79it/s, train_loss=0.5020, val_loss=0.5090] \u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with AdamW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:29,  3.51it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:57, 34.83it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:36, 54.51it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   2%|▏         | 31/2000 [00:00<00:29, 67.57it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   2%|▏         | 41/2000 [00:00<00:25, 75.83it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   2%|▎         | 50/2000 [00:00<00:25, 77.10it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   3%|▎         | 60/2000 [00:00<00:23, 82.48it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   4%|▎         | 70/2000 [00:01<00:22, 85.99it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   4%|▍         | 80/2000 [00:01<00:21, 89.26it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   4%|▍         | 90/2000 [00:01<00:20, 91.33it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:20, 93.09it/s, train_loss=3.7052, val_loss=3.7045]\u001B[A\n",
      "Training (adam):   5%|▌         | 100/2000 [00:01<00:20, 93.09it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):   6%|▌         | 110/2000 [00:01<00:34, 54.20it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):   6%|▌         | 120/2000 [00:01<00:30, 62.57it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):   6%|▋         | 130/2000 [00:01<00:26, 70.38it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):   7%|▋         | 140/2000 [00:02<00:24, 76.58it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):   8%|▊         | 150/2000 [00:02<00:22, 81.75it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):   8%|▊         | 160/2000 [00:02<00:21, 85.53it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):   8%|▊         | 170/2000 [00:02<00:20, 88.02it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:21, 85.97it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):  10%|▉         | 190/2000 [00:02<00:20, 89.33it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):  10%|█         | 200/2000 [00:02<00:19, 91.90it/s, train_loss=2.3176, val_loss=2.3216]\u001B[A\n",
      "Training (adam):  10%|█         | 200/2000 [00:02<00:19, 91.90it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  10%|█         | 210/2000 [00:03<00:33, 53.40it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  11%|█         | 221/2000 [00:03<00:28, 62.83it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  12%|█▏        | 232/2000 [00:03<00:24, 71.45it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:22, 78.66it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  13%|█▎        | 254/2000 [00:03<00:20, 84.45it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  13%|█▎        | 265/2000 [00:03<00:19, 88.91it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  14%|█▍        | 276/2000 [00:03<00:18, 92.45it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  14%|█▍        | 286/2000 [00:03<00:18, 94.41it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:03<00:17, 96.71it/s, train_loss=1.8260, val_loss=1.8221]\u001B[A\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:17, 96.71it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  15%|█▌        | 307/2000 [00:04<00:30, 55.91it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  16%|█▌        | 318/2000 [00:04<00:25, 64.97it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  16%|█▋        | 329/2000 [00:04<00:22, 72.96it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  17%|█▋        | 339/2000 [00:04<00:21, 78.95it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  18%|█▊        | 350/2000 [00:04<00:19, 84.47it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  18%|█▊        | 361/2000 [00:04<00:18, 88.85it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  19%|█▊        | 372/2000 [00:04<00:17, 92.34it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  19%|█▉        | 382/2000 [00:04<00:17, 93.80it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:16, 95.30it/s, train_loss=1.2701, val_loss=1.2796]\u001B[A\n",
      "Training (adam):  20%|█▉        | 392/2000 [00:05<00:16, 95.30it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  20%|██        | 402/2000 [00:05<00:29, 53.93it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  21%|██        | 412/2000 [00:05<00:25, 61.94it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  21%|██        | 422/2000 [00:05<00:22, 69.46it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  22%|██▏       | 433/2000 [00:05<00:20, 76.92it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  22%|██▏       | 443/2000 [00:05<00:18, 82.12it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  23%|██▎       | 453/2000 [00:05<00:17, 86.68it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  23%|██▎       | 463/2000 [00:06<00:17, 89.85it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:06<00:16, 92.32it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  24%|██▍       | 483/2000 [00:06<00:16, 94.14it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:15, 95.72it/s, train_loss=0.8051, val_loss=0.8136]\u001B[A\n",
      "Training (adam):  25%|██▍       | 493/2000 [00:06<00:15, 95.72it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  25%|██▌       | 503/2000 [00:06<00:29, 50.58it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  26%|██▌       | 513/2000 [00:06<00:25, 59.06it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  26%|██▌       | 523/2000 [00:07<00:21, 67.24it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  27%|██▋       | 534/2000 [00:07<00:19, 75.09it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  27%|██▋       | 544/2000 [00:07<00:17, 80.96it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  28%|██▊       | 555/2000 [00:07<00:16, 86.35it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  28%|██▊       | 566/2000 [00:07<00:15, 90.35it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  29%|██▉       | 576/2000 [00:07<00:15, 92.77it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  29%|██▉       | 586/2000 [00:07<00:14, 94.63it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  30%|██▉       | 597/2000 [00:07<00:14, 96.65it/s, train_loss=0.6272, val_loss=0.6451]\u001B[A\n",
      "Training (adam):  30%|██▉       | 597/2000 [00:08<00:14, 96.65it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  30%|███       | 607/2000 [00:08<00:25, 55.58it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  31%|███       | 618/2000 [00:08<00:21, 64.65it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  31%|███▏      | 628/2000 [00:08<00:19, 71.96it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  32%|███▏      | 638/2000 [00:08<00:17, 78.05it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  32%|███▏      | 649/2000 [00:08<00:16, 84.15it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  33%|███▎      | 659/2000 [00:08<00:15, 88.19it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  34%|███▎      | 670/2000 [00:08<00:14, 91.80it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  34%|███▍      | 681/2000 [00:08<00:13, 94.72it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:08<00:13, 95.63it/s, train_loss=0.5643, val_loss=0.5707]\u001B[A\n",
      "Training (adam):  35%|███▍      | 691/2000 [00:09<00:13, 95.63it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  35%|███▌      | 701/2000 [00:09<00:24, 52.37it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  36%|███▌      | 711/2000 [00:09<00:21, 60.34it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  36%|███▌      | 721/2000 [00:09<00:18, 68.21it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  37%|███▋      | 731/2000 [00:09<00:16, 75.13it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  37%|███▋      | 741/2000 [00:09<00:15, 80.92it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  38%|███▊      | 751/2000 [00:09<00:14, 85.75it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  38%|███▊      | 761/2000 [00:09<00:14, 88.48it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  39%|███▊      | 772/2000 [00:10<00:13, 91.97it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  39%|███▉      | 783/2000 [00:10<00:12, 94.31it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 96.05it/s, train_loss=0.5543, val_loss=0.5535]\u001B[A\n",
      "Training (adam):  40%|███▉      | 794/2000 [00:10<00:12, 96.05it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  40%|████      | 804/2000 [00:10<00:21, 54.52it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  41%|████      | 814/2000 [00:10<00:18, 62.47it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  41%|████      | 824/2000 [00:10<00:16, 70.10it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  42%|████▏     | 834/2000 [00:10<00:15, 76.73it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  42%|████▏     | 845/2000 [00:11<00:13, 82.93it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  43%|████▎     | 855/2000 [00:11<00:13, 86.84it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  43%|████▎     | 866/2000 [00:11<00:12, 90.68it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  44%|████▍     | 877/2000 [00:11<00:11, 93.84it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  44%|████▍     | 887/2000 [00:11<00:11, 94.93it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:11<00:11, 96.06it/s, train_loss=0.5088, val_loss=0.5224]\u001B[A\n",
      "Training (adam):  45%|████▍     | 897/2000 [00:11<00:11, 96.06it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  45%|████▌     | 907/2000 [00:12<00:20, 54.58it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  46%|████▌     | 917/2000 [00:12<00:17, 62.90it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  46%|████▋     | 928/2000 [00:12<00:14, 71.64it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  47%|████▋     | 939/2000 [00:12<00:13, 78.71it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  47%|████▋     | 949/2000 [00:12<00:12, 83.44it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  48%|████▊     | 959/2000 [00:12<00:11, 87.63it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  48%|████▊     | 969/2000 [00:12<00:11, 90.79it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  49%|████▉     | 979/2000 [00:12<00:10, 93.13it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:12<00:10, 94.19it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:12<00:10, 95.49it/s, train_loss=0.5059, val_loss=0.5062]\u001B[A\n",
      "Training (adam):  50%|████▉     | 999/2000 [00:13<00:10, 95.49it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  50%|█████     | 1009/2000 [00:13<00:18, 53.99it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  51%|█████     | 1019/2000 [00:13<00:15, 62.57it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  52%|█████▏    | 1030/2000 [00:13<00:13, 71.25it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  52%|█████▏    | 1041/2000 [00:13<00:12, 78.46it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  53%|█████▎    | 1051/2000 [00:13<00:11, 83.05it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  53%|█████▎    | 1061/2000 [00:13<00:10, 87.10it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  54%|█████▎    | 1071/2000 [00:13<00:10, 89.84it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  54%|█████▍    | 1081/2000 [00:14<00:09, 91.94it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:14<00:09, 94.19it/s, train_loss=0.4850, val_loss=0.4994]\u001B[A\n",
      "Training (adam):  55%|█████▍    | 1091/2000 [00:14<00:09, 94.19it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  55%|█████▌    | 1101/2000 [00:14<00:17, 52.68it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  56%|█████▌    | 1111/2000 [00:14<00:14, 60.22it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:14<00:14, 62.56it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  56%|█████▋    | 1129/2000 [00:14<00:12, 68.22it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  57%|█████▋    | 1139/2000 [00:14<00:11, 74.66it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  57%|█████▋    | 1148/2000 [00:15<00:10, 78.42it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  58%|█████▊    | 1158/2000 [00:15<00:10, 83.65it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  58%|█████▊    | 1168/2000 [00:15<00:09, 87.30it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  59%|█████▉    | 1178/2000 [00:15<00:09, 89.93it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  59%|█████▉    | 1188/2000 [00:15<00:08, 92.67it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 94.61it/s, train_loss=0.4774, val_loss=0.4847]\u001B[A\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:15<00:08, 94.61it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  60%|██████    | 1208/2000 [00:16<00:16, 48.50it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  61%|██████    | 1216/2000 [00:16<00:15, 51.71it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  61%|██████    | 1224/2000 [00:16<00:13, 56.07it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  62%|██████▏   | 1232/2000 [00:16<00:12, 59.70it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:16<00:12, 62.73it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  62%|██████▏   | 1248/2000 [00:16<00:11, 65.73it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:16<00:10, 69.83it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  63%|██████▎   | 1265/2000 [00:16<00:10, 72.43it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  64%|██████▎   | 1274/2000 [00:16<00:09, 74.66it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  64%|██████▍   | 1282/2000 [00:17<00:09, 76.00it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:17<00:08, 81.10it/s, train_loss=0.4941, val_loss=0.4935]\u001B[A\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:17<00:08, 81.10it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  65%|██████▌   | 1301/2000 [00:17<00:14, 47.57it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  66%|██████▌   | 1311/2000 [00:17<00:12, 56.55it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  66%|██████▌   | 1321/2000 [00:17<00:10, 65.09it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  67%|██████▋   | 1331/2000 [00:17<00:09, 72.47it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  67%|██████▋   | 1341/2000 [00:17<00:08, 78.24it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  68%|██████▊   | 1351/2000 [00:18<00:07, 82.04it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  68%|██████▊   | 1361/2000 [00:18<00:07, 85.82it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  69%|██████▊   | 1371/2000 [00:18<00:07, 88.31it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  69%|██████▉   | 1381/2000 [00:18<00:06, 90.60it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:18<00:06, 90.15it/s, train_loss=0.4682, val_loss=0.4651]\u001B[A\n",
      "Training (adam):  70%|██████▉   | 1391/2000 [00:18<00:06, 90.15it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  70%|███████   | 1401/2000 [00:18<00:14, 40.36it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  70%|███████   | 1409/2000 [00:19<00:12, 46.04it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  71%|███████   | 1417/2000 [00:19<00:11, 51.51it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  71%|███████▏  | 1425/2000 [00:19<00:10, 56.80it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  72%|███████▏  | 1433/2000 [00:19<00:09, 59.96it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  72%|███████▏  | 1441/2000 [00:19<00:08, 64.45it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  72%|███████▏  | 1449/2000 [00:19<00:08, 67.66it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  73%|███████▎  | 1457/2000 [00:19<00:07, 68.87it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  73%|███████▎  | 1465/2000 [00:19<00:07, 70.49it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  74%|███████▎  | 1473/2000 [00:19<00:07, 72.90it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  74%|███████▍  | 1482/2000 [00:20<00:06, 74.96it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  74%|███████▍  | 1490/2000 [00:20<00:06, 75.57it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:20<00:06, 76.78it/s, train_loss=0.4590, val_loss=0.4669]\u001B[A\n",
      "Training (adam):  75%|███████▍  | 1498/2000 [00:20<00:06, 76.78it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  75%|███████▌  | 1506/2000 [00:20<00:12, 40.11it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  76%|███████▌  | 1514/2000 [00:20<00:10, 45.78it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  76%|███████▌  | 1522/2000 [00:20<00:09, 52.27it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  76%|███████▋  | 1530/2000 [00:21<00:08, 58.13it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  77%|███████▋  | 1538/2000 [00:21<00:07, 62.41it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  77%|███████▋  | 1546/2000 [00:21<00:06, 65.92it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  78%|███████▊  | 1554/2000 [00:21<00:06, 68.50it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  78%|███████▊  | 1562/2000 [00:21<00:06, 69.39it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  78%|███████▊  | 1570/2000 [00:21<00:06, 70.69it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  79%|███████▉  | 1578/2000 [00:21<00:05, 72.65it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  79%|███████▉  | 1586/2000 [00:21<00:05, 74.31it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:21<00:05, 75.37it/s, train_loss=0.4673, val_loss=0.4683]\u001B[A\n",
      "Training (adam):  80%|███████▉  | 1594/2000 [00:22<00:05, 75.37it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  80%|████████  | 1602/2000 [00:22<00:10, 39.01it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  80%|████████  | 1610/2000 [00:22<00:08, 45.83it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  81%|████████  | 1618/2000 [00:22<00:07, 52.34it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  81%|████████▏ | 1626/2000 [00:22<00:06, 57.93it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  82%|████████▏ | 1634/2000 [00:22<00:05, 62.95it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  82%|████████▏ | 1642/2000 [00:22<00:05, 66.52it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  82%|████████▎ | 1650/2000 [00:22<00:05, 69.52it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:23<00:04, 72.08it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  83%|████████▎ | 1666/2000 [00:23<00:04, 73.46it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:23<00:04, 71.00it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  84%|████████▍ | 1682/2000 [00:23<00:04, 72.20it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  84%|████████▍ | 1690/2000 [00:23<00:04, 73.21it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:23<00:04, 74.70it/s, train_loss=0.4827, val_loss=0.4832]\u001B[A\n",
      "Training (adam):  85%|████████▍ | 1698/2000 [00:23<00:04, 74.70it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  85%|████████▌ | 1706/2000 [00:23<00:07, 40.81it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  86%|████████▌ | 1714/2000 [00:24<00:06, 47.36it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  86%|████████▌ | 1722/2000 [00:24<00:05, 53.84it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  86%|████████▋ | 1730/2000 [00:24<00:04, 59.40it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  87%|████████▋ | 1738/2000 [00:24<00:04, 64.02it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  87%|████████▋ | 1746/2000 [00:24<00:03, 67.63it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:24<00:03, 70.12it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  88%|████████▊ | 1762/2000 [00:24<00:03, 72.20it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  88%|████████▊ | 1770/2000 [00:24<00:03, 73.00it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  89%|████████▉ | 1778/2000 [00:24<00:02, 74.28it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  89%|████████▉ | 1786/2000 [00:24<00:02, 75.42it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:25<00:02, 76.56it/s, train_loss=0.4750, val_loss=0.4750]\u001B[A\n",
      "Training (adam):  90%|████████▉ | 1794/2000 [00:25<00:02, 76.56it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  90%|█████████ | 1802/2000 [00:25<00:05, 37.76it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  90%|█████████ | 1810/2000 [00:25<00:04, 44.54it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  91%|█████████ | 1818/2000 [00:25<00:03, 50.73it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  91%|█████████▏| 1826/2000 [00:25<00:03, 55.90it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  92%|█████████▏| 1834/2000 [00:25<00:02, 59.76it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  92%|█████████▏| 1842/2000 [00:26<00:02, 64.59it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  93%|█████████▎| 1851/2000 [00:26<00:02, 70.85it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  93%|█████████▎| 1861/2000 [00:26<00:01, 76.49it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  94%|█████████▎| 1871/2000 [00:26<00:01, 81.38it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  94%|█████████▍| 1881/2000 [00:26<00:01, 84.34it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:26<00:01, 86.79it/s, train_loss=0.4507, val_loss=0.4528]\u001B[A\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:26<00:01, 86.79it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:27<00:01, 49.82it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:27<00:01, 58.15it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  96%|█████████▌| 1921/2000 [00:27<00:01, 65.87it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  97%|█████████▋| 1931/2000 [00:27<00:00, 72.33it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  97%|█████████▋| 1941/2000 [00:27<00:00, 78.09it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  98%|█████████▊| 1951/2000 [00:27<00:00, 82.58it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  98%|█████████▊| 1961/2000 [00:27<00:00, 86.03it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  99%|█████████▊| 1971/2000 [00:27<00:00, 89.06it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam):  99%|█████████▉| 1981/2000 [00:27<00:00, 91.10it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:28<00:00, 71.29it/s, train_loss=0.4638, val_loss=0.4699]\u001B[A\n",
      "Trials:  33%|███▎      | 1/3 [01:06<02:12, 66.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 2/3:\n",
      "Training with Constant SGD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<09:18,  3.58it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   1%|          | 19/2000 [00:00<00:31, 62.97it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   2%|▏         | 37/2000 [00:00<00:19, 99.28it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   3%|▎         | 55/2000 [00:00<00:15, 123.50it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   4%|▎         | 73/2000 [00:00<00:13, 139.54it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   5%|▍         | 91/2000 [00:00<00:12, 151.09it/s, train_loss=3.6919, val_loss=3.6935]\u001B[A\n",
      "Training (constant):   5%|▍         | 91/2000 [00:01<00:12, 151.09it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   5%|▌         | 108/2000 [00:01<00:21, 88.14it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   6%|▋         | 126/2000 [00:01<00:17, 105.71it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   7%|▋         | 141/2000 [00:01<00:16, 114.27it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   8%|▊         | 156/2000 [00:01<00:15, 116.52it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   9%|▊         | 171/2000 [00:01<00:14, 123.49it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   9%|▉         | 188/2000 [00:01<00:13, 134.21it/s, train_loss=3.2617, val_loss=3.2641]\u001B[A\n",
      "Training (constant):   9%|▉         | 188/2000 [00:02<00:13, 134.21it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  10%|█         | 203/2000 [00:02<00:23, 77.79it/s, train_loss=3.0697, val_loss=3.0754] \u001B[A\n",
      "Training (constant):  11%|█         | 220/2000 [00:02<00:19, 93.60it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  12%|█▏        | 237/2000 [00:02<00:16, 108.35it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  13%|█▎        | 254/2000 [00:02<00:14, 121.26it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  14%|█▎        | 271/2000 [00:02<00:13, 132.22it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  14%|█▍        | 288/2000 [00:02<00:12, 141.04it/s, train_loss=3.0697, val_loss=3.0754]\u001B[A\n",
      "Training (constant):  14%|█▍        | 288/2000 [00:02<00:12, 141.04it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  15%|█▌        | 304/2000 [00:02<00:19, 85.64it/s, train_loss=2.9452, val_loss=2.9493] \u001B[A\n",
      "Training (constant):  16%|█▌        | 322/2000 [00:03<00:16, 102.53it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  17%|█▋        | 340/2000 [00:03<00:14, 118.12it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  18%|█▊        | 357/2000 [00:03<00:12, 129.80it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  19%|█▉        | 375/2000 [00:03<00:11, 141.80it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  20%|█▉        | 393/2000 [00:03<00:10, 150.56it/s, train_loss=2.9452, val_loss=2.9493]\u001B[A\n",
      "Training (constant):  20%|█▉        | 393/2000 [00:03<00:10, 150.56it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  20%|██        | 410/2000 [00:03<00:17, 92.35it/s, train_loss=2.8582, val_loss=2.8591] \u001B[A\n",
      "Training (constant):  21%|██▏       | 427/2000 [00:03<00:14, 105.84it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  22%|██▏       | 445/2000 [00:04<00:12, 120.25it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  23%|██▎       | 463/2000 [00:04<00:11, 133.45it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  24%|██▍       | 482/2000 [00:04<00:10, 145.44it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  25%|██▌       | 500/2000 [00:04<00:09, 152.68it/s, train_loss=2.8582, val_loss=2.8591]\u001B[A\n",
      "Training (constant):  25%|██▌       | 500/2000 [00:04<00:09, 152.68it/s, train_loss=2.7840, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  26%|██▌       | 517/2000 [00:04<00:16, 92.12it/s, train_loss=2.7840, val_loss=2.7792] \u001B[A\n",
      "Training (constant):  27%|██▋       | 535/2000 [00:04<00:13, 107.69it/s, train_loss=2.7840, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  28%|██▊       | 553/2000 [00:04<00:11, 122.03it/s, train_loss=2.7840, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  29%|██▊       | 571/2000 [00:05<00:10, 134.66it/s, train_loss=2.7840, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  29%|██▉       | 589/2000 [00:05<00:09, 144.56it/s, train_loss=2.7840, val_loss=2.7792]\u001B[A\n",
      "Training (constant):  29%|██▉       | 589/2000 [00:05<00:09, 144.56it/s, train_loss=2.7083, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  30%|███       | 606/2000 [00:05<00:15, 87.46it/s, train_loss=2.7083, val_loss=2.7145] \u001B[A\n",
      "Training (constant):  31%|███       | 624/2000 [00:05<00:13, 103.18it/s, train_loss=2.7083, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  32%|███▏      | 642/2000 [00:05<00:11, 117.86it/s, train_loss=2.7083, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  33%|███▎      | 660/2000 [00:05<00:10, 131.16it/s, train_loss=2.7083, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  34%|███▍      | 678/2000 [00:05<00:09, 142.31it/s, train_loss=2.7083, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  35%|███▍      | 696/2000 [00:06<00:08, 151.82it/s, train_loss=2.7083, val_loss=2.7145]\u001B[A\n",
      "Training (constant):  35%|███▍      | 696/2000 [00:06<00:08, 151.82it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  36%|███▌      | 713/2000 [00:06<00:14, 90.87it/s, train_loss=2.6599, val_loss=2.6596] \u001B[A\n",
      "Training (constant):  37%|███▋      | 731/2000 [00:06<00:11, 106.58it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  37%|███▋      | 749/2000 [00:06<00:10, 121.57it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  38%|███▊      | 767/2000 [00:06<00:09, 134.34it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  39%|███▉      | 785/2000 [00:06<00:08, 145.25it/s, train_loss=2.6599, val_loss=2.6596]\u001B[A\n",
      "Training (constant):  39%|███▉      | 785/2000 [00:07<00:08, 145.25it/s, train_loss=2.5898, val_loss=2.5838]\u001B[A\n",
      "Training (constant):  40%|████      | 802/2000 [00:07<00:13, 85.77it/s, train_loss=2.5898, val_loss=2.5838] \u001B[A\n",
      "Training (constant):  41%|████      | 819/2000 [00:07<00:11, 100.21it/s, train_loss=2.5898, val_loss=2.5838]\u001B[A\n",
      "Training (constant):  42%|████▏     | 837/2000 [00:07<00:10, 115.02it/s, train_loss=2.5898, val_loss=2.5838]\u001B[A\n",
      "Training (constant):  43%|████▎     | 855/2000 [00:07<00:08, 128.70it/s, train_loss=2.5898, val_loss=2.5838]\u001B[A\n",
      "Training (constant):  44%|████▎     | 873/2000 [00:07<00:08, 139.94it/s, train_loss=2.5898, val_loss=2.5838]\u001B[A\n",
      "Training (constant):  45%|████▍     | 891/2000 [00:07<00:07, 149.57it/s, train_loss=2.5898, val_loss=2.5838]\u001B[A\n",
      "Training (constant):  45%|████▍     | 891/2000 [00:08<00:07, 149.57it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  45%|████▌     | 908/2000 [00:08<00:12, 90.41it/s, train_loss=2.5364, val_loss=2.5307] \u001B[A\n",
      "Training (constant):  46%|████▋     | 926/2000 [00:08<00:10, 105.96it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  47%|████▋     | 944/2000 [00:08<00:08, 120.87it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  48%|████▊     | 962/2000 [00:08<00:07, 134.16it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  49%|████▉     | 980/2000 [00:08<00:07, 144.30it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  50%|████▉     | 998/2000 [00:08<00:06, 153.43it/s, train_loss=2.5364, val_loss=2.5307]\u001B[A\n",
      "Training (constant):  50%|████▉     | 998/2000 [00:08<00:06, 153.43it/s, train_loss=2.4912, val_loss=2.4874]\u001B[A\n",
      "Training (constant):  51%|█████     | 1016/2000 [00:08<00:10, 93.94it/s, train_loss=2.4912, val_loss=2.4874]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1034/2000 [00:09<00:08, 109.15it/s, train_loss=2.4912, val_loss=2.4874]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1052/2000 [00:09<00:07, 123.15it/s, train_loss=2.4912, val_loss=2.4874]\u001B[A\n",
      "Training (constant):  54%|█████▎    | 1070/2000 [00:09<00:06, 135.37it/s, train_loss=2.4912, val_loss=2.4874]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1088/2000 [00:09<00:06, 144.44it/s, train_loss=2.4912, val_loss=2.4874]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1088/2000 [00:09<00:06, 144.44it/s, train_loss=2.4425, val_loss=2.4445]\u001B[A\n",
      "Training (constant):  55%|█████▌    | 1105/2000 [00:09<00:09, 89.61it/s, train_loss=2.4425, val_loss=2.4445] \u001B[A\n",
      "Training (constant):  56%|█████▌    | 1123/2000 [00:09<00:08, 105.49it/s, train_loss=2.4425, val_loss=2.4445]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1141/2000 [00:09<00:07, 120.58it/s, train_loss=2.4425, val_loss=2.4445]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1159/2000 [00:10<00:06, 133.22it/s, train_loss=2.4425, val_loss=2.4445]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1177/2000 [00:10<00:05, 144.19it/s, train_loss=2.4425, val_loss=2.4445]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1195/2000 [00:10<00:05, 152.56it/s, train_loss=2.4425, val_loss=2.4445]\u001B[A\n",
      "Training (constant):  60%|█████▉    | 1195/2000 [00:10<00:05, 152.56it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  61%|██████    | 1212/2000 [00:10<00:08, 93.50it/s, train_loss=2.3896, val_loss=2.3979] \u001B[A\n",
      "Training (constant):  62%|██████▏   | 1230/2000 [00:10<00:07, 109.14it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1248/2000 [00:10<00:06, 123.79it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1266/2000 [00:10<00:05, 136.04it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1284/2000 [00:10<00:04, 146.79it/s, train_loss=2.3896, val_loss=2.3979]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1284/2000 [00:11<00:04, 146.79it/s, train_loss=2.3401, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1301/2000 [00:11<00:07, 90.00it/s, train_loss=2.3401, val_loss=2.3533] \u001B[A\n",
      "Training (constant):  66%|██████▌   | 1319/2000 [00:11<00:06, 105.34it/s, train_loss=2.3401, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  67%|██████▋   | 1334/2000 [00:11<00:05, 113.69it/s, train_loss=2.3401, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1351/2000 [00:11<00:05, 124.89it/s, train_loss=2.3401, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1368/2000 [00:11<00:04, 134.06it/s, train_loss=2.3401, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1384/2000 [00:11<00:04, 137.34it/s, train_loss=2.3401, val_loss=2.3533]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1384/2000 [00:12<00:04, 137.34it/s, train_loss=2.3477, val_loss=2.3469]\u001B[A\n",
      "Training (constant):  70%|███████   | 1401/2000 [00:12<00:07, 79.51it/s, train_loss=2.3477, val_loss=2.3469] \u001B[A\n",
      "Training (constant):  71%|███████   | 1418/2000 [00:12<00:06, 94.19it/s, train_loss=2.3477, val_loss=2.3469]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1435/2000 [00:12<00:05, 108.61it/s, train_loss=2.3477, val_loss=2.3469]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1453/2000 [00:12<00:04, 123.19it/s, train_loss=2.3477, val_loss=2.3469]\u001B[A\n",
      "Training (constant):  74%|███████▎  | 1471/2000 [00:12<00:03, 134.90it/s, train_loss=2.3477, val_loss=2.3469]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1489/2000 [00:12<00:03, 144.96it/s, train_loss=2.3477, val_loss=2.3469]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1489/2000 [00:13<00:03, 144.96it/s, train_loss=2.2825, val_loss=2.2774]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1506/2000 [00:13<00:05, 87.32it/s, train_loss=2.2825, val_loss=2.2774] \u001B[A\n",
      "Training (constant):  76%|███████▌  | 1522/2000 [00:13<00:04, 100.05it/s, train_loss=2.2825, val_loss=2.2774]\u001B[A\n",
      "Training (constant):  77%|███████▋  | 1536/2000 [00:13<00:04, 107.86it/s, train_loss=2.2825, val_loss=2.2774]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1550/2000 [00:13<00:04, 105.96it/s, train_loss=2.2825, val_loss=2.2774]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1567/2000 [00:13<00:03, 118.90it/s, train_loss=2.2825, val_loss=2.2774]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1584/2000 [00:13<00:03, 129.94it/s, train_loss=2.2825, val_loss=2.2774]\u001B[A\n",
      "Training (constant):  79%|███████▉  | 1584/2000 [00:14<00:03, 129.94it/s, train_loss=2.2806, val_loss=2.2932]\u001B[A\n",
      "Training (constant):  80%|████████  | 1601/2000 [00:14<00:05, 71.13it/s, train_loss=2.2806, val_loss=2.2932] \u001B[A\n",
      "Training (constant):  81%|████████  | 1616/2000 [00:14<00:04, 82.49it/s, train_loss=2.2806, val_loss=2.2932]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1630/2000 [00:14<00:03, 92.63it/s, train_loss=2.2806, val_loss=2.2932]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1647/2000 [00:14<00:03, 107.49it/s, train_loss=2.2806, val_loss=2.2932]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1663/2000 [00:14<00:02, 118.55it/s, train_loss=2.2806, val_loss=2.2932]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1680/2000 [00:14<00:02, 129.95it/s, train_loss=2.2806, val_loss=2.2932]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1697/2000 [00:14<00:02, 139.30it/s, train_loss=2.2806, val_loss=2.2932]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1697/2000 [00:15<00:02, 139.30it/s, train_loss=2.2439, val_loss=2.2583]\u001B[A\n",
      "Training (constant):  86%|████████▌ | 1713/2000 [00:15<00:03, 80.62it/s, train_loss=2.2439, val_loss=2.2583] \u001B[A\n",
      "Training (constant):  86%|████████▋ | 1729/2000 [00:15<00:02, 94.43it/s, train_loss=2.2439, val_loss=2.2583]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1743/2000 [00:15<00:02, 102.02it/s, train_loss=2.2439, val_loss=2.2583]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1760/2000 [00:15<00:02, 115.63it/s, train_loss=2.2439, val_loss=2.2583]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1778/2000 [00:15<00:01, 129.16it/s, train_loss=2.2439, val_loss=2.2583]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1793/2000 [00:15<00:01, 133.04it/s, train_loss=2.2439, val_loss=2.2583]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1793/2000 [00:16<00:01, 133.04it/s, train_loss=2.1990, val_loss=2.2021]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1808/2000 [00:16<00:02, 81.33it/s, train_loss=2.1990, val_loss=2.2021] \u001B[A\n",
      "Training (constant):  91%|█████████▏| 1825/2000 [00:16<00:01, 96.75it/s, train_loss=2.1990, val_loss=2.2021]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1842/2000 [00:16<00:01, 111.42it/s, train_loss=2.1990, val_loss=2.2021]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1860/2000 [00:16<00:01, 125.78it/s, train_loss=2.1990, val_loss=2.2021]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1877/2000 [00:16<00:00, 135.59it/s, train_loss=2.1990, val_loss=2.2021]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1895/2000 [00:16<00:00, 145.78it/s, train_loss=2.1990, val_loss=2.2021]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1895/2000 [00:17<00:00, 145.78it/s, train_loss=2.1733, val_loss=2.1640]\u001B[A\n",
      "Training (constant):  96%|█████████▌| 1912/2000 [00:17<00:01, 78.09it/s, train_loss=2.1733, val_loss=2.1640] \u001B[A\n",
      "Training (constant):  96%|█████████▋| 1928/2000 [00:17<00:00, 90.92it/s, train_loss=2.1733, val_loss=2.1640]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1945/2000 [00:17<00:00, 105.14it/s, train_loss=2.1733, val_loss=2.1640]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1962/2000 [00:17<00:00, 118.06it/s, train_loss=2.1733, val_loss=2.1640]\u001B[A\n",
      "Training (constant):  99%|█████████▉| 1977/2000 [00:17<00:00, 122.21it/s, train_loss=2.1733, val_loss=2.1640]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:17<00:00, 112.96it/s, train_loss=2.1733, val_loss=2.1640]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Polyak SGD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (polyak):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (polyak):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   0%|          | 1/2000 [00:00<14:16,  2.34it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   1%|          | 13/2000 [00:00<01:03, 31.45it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   1%|▏         | 27/2000 [00:00<00:33, 58.87it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   2%|▏         | 41/2000 [00:00<00:24, 79.26it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   3%|▎         | 54/2000 [00:00<00:21, 91.42it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   3%|▎         | 68/2000 [00:00<00:18, 102.92it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   4%|▍         | 81/2000 [00:01<00:17, 110.35it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   5%|▍         | 94/2000 [00:01<00:16, 113.58it/s, train_loss=3.6927, val_loss=3.6925]\u001B[A\n",
      "Training (polyak):   5%|▍         | 94/2000 [00:01<00:16, 113.58it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):   5%|▌         | 107/2000 [00:01<00:30, 62.70it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):   6%|▌         | 119/2000 [00:01<00:26, 71.94it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):   7%|▋         | 131/2000 [00:01<00:23, 81.15it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):   7%|▋         | 142/2000 [00:01<00:21, 85.87it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):   8%|▊         | 154/2000 [00:01<00:19, 93.86it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):   8%|▊         | 167/2000 [00:02<00:17, 102.12it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):   9%|▉         | 180/2000 [00:02<00:16, 108.19it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):  10%|▉         | 193/2000 [00:02<00:16, 112.89it/s, train_loss=2.3020, val_loss=2.3197]\u001B[A\n",
      "Training (polyak):  10%|▉         | 193/2000 [00:02<00:16, 112.89it/s, train_loss=2.0617, val_loss=2.0664]\u001B[A\n",
      "Training (polyak):  10%|█         | 205/2000 [00:02<00:27, 64.67it/s, train_loss=2.0617, val_loss=2.0664] \u001B[A\n",
      "Training (polyak):  11%|█         | 219/2000 [00:02<00:22, 77.56it/s, train_loss=2.0617, val_loss=2.0664]\u001B[A\n",
      "Training (polyak):  12%|█▏        | 232/2000 [00:02<00:20, 87.47it/s, train_loss=2.0617, val_loss=2.0664]\u001B[A\n",
      "Training (polyak):  12%|█▏        | 245/2000 [00:03<00:18, 96.72it/s, train_loss=2.0617, val_loss=2.0664]\u001B[A\n",
      "Training (polyak):  13%|█▎        | 258/2000 [00:03<00:16, 104.69it/s, train_loss=2.0617, val_loss=2.0664]\u001B[A\n",
      "Training (polyak):  14%|█▎        | 271/2000 [00:03<00:15, 110.90it/s, train_loss=2.0617, val_loss=2.0664]\u001B[A\n",
      "Training (polyak):  14%|█▍        | 285/2000 [00:03<00:14, 116.31it/s, train_loss=2.0617, val_loss=2.0664]\u001B[A\n",
      "Training (polyak):  15%|█▍        | 298/2000 [00:03<00:14, 119.94it/s, train_loss=2.0617, val_loss=2.0664]\u001B[A\n",
      "Training (polyak):  15%|█▍        | 298/2000 [00:03<00:14, 119.94it/s, train_loss=1.9638, val_loss=1.9628]\u001B[A\n",
      "Training (polyak):  16%|█▌        | 311/2000 [00:03<00:24, 68.83it/s, train_loss=1.9638, val_loss=1.9628] \u001B[A\n",
      "Training (polyak):  16%|█▌        | 324/2000 [00:03<00:21, 79.34it/s, train_loss=1.9638, val_loss=1.9628]\u001B[A\n",
      "Training (polyak):  17%|█▋        | 338/2000 [00:04<00:18, 90.66it/s, train_loss=1.9638, val_loss=1.9628]\u001B[A\n",
      "Training (polyak):  18%|█▊        | 351/2000 [00:04<00:16, 99.34it/s, train_loss=1.9638, val_loss=1.9628]\u001B[A\n",
      "Training (polyak):  18%|█▊        | 365/2000 [00:04<00:15, 107.49it/s, train_loss=1.9638, val_loss=1.9628]\u001B[A\n",
      "Training (polyak):  19%|█▉        | 379/2000 [00:04<00:14, 113.53it/s, train_loss=1.9638, val_loss=1.9628]\u001B[A\n",
      "Training (polyak):  20%|█▉        | 392/2000 [00:04<00:13, 116.14it/s, train_loss=1.9638, val_loss=1.9628]\u001B[A\n",
      "Training (polyak):  20%|█▉        | 392/2000 [00:04<00:13, 116.14it/s, train_loss=1.8688, val_loss=1.8737]\u001B[A\n",
      "Training (polyak):  20%|██        | 405/2000 [00:04<00:24, 65.96it/s, train_loss=1.8688, val_loss=1.8737] \u001B[A\n",
      "Training (polyak):  21%|██        | 417/2000 [00:04<00:21, 75.20it/s, train_loss=1.8688, val_loss=1.8737]\u001B[A\n",
      "Training (polyak):  21%|██▏       | 429/2000 [00:05<00:18, 84.13it/s, train_loss=1.8688, val_loss=1.8737]\u001B[A\n",
      "Training (polyak):  22%|██▏       | 443/2000 [00:05<00:16, 95.11it/s, train_loss=1.8688, val_loss=1.8737]\u001B[A\n",
      "Training (polyak):  23%|██▎       | 456/2000 [00:05<00:14, 103.17it/s, train_loss=1.8688, val_loss=1.8737]\u001B[A\n",
      "Training (polyak):  24%|██▎       | 470/2000 [00:05<00:13, 111.15it/s, train_loss=1.8688, val_loss=1.8737]\u001B[A\n",
      "Training (polyak):  24%|██▍       | 483/2000 [00:05<00:13, 115.99it/s, train_loss=1.8688, val_loss=1.8737]\u001B[A\n",
      "Training (polyak):  25%|██▍       | 497/2000 [00:05<00:12, 120.04it/s, train_loss=1.8688, val_loss=1.8737]\u001B[A\n",
      "Training (polyak):  25%|██▍       | 497/2000 [00:05<00:12, 120.04it/s, train_loss=1.8880, val_loss=1.8804]\u001B[A\n",
      "Training (polyak):  26%|██▌       | 510/2000 [00:05<00:20, 71.16it/s, train_loss=1.8880, val_loss=1.8804] \u001B[A\n",
      "Training (polyak):  26%|██▌       | 523/2000 [00:06<00:18, 81.74it/s, train_loss=1.8880, val_loss=1.8804]\u001B[A\n",
      "Training (polyak):  27%|██▋       | 537/2000 [00:06<00:15, 93.16it/s, train_loss=1.8880, val_loss=1.8804]\u001B[A\n",
      "Training (polyak):  28%|██▊       | 551/2000 [00:06<00:14, 102.10it/s, train_loss=1.8880, val_loss=1.8804]\u001B[A\n",
      "Training (polyak):  28%|██▊       | 565/2000 [00:06<00:13, 109.91it/s, train_loss=1.8880, val_loss=1.8804]\u001B[A\n",
      "Training (polyak):  29%|██▉       | 579/2000 [00:06<00:12, 115.66it/s, train_loss=1.8880, val_loss=1.8804]\u001B[A\n",
      "Training (polyak):  30%|██▉       | 592/2000 [00:06<00:11, 118.90it/s, train_loss=1.8880, val_loss=1.8804]\u001B[A\n",
      "Training (polyak):  30%|██▉       | 592/2000 [00:06<00:11, 118.90it/s, train_loss=1.5034, val_loss=1.4964]\u001B[A\n",
      "Training (polyak):  30%|███       | 605/2000 [00:06<00:20, 69.13it/s, train_loss=1.5034, val_loss=1.4964] \u001B[A\n",
      "Training (polyak):  31%|███       | 618/2000 [00:07<00:17, 79.56it/s, train_loss=1.5034, val_loss=1.4964]\u001B[A\n",
      "Training (polyak):  32%|███▏      | 632/2000 [00:07<00:15, 90.98it/s, train_loss=1.5034, val_loss=1.4964]\u001B[A\n",
      "Training (polyak):  32%|███▏      | 646/2000 [00:07<00:13, 101.16it/s, train_loss=1.5034, val_loss=1.4964]\u001B[A\n",
      "Training (polyak):  33%|███▎      | 660/2000 [00:07<00:12, 109.38it/s, train_loss=1.5034, val_loss=1.4964]\u001B[A\n",
      "Training (polyak):  34%|███▎      | 674/2000 [00:07<00:11, 115.31it/s, train_loss=1.5034, val_loss=1.4964]\u001B[A\n",
      "Training (polyak):  34%|███▍      | 687/2000 [00:07<00:11, 118.98it/s, train_loss=1.5034, val_loss=1.4964]\u001B[A\n",
      "Training (polyak):  34%|███▍      | 687/2000 [00:07<00:11, 118.98it/s, train_loss=1.0554, val_loss=1.0571]\u001B[A\n",
      "Training (polyak):  35%|███▌      | 701/2000 [00:07<00:18, 68.98it/s, train_loss=1.0554, val_loss=1.0571] \u001B[A\n",
      "Training (polyak):  36%|███▌      | 715/2000 [00:08<00:15, 80.54it/s, train_loss=1.0554, val_loss=1.0571]\u001B[A\n",
      "Training (polyak):  36%|███▋      | 729/2000 [00:08<00:13, 91.18it/s, train_loss=1.0554, val_loss=1.0571]\u001B[A\n",
      "Training (polyak):  37%|███▋      | 743/2000 [00:08<00:12, 100.69it/s, train_loss=1.0554, val_loss=1.0571]\u001B[A\n",
      "Training (polyak):  38%|███▊      | 756/2000 [00:08<00:11, 107.61it/s, train_loss=1.0554, val_loss=1.0571]\u001B[A\n",
      "Training (polyak):  38%|███▊      | 770/2000 [00:08<00:10, 113.88it/s, train_loss=1.0554, val_loss=1.0571]\u001B[A\n",
      "Training (polyak):  39%|███▉      | 783/2000 [00:08<00:11, 109.01it/s, train_loss=1.0554, val_loss=1.0571]\u001B[A\n",
      "Training (polyak):  40%|███▉      | 796/2000 [00:08<00:10, 113.88it/s, train_loss=1.0554, val_loss=1.0571]\u001B[A\n",
      "Training (polyak):  40%|███▉      | 796/2000 [00:08<00:10, 113.88it/s, train_loss=0.8615, val_loss=0.8564]\u001B[A\n",
      "Training (polyak):  40%|████      | 809/2000 [00:09<00:17, 69.12it/s, train_loss=0.8615, val_loss=0.8564] \u001B[A\n",
      "Training (polyak):  41%|████      | 823/2000 [00:09<00:14, 80.99it/s, train_loss=0.8615, val_loss=0.8564]\u001B[A\n",
      "Training (polyak):  42%|████▏     | 836/2000 [00:09<00:12, 90.02it/s, train_loss=0.8615, val_loss=0.8564]\u001B[A\n",
      "Training (polyak):  42%|████▎     | 850/2000 [00:09<00:11, 100.47it/s, train_loss=0.8615, val_loss=0.8564]\u001B[A\n",
      "Training (polyak):  43%|████▎     | 863/2000 [00:09<00:10, 107.44it/s, train_loss=0.8615, val_loss=0.8564]\u001B[A\n",
      "Training (polyak):  44%|████▍     | 876/2000 [00:09<00:09, 112.75it/s, train_loss=0.8615, val_loss=0.8564]\u001B[A\n",
      "Training (polyak):  44%|████▍     | 890/2000 [00:09<00:09, 118.36it/s, train_loss=0.8615, val_loss=0.8564]\u001B[A\n",
      "Training (polyak):  44%|████▍     | 890/2000 [00:10<00:09, 118.36it/s, train_loss=0.6868, val_loss=0.6964]\u001B[A\n",
      "Training (polyak):  45%|████▌     | 903/2000 [00:10<00:15, 69.11it/s, train_loss=0.6868, val_loss=0.6964] \u001B[A\n",
      "Training (polyak):  46%|████▌     | 916/2000 [00:10<00:13, 80.06it/s, train_loss=0.6868, val_loss=0.6964]\u001B[A\n",
      "Training (polyak):  46%|████▋     | 930/2000 [00:10<00:11, 91.44it/s, train_loss=0.6868, val_loss=0.6964]\u001B[A\n",
      "Training (polyak):  47%|████▋     | 943/2000 [00:10<00:10, 100.06it/s, train_loss=0.6868, val_loss=0.6964]\u001B[A\n",
      "Training (polyak):  48%|████▊     | 957/2000 [00:10<00:09, 108.32it/s, train_loss=0.6868, val_loss=0.6964]\u001B[A\n",
      "Training (polyak):  48%|████▊     | 970/2000 [00:10<00:09, 113.05it/s, train_loss=0.6868, val_loss=0.6964]\u001B[A\n",
      "Training (polyak):  49%|████▉     | 983/2000 [00:10<00:08, 117.34it/s, train_loss=0.6868, val_loss=0.6964]\u001B[A\n",
      "Training (polyak):  50%|████▉     | 996/2000 [00:10<00:08, 120.44it/s, train_loss=0.6868, val_loss=0.6964]\u001B[A\n",
      "Training (polyak):  50%|████▉     | 996/2000 [00:11<00:08, 120.44it/s, train_loss=0.6410, val_loss=0.6423]\u001B[A\n",
      "Training (polyak):  50%|█████     | 1009/2000 [00:11<00:14, 69.74it/s, train_loss=0.6410, val_loss=0.6423]\u001B[A\n",
      "Training (polyak):  51%|█████     | 1022/2000 [00:11<00:12, 80.37it/s, train_loss=0.6410, val_loss=0.6423]\u001B[A\n",
      "Training (polyak):  52%|█████▏    | 1035/2000 [00:11<00:10, 89.89it/s, train_loss=0.6410, val_loss=0.6423]\u001B[A\n",
      "Training (polyak):  52%|█████▏    | 1049/2000 [00:11<00:09, 99.50it/s, train_loss=0.6410, val_loss=0.6423]\u001B[A\n",
      "Training (polyak):  53%|█████▎    | 1062/2000 [00:11<00:08, 106.61it/s, train_loss=0.6410, val_loss=0.6423]\u001B[A\n",
      "Training (polyak):  54%|█████▍    | 1076/2000 [00:11<00:08, 113.70it/s, train_loss=0.6410, val_loss=0.6423]\u001B[A\n",
      "Training (polyak):  55%|█████▍    | 1090/2000 [00:11<00:07, 119.42it/s, train_loss=0.6410, val_loss=0.6423]\u001B[A\n",
      "Training (polyak):  55%|█████▍    | 1090/2000 [00:12<00:07, 119.42it/s, train_loss=0.6807, val_loss=0.6756]\u001B[A\n",
      "Training (polyak):  55%|█████▌    | 1103/2000 [00:12<00:12, 70.33it/s, train_loss=0.6807, val_loss=0.6756] \u001B[A\n",
      "Training (polyak):  56%|█████▌    | 1116/2000 [00:12<00:10, 81.06it/s, train_loss=0.6807, val_loss=0.6756]\u001B[A\n",
      "Training (polyak):  56%|█████▋    | 1130/2000 [00:12<00:09, 91.80it/s, train_loss=0.6807, val_loss=0.6756]\u001B[A\n",
      "Training (polyak):  57%|█████▋    | 1143/2000 [00:12<00:08, 100.36it/s, train_loss=0.6807, val_loss=0.6756]\u001B[A\n",
      "Training (polyak):  58%|█████▊    | 1156/2000 [00:12<00:07, 107.26it/s, train_loss=0.6807, val_loss=0.6756]\u001B[A\n",
      "Training (polyak):  58%|█████▊    | 1169/2000 [00:12<00:07, 112.98it/s, train_loss=0.6807, val_loss=0.6756]\u001B[A\n",
      "Training (polyak):  59%|█████▉    | 1183/2000 [00:12<00:06, 118.15it/s, train_loss=0.6807, val_loss=0.6756]\u001B[A\n",
      "Training (polyak):  60%|█████▉    | 1197/2000 [00:12<00:06, 121.84it/s, train_loss=0.6807, val_loss=0.6756]\u001B[A\n",
      "Training (polyak):  60%|█████▉    | 1197/2000 [00:13<00:06, 121.84it/s, train_loss=0.6257, val_loss=0.6318]\u001B[A\n",
      "Training (polyak):  60%|██████    | 1210/2000 [00:13<00:11, 70.92it/s, train_loss=0.6257, val_loss=0.6318] \u001B[A\n",
      "Training (polyak):  61%|██████    | 1223/2000 [00:13<00:09, 81.70it/s, train_loss=0.6257, val_loss=0.6318]\u001B[A\n",
      "Training (polyak):  62%|██████▏   | 1237/2000 [00:13<00:08, 92.76it/s, train_loss=0.6257, val_loss=0.6318]\u001B[A\n",
      "Training (polyak):  62%|██████▎   | 1250/2000 [00:13<00:07, 101.17it/s, train_loss=0.6257, val_loss=0.6318]\u001B[A\n",
      "Training (polyak):  63%|██████▎   | 1263/2000 [00:13<00:06, 107.87it/s, train_loss=0.6257, val_loss=0.6318]\u001B[A\n",
      "Training (polyak):  64%|██████▍   | 1277/2000 [00:13<00:06, 114.19it/s, train_loss=0.6257, val_loss=0.6318]\u001B[A\n",
      "Training (polyak):  64%|██████▍   | 1290/2000 [00:13<00:06, 117.67it/s, train_loss=0.6257, val_loss=0.6318]\u001B[A\n",
      "Training (polyak):  64%|██████▍   | 1290/2000 [00:14<00:06, 117.67it/s, train_loss=0.5991, val_loss=0.6040]\u001B[A\n",
      "Training (polyak):  65%|██████▌   | 1303/2000 [00:14<00:10, 66.75it/s, train_loss=0.5991, val_loss=0.6040] \u001B[A\n",
      "Training (polyak):  66%|██████▌   | 1314/2000 [00:14<00:09, 74.28it/s, train_loss=0.5991, val_loss=0.6040]\u001B[A\n",
      "Training (polyak):  66%|██████▋   | 1326/2000 [00:14<00:08, 82.30it/s, train_loss=0.5991, val_loss=0.6040]\u001B[A\n",
      "Training (polyak):  67%|██████▋   | 1339/2000 [00:14<00:07, 91.47it/s, train_loss=0.5991, val_loss=0.6040]\u001B[A\n",
      "Training (polyak):  68%|██████▊   | 1352/2000 [00:14<00:06, 99.97it/s, train_loss=0.5991, val_loss=0.6040]\u001B[A\n",
      "Training (polyak):  68%|██████▊   | 1366/2000 [00:14<00:05, 109.12it/s, train_loss=0.5991, val_loss=0.6040]\u001B[A\n",
      "Training (polyak):  69%|██████▉   | 1380/2000 [00:14<00:05, 116.72it/s, train_loss=0.5991, val_loss=0.6040]\u001B[A\n",
      "Training (polyak):  70%|██████▉   | 1394/2000 [00:14<00:04, 121.98it/s, train_loss=0.5991, val_loss=0.6040]\u001B[A\n",
      "Training (polyak):  70%|██████▉   | 1394/2000 [00:15<00:04, 121.98it/s, train_loss=0.5989, val_loss=0.6030]\u001B[A\n",
      "Training (polyak):  70%|███████   | 1407/2000 [00:15<00:08, 72.07it/s, train_loss=0.5989, val_loss=0.6030] \u001B[A\n",
      "Training (polyak):  71%|███████   | 1421/2000 [00:15<00:06, 83.70it/s, train_loss=0.5989, val_loss=0.6030]\u001B[A\n",
      "Training (polyak):  72%|███████▏  | 1435/2000 [00:15<00:05, 94.45it/s, train_loss=0.5989, val_loss=0.6030]\u001B[A\n",
      "Training (polyak):  72%|███████▏  | 1449/2000 [00:15<00:05, 103.84it/s, train_loss=0.5989, val_loss=0.6030]\u001B[A\n",
      "Training (polyak):  73%|███████▎  | 1463/2000 [00:15<00:04, 110.86it/s, train_loss=0.5989, val_loss=0.6030]\u001B[A\n",
      "Training (polyak):  74%|███████▍  | 1476/2000 [00:15<00:04, 115.29it/s, train_loss=0.5989, val_loss=0.6030]\u001B[A\n",
      "Training (polyak):  74%|███████▍  | 1490/2000 [00:15<00:04, 120.53it/s, train_loss=0.5989, val_loss=0.6030]\u001B[A\n",
      "Training (polyak):  74%|███████▍  | 1490/2000 [00:16<00:04, 120.53it/s, train_loss=0.6084, val_loss=0.6056]\u001B[A\n",
      "Training (polyak):  75%|███████▌  | 1503/2000 [00:16<00:07, 68.83it/s, train_loss=0.6084, val_loss=0.6056] \u001B[A\n",
      "Training (polyak):  76%|███████▌  | 1516/2000 [00:16<00:06, 79.77it/s, train_loss=0.6084, val_loss=0.6056]\u001B[A\n",
      "Training (polyak):  76%|███████▋  | 1530/2000 [00:16<00:05, 90.90it/s, train_loss=0.6084, val_loss=0.6056]\u001B[A\n",
      "Training (polyak):  77%|███████▋  | 1544/2000 [00:16<00:04, 100.53it/s, train_loss=0.6084, val_loss=0.6056]\u001B[A\n",
      "Training (polyak):  78%|███████▊  | 1558/2000 [00:16<00:04, 108.86it/s, train_loss=0.6084, val_loss=0.6056]\u001B[A\n",
      "Training (polyak):  79%|███████▊  | 1571/2000 [00:16<00:03, 113.97it/s, train_loss=0.6084, val_loss=0.6056]\u001B[A\n",
      "Training (polyak):  79%|███████▉  | 1584/2000 [00:16<00:03, 118.20it/s, train_loss=0.6084, val_loss=0.6056]\u001B[A\n",
      "Training (polyak):  80%|███████▉  | 1598/2000 [00:17<00:03, 121.80it/s, train_loss=0.6084, val_loss=0.6056]\u001B[A\n",
      "Training (polyak):  80%|███████▉  | 1598/2000 [00:17<00:03, 121.80it/s, train_loss=0.5539, val_loss=0.5718]\u001B[A\n",
      "Training (polyak):  81%|████████  | 1611/2000 [00:17<00:05, 71.14it/s, train_loss=0.5539, val_loss=0.5718] \u001B[A\n",
      "Training (polyak):  81%|████████▏ | 1625/2000 [00:17<00:04, 83.00it/s, train_loss=0.5539, val_loss=0.5718]\u001B[A\n",
      "Training (polyak):  82%|████████▏ | 1639/2000 [00:17<00:03, 93.74it/s, train_loss=0.5539, val_loss=0.5718]\u001B[A\n",
      "Training (polyak):  83%|████████▎ | 1652/2000 [00:17<00:03, 101.29it/s, train_loss=0.5539, val_loss=0.5718]\u001B[A\n",
      "Training (polyak):  83%|████████▎ | 1665/2000 [00:17<00:03, 106.39it/s, train_loss=0.5539, val_loss=0.5718]\u001B[A\n",
      "Training (polyak):  84%|████████▍ | 1678/2000 [00:17<00:02, 110.64it/s, train_loss=0.5539, val_loss=0.5718]\u001B[A\n",
      "Training (polyak):  85%|████████▍ | 1691/2000 [00:18<00:02, 113.84it/s, train_loss=0.5539, val_loss=0.5718]\u001B[A\n",
      "Training (polyak):  85%|████████▍ | 1691/2000 [00:18<00:02, 113.84it/s, train_loss=0.5400, val_loss=0.5506]\u001B[A\n",
      "Training (polyak):  85%|████████▌ | 1704/2000 [00:18<00:04, 65.88it/s, train_loss=0.5400, val_loss=0.5506] \u001B[A\n",
      "Training (polyak):  86%|████████▌ | 1718/2000 [00:18<00:03, 78.55it/s, train_loss=0.5400, val_loss=0.5506]\u001B[A\n",
      "Training (polyak):  87%|████████▋ | 1731/2000 [00:18<00:03, 86.95it/s, train_loss=0.5400, val_loss=0.5506]\u001B[A\n",
      "Training (polyak):  87%|████████▋ | 1743/2000 [00:18<00:02, 93.79it/s, train_loss=0.5400, val_loss=0.5506]\u001B[A\n",
      "Training (polyak):  88%|████████▊ | 1756/2000 [00:18<00:02, 100.56it/s, train_loss=0.5400, val_loss=0.5506]\u001B[A\n",
      "Training (polyak):  88%|████████▊ | 1768/2000 [00:19<00:02, 104.97it/s, train_loss=0.5400, val_loss=0.5506]\u001B[A\n",
      "Training (polyak):  89%|████████▉ | 1780/2000 [00:19<00:02, 108.26it/s, train_loss=0.5400, val_loss=0.5506]\u001B[A\n",
      "Training (polyak):  90%|████████▉ | 1792/2000 [00:19<00:01, 109.11it/s, train_loss=0.5400, val_loss=0.5506]\u001B[A\n",
      "Training (polyak):  90%|████████▉ | 1792/2000 [00:19<00:01, 109.11it/s, train_loss=0.5554, val_loss=0.5498]\u001B[A\n",
      "Training (polyak):  90%|█████████ | 1804/2000 [00:19<00:03, 63.66it/s, train_loss=0.5554, val_loss=0.5498] \u001B[A\n",
      "Training (polyak):  91%|█████████ | 1817/2000 [00:19<00:02, 74.81it/s, train_loss=0.5554, val_loss=0.5498]\u001B[A\n",
      "Training (polyak):  92%|█████████▏| 1830/2000 [00:19<00:02, 84.80it/s, train_loss=0.5554, val_loss=0.5498]\u001B[A\n",
      "Training (polyak):  92%|█████████▏| 1843/2000 [00:19<00:01, 93.34it/s, train_loss=0.5554, val_loss=0.5498]\u001B[A\n",
      "Training (polyak):  93%|█████████▎| 1856/2000 [00:20<00:01, 100.44it/s, train_loss=0.5554, val_loss=0.5498]\u001B[A\n",
      "Training (polyak):  93%|█████████▎| 1868/2000 [00:20<00:01, 100.40it/s, train_loss=0.5554, val_loss=0.5498]\u001B[A\n",
      "Training (polyak):  94%|█████████▍| 1879/2000 [00:20<00:01, 101.64it/s, train_loss=0.5554, val_loss=0.5498]\u001B[A\n",
      "Training (polyak):  95%|█████████▍| 1891/2000 [00:20<00:01, 105.02it/s, train_loss=0.5554, val_loss=0.5498]\u001B[A\n",
      "Training (polyak):  95%|█████████▍| 1891/2000 [00:20<00:01, 105.02it/s, train_loss=0.5054, val_loss=0.5010]\u001B[A\n",
      "Training (polyak):  95%|█████████▌| 1902/2000 [00:20<00:01, 54.60it/s, train_loss=0.5054, val_loss=0.5010] \u001B[A\n",
      "Training (polyak):  96%|█████████▌| 1913/2000 [00:20<00:01, 63.60it/s, train_loss=0.5054, val_loss=0.5010]\u001B[A\n",
      "Training (polyak):  96%|█████████▌| 1923/2000 [00:21<00:01, 69.62it/s, train_loss=0.5054, val_loss=0.5010]\u001B[A\n",
      "Training (polyak):  97%|█████████▋| 1935/2000 [00:21<00:00, 79.35it/s, train_loss=0.5054, val_loss=0.5010]\u001B[A\n",
      "Training (polyak):  97%|█████████▋| 1947/2000 [00:21<00:00, 88.20it/s, train_loss=0.5054, val_loss=0.5010]\u001B[A\n",
      "Training (polyak):  98%|█████████▊| 1960/2000 [00:21<00:00, 97.34it/s, train_loss=0.5054, val_loss=0.5010]\u001B[A\n",
      "Training (polyak):  99%|█████████▊| 1973/2000 [00:21<00:00, 105.28it/s, train_loss=0.5054, val_loss=0.5010]\u001B[A\n",
      "Training (polyak):  99%|█████████▉| 1986/2000 [00:21<00:00, 111.88it/s, train_loss=0.5054, val_loss=0.5010]\u001B[A\n",
      "Training (polyak): 100%|██████████| 2000/2000 [00:21<00:00, 92.39it/s, train_loss=0.5054, val_loss=0.5010] \u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with AdamW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   0%|          | 1/2000 [00:00<10:44,  3.10it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   0%|          | 10/2000 [00:00<01:09, 28.59it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   1%|          | 17/2000 [00:00<00:48, 40.54it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   1%|▏         | 26/2000 [00:00<00:36, 53.84it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   2%|▏         | 35/2000 [00:00<00:31, 63.12it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   2%|▏         | 44/2000 [00:00<00:28, 69.61it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   3%|▎         | 53/2000 [00:00<00:25, 74.95it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   3%|▎         | 62/2000 [00:01<00:24, 77.67it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   4%|▎         | 72/2000 [00:01<00:23, 81.56it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   4%|▍         | 81/2000 [00:01<00:23, 82.53it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   4%|▍         | 90/2000 [00:01<00:23, 80.62it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   5%|▍         | 99/2000 [00:01<00:25, 75.67it/s, train_loss=3.6922, val_loss=3.6926]\u001B[A\n",
      "Training (adam):   5%|▍         | 99/2000 [00:02<00:25, 75.67it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   5%|▌         | 107/2000 [00:02<00:54, 34.86it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   6%|▌         | 116/2000 [00:02<00:44, 42.76it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   6%|▋         | 125/2000 [00:02<00:36, 50.85it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   7%|▋         | 135/2000 [00:02<00:31, 59.37it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   7%|▋         | 144/2000 [00:02<00:28, 65.74it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   8%|▊         | 153/2000 [00:02<00:25, 71.37it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   8%|▊         | 162/2000 [00:02<00:24, 75.99it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   9%|▊         | 171/2000 [00:02<00:23, 79.27it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   9%|▉         | 180/2000 [00:02<00:22, 79.36it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):   9%|▉         | 189/2000 [00:03<00:22, 81.58it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:21, 83.13it/s, train_loss=2.3535, val_loss=2.3484]\u001B[A\n",
      "Training (adam):  10%|▉         | 198/2000 [00:03<00:21, 83.13it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:37, 47.89it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  11%|█         | 216/2000 [00:03<00:32, 55.65it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  11%|█▏        | 225/2000 [00:03<00:28, 62.69it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  12%|█▏        | 234/2000 [00:03<00:25, 68.69it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  12%|█▏        | 243/2000 [00:03<00:24, 71.85it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  13%|█▎        | 252/2000 [00:04<00:23, 74.40it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  13%|█▎        | 261/2000 [00:04<00:23, 73.26it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  14%|█▎        | 270/2000 [00:04<00:22, 76.82it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  14%|█▍        | 279/2000 [00:04<00:21, 80.08it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  14%|█▍        | 288/2000 [00:04<00:20, 82.49it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:20, 84.12it/s, train_loss=1.8568, val_loss=1.8760]\u001B[A\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:20, 84.12it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  15%|█▌        | 306/2000 [00:04<00:35, 47.60it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  16%|█▌        | 315/2000 [00:05<00:30, 55.13it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  16%|█▌        | 324/2000 [00:05<00:26, 62.16it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  17%|█▋        | 333/2000 [00:05<00:24, 68.22it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  17%|█▋        | 342/2000 [00:05<00:22, 72.73it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  18%|█▊        | 351/2000 [00:05<00:21, 76.15it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  18%|█▊        | 360/2000 [00:05<00:20, 79.16it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  18%|█▊        | 369/2000 [00:05<00:21, 75.39it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  19%|█▉        | 377/2000 [00:05<00:22, 72.37it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  19%|█▉        | 385/2000 [00:05<00:23, 67.99it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:06<00:24, 66.03it/s, train_loss=1.3605, val_loss=1.3544]\u001B[A\n",
      "Training (adam):  20%|█▉        | 393/2000 [00:06<00:24, 66.03it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  20%|██        | 401/2000 [00:06<00:39, 40.92it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  20%|██        | 410/2000 [00:06<00:32, 49.19it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  21%|██        | 419/2000 [00:06<00:27, 57.07it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  21%|██▏       | 428/2000 [00:06<00:24, 63.65it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  22%|██▏       | 437/2000 [00:06<00:22, 69.24it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  22%|██▏       | 446/2000 [00:06<00:21, 73.75it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  23%|██▎       | 455/2000 [00:07<00:19, 77.37it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  23%|██▎       | 464/2000 [00:07<00:19, 80.28it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  24%|██▎       | 473/2000 [00:07<00:18, 82.33it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  24%|██▍       | 482/2000 [00:07<00:18, 83.55it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  25%|██▍       | 491/2000 [00:07<00:17, 84.28it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:07<00:17, 85.18it/s, train_loss=0.8164, val_loss=0.8227]\u001B[A\n",
      "Training (adam):  25%|██▌       | 500/2000 [00:07<00:17, 85.18it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  25%|██▌       | 509/2000 [00:07<00:30, 48.30it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  26%|██▌       | 518/2000 [00:08<00:26, 55.82it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  26%|██▋       | 527/2000 [00:08<00:23, 62.52it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:08<00:21, 68.19it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  27%|██▋       | 545/2000 [00:08<00:19, 72.84it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  28%|██▊       | 554/2000 [00:08<00:19, 75.92it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  28%|██▊       | 563/2000 [00:08<00:18, 78.92it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  29%|██▊       | 572/2000 [00:08<00:17, 81.04it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  29%|██▉       | 581/2000 [00:08<00:17, 82.87it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  30%|██▉       | 590/2000 [00:08<00:16, 83.81it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  30%|██▉       | 599/2000 [00:09<00:16, 84.24it/s, train_loss=0.6754, val_loss=0.6736]\u001B[A\n",
      "Training (adam):  30%|██▉       | 599/2000 [00:09<00:16, 84.24it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  30%|███       | 608/2000 [00:09<00:33, 41.13it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  31%|███       | 615/2000 [00:09<00:30, 45.26it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  31%|███       | 622/2000 [00:09<00:29, 46.70it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  31%|███▏      | 629/2000 [00:09<00:27, 50.47it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  32%|███▏      | 636/2000 [00:09<00:24, 54.65it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  32%|███▏      | 644/2000 [00:10<00:22, 59.10it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  33%|███▎      | 652/2000 [00:10<00:21, 63.80it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  33%|███▎      | 661/2000 [00:10<00:19, 68.75it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  34%|███▎      | 670/2000 [00:10<00:18, 72.06it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  34%|███▍      | 679/2000 [00:10<00:17, 74.13it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  34%|███▍      | 688/2000 [00:10<00:17, 76.00it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:10<00:19, 67.40it/s, train_loss=0.5780, val_loss=0.5833]\u001B[A\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:11<00:19, 67.40it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  35%|███▌      | 704/2000 [00:11<00:39, 33.04it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  36%|███▌      | 712/2000 [00:11<00:32, 39.47it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  36%|███▌      | 720/2000 [00:11<00:28, 45.32it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  36%|███▋      | 728/2000 [00:11<00:24, 50.94it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  37%|███▋      | 736/2000 [00:11<00:22, 56.74it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  37%|███▋      | 745/2000 [00:11<00:19, 62.88it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  38%|███▊      | 754/2000 [00:11<00:18, 67.26it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  38%|███▊      | 762/2000 [00:12<00:19, 63.27it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  38%|███▊      | 770/2000 [00:12<00:18, 65.20it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  39%|███▉      | 777/2000 [00:12<00:19, 63.24it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  39%|███▉      | 784/2000 [00:12<00:19, 62.14it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  40%|███▉      | 792/2000 [00:12<00:18, 66.10it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  40%|████      | 800/2000 [00:12<00:17, 69.36it/s, train_loss=0.5269, val_loss=0.5313]\u001B[A\n",
      "Training (adam):  40%|████      | 800/2000 [00:12<00:17, 69.36it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  40%|████      | 808/2000 [00:13<00:29, 40.55it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  41%|████      | 817/2000 [00:13<00:24, 48.98it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  41%|████▏     | 826/2000 [00:13<00:20, 56.74it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  42%|████▏     | 835/2000 [00:13<00:18, 63.20it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  42%|████▏     | 844/2000 [00:13<00:16, 68.84it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  43%|████▎     | 853/2000 [00:13<00:15, 72.84it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  43%|████▎     | 862/2000 [00:13<00:14, 76.34it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  44%|████▎     | 871/2000 [00:13<00:14, 78.85it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  44%|████▍     | 880/2000 [00:13<00:13, 80.40it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  44%|████▍     | 889/2000 [00:14<00:14, 78.49it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  45%|████▍     | 898/2000 [00:14<00:14, 75.92it/s, train_loss=0.5246, val_loss=0.5165]\u001B[A\n",
      "Training (adam):  45%|████▍     | 898/2000 [00:14<00:14, 75.92it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  45%|████▌     | 906/2000 [00:14<00:26, 41.78it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  46%|████▌     | 914/2000 [00:14<00:22, 47.67it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  46%|████▌     | 923/2000 [00:14<00:19, 54.73it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  47%|████▋     | 932/2000 [00:14<00:17, 61.18it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  47%|████▋     | 941/2000 [00:15<00:16, 65.55it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  47%|████▋     | 949/2000 [00:15<00:15, 67.42it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  48%|████▊     | 957/2000 [00:15<00:14, 69.94it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  48%|████▊     | 965/2000 [00:15<00:14, 70.50it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  49%|████▊     | 973/2000 [00:15<00:14, 71.64it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  49%|████▉     | 981/2000 [00:15<00:14, 71.24it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  49%|████▉     | 989/2000 [00:15<00:14, 71.45it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:15<00:14, 71.03it/s, train_loss=0.5122, val_loss=0.5092]\u001B[A\n",
      "Training (adam):  50%|████▉     | 997/2000 [00:16<00:14, 71.03it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:16<00:28, 35.22it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  51%|█████     | 1012/2000 [00:16<00:24, 40.50it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  51%|█████     | 1020/2000 [00:16<00:20, 47.42it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  51%|█████▏    | 1029/2000 [00:16<00:17, 55.06it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  52%|█████▏    | 1037/2000 [00:16<00:15, 60.62it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  52%|█████▏    | 1045/2000 [00:16<00:14, 63.83it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  53%|█████▎    | 1054/2000 [00:16<00:13, 68.64it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  53%|█████▎    | 1063/2000 [00:17<00:12, 73.25it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  54%|█████▎    | 1071/2000 [00:17<00:13, 70.05it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  54%|█████▍    | 1079/2000 [00:17<00:13, 69.99it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  54%|█████▍    | 1087/2000 [00:17<00:12, 71.94it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:17<00:12, 74.47it/s, train_loss=0.5018, val_loss=0.4915]\u001B[A\n",
      "Training (adam):  55%|█████▍    | 1096/2000 [00:17<00:12, 74.47it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  55%|█████▌    | 1104/2000 [00:17<00:21, 41.87it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  56%|█████▌    | 1113/2000 [00:17<00:17, 50.00it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  56%|█████▌    | 1120/2000 [00:18<00:16, 53.96it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  56%|█████▋    | 1127/2000 [00:18<00:16, 53.36it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  57%|█████▋    | 1134/2000 [00:18<00:15, 56.09it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  57%|█████▋    | 1141/2000 [00:18<00:14, 59.04it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  57%|█████▋    | 1149/2000 [00:18<00:13, 62.26it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  58%|█████▊    | 1156/2000 [00:18<00:14, 58.79it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  58%|█████▊    | 1163/2000 [00:18<00:14, 58.36it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  59%|█████▊    | 1171/2000 [00:18<00:13, 63.75it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  59%|█████▉    | 1180/2000 [00:19<00:11, 69.51it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  59%|█████▉    | 1189/2000 [00:19<00:10, 74.40it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:19<00:10, 76.37it/s, train_loss=0.4808, val_loss=0.4896]\u001B[A\n",
      "Training (adam):  60%|█████▉    | 1198/2000 [00:19<00:10, 76.37it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  60%|██████    | 1206/2000 [00:19<00:22, 34.72it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  61%|██████    | 1214/2000 [00:19<00:19, 40.76it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  61%|██████    | 1222/2000 [00:19<00:16, 47.35it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  62%|██████▏   | 1231/2000 [00:20<00:14, 54.57it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:20<00:12, 61.11it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  62%|██████▏   | 1249/2000 [00:20<00:11, 66.76it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  63%|██████▎   | 1257/2000 [00:20<00:10, 69.14it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  63%|██████▎   | 1265/2000 [00:20<00:10, 70.24it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  64%|██████▎   | 1274/2000 [00:20<00:09, 74.14it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  64%|██████▍   | 1283/2000 [00:20<00:09, 77.15it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:20<00:08, 78.92it/s, train_loss=0.4891, val_loss=0.4937]\u001B[A\n",
      "Training (adam):  65%|██████▍   | 1292/2000 [00:21<00:08, 78.92it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  65%|██████▌   | 1301/2000 [00:21<00:16, 41.81it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  65%|██████▌   | 1309/2000 [00:21<00:14, 47.77it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  66%|██████▌   | 1317/2000 [00:21<00:12, 52.87it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  66%|██████▋   | 1326/2000 [00:21<00:11, 59.69it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  67%|██████▋   | 1335/2000 [00:21<00:10, 65.58it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  67%|██████▋   | 1344/2000 [00:21<00:09, 69.83it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  68%|██████▊   | 1352/2000 [00:21<00:09, 70.82it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  68%|██████▊   | 1360/2000 [00:22<00:08, 71.73it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  68%|██████▊   | 1369/2000 [00:22<00:08, 74.83it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  69%|██████▉   | 1378/2000 [00:22<00:08, 77.06it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  69%|██████▉   | 1387/2000 [00:22<00:07, 79.16it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:22<00:07, 80.83it/s, train_loss=0.4808, val_loss=0.4917]\u001B[A\n",
      "Training (adam):  70%|██████▉   | 1396/2000 [00:22<00:07, 80.83it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  70%|███████   | 1405/2000 [00:22<00:13, 44.07it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  71%|███████   | 1414/2000 [00:23<00:11, 51.56it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  71%|███████   | 1422/2000 [00:23<00:10, 57.13it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  72%|███████▏  | 1431/2000 [00:23<00:08, 63.27it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  72%|███████▏  | 1440/2000 [00:23<00:08, 68.70it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  72%|███████▏  | 1449/2000 [00:23<00:07, 72.44it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  73%|███████▎  | 1458/2000 [00:23<00:07, 75.78it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  73%|███████▎  | 1467/2000 [00:23<00:06, 78.37it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  74%|███████▍  | 1476/2000 [00:23<00:06, 80.44it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  74%|███████▍  | 1485/2000 [00:23<00:06, 81.81it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  75%|███████▍  | 1494/2000 [00:23<00:06, 82.62it/s, train_loss=0.4649, val_loss=0.4672]\u001B[A\n",
      "Training (adam):  75%|███████▍  | 1494/2000 [00:24<00:06, 82.62it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  75%|███████▌  | 1503/2000 [00:24<00:10, 46.31it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  76%|███████▌  | 1512/2000 [00:24<00:09, 53.67it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  76%|███████▌  | 1521/2000 [00:24<00:07, 60.51it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  76%|███████▋  | 1530/2000 [00:24<00:07, 66.29it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  77%|███████▋  | 1539/2000 [00:24<00:06, 71.21it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  77%|███████▋  | 1548/2000 [00:24<00:06, 75.28it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  78%|███████▊  | 1557/2000 [00:24<00:05, 78.18it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  78%|███████▊  | 1566/2000 [00:25<00:05, 79.97it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  79%|███████▉  | 1575/2000 [00:25<00:05, 81.64it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  79%|███████▉  | 1584/2000 [00:25<00:04, 83.26it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  80%|███████▉  | 1593/2000 [00:25<00:04, 84.04it/s, train_loss=0.4689, val_loss=0.4747]\u001B[A\n",
      "Training (adam):  80%|███████▉  | 1593/2000 [00:25<00:04, 84.04it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  80%|████████  | 1602/2000 [00:25<00:08, 45.55it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  81%|████████  | 1611/2000 [00:25<00:07, 53.00it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  81%|████████  | 1620/2000 [00:26<00:06, 59.81it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  81%|████████▏ | 1629/2000 [00:26<00:05, 65.85it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  82%|████████▏ | 1638/2000 [00:26<00:05, 71.09it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  82%|████████▏ | 1647/2000 [00:26<00:04, 74.70it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  83%|████████▎ | 1656/2000 [00:26<00:04, 77.63it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  83%|████████▎ | 1665/2000 [00:26<00:04, 79.93it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  84%|████████▎ | 1674/2000 [00:26<00:03, 81.73it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  84%|████████▍ | 1683/2000 [00:26<00:03, 83.25it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:26<00:03, 84.16it/s, train_loss=0.4693, val_loss=0.4732]\u001B[A\n",
      "Training (adam):  85%|████████▍ | 1692/2000 [00:27<00:03, 84.16it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:27<00:06, 45.33it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  86%|████████▌ | 1710/2000 [00:27<00:05, 52.84it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  86%|████████▌ | 1719/2000 [00:27<00:04, 59.12it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  86%|████████▋ | 1728/2000 [00:27<00:04, 65.37it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  87%|████████▋ | 1736/2000 [00:27<00:03, 67.97it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  87%|████████▋ | 1745/2000 [00:27<00:03, 72.61it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:27<00:03, 75.92it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  88%|████████▊ | 1763/2000 [00:28<00:02, 79.02it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  89%|████████▊ | 1772/2000 [00:28<00:02, 81.03it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  89%|████████▉ | 1781/2000 [00:28<00:02, 82.34it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  90%|████████▉ | 1790/2000 [00:28<00:02, 83.43it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:28<00:02, 84.58it/s, train_loss=0.4552, val_loss=0.4570]\u001B[A\n",
      "Training (adam):  90%|████████▉ | 1799/2000 [00:28<00:02, 84.58it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  90%|█████████ | 1808/2000 [00:28<00:03, 48.48it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  91%|█████████ | 1817/2000 [00:28<00:03, 55.99it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  91%|█████████▏| 1826/2000 [00:28<00:02, 62.59it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  92%|█████████▏| 1835/2000 [00:29<00:02, 68.44it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  92%|█████████▏| 1844/2000 [00:29<00:02, 70.79it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  93%|█████████▎| 1853/2000 [00:29<00:01, 73.53it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  93%|█████████▎| 1862/2000 [00:29<00:01, 76.51it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  94%|█████████▎| 1871/2000 [00:29<00:01, 77.92it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:29<00:01, 75.66it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  94%|█████████▍| 1888/2000 [00:29<00:01, 74.91it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:29<00:01, 76.84it/s, train_loss=0.4682, val_loss=0.4706]\u001B[A\n",
      "Training (adam):  95%|█████████▍| 1897/2000 [00:30<00:01, 76.84it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  95%|█████████▌| 1905/2000 [00:30<00:02, 44.16it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  96%|█████████▌| 1914/2000 [00:30<00:01, 51.88it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  96%|█████████▌| 1923/2000 [00:30<00:01, 58.89it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  97%|█████████▋| 1932/2000 [00:30<00:01, 65.11it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  97%|█████████▋| 1941/2000 [00:30<00:00, 69.69it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  98%|█████████▊| 1950/2000 [00:30<00:00, 73.55it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  98%|█████████▊| 1959/2000 [00:30<00:00, 76.63it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  98%|█████████▊| 1968/2000 [00:31<00:00, 79.12it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  99%|█████████▉| 1977/2000 [00:31<00:00, 81.14it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam):  99%|█████████▉| 1986/2000 [00:31<00:00, 77.12it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:31<00:00, 63.65it/s, train_loss=0.4669, val_loss=0.4609]\u001B[A\n",
      "Trials:  67%|██████▋   | 2/3 [02:17<01:09, 69.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 3/3:\n",
      "Training with Constant SGD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (constant):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   0%|          | 1/2000 [00:00<09:30,  3.50it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   1%|          | 19/2000 [00:00<00:31, 62.27it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   2%|▏         | 35/2000 [00:00<00:21, 89.55it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   2%|▏         | 48/2000 [00:00<00:20, 96.03it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   3%|▎         | 60/2000 [00:00<00:25, 75.53it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   4%|▎         | 70/2000 [00:00<00:24, 79.19it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   4%|▍         | 84/2000 [00:01<00:20, 92.89it/s, train_loss=3.6821, val_loss=3.6828]\u001B[A\n",
      "Training (constant):   4%|▍         | 84/2000 [00:01<00:20, 92.89it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   5%|▌         | 101/2000 [00:01<00:33, 56.53it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   6%|▌         | 118/2000 [00:01<00:25, 73.75it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   7%|▋         | 135/2000 [00:01<00:20, 90.78it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   8%|▊         | 152/2000 [00:01<00:17, 106.41it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   8%|▊         | 169/2000 [00:01<00:15, 119.39it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   9%|▉         | 185/2000 [00:02<00:14, 129.02it/s, train_loss=3.2751, val_loss=3.2775]\u001B[A\n",
      "Training (constant):   9%|▉         | 185/2000 [00:02<00:14, 129.02it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  10%|█         | 201/2000 [00:02<00:22, 78.36it/s, train_loss=3.0934, val_loss=3.0878] \u001B[A\n",
      "Training (constant):  11%|█         | 218/2000 [00:02<00:19, 93.22it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  12%|█▏        | 235/2000 [00:02<00:16, 107.97it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  13%|█▎        | 251/2000 [00:02<00:14, 118.44it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  13%|█▎        | 268/2000 [00:02<00:13, 129.53it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  14%|█▍        | 285/2000 [00:02<00:12, 138.36it/s, train_loss=3.0934, val_loss=3.0878]\u001B[A\n",
      "Training (constant):  14%|█▍        | 285/2000 [00:03<00:12, 138.36it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  15%|█▌        | 301/2000 [00:03<00:21, 77.91it/s, train_loss=2.9572, val_loss=2.9528] \u001B[A\n",
      "Training (constant):  16%|█▌        | 318/2000 [00:03<00:18, 93.03it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  17%|█▋        | 335/2000 [00:03<00:15, 107.21it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  18%|█▊        | 352/2000 [00:03<00:13, 119.79it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  18%|█▊        | 369/2000 [00:03<00:12, 130.16it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  19%|█▉        | 386/2000 [00:03<00:11, 139.72it/s, train_loss=2.9572, val_loss=2.9528]\u001B[A\n",
      "Training (constant):  19%|█▉        | 386/2000 [00:04<00:11, 139.72it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  20%|██        | 402/2000 [00:04<00:18, 84.34it/s, train_loss=2.8577, val_loss=2.8565] \u001B[A\n",
      "Training (constant):  21%|██        | 419/2000 [00:04<00:15, 99.21it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  22%|██▏       | 437/2000 [00:04<00:13, 114.34it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  23%|██▎       | 454/2000 [00:04<00:12, 126.65it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  24%|██▎       | 471/2000 [00:04<00:11, 136.87it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  24%|██▍       | 488/2000 [00:04<00:10, 145.24it/s, train_loss=2.8577, val_loss=2.8565]\u001B[A\n",
      "Training (constant):  24%|██▍       | 488/2000 [00:05<00:10, 145.24it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  25%|██▌       | 505/2000 [00:05<00:19, 76.92it/s, train_loss=2.7661, val_loss=2.7745] \u001B[A\n",
      "Training (constant):  26%|██▌       | 522/2000 [00:05<00:16, 91.62it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  27%|██▋       | 540/2000 [00:05<00:13, 107.27it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  28%|██▊       | 558/2000 [00:05<00:11, 121.10it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  29%|██▉       | 575/2000 [00:05<00:10, 130.65it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  30%|██▉       | 592/2000 [00:05<00:10, 139.22it/s, train_loss=2.7661, val_loss=2.7745]\u001B[A\n",
      "Training (constant):  30%|██▉       | 592/2000 [00:06<00:10, 139.22it/s, train_loss=2.6927, val_loss=2.6949]\u001B[A\n",
      "Training (constant):  30%|███       | 608/2000 [00:06<00:16, 82.72it/s, train_loss=2.6927, val_loss=2.6949] \u001B[A\n",
      "Training (constant):  31%|███▏      | 625/2000 [00:06<00:14, 97.34it/s, train_loss=2.6927, val_loss=2.6949]\u001B[A\n",
      "Training (constant):  32%|███▏      | 643/2000 [00:06<00:12, 112.78it/s, train_loss=2.6927, val_loss=2.6949]\u001B[A\n",
      "Training (constant):  33%|███▎      | 660/2000 [00:06<00:10, 125.07it/s, train_loss=2.6927, val_loss=2.6949]\u001B[A\n",
      "Training (constant):  34%|███▍      | 677/2000 [00:06<00:09, 135.45it/s, train_loss=2.6927, val_loss=2.6949]\u001B[A\n",
      "Training (constant):  35%|███▍      | 695/2000 [00:06<00:09, 144.93it/s, train_loss=2.6927, val_loss=2.6949]\u001B[A\n",
      "Training (constant):  35%|███▍      | 695/2000 [00:06<00:09, 144.93it/s, train_loss=2.6324, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  36%|███▌      | 712/2000 [00:07<00:14, 89.90it/s, train_loss=2.6324, val_loss=2.6388] \u001B[A\n",
      "Training (constant):  36%|███▋      | 730/2000 [00:07<00:12, 105.41it/s, train_loss=2.6324, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  37%|███▋      | 748/2000 [00:07<00:10, 120.22it/s, train_loss=2.6324, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  38%|███▊      | 766/2000 [00:07<00:09, 132.32it/s, train_loss=2.6324, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  39%|███▉      | 783/2000 [00:07<00:08, 140.70it/s, train_loss=2.6324, val_loss=2.6388]\u001B[A\n",
      "Training (constant):  39%|███▉      | 783/2000 [00:07<00:08, 140.70it/s, train_loss=2.5564, val_loss=2.5607]\u001B[A\n",
      "Training (constant):  40%|████      | 801/2000 [00:07<00:14, 85.43it/s, train_loss=2.5564, val_loss=2.5607] \u001B[A\n",
      "Training (constant):  41%|████      | 818/2000 [00:07<00:11, 99.89it/s, train_loss=2.5564, val_loss=2.5607]\u001B[A\n",
      "Training (constant):  42%|████▏     | 836/2000 [00:08<00:10, 114.30it/s, train_loss=2.5564, val_loss=2.5607]\u001B[A\n",
      "Training (constant):  43%|████▎     | 854/2000 [00:08<00:08, 127.62it/s, train_loss=2.5564, val_loss=2.5607]\u001B[A\n",
      "Training (constant):  44%|████▎     | 872/2000 [00:08<00:08, 139.23it/s, train_loss=2.5564, val_loss=2.5607]\u001B[A\n",
      "Training (constant):  44%|████▍     | 889/2000 [00:08<00:07, 146.86it/s, train_loss=2.5564, val_loss=2.5607]\u001B[A\n",
      "Training (constant):  44%|████▍     | 889/2000 [00:08<00:07, 146.86it/s, train_loss=2.5184, val_loss=2.5277]\u001B[A\n",
      "Training (constant):  45%|████▌     | 906/2000 [00:08<00:12, 88.97it/s, train_loss=2.5184, val_loss=2.5277] \u001B[A\n",
      "Training (constant):  46%|████▌     | 924/2000 [00:08<00:10, 104.36it/s, train_loss=2.5184, val_loss=2.5277]\u001B[A\n",
      "Training (constant):  47%|████▋     | 942/2000 [00:08<00:08, 119.14it/s, train_loss=2.5184, val_loss=2.5277]\u001B[A\n",
      "Training (constant):  48%|████▊     | 960/2000 [00:09<00:07, 131.09it/s, train_loss=2.5184, val_loss=2.5277]\u001B[A\n",
      "Training (constant):  49%|████▉     | 978/2000 [00:09<00:07, 141.33it/s, train_loss=2.5184, val_loss=2.5277]\u001B[A\n",
      "Training (constant):  50%|████▉     | 996/2000 [00:09<00:06, 149.73it/s, train_loss=2.5184, val_loss=2.5277]\u001B[A\n",
      "Training (constant):  50%|████▉     | 996/2000 [00:09<00:06, 149.73it/s, train_loss=2.4611, val_loss=2.4622]\u001B[A\n",
      "Training (constant):  51%|█████     | 1013/2000 [00:09<00:10, 92.30it/s, train_loss=2.4611, val_loss=2.4622]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1031/2000 [00:09<00:08, 107.74it/s, train_loss=2.4611, val_loss=2.4622]\u001B[A\n",
      "Training (constant):  52%|█████▏    | 1048/2000 [00:09<00:07, 120.41it/s, train_loss=2.4611, val_loss=2.4622]\u001B[A\n",
      "Training (constant):  53%|█████▎    | 1066/2000 [00:09<00:07, 133.23it/s, train_loss=2.4611, val_loss=2.4622]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1084/2000 [00:10<00:06, 143.68it/s, train_loss=2.4611, val_loss=2.4622]\u001B[A\n",
      "Training (constant):  54%|█████▍    | 1084/2000 [00:10<00:06, 143.68it/s, train_loss=2.4115, val_loss=2.4137]\u001B[A\n",
      "Training (constant):  55%|█████▌    | 1101/2000 [00:10<00:10, 86.49it/s, train_loss=2.4115, val_loss=2.4137] \u001B[A\n",
      "Training (constant):  56%|█████▌    | 1119/2000 [00:10<00:08, 102.00it/s, train_loss=2.4115, val_loss=2.4137]\u001B[A\n",
      "Training (constant):  57%|█████▋    | 1137/2000 [00:10<00:07, 116.42it/s, train_loss=2.4115, val_loss=2.4137]\u001B[A\n",
      "Training (constant):  58%|█████▊    | 1153/2000 [00:10<00:06, 125.15it/s, train_loss=2.4115, val_loss=2.4137]\u001B[A\n",
      "Training (constant):  59%|█████▊    | 1171/2000 [00:10<00:06, 136.63it/s, train_loss=2.4115, val_loss=2.4137]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1189/2000 [00:10<00:05, 146.74it/s, train_loss=2.4115, val_loss=2.4137]\u001B[A\n",
      "Training (constant):  59%|█████▉    | 1189/2000 [00:11<00:05, 146.74it/s, train_loss=2.3799, val_loss=2.3840]\u001B[A\n",
      "Training (constant):  60%|██████    | 1206/2000 [00:11<00:09, 87.99it/s, train_loss=2.3799, val_loss=2.3840] \u001B[A\n",
      "Training (constant):  61%|██████    | 1224/2000 [00:11<00:07, 103.51it/s, train_loss=2.3799, val_loss=2.3840]\u001B[A\n",
      "Training (constant):  62%|██████▏   | 1242/2000 [00:11<00:06, 118.03it/s, train_loss=2.3799, val_loss=2.3840]\u001B[A\n",
      "Training (constant):  63%|██████▎   | 1258/2000 [00:11<00:05, 124.08it/s, train_loss=2.3799, val_loss=2.3840]\u001B[A\n",
      "Training (constant):  64%|██████▎   | 1273/2000 [00:11<00:05, 129.28it/s, train_loss=2.3799, val_loss=2.3840]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1289/2000 [00:11<00:05, 135.66it/s, train_loss=2.3799, val_loss=2.3840]\u001B[A\n",
      "Training (constant):  64%|██████▍   | 1289/2000 [00:12<00:05, 135.66it/s, train_loss=2.3370, val_loss=2.3505]\u001B[A\n",
      "Training (constant):  65%|██████▌   | 1304/2000 [00:12<00:09, 74.20it/s, train_loss=2.3370, val_loss=2.3505] \u001B[A\n",
      "Training (constant):  66%|██████▌   | 1320/2000 [00:12<00:07, 88.42it/s, train_loss=2.3370, val_loss=2.3505]\u001B[A\n",
      "Training (constant):  67%|██████▋   | 1337/2000 [00:12<00:06, 103.21it/s, train_loss=2.3370, val_loss=2.3505]\u001B[A\n",
      "Training (constant):  68%|██████▊   | 1354/2000 [00:12<00:05, 116.27it/s, train_loss=2.3370, val_loss=2.3505]\u001B[A\n",
      "Training (constant):  69%|██████▊   | 1371/2000 [00:12<00:04, 127.74it/s, train_loss=2.3370, val_loss=2.3505]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1387/2000 [00:12<00:04, 135.06it/s, train_loss=2.3370, val_loss=2.3505]\u001B[A\n",
      "Training (constant):  69%|██████▉   | 1387/2000 [00:13<00:04, 135.06it/s, train_loss=2.2980, val_loss=2.3101]\u001B[A\n",
      "Training (constant):  70%|███████   | 1403/2000 [00:13<00:08, 68.13it/s, train_loss=2.2980, val_loss=2.3101] \u001B[A\n",
      "Training (constant):  71%|███████   | 1420/2000 [00:13<00:06, 83.09it/s, train_loss=2.2980, val_loss=2.3101]\u001B[A\n",
      "Training (constant):  72%|███████▏  | 1438/2000 [00:13<00:05, 99.43it/s, train_loss=2.2980, val_loss=2.3101]\u001B[A\n",
      "Training (constant):  73%|███████▎  | 1455/2000 [00:13<00:04, 112.66it/s, train_loss=2.2980, val_loss=2.3101]\u001B[A\n",
      "Training (constant):  74%|███████▎  | 1472/2000 [00:13<00:04, 125.02it/s, train_loss=2.2980, val_loss=2.3101]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1489/2000 [00:13<00:03, 135.21it/s, train_loss=2.2980, val_loss=2.3101]\u001B[A\n",
      "Training (constant):  74%|███████▍  | 1489/2000 [00:14<00:03, 135.21it/s, train_loss=2.2798, val_loss=2.2804]\u001B[A\n",
      "Training (constant):  75%|███████▌  | 1505/2000 [00:14<00:05, 83.73it/s, train_loss=2.2798, val_loss=2.2804] \u001B[A\n",
      "Training (constant):  76%|███████▌  | 1522/2000 [00:14<00:04, 98.69it/s, train_loss=2.2798, val_loss=2.2804]\u001B[A\n",
      "Training (constant):  77%|███████▋  | 1540/2000 [00:14<00:04, 113.70it/s, train_loss=2.2798, val_loss=2.2804]\u001B[A\n",
      "Training (constant):  78%|███████▊  | 1557/2000 [00:14<00:03, 126.03it/s, train_loss=2.2798, val_loss=2.2804]\u001B[A\n",
      "Training (constant):  79%|███████▊  | 1574/2000 [00:14<00:03, 136.36it/s, train_loss=2.2798, val_loss=2.2804]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1592/2000 [00:14<00:02, 146.10it/s, train_loss=2.2798, val_loss=2.2804]\u001B[A\n",
      "Training (constant):  80%|███████▉  | 1592/2000 [00:15<00:02, 146.10it/s, train_loss=2.2326, val_loss=2.2494]\u001B[A\n",
      "Training (constant):  80%|████████  | 1609/2000 [00:15<00:04, 86.56it/s, train_loss=2.2326, val_loss=2.2494] \u001B[A\n",
      "Training (constant):  81%|████████▏ | 1627/2000 [00:15<00:03, 102.59it/s, train_loss=2.2326, val_loss=2.2494]\u001B[A\n",
      "Training (constant):  82%|████████▏ | 1644/2000 [00:15<00:03, 116.04it/s, train_loss=2.2326, val_loss=2.2494]\u001B[A\n",
      "Training (constant):  83%|████████▎ | 1661/2000 [00:15<00:02, 127.90it/s, train_loss=2.2326, val_loss=2.2494]\u001B[A\n",
      "Training (constant):  84%|████████▍ | 1678/2000 [00:15<00:02, 137.99it/s, train_loss=2.2326, val_loss=2.2494]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1696/2000 [00:15<00:02, 146.67it/s, train_loss=2.2326, val_loss=2.2494]\u001B[A\n",
      "Training (constant):  85%|████████▍ | 1696/2000 [00:15<00:02, 146.67it/s, train_loss=2.2343, val_loss=2.2389]\u001B[A\n",
      "Training (constant):  86%|████████▌ | 1713/2000 [00:15<00:03, 89.80it/s, train_loss=2.2343, val_loss=2.2389] \u001B[A\n",
      "Training (constant):  87%|████████▋ | 1731/2000 [00:16<00:02, 105.35it/s, train_loss=2.2343, val_loss=2.2389]\u001B[A\n",
      "Training (constant):  87%|████████▋ | 1746/2000 [00:16<00:02, 111.36it/s, train_loss=2.2343, val_loss=2.2389]\u001B[A\n",
      "Training (constant):  88%|████████▊ | 1760/2000 [00:16<00:02, 114.70it/s, train_loss=2.2343, val_loss=2.2389]\u001B[A\n",
      "Training (constant):  89%|████████▉ | 1775/2000 [00:16<00:01, 120.95it/s, train_loss=2.2343, val_loss=2.2389]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1792/2000 [00:16<00:01, 132.22it/s, train_loss=2.2343, val_loss=2.2389]\u001B[A\n",
      "Training (constant):  90%|████████▉ | 1792/2000 [00:16<00:01, 132.22it/s, train_loss=2.1780, val_loss=2.1764]\u001B[A\n",
      "Training (constant):  90%|█████████ | 1807/2000 [00:16<00:02, 80.90it/s, train_loss=2.1780, val_loss=2.1764] \u001B[A\n",
      "Training (constant):  91%|█████████ | 1824/2000 [00:16<00:01, 96.69it/s, train_loss=2.1780, val_loss=2.1764]\u001B[A\n",
      "Training (constant):  92%|█████████▏| 1841/2000 [00:17<00:01, 111.63it/s, train_loss=2.1780, val_loss=2.1764]\u001B[A\n",
      "Training (constant):  93%|█████████▎| 1859/2000 [00:17<00:01, 126.23it/s, train_loss=2.1780, val_loss=2.1764]\u001B[A\n",
      "Training (constant):  94%|█████████▍| 1877/2000 [00:17<00:00, 137.75it/s, train_loss=2.1780, val_loss=2.1764]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1895/2000 [00:17<00:00, 146.53it/s, train_loss=2.1780, val_loss=2.1764]\u001B[A\n",
      "Training (constant):  95%|█████████▍| 1895/2000 [00:17<00:00, 146.53it/s, train_loss=2.1877, val_loss=2.1921]\u001B[A\n",
      "Training (constant):  96%|█████████▌| 1912/2000 [00:17<00:01, 86.24it/s, train_loss=2.1877, val_loss=2.1921] \u001B[A\n",
      "Training (constant):  96%|█████████▋| 1927/2000 [00:17<00:00, 97.16it/s, train_loss=2.1877, val_loss=2.1921]\u001B[A\n",
      "Training (constant):  97%|█████████▋| 1944/2000 [00:17<00:00, 110.70it/s, train_loss=2.1877, val_loss=2.1921]\u001B[A\n",
      "Training (constant):  98%|█████████▊| 1961/2000 [00:18<00:00, 123.36it/s, train_loss=2.1877, val_loss=2.1921]\u001B[A\n",
      "Training (constant):  99%|█████████▉| 1979/2000 [00:18<00:00, 135.28it/s, train_loss=2.1877, val_loss=2.1921]\u001B[A\n",
      "Training (constant): 100%|██████████| 2000/2000 [00:18<00:00, 109.16it/s, train_loss=2.1877, val_loss=2.1921]\u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Polyak SGD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (polyak):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (polyak):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   0%|          | 1/2000 [00:00<11:40,  2.85it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   1%|          | 14/2000 [00:00<00:50, 39.56it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   1%|▏         | 28/2000 [00:00<00:29, 67.51it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   2%|▏         | 41/2000 [00:00<00:23, 85.06it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   3%|▎         | 54/2000 [00:00<00:19, 98.15it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   3%|▎         | 68/2000 [00:00<00:17, 108.54it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   4%|▍         | 82/2000 [00:00<00:16, 115.43it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   5%|▍         | 96/2000 [00:01<00:15, 120.02it/s, train_loss=3.6881, val_loss=3.6877]\u001B[A\n",
      "Training (polyak):   5%|▍         | 96/2000 [00:01<00:15, 120.02it/s, train_loss=2.3140, val_loss=2.3157]\u001B[A\n",
      "Training (polyak):   5%|▌         | 109/2000 [00:01<00:27, 68.96it/s, train_loss=2.3140, val_loss=2.3157]\u001B[A\n",
      "Training (polyak):   6%|▌         | 123/2000 [00:01<00:23, 81.28it/s, train_loss=2.3140, val_loss=2.3157]\u001B[A\n",
      "Training (polyak):   7%|▋         | 137/2000 [00:01<00:20, 92.16it/s, train_loss=2.3140, val_loss=2.3157]\u001B[A\n",
      "Training (polyak):   8%|▊         | 151/2000 [00:01<00:18, 101.89it/s, train_loss=2.3140, val_loss=2.3157]\u001B[A\n",
      "Training (polyak):   8%|▊         | 164/2000 [00:01<00:16, 108.47it/s, train_loss=2.3140, val_loss=2.3157]\u001B[A\n",
      "Training (polyak):   9%|▉         | 178/2000 [00:01<00:15, 114.56it/s, train_loss=2.3140, val_loss=2.3157]\u001B[A\n",
      "Training (polyak):  10%|▉         | 191/2000 [00:02<00:15, 118.46it/s, train_loss=2.3140, val_loss=2.3157]\u001B[A\n",
      "Training (polyak):  10%|▉         | 191/2000 [00:02<00:15, 118.46it/s, train_loss=2.2364, val_loss=2.2414]\u001B[A\n",
      "Training (polyak):  10%|█         | 204/2000 [00:02<00:26, 66.80it/s, train_loss=2.2364, val_loss=2.2414] \u001B[A\n",
      "Training (polyak):  11%|█         | 218/2000 [00:02<00:22, 78.93it/s, train_loss=2.2364, val_loss=2.2414]\u001B[A\n",
      "Training (polyak):  12%|█▏        | 232/2000 [00:02<00:19, 89.98it/s, train_loss=2.2364, val_loss=2.2414]\u001B[A\n",
      "Training (polyak):  12%|█▏        | 245/2000 [00:02<00:17, 98.44it/s, train_loss=2.2364, val_loss=2.2414]\u001B[A\n",
      "Training (polyak):  13%|█▎        | 259/2000 [00:02<00:16, 107.17it/s, train_loss=2.2364, val_loss=2.2414]\u001B[A\n",
      "Training (polyak):  14%|█▎        | 273/2000 [00:03<00:15, 114.09it/s, train_loss=2.2364, val_loss=2.2414]\u001B[A\n",
      "Training (polyak):  14%|█▍        | 287/2000 [00:03<00:14, 118.63it/s, train_loss=2.2364, val_loss=2.2414]\u001B[A\n",
      "Training (polyak):  15%|█▌        | 300/2000 [00:03<00:14, 121.05it/s, train_loss=2.2364, val_loss=2.2414]\u001B[A\n",
      "Training (polyak):  15%|█▌        | 300/2000 [00:03<00:14, 121.05it/s, train_loss=1.9879, val_loss=1.9751]\u001B[A\n",
      "Training (polyak):  16%|█▌        | 313/2000 [00:03<00:23, 70.59it/s, train_loss=1.9879, val_loss=1.9751] \u001B[A\n",
      "Training (polyak):  16%|█▋        | 327/2000 [00:03<00:20, 82.73it/s, train_loss=1.9879, val_loss=1.9751]\u001B[A\n",
      "Training (polyak):  17%|█▋        | 341/2000 [00:03<00:17, 93.67it/s, train_loss=1.9879, val_loss=1.9751]\u001B[A\n",
      "Training (polyak):  18%|█▊        | 355/2000 [00:03<00:15, 103.16it/s, train_loss=1.9879, val_loss=1.9751]\u001B[A\n",
      "Training (polyak):  18%|█▊        | 369/2000 [00:04<00:14, 110.55it/s, train_loss=1.9879, val_loss=1.9751]\u001B[A\n",
      "Training (polyak):  19%|█▉        | 383/2000 [00:04<00:13, 116.52it/s, train_loss=1.9879, val_loss=1.9751]\u001B[A\n",
      "Training (polyak):  20%|█▉        | 397/2000 [00:04<00:13, 120.46it/s, train_loss=1.9879, val_loss=1.9751]\u001B[A\n",
      "Training (polyak):  20%|█▉        | 397/2000 [00:04<00:13, 120.46it/s, train_loss=1.8129, val_loss=1.8313]\u001B[A\n",
      "Training (polyak):  20%|██        | 410/2000 [00:04<00:22, 71.43it/s, train_loss=1.8129, val_loss=1.8313] \u001B[A\n",
      "Training (polyak):  21%|██        | 423/2000 [00:04<00:19, 81.97it/s, train_loss=1.8129, val_loss=1.8313]\u001B[A\n",
      "Training (polyak):  22%|██▏       | 437/2000 [00:04<00:16, 93.36it/s, train_loss=1.8129, val_loss=1.8313]\u001B[A\n",
      "Training (polyak):  23%|██▎       | 451/2000 [00:04<00:15, 102.89it/s, train_loss=1.8129, val_loss=1.8313]\u001B[A\n",
      "Training (polyak):  23%|██▎       | 465/2000 [00:04<00:13, 110.29it/s, train_loss=1.8129, val_loss=1.8313]\u001B[A\n",
      "Training (polyak):  24%|██▍       | 479/2000 [00:05<00:13, 116.03it/s, train_loss=1.8129, val_loss=1.8313]\u001B[A\n",
      "Training (polyak):  25%|██▍       | 493/2000 [00:05<00:12, 120.38it/s, train_loss=1.8129, val_loss=1.8313]\u001B[A\n",
      "Training (polyak):  25%|██▍       | 493/2000 [00:05<00:12, 120.38it/s, train_loss=1.8783, val_loss=1.8923]\u001B[A\n",
      "Training (polyak):  25%|██▌       | 506/2000 [00:05<00:20, 72.01it/s, train_loss=1.8783, val_loss=1.8923] \u001B[A\n",
      "Training (polyak):  26%|██▌       | 520/2000 [00:05<00:17, 83.82it/s, train_loss=1.8783, val_loss=1.8923]\u001B[A\n",
      "Training (polyak):  27%|██▋       | 534/2000 [00:05<00:15, 94.64it/s, train_loss=1.8783, val_loss=1.8923]\u001B[A\n",
      "Training (polyak):  27%|██▋       | 548/2000 [00:05<00:14, 103.41it/s, train_loss=1.8783, val_loss=1.8923]\u001B[A\n",
      "Training (polyak):  28%|██▊       | 562/2000 [00:05<00:12, 111.60it/s, train_loss=1.8783, val_loss=1.8923]\u001B[A\n",
      "Training (polyak):  29%|██▉       | 576/2000 [00:06<00:12, 117.76it/s, train_loss=1.8783, val_loss=1.8923]\u001B[A\n",
      "Training (polyak):  30%|██▉       | 590/2000 [00:06<00:11, 121.94it/s, train_loss=1.8783, val_loss=1.8923]\u001B[A\n",
      "Training (polyak):  30%|██▉       | 590/2000 [00:06<00:11, 121.94it/s, train_loss=1.4153, val_loss=1.4278]\u001B[A\n",
      "Training (polyak):  30%|███       | 604/2000 [00:06<00:19, 72.15it/s, train_loss=1.4153, val_loss=1.4278] \u001B[A\n",
      "Training (polyak):  31%|███       | 618/2000 [00:06<00:16, 83.59it/s, train_loss=1.4153, val_loss=1.4278]\u001B[A\n",
      "Training (polyak):  32%|███▏      | 632/2000 [00:06<00:14, 94.45it/s, train_loss=1.4153, val_loss=1.4278]\u001B[A\n",
      "Training (polyak):  32%|███▏      | 646/2000 [00:06<00:13, 103.44it/s, train_loss=1.4153, val_loss=1.4278]\u001B[A\n",
      "Training (polyak):  33%|███▎      | 660/2000 [00:06<00:12, 111.06it/s, train_loss=1.4153, val_loss=1.4278]\u001B[A\n",
      "Training (polyak):  34%|███▎      | 674/2000 [00:07<00:11, 117.36it/s, train_loss=1.4153, val_loss=1.4278]\u001B[A\n",
      "Training (polyak):  34%|███▍      | 688/2000 [00:07<00:10, 122.06it/s, train_loss=1.4153, val_loss=1.4278]\u001B[A\n",
      "Training (polyak):  34%|███▍      | 688/2000 [00:07<00:10, 122.06it/s, train_loss=0.9840, val_loss=0.9745]\u001B[A\n",
      "Training (polyak):  35%|███▌      | 702/2000 [00:07<00:18, 70.97it/s, train_loss=0.9840, val_loss=0.9745] \u001B[A\n",
      "Training (polyak):  36%|███▌      | 715/2000 [00:07<00:15, 81.43it/s, train_loss=0.9840, val_loss=0.9745]\u001B[A\n",
      "Training (polyak):  36%|███▋      | 729/2000 [00:07<00:13, 92.32it/s, train_loss=0.9840, val_loss=0.9745]\u001B[A\n",
      "Training (polyak):  37%|███▋      | 743/2000 [00:07<00:12, 101.99it/s, train_loss=0.9840, val_loss=0.9745]\u001B[A\n",
      "Training (polyak):  38%|███▊      | 757/2000 [00:08<00:11, 109.56it/s, train_loss=0.9840, val_loss=0.9745]\u001B[A\n",
      "Training (polyak):  39%|███▊      | 771/2000 [00:08<00:10, 115.98it/s, train_loss=0.9840, val_loss=0.9745]\u001B[A\n",
      "Training (polyak):  39%|███▉      | 785/2000 [00:08<00:10, 121.18it/s, train_loss=0.9840, val_loss=0.9745]\u001B[A\n",
      "Training (polyak):  40%|███▉      | 799/2000 [00:08<00:09, 124.46it/s, train_loss=0.9840, val_loss=0.9745]\u001B[A\n",
      "Training (polyak):  40%|███▉      | 799/2000 [00:08<00:09, 124.46it/s, train_loss=0.8525, val_loss=0.8406]\u001B[A\n",
      "Training (polyak):  41%|████      | 813/2000 [00:08<00:15, 74.24it/s, train_loss=0.8525, val_loss=0.8406] \u001B[A\n",
      "Training (polyak):  41%|████▏     | 827/2000 [00:08<00:13, 85.52it/s, train_loss=0.8525, val_loss=0.8406]\u001B[A\n",
      "Training (polyak):  42%|████▏     | 841/2000 [00:08<00:12, 95.95it/s, train_loss=0.8525, val_loss=0.8406]\u001B[A\n",
      "Training (polyak):  43%|████▎     | 854/2000 [00:08<00:11, 103.52it/s, train_loss=0.8525, val_loss=0.8406]\u001B[A\n",
      "Training (polyak):  43%|████▎     | 868/2000 [00:09<00:10, 110.88it/s, train_loss=0.8525, val_loss=0.8406]\u001B[A\n",
      "Training (polyak):  44%|████▍     | 882/2000 [00:09<00:09, 117.20it/s, train_loss=0.8525, val_loss=0.8406]\u001B[A\n",
      "Training (polyak):  45%|████▍     | 896/2000 [00:09<00:09, 122.41it/s, train_loss=0.8525, val_loss=0.8406]\u001B[A\n",
      "Training (polyak):  45%|████▍     | 896/2000 [00:09<00:09, 122.41it/s, train_loss=0.6535, val_loss=0.6684]\u001B[A\n",
      "Training (polyak):  46%|████▌     | 910/2000 [00:09<00:14, 72.84it/s, train_loss=0.6535, val_loss=0.6684] \u001B[A\n",
      "Training (polyak):  46%|████▌     | 924/2000 [00:09<00:12, 84.38it/s, train_loss=0.6535, val_loss=0.6684]\u001B[A\n",
      "Training (polyak):  47%|████▋     | 938/2000 [00:09<00:11, 95.03it/s, train_loss=0.6535, val_loss=0.6684]\u001B[A\n",
      "Training (polyak):  48%|████▊     | 952/2000 [00:09<00:10, 104.02it/s, train_loss=0.6535, val_loss=0.6684]\u001B[A\n",
      "Training (polyak):  48%|████▊     | 966/2000 [00:10<00:09, 111.80it/s, train_loss=0.6535, val_loss=0.6684]\u001B[A\n",
      "Training (polyak):  49%|████▉     | 980/2000 [00:10<00:08, 117.62it/s, train_loss=0.6535, val_loss=0.6684]\u001B[A\n",
      "Training (polyak):  50%|████▉     | 994/2000 [00:10<00:08, 121.57it/s, train_loss=0.6535, val_loss=0.6684]\u001B[A\n",
      "Training (polyak):  50%|████▉     | 994/2000 [00:10<00:08, 121.57it/s, train_loss=0.6468, val_loss=0.6496]\u001B[A\n",
      "Training (polyak):  50%|█████     | 1007/2000 [00:10<00:13, 72.20it/s, train_loss=0.6468, val_loss=0.6496]\u001B[A\n",
      "Training (polyak):  51%|█████     | 1021/2000 [00:10<00:11, 84.08it/s, train_loss=0.6468, val_loss=0.6496]\u001B[A\n",
      "Training (polyak):  52%|█████▏    | 1035/2000 [00:10<00:10, 94.80it/s, train_loss=0.6468, val_loss=0.6496]\u001B[A\n",
      "Training (polyak):  52%|█████▏    | 1049/2000 [00:10<00:09, 104.13it/s, train_loss=0.6468, val_loss=0.6496]\u001B[A\n",
      "Training (polyak):  53%|█████▎    | 1063/2000 [00:11<00:08, 111.13it/s, train_loss=0.6468, val_loss=0.6496]\u001B[A\n",
      "Training (polyak):  54%|█████▍    | 1076/2000 [00:11<00:08, 115.47it/s, train_loss=0.6468, val_loss=0.6496]\u001B[A\n",
      "Training (polyak):  55%|█████▍    | 1090/2000 [00:11<00:07, 120.63it/s, train_loss=0.6468, val_loss=0.6496]\u001B[A\n",
      "Training (polyak):  55%|█████▍    | 1090/2000 [00:11<00:07, 120.63it/s, train_loss=0.6686, val_loss=0.6776]\u001B[A\n",
      "Training (polyak):  55%|█████▌    | 1103/2000 [00:11<00:12, 69.03it/s, train_loss=0.6686, val_loss=0.6776] \u001B[A\n",
      "Training (polyak):  56%|█████▌    | 1117/2000 [00:11<00:10, 80.95it/s, train_loss=0.6686, val_loss=0.6776]\u001B[A\n",
      "Training (polyak):  57%|█████▋    | 1131/2000 [00:11<00:09, 91.73it/s, train_loss=0.6686, val_loss=0.6776]\u001B[A\n",
      "Training (polyak):  57%|█████▋    | 1145/2000 [00:12<00:08, 101.06it/s, train_loss=0.6686, val_loss=0.6776]\u001B[A\n",
      "Training (polyak):  58%|█████▊    | 1159/2000 [00:12<00:07, 108.73it/s, train_loss=0.6686, val_loss=0.6776]\u001B[A\n",
      "Training (polyak):  59%|█████▊    | 1173/2000 [00:12<00:07, 115.27it/s, train_loss=0.6686, val_loss=0.6776]\u001B[A\n",
      "Training (polyak):  59%|█████▉    | 1187/2000 [00:12<00:06, 120.63it/s, train_loss=0.6686, val_loss=0.6776]\u001B[A\n",
      "Training (polyak):  59%|█████▉    | 1187/2000 [00:12<00:06, 120.63it/s, train_loss=0.5876, val_loss=0.5838]\u001B[A\n",
      "Training (polyak):  60%|██████    | 1201/2000 [00:12<00:11, 70.95it/s, train_loss=0.5876, val_loss=0.5838] \u001B[A\n",
      "Training (polyak):  61%|██████    | 1214/2000 [00:12<00:09, 81.26it/s, train_loss=0.5876, val_loss=0.5838]\u001B[A\n",
      "Training (polyak):  61%|██████▏   | 1227/2000 [00:12<00:08, 91.10it/s, train_loss=0.5876, val_loss=0.5838]\u001B[A\n",
      "Training (polyak):  62%|██████▏   | 1240/2000 [00:13<00:07, 99.75it/s, train_loss=0.5876, val_loss=0.5838]\u001B[A\n",
      "Training (polyak):  63%|██████▎   | 1254/2000 [00:13<00:06, 108.35it/s, train_loss=0.5876, val_loss=0.5838]\u001B[A\n",
      "Training (polyak):  63%|██████▎   | 1268/2000 [00:13<00:06, 115.29it/s, train_loss=0.5876, val_loss=0.5838]\u001B[A\n",
      "Training (polyak):  64%|██████▍   | 1282/2000 [00:13<00:05, 119.88it/s, train_loss=0.5876, val_loss=0.5838]\u001B[A\n",
      "Training (polyak):  65%|██████▍   | 1296/2000 [00:13<00:05, 123.19it/s, train_loss=0.5876, val_loss=0.5838]\u001B[A\n",
      "Training (polyak):  65%|██████▍   | 1296/2000 [00:13<00:05, 123.19it/s, train_loss=0.6221, val_loss=0.6189]\u001B[A\n",
      "Training (polyak):  65%|██████▌   | 1309/2000 [00:13<00:09, 72.20it/s, train_loss=0.6221, val_loss=0.6189] \u001B[A\n",
      "Training (polyak):  66%|██████▌   | 1323/2000 [00:13<00:08, 83.92it/s, train_loss=0.6221, val_loss=0.6189]\u001B[A\n",
      "Training (polyak):  67%|██████▋   | 1337/2000 [00:14<00:06, 95.14it/s, train_loss=0.6221, val_loss=0.6189]\u001B[A\n",
      "Training (polyak):  68%|██████▊   | 1351/2000 [00:14<00:06, 103.79it/s, train_loss=0.6221, val_loss=0.6189]\u001B[A\n",
      "Training (polyak):  68%|██████▊   | 1365/2000 [00:14<00:05, 110.78it/s, train_loss=0.6221, val_loss=0.6189]\u001B[A\n",
      "Training (polyak):  69%|██████▉   | 1379/2000 [00:14<00:05, 116.80it/s, train_loss=0.6221, val_loss=0.6189]\u001B[A\n",
      "Training (polyak):  70%|██████▉   | 1392/2000 [00:14<00:05, 118.51it/s, train_loss=0.6221, val_loss=0.6189]\u001B[A\n",
      "Training (polyak):  70%|██████▉   | 1392/2000 [00:14<00:05, 118.51it/s, train_loss=0.5547, val_loss=0.5636]\u001B[A\n",
      "Training (polyak):  70%|███████   | 1405/2000 [00:14<00:09, 64.36it/s, train_loss=0.5547, val_loss=0.5636] \u001B[A\n",
      "Training (polyak):  71%|███████   | 1419/2000 [00:14<00:07, 76.68it/s, train_loss=0.5547, val_loss=0.5636]\u001B[A\n",
      "Training (polyak):  72%|███████▏  | 1433/2000 [00:15<00:06, 87.96it/s, train_loss=0.5547, val_loss=0.5636]\u001B[A\n",
      "Training (polyak):  72%|███████▏  | 1447/2000 [00:15<00:05, 98.33it/s, train_loss=0.5547, val_loss=0.5636]\u001B[A\n",
      "Training (polyak):  73%|███████▎  | 1461/2000 [00:15<00:05, 107.18it/s, train_loss=0.5547, val_loss=0.5636]\u001B[A\n",
      "Training (polyak):  74%|███████▍  | 1475/2000 [00:15<00:04, 114.23it/s, train_loss=0.5547, val_loss=0.5636]\u001B[A\n",
      "Training (polyak):  74%|███████▍  | 1489/2000 [00:15<00:04, 119.45it/s, train_loss=0.5547, val_loss=0.5636]\u001B[A\n",
      "Training (polyak):  74%|███████▍  | 1489/2000 [00:15<00:04, 119.45it/s, train_loss=0.5619, val_loss=0.5490]\u001B[A\n",
      "Training (polyak):  75%|███████▌  | 1502/2000 [00:15<00:07, 70.99it/s, train_loss=0.5619, val_loss=0.5490] \u001B[A\n",
      "Training (polyak):  76%|███████▌  | 1515/2000 [00:15<00:05, 81.61it/s, train_loss=0.5619, val_loss=0.5490]\u001B[A\n",
      "Training (polyak):  76%|███████▋  | 1529/2000 [00:16<00:05, 92.71it/s, train_loss=0.5619, val_loss=0.5490]\u001B[A\n",
      "Training (polyak):  77%|███████▋  | 1543/2000 [00:16<00:04, 102.66it/s, train_loss=0.5619, val_loss=0.5490]\u001B[A\n",
      "Training (polyak):  78%|███████▊  | 1557/2000 [00:16<00:04, 109.79it/s, train_loss=0.5619, val_loss=0.5490]\u001B[A\n",
      "Training (polyak):  79%|███████▊  | 1571/2000 [00:16<00:03, 116.04it/s, train_loss=0.5619, val_loss=0.5490]\u001B[A\n",
      "Training (polyak):  79%|███████▉  | 1585/2000 [00:16<00:03, 120.68it/s, train_loss=0.5619, val_loss=0.5490]\u001B[A\n",
      "Training (polyak):  80%|███████▉  | 1599/2000 [00:16<00:03, 124.34it/s, train_loss=0.5619, val_loss=0.5490]\u001B[A\n",
      "Training (polyak):  80%|███████▉  | 1599/2000 [00:16<00:03, 124.34it/s, train_loss=0.5808, val_loss=0.5697]\u001B[A\n",
      "Training (polyak):  81%|████████  | 1613/2000 [00:16<00:05, 73.88it/s, train_loss=0.5808, val_loss=0.5697] \u001B[A\n",
      "Training (polyak):  81%|████████▏ | 1627/2000 [00:17<00:04, 85.28it/s, train_loss=0.5808, val_loss=0.5697]\u001B[A\n",
      "Training (polyak):  82%|████████▏ | 1641/2000 [00:17<00:03, 95.91it/s, train_loss=0.5808, val_loss=0.5697]\u001B[A\n",
      "Training (polyak):  83%|████████▎ | 1655/2000 [00:17<00:03, 104.38it/s, train_loss=0.5808, val_loss=0.5697]\u001B[A\n",
      "Training (polyak):  83%|████████▎ | 1669/2000 [00:17<00:02, 111.59it/s, train_loss=0.5808, val_loss=0.5697]\u001B[A\n",
      "Training (polyak):  84%|████████▍ | 1682/2000 [00:17<00:02, 116.05it/s, train_loss=0.5808, val_loss=0.5697]\u001B[A\n",
      "Training (polyak):  85%|████████▍ | 1696/2000 [00:17<00:02, 120.72it/s, train_loss=0.5808, val_loss=0.5697]\u001B[A\n",
      "Training (polyak):  85%|████████▍ | 1696/2000 [00:17<00:02, 120.72it/s, train_loss=0.5246, val_loss=0.5382]\u001B[A\n",
      "Training (polyak):  85%|████████▌ | 1709/2000 [00:17<00:04, 72.12it/s, train_loss=0.5246, val_loss=0.5382] \u001B[A\n",
      "Training (polyak):  86%|████████▌ | 1723/2000 [00:18<00:03, 84.22it/s, train_loss=0.5246, val_loss=0.5382]\u001B[A\n",
      "Training (polyak):  87%|████████▋ | 1737/2000 [00:18<00:02, 95.21it/s, train_loss=0.5246, val_loss=0.5382]\u001B[A\n",
      "Training (polyak):  88%|████████▊ | 1751/2000 [00:18<00:02, 104.61it/s, train_loss=0.5246, val_loss=0.5382]\u001B[A\n",
      "Training (polyak):  88%|████████▊ | 1765/2000 [00:18<00:02, 111.64it/s, train_loss=0.5246, val_loss=0.5382]\u001B[A\n",
      "Training (polyak):  89%|████████▉ | 1779/2000 [00:18<00:01, 117.32it/s, train_loss=0.5246, val_loss=0.5382]\u001B[A\n",
      "Training (polyak):  90%|████████▉ | 1793/2000 [00:18<00:01, 121.94it/s, train_loss=0.5246, val_loss=0.5382]\u001B[A\n",
      "Training (polyak):  90%|████████▉ | 1793/2000 [00:18<00:01, 121.94it/s, train_loss=0.5341, val_loss=0.5273]\u001B[A\n",
      "Training (polyak):  90%|█████████ | 1806/2000 [00:18<00:02, 72.17it/s, train_loss=0.5341, val_loss=0.5273] \u001B[A\n",
      "Training (polyak):  91%|█████████ | 1820/2000 [00:19<00:02, 83.66it/s, train_loss=0.5341, val_loss=0.5273]\u001B[A\n",
      "Training (polyak):  92%|█████████▏| 1834/2000 [00:19<00:01, 94.61it/s, train_loss=0.5341, val_loss=0.5273]\u001B[A\n",
      "Training (polyak):  92%|█████████▏| 1848/2000 [00:19<00:01, 104.51it/s, train_loss=0.5341, val_loss=0.5273]\u001B[A\n",
      "Training (polyak):  93%|█████████▎| 1862/2000 [00:19<00:01, 111.27it/s, train_loss=0.5341, val_loss=0.5273]\u001B[A\n",
      "Training (polyak):  94%|█████████▍| 1876/2000 [00:19<00:01, 118.03it/s, train_loss=0.5341, val_loss=0.5273]\u001B[A\n",
      "Training (polyak):  94%|█████████▍| 1890/2000 [00:19<00:00, 121.72it/s, train_loss=0.5341, val_loss=0.5273]\u001B[A\n",
      "Training (polyak):  94%|█████████▍| 1890/2000 [00:19<00:00, 121.72it/s, train_loss=0.5431, val_loss=0.5447]\u001B[A\n",
      "Training (polyak):  95%|█████████▌| 1903/2000 [00:19<00:01, 70.02it/s, train_loss=0.5431, val_loss=0.5447] \u001B[A\n",
      "Training (polyak):  96%|█████████▌| 1917/2000 [00:20<00:01, 82.05it/s, train_loss=0.5431, val_loss=0.5447]\u001B[A\n",
      "Training (polyak):  97%|█████████▋| 1931/2000 [00:20<00:00, 93.12it/s, train_loss=0.5431, val_loss=0.5447]\u001B[A\n",
      "Training (polyak):  97%|█████████▋| 1945/2000 [00:20<00:00, 102.59it/s, train_loss=0.5431, val_loss=0.5447]\u001B[A\n",
      "Training (polyak):  98%|█████████▊| 1959/2000 [00:20<00:00, 110.29it/s, train_loss=0.5431, val_loss=0.5447]\u001B[A\n",
      "Training (polyak):  99%|█████████▊| 1973/2000 [00:20<00:00, 115.87it/s, train_loss=0.5431, val_loss=0.5447]\u001B[A\n",
      "Training (polyak): 100%|██████████| 2000/2000 [00:20<00:00, 96.82it/s, train_loss=0.5431, val_loss=0.5447] \u001B[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with AdamW...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s]\u001B[A\n",
      "Training (adam):   0%|          | 0/2000 [00:00<?, ?it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   0%|          | 1/2000 [00:00<09:52,  3.38it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   1%|          | 11/2000 [00:00<00:58, 33.84it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   1%|          | 21/2000 [00:00<00:37, 52.89it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   2%|▏         | 30/2000 [00:00<00:30, 63.95it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   2%|▏         | 40/2000 [00:00<00:26, 73.11it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   2%|▎         | 50/2000 [00:00<00:24, 78.62it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   3%|▎         | 60/2000 [00:00<00:23, 82.39it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   3%|▎         | 69/2000 [00:01<00:22, 84.12it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   4%|▍         | 79/2000 [00:01<00:22, 86.55it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   4%|▍         | 88/2000 [00:01<00:21, 87.15it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   5%|▍         | 98/2000 [00:01<00:21, 88.94it/s, train_loss=3.6912, val_loss=3.6918]\u001B[A\n",
      "Training (adam):   5%|▍         | 98/2000 [00:01<00:21, 88.94it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   5%|▌         | 108/2000 [00:01<00:37, 50.45it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   6%|▌         | 117/2000 [00:01<00:32, 57.61it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   6%|▋         | 127/2000 [00:01<00:28, 65.58it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   7%|▋         | 137/2000 [00:02<00:25, 71.69it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   7%|▋         | 147/2000 [00:02<00:23, 77.69it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   8%|▊         | 157/2000 [00:02<00:22, 81.88it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   8%|▊         | 167/2000 [00:02<00:21, 85.07it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   9%|▉         | 177/2000 [00:02<00:20, 87.66it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):   9%|▉         | 187/2000 [00:02<00:20, 89.64it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):  10%|▉         | 197/2000 [00:02<00:19, 90.45it/s, train_loss=2.3479, val_loss=2.3522]\u001B[A\n",
      "Training (adam):  10%|▉         | 197/2000 [00:03<00:19, 90.45it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  10%|█         | 207/2000 [00:03<00:34, 52.37it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  11%|█         | 217/2000 [00:03<00:29, 60.34it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  11%|█▏        | 227/2000 [00:03<00:26, 67.29it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  12%|█▏        | 237/2000 [00:03<00:24, 73.03it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  12%|█▏        | 247/2000 [00:03<00:22, 78.20it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  13%|█▎        | 257/2000 [00:03<00:21, 82.22it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  13%|█▎        | 267/2000 [00:03<00:20, 85.16it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  14%|█▍        | 277/2000 [00:03<00:19, 87.61it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  14%|█▍        | 287/2000 [00:03<00:19, 89.41it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:19, 89.26it/s, train_loss=1.8475, val_loss=1.8357]\u001B[A\n",
      "Training (adam):  15%|█▍        | 297/2000 [00:04<00:19, 89.26it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  15%|█▌        | 307/2000 [00:04<00:32, 52.63it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  16%|█▌        | 317/2000 [00:04<00:27, 60.64it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  16%|█▋        | 327/2000 [00:04<00:24, 68.06it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  17%|█▋        | 337/2000 [00:04<00:22, 73.93it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  17%|█▋        | 347/2000 [00:04<00:20, 79.10it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  18%|█▊        | 357/2000 [00:04<00:19, 83.01it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  18%|█▊        | 367/2000 [00:05<00:19, 84.42it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  19%|█▉        | 376/2000 [00:05<00:18, 85.53it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  19%|█▉        | 386/2000 [00:05<00:18, 88.41it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:17, 89.84it/s, train_loss=1.3896, val_loss=1.4055]\u001B[A\n",
      "Training (adam):  20%|█▉        | 396/2000 [00:05<00:17, 89.84it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  20%|██        | 406/2000 [00:05<00:30, 53.04it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  21%|██        | 416/2000 [00:05<00:25, 60.96it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  21%|██▏       | 426/2000 [00:05<00:23, 67.81it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  22%|██▏       | 436/2000 [00:06<00:21, 74.00it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  22%|██▏       | 446/2000 [00:06<00:19, 79.11it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  23%|██▎       | 456/2000 [00:06<00:18, 83.87it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  23%|██▎       | 466/2000 [00:06<00:17, 87.09it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  24%|██▍       | 476/2000 [00:06<00:17, 88.46it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  24%|██▍       | 486/2000 [00:06<00:16, 89.21it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:06<00:16, 89.72it/s, train_loss=0.8362, val_loss=0.8387]\u001B[A\n",
      "Training (adam):  25%|██▍       | 496/2000 [00:07<00:16, 89.72it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  25%|██▌       | 506/2000 [00:07<00:28, 52.94it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  26%|██▌       | 516/2000 [00:07<00:24, 60.90it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  26%|██▋       | 526/2000 [00:07<00:21, 68.07it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  27%|██▋       | 536/2000 [00:07<00:19, 74.31it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  27%|██▋       | 546/2000 [00:07<00:18, 79.06it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  28%|██▊       | 556/2000 [00:07<00:17, 83.34it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  28%|██▊       | 566/2000 [00:07<00:16, 85.48it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  29%|██▉       | 576/2000 [00:07<00:16, 88.41it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  29%|██▉       | 586/2000 [00:07<00:15, 89.78it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  30%|██▉       | 596/2000 [00:08<00:15, 90.67it/s, train_loss=0.6028, val_loss=0.6147]\u001B[A\n",
      "Training (adam):  30%|██▉       | 596/2000 [00:08<00:15, 90.67it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  30%|███       | 606/2000 [00:08<00:26, 52.98it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  31%|███       | 616/2000 [00:08<00:22, 60.82it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  31%|███▏      | 626/2000 [00:08<00:20, 68.23it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  32%|███▏      | 636/2000 [00:08<00:18, 73.89it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  32%|███▏      | 646/2000 [00:08<00:17, 78.36it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  33%|███▎      | 656/2000 [00:08<00:16, 83.00it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  33%|███▎      | 666/2000 [00:09<00:15, 85.34it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  34%|███▍      | 676/2000 [00:09<00:15, 88.22it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  34%|███▍      | 686/2000 [00:09<00:14, 89.74it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:09<00:14, 90.91it/s, train_loss=0.5488, val_loss=0.5706]\u001B[A\n",
      "Training (adam):  35%|███▍      | 696/2000 [00:09<00:14, 90.91it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  35%|███▌      | 706/2000 [00:09<00:24, 52.36it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  36%|███▌      | 716/2000 [00:09<00:21, 60.73it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  36%|███▋      | 726/2000 [00:09<00:18, 68.12it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  37%|███▋      | 736/2000 [00:10<00:17, 74.22it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  37%|███▋      | 746/2000 [00:10<00:15, 78.58it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  38%|███▊      | 756/2000 [00:10<00:15, 82.18it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  38%|███▊      | 766/2000 [00:10<00:14, 85.31it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  39%|███▉      | 776/2000 [00:10<00:13, 88.27it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  39%|███▉      | 786/2000 [00:10<00:13, 90.49it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:10<00:13, 86.09it/s, train_loss=0.5143, val_loss=0.5239]\u001B[A\n",
      "Training (adam):  40%|███▉      | 796/2000 [00:11<00:13, 86.09it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  40%|████      | 805/2000 [00:11<00:25, 47.23it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  41%|████      | 815/2000 [00:11<00:21, 55.52it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  41%|████▏     | 825/2000 [00:11<00:18, 62.92it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  42%|████▏     | 835/2000 [00:11<00:16, 69.84it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  42%|████▏     | 845/2000 [00:11<00:15, 75.37it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  43%|████▎     | 855/2000 [00:11<00:14, 80.28it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  43%|████▎     | 865/2000 [00:11<00:13, 83.69it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  44%|████▍     | 875/2000 [00:11<00:13, 86.46it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  44%|████▍     | 885/2000 [00:12<00:12, 88.57it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:12<00:12, 89.94it/s, train_loss=0.5175, val_loss=0.5081]\u001B[A\n",
      "Training (adam):  45%|████▍     | 895/2000 [00:12<00:12, 89.94it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  45%|████▌     | 905/2000 [00:12<00:21, 51.64it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  46%|████▌     | 915/2000 [00:12<00:18, 59.71it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  46%|████▋     | 925/2000 [00:12<00:15, 67.40it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  47%|████▋     | 935/2000 [00:12<00:14, 73.56it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  47%|████▋     | 945/2000 [00:12<00:13, 78.85it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  48%|████▊     | 955/2000 [00:13<00:12, 82.25it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  48%|████▊     | 965/2000 [00:13<00:12, 85.45it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  49%|████▉     | 975/2000 [00:13<00:11, 88.12it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  49%|████▉     | 985/2000 [00:13<00:11, 89.37it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:13<00:11, 90.80it/s, train_loss=0.5101, val_loss=0.5122]\u001B[A\n",
      "Training (adam):  50%|████▉     | 995/2000 [00:13<00:11, 90.80it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  50%|█████     | 1005/2000 [00:13<00:19, 51.86it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  51%|█████     | 1015/2000 [00:13<00:16, 59.59it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  51%|█████▏    | 1025/2000 [00:14<00:14, 66.55it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  52%|█████▏    | 1035/2000 [00:14<00:13, 72.87it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  52%|█████▏    | 1045/2000 [00:14<00:12, 78.04it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  53%|█████▎    | 1055/2000 [00:14<00:11, 81.76it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  53%|█████▎    | 1065/2000 [00:14<00:10, 85.46it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  54%|█████▍    | 1075/2000 [00:14<00:10, 87.73it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  54%|█████▍    | 1085/2000 [00:14<00:10, 88.75it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  55%|█████▍    | 1095/2000 [00:14<00:10, 89.87it/s, train_loss=0.4852, val_loss=0.5019]\u001B[A\n",
      "Training (adam):  55%|█████▍    | 1095/2000 [00:15<00:10, 89.87it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  55%|█████▌    | 1105/2000 [00:15<00:16, 52.73it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  56%|█████▌    | 1115/2000 [00:15<00:14, 60.62it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  56%|█████▋    | 1125/2000 [00:15<00:12, 67.99it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  57%|█████▋    | 1135/2000 [00:15<00:11, 73.60it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  57%|█████▋    | 1145/2000 [00:15<00:10, 78.17it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  58%|█████▊    | 1155/2000 [00:15<00:10, 82.87it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  58%|█████▊    | 1166/2000 [00:15<00:09, 87.82it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  59%|█████▉    | 1176/2000 [00:15<00:09, 90.94it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  59%|█████▉    | 1186/2000 [00:16<00:08, 92.50it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:16<00:08, 95.33it/s, train_loss=0.4891, val_loss=0.4858]\u001B[A\n",
      "Training (adam):  60%|█████▉    | 1197/2000 [00:16<00:08, 95.33it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  60%|██████    | 1207/2000 [00:16<00:14, 55.28it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  61%|██████    | 1218/2000 [00:16<00:12, 64.43it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  61%|██████▏   | 1229/2000 [00:16<00:10, 72.69it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  62%|██████▏   | 1240/2000 [00:16<00:09, 79.74it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  63%|██████▎   | 1251/2000 [00:16<00:08, 85.42it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  63%|██████▎   | 1262/2000 [00:17<00:08, 89.87it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  64%|██████▎   | 1273/2000 [00:17<00:07, 92.88it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  64%|██████▍   | 1284/2000 [00:17<00:07, 95.25it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  65%|██████▍   | 1294/2000 [00:17<00:07, 96.52it/s, train_loss=0.4761, val_loss=0.4802]\u001B[A\n",
      "Training (adam):  65%|██████▍   | 1294/2000 [00:17<00:07, 96.52it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  65%|██████▌   | 1304/2000 [00:17<00:12, 55.46it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  66%|██████▌   | 1314/2000 [00:17<00:10, 63.40it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  66%|██████▋   | 1325/2000 [00:17<00:09, 71.77it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  67%|██████▋   | 1335/2000 [00:18<00:08, 78.13it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  67%|██████▋   | 1345/2000 [00:18<00:07, 83.44it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  68%|██████▊   | 1356/2000 [00:18<00:07, 88.45it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  68%|██████▊   | 1367/2000 [00:18<00:06, 92.22it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  69%|██████▉   | 1378/2000 [00:18<00:06, 95.06it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  69%|██████▉   | 1389/2000 [00:18<00:06, 97.19it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  70%|███████   | 1400/2000 [00:18<00:06, 98.60it/s, train_loss=0.4760, val_loss=0.4713]\u001B[A\n",
      "Training (adam):  70%|███████   | 1400/2000 [00:18<00:06, 98.60it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  71%|███████   | 1411/2000 [00:19<00:10, 57.40it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  71%|███████   | 1422/2000 [00:19<00:08, 65.98it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  72%|███████▏  | 1433/2000 [00:19<00:07, 73.67it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  72%|███████▏  | 1444/2000 [00:19<00:06, 80.27it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  73%|███████▎  | 1455/2000 [00:19<00:06, 85.40it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  73%|███████▎  | 1466/2000 [00:19<00:05, 89.75it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  74%|███████▍  | 1477/2000 [00:19<00:05, 92.92it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  74%|███████▍  | 1488/2000 [00:19<00:05, 95.22it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:19<00:05, 96.93it/s, train_loss=0.4840, val_loss=0.4794]\u001B[A\n",
      "Training (adam):  75%|███████▍  | 1499/2000 [00:20<00:05, 96.93it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  75%|███████▌  | 1509/2000 [00:20<00:08, 55.91it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  76%|███████▌  | 1520/2000 [00:20<00:07, 64.69it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  77%|███████▋  | 1531/2000 [00:20<00:06, 72.63it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  77%|███████▋  | 1541/2000 [00:20<00:05, 78.71it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  78%|███████▊  | 1552/2000 [00:20<00:05, 84.50it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  78%|███████▊  | 1563/2000 [00:20<00:04, 88.79it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  79%|███████▊  | 1574/2000 [00:20<00:04, 92.26it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  79%|███████▉  | 1584/2000 [00:21<00:04, 94.15it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:21<00:04, 96.11it/s, train_loss=0.4627, val_loss=0.4585]\u001B[A\n",
      "Training (adam):  80%|███████▉  | 1595/2000 [00:21<00:04, 96.11it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  80%|████████  | 1605/2000 [00:21<00:07, 55.37it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  81%|████████  | 1616/2000 [00:21<00:05, 64.37it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  81%|████████▏ | 1627/2000 [00:21<00:05, 72.37it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  82%|████████▏ | 1637/2000 [00:21<00:04, 78.52it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  82%|████████▏ | 1647/2000 [00:21<00:04, 83.64it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  83%|████████▎ | 1658/2000 [00:22<00:03, 88.24it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  83%|████████▎ | 1669/2000 [00:22<00:03, 91.74it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  84%|████████▍ | 1680/2000 [00:22<00:03, 94.28it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:22<00:03, 95.97it/s, train_loss=0.4578, val_loss=0.4697]\u001B[A\n",
      "Training (adam):  85%|████████▍ | 1691/2000 [00:22<00:03, 95.97it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  85%|████████▌ | 1701/2000 [00:22<00:05, 54.96it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  86%|████████▌ | 1711/2000 [00:22<00:04, 62.79it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  86%|████████▌ | 1722/2000 [00:22<00:03, 71.17it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  87%|████████▋ | 1733/2000 [00:23<00:03, 78.21it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  87%|████████▋ | 1743/2000 [00:23<00:03, 83.20it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  88%|████████▊ | 1754/2000 [00:23<00:02, 88.07it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  88%|████████▊ | 1765/2000 [00:23<00:02, 91.80it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  89%|████████▉ | 1775/2000 [00:23<00:02, 93.86it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  89%|████████▉ | 1785/2000 [00:23<00:02, 95.51it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  90%|████████▉ | 1796/2000 [00:23<00:02, 97.29it/s, train_loss=0.4580, val_loss=0.4716]\u001B[A\n",
      "Training (adam):  90%|████████▉ | 1796/2000 [00:24<00:02, 97.29it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  90%|█████████ | 1806/2000 [00:24<00:03, 55.14it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  91%|█████████ | 1817/2000 [00:24<00:02, 64.21it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  91%|█████████▏| 1826/2000 [00:24<00:02, 66.82it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  92%|█████████▏| 1836/2000 [00:24<00:02, 73.58it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  92%|█████████▏| 1847/2000 [00:24<00:01, 80.51it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  93%|█████████▎| 1858/2000 [00:24<00:01, 86.14it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  93%|█████████▎| 1869/2000 [00:24<00:01, 90.37it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  94%|█████████▍| 1880/2000 [00:24<00:01, 93.61it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:24<00:01, 96.01it/s, train_loss=0.4607, val_loss=0.4600]\u001B[A\n",
      "Training (adam):  95%|█████████▍| 1891/2000 [00:25<00:01, 96.01it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  95%|█████████▌| 1901/2000 [00:25<00:01, 55.29it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  96%|█████████▌| 1911/2000 [00:25<00:01, 63.22it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  96%|█████████▌| 1922/2000 [00:25<00:01, 71.59it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  97%|█████████▋| 1933/2000 [00:25<00:00, 78.57it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  97%|█████████▋| 1944/2000 [00:25<00:00, 84.25it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  98%|█████████▊| 1954/2000 [00:25<00:00, 88.16it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  98%|█████████▊| 1965/2000 [00:25<00:00, 91.76it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  99%|█████████▉| 1976/2000 [00:26<00:00, 94.48it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam):  99%|█████████▉| 1987/2000 [00:26<00:00, 96.51it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Training (adam): 100%|██████████| 2000/2000 [00:26<00:00, 75.94it/s, train_loss=0.4611, val_loss=0.4611]\u001B[A\n",
      "Trials: 100%|██████████| 3/3 [03:22<00:00, 67.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Runtime Profiling:\n",
      "| Optimizer    | Time/step (ms)   |   Time/2000 iters (min) |\n",
      "|:-------------|:-----------------|------------------------:|\n",
      "| SGD Constant | 8.85             |                    0.29 |\n",
      "| SGD Polyak   | 10.52 (+18.9%)   |                    0.35 |\n",
      "| AdamW        | 14.23 (+60.8%)   |                    0.47 |\n",
      "\n",
      "5. Step-Size Instability Analysis for Polyak SGD:\n",
      "   α_min (1e-4) hits: 0.0 times (0.0% of iterations)\n",
      "   α_max (1.0) hits: 1072.0 times (53.6% of iterations)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1500 with 4 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Training Results:\n",
      "Constant SGD: Train Loss=2.1689, Val Loss=2.1707, Overfitting=0.0018\n",
      "Polyak SGD: Train Loss=0.5168, Val Loss=0.5182, Overfitting=0.0014\n",
      "AdamW: Train Loss=0.4640, Val Loss=0.4639, Overfitting=-0.0000\n",
      "Tests passed: AdamW losses within expected range\n",
      "\n",
      "6. Appendix 1(d) - Hyperparameter Tuning Results\n",
      "| optimizer    |     lr | beta2   |   avg_val_loss |\n",
      "|:-------------|-------:|:--------|---------------:|\n",
      "| SGD Constant | 0.005  | N/A     |       2.17043  |\n",
      "| AdamW        | 0.0001 | 0.98    |       1.43556  |\n",
      "| AdamW        | 0.0001 | 0.999   |       0.919496 |\n",
      "| AdamW        | 0.0005 | 0.98    |       0.477582 |\n",
      "| AdamW        | 0.0005 | 0.999   |       0.469358 |\n",
      "| AdamW        | 0.001  | 0.98    |       0.472907 |\n",
      "| AdamW        | 0.001  | 0.999   |       0.466452 |\n",
      "| SGD Constant | 0.01   | N/A     |       1.83359  |\n",
      "| AdamW        | 0.0001 | 0.98    |       1.43669  |\n",
      "| AdamW        | 0.0001 | 0.999   |       0.9183   |\n",
      "| AdamW        | 0.0005 | 0.98    |       0.478004 |\n",
      "| AdamW        | 0.0005 | 0.999   |       0.472984 |\n",
      "| AdamW        | 0.001  | 0.98    |       0.472111 |\n",
      "| AdamW        | 0.001  | 0.999   |       0.465839 |\n",
      "| SGD Constant | 0.02   | N/A     |       1.47112  |\n",
      "| AdamW        | 0.0001 | 0.98    |       1.43507  |\n",
      "| AdamW        | 0.0001 | 0.999   |       0.916789 |\n",
      "| AdamW        | 0.0005 | 0.98    |       0.480777 |\n",
      "| AdamW        | 0.0005 | 0.999   |       0.473485 |\n",
      "| AdamW        | 0.001  | 0.98    |       0.476877 |\n",
      "| AdamW        | 0.001  | 0.999   |       0.463522 |\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T23:49:27.409003Z",
     "start_time": "2025-04-28T23:49:27.406478Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d1934469ecbc05a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fb596f3420e8d87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-29T00:37:40.067907Z",
     "start_time": "2025-04-29T00:37:26.517824Z"
    }
   },
   "source": [
    "# Cell 16: Polyak-Adam vs Adam for 1(e)\n",
    "'''\n",
    "1(e): Brief investigation of Polyak-Adam vs Adam on noisy linear regression.\n",
    "\n",
    "Polyak-Adam combines Polyak step sizing with Adam momentum estimation.\n",
    "Because Adam rescales updates by √vₖ, combining with Polyak's (loss/‖∇‖²) can lead to \n",
    "overly large effective steps unless bounded by α_max and α_min constraints.\n",
    "'''\n",
    "\n",
    "# Synthetic data (same as 1(b))\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "N = 200\n",
    "X = torch.linspace(-5, 5, N).unsqueeze(1)\n",
    "y = 4 * X - 2 + 0.5 * torch.randn_like(X)\n",
    "\n",
    "# Split into train/test\n",
    "train_size = int(0.8 * len(X))\n",
    "test_size = len(X) - train_size\n",
    "train_ds, test_ds = random_split(TensorDataset(X, y), [train_size, test_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=20, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=20)\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Polyak-Adam implementation\n",
    "def train_polyak_adam(model, dataloader, loss_fn, num_epochs=50, beta1=0.9, beta2=0.999, \n",
    "                      eps=1e-8, lr_lb=1e-4, f_star=0.0, alpha_max=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train model with Polyak-Adam (Polyak step size with Adam momentum).\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        dataloader: DataLoader for training data\n",
    "        loss_fn: Loss function (e.g., MSELoss)\n",
    "        num_epochs: Number of epochs\n",
    "        beta1, beta2: Adam momentum parameters\n",
    "        eps: Adam numerical stability term\n",
    "        lr_lb: Lower bound on Polyak step size\n",
    "        f_star: Assumed minimum loss (0.0)\n",
    "        alpha_max: Upper bound on Polyak step size\n",
    "        device: Device for training\n",
    "    \n",
    "    Returns:\n",
    "        history: List of per-epoch losses\n",
    "        metrics: Dict with final loss and convergence epoch\n",
    "        alpha_history: List of Polyak step sizes used\n",
    "    \"\"\"\n",
    "    model.to(device).train()\n",
    "    m = [torch.zeros_like(p, device=device) for p in model.parameters()]\n",
    "    v = [torch.zeros_like(p, device=device) for p in model.parameters()]\n",
    "    step, history = 0, []\n",
    "    convergence_epoch = None\n",
    "    threshold = 0.5  # MSE threshold for convergence\n",
    "    \n",
    "    # Track Polyak step sizes\n",
    "    alpha_history = []\n",
    "    alpha_min_count = 0\n",
    "    alpha_max_count = 0\n",
    "    total_steps = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running = 0.0\n",
    "        epoch_alphas = []\n",
    "        \n",
    "        for xb, yb in dataloader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            step += 1\n",
    "            total_steps += 1\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Compute Polyak step size\n",
    "            grads = torch.cat([p.grad.view(-1) for p in model.parameters() if p.grad is not None])\n",
    "            gnorm2 = grads.dot(grads) + 1e-12\n",
    "            raw_alpha = (loss.item() - f_star) / gnorm2.item()\n",
    "            alpha_k = max(min(raw_alpha, alpha_max), lr_lb)\n",
    "            \n",
    "            # Track when bounds are hit\n",
    "            if alpha_k == lr_lb:\n",
    "                alpha_min_count += 1\n",
    "            elif alpha_k == alpha_max:\n",
    "                alpha_max_count += 1\n",
    "            \n",
    "            epoch_alphas.append(alpha_k)\n",
    "            \n",
    "            # Adam update with Polyak step size\n",
    "            with torch.no_grad():\n",
    "                for i, p in enumerate(model.parameters()):\n",
    "                    if p.grad is None: continue\n",
    "                    m[i] = beta1 * m[i] + (1 - beta1) * p.grad\n",
    "                    v[i] = beta2 * v[i] + (1 - beta2) * (p.grad * p.grad)\n",
    "                    m_hat = m[i] / (1 - beta1 ** step)\n",
    "                    v_hat = v[i] / (1 - beta2 ** step)\n",
    "                    p -= alpha_k * m_hat / (v_hat.sqrt() + eps)\n",
    "            \n",
    "            running += loss.item() * xb.size(0)\n",
    "        \n",
    "        # Store average alpha for this epoch\n",
    "        alpha_history.append(np.mean(epoch_alphas))\n",
    "        \n",
    "        epoch_loss = running / len(dataloader.dataset)\n",
    "        history.append(epoch_loss)\n",
    "        if convergence_epoch is None and epoch_loss < threshold:\n",
    "            convergence_epoch = epoch + 1\n",
    "    \n",
    "    # Calculate percentage of times alpha hit bounds\n",
    "    alpha_min_pct = (alpha_min_count / total_steps) * 100\n",
    "    alpha_max_pct = (alpha_max_count / total_steps) * 100\n",
    "    \n",
    "    metrics = {\n",
    "        'final_loss': history[-1], \n",
    "        'convergence_epoch': convergence_epoch or num_epochs,\n",
    "        'alpha_min_pct': alpha_min_pct,\n",
    "        'alpha_max_pct': alpha_max_pct\n",
    "    }\n",
    "    \n",
    "    return history, metrics, alpha_history\n",
    "\n",
    "# Function to run experiment with different noise levels\n",
    "def run_experiment(noise_level):\n",
    "    # Generate data with specified noise level\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    X = torch.linspace(-5, 5, N).unsqueeze(1)\n",
    "    y = 4 * X - 2 + noise_level * torch.randn_like(X)\n",
    "    \n",
    "    # Split into train/test\n",
    "    train_size = int(0.8 * len(X))\n",
    "    test_size = len(X) - train_size\n",
    "    train_ds, test_ds = random_split(TensorDataset(X, y), [train_size, test_size])\n",
    "    train_loader = DataLoader(train_ds, batch_size=20, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=20)\n",
    "    \n",
    "    # Run trials\n",
    "    hist_adam_all = []\n",
    "    hist_padam_all = []\n",
    "    val_adam_all = []\n",
    "    val_padam_all = []\n",
    "    alpha_histories = []\n",
    "    metrics_all = []\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        # Set seed for reproducibility\n",
    "        torch.manual_seed(42 + trial)\n",
    "        np.random.seed(42 + trial)\n",
    "        \n",
    "        # Adam baseline\n",
    "        model_adam = LinearModel().to(device)\n",
    "        opt = optim.Adam(model_adam.parameters(), lr=best_lr)\n",
    "        hist_adam = []\n",
    "        for epoch in range(50):\n",
    "            running = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model_adam(xb)\n",
    "                loss = mse(pred, yb)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                running += loss.item() * xb.size(0)\n",
    "            hist_adam.append(running / len(train_loader.dataset))\n",
    "        val_adam = compute_val_loss(model_adam, test_loader, mse)\n",
    "        \n",
    "        # Polyak-Adam\n",
    "        model_padam = LinearModel().to(device)\n",
    "        hist_padam, metrics_padam, alpha_hist = train_polyak_adam(\n",
    "            model_padam, train_loader, mse,\n",
    "            num_epochs=50, lr_lb=1e-4, alpha_max=1.0, device=device\n",
    "        )\n",
    "        val_padam = compute_val_loss(model_padam, test_loader, mse)\n",
    "        \n",
    "        hist_adam_all.append(hist_adam)\n",
    "        hist_padam_all.append(hist_padam)\n",
    "        val_adam_all.append(val_adam)\n",
    "        val_padam_all.append(val_padam)\n",
    "        alpha_histories.append(alpha_hist)\n",
    "        metrics_all.append(metrics_padam)\n",
    "        \n",
    "        # Log trial metrics\n",
    "        print(f\"1(e) σ={noise_level} Trial {trial+1}: Adam - Train={hist_adam[-1]:.4f}, Val={val_adam:.4f}\")\n",
    "        print(f\"1(e) σ={noise_level} Trial {trial+1}: Polyak-Adam - Train={hist_padam[-1]:.4f}, Val={val_padam:.4f}, {metrics_padam}\")\n",
    "    \n",
    "    # Average results\n",
    "    hist_adam = np.mean(hist_adam_all, axis=0)\n",
    "    hist_padam = np.mean(hist_padam_all, axis=0)\n",
    "    val_adam = np.mean(val_adam_all)\n",
    "    val_padam = np.mean(val_padam_all)\n",
    "    alpha_history = np.mean(alpha_histories, axis=0)\n",
    "    \n",
    "    # Use the metrics from the last trial (or could average them)\n",
    "    final_metrics = metrics_all[-1]\n",
    "    \n",
    "    # Compute overfitting metrics\n",
    "    overfit_adam = val_adam - hist_adam[-1]\n",
    "    overfit_padam = val_padam - hist_padam[-1]\n",
    "    \n",
    "    return {\n",
    "        'hist_adam': hist_adam,\n",
    "        'hist_padam': hist_padam,\n",
    "        'val_adam': val_adam,\n",
    "        'val_padam': val_padam,\n",
    "        'overfit_adam': overfit_adam,\n",
    "        'overfit_padam': overfit_padam,\n",
    "        'alpha_history': alpha_history,\n",
    "        'metrics_padam': final_metrics\n",
    "    }\n",
    "\n",
    "# Hyperparameter tuning for Adam\n",
    "lr_candidates = [1e-3, 1e-2, 5e-2]\n",
    "best_lr = 1e-2\n",
    "best_mse = float('inf')\n",
    "num_trials = 3\n",
    "\n",
    "# Create a table for Adam lr tuning results\n",
    "adam_tuning_results = []\n",
    "\n",
    "for lr in lr_candidates:\n",
    "    mse_sum = 0\n",
    "    for trial in range(num_trials):\n",
    "        set_seed(42 + trial)\n",
    "        model = LinearModel().to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "        hist = []\n",
    "        for epoch in range(50):\n",
    "            running = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                loss = mse(pred, yb)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                running += loss.item() * xb.size(0)\n",
    "            hist.append(running / len(train_loader.dataset))\n",
    "        mse_sum += hist[-1]\n",
    "    \n",
    "    avg_mse = mse_sum / num_trials\n",
    "    adam_tuning_results.append((lr, avg_mse))\n",
    "    \n",
    "    if avg_mse < best_mse:\n",
    "        best_mse = avg_mse\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"Best Adam LR: {best_lr}\")\n",
    "print(f\"1(e) Adam learning rate tuning:\")\n",
    "for lr, mse_val in adam_tuning_results:\n",
    "    print(f\"  lr={lr:.1e}: Avg MSE={mse_val:.6f}\")\n",
    "\n",
    "# Helper function for validation\n",
    "def compute_val_loss(model, loader, loss_fn):\n",
    "    \"\"\"Compute validation loss.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            out = model(Xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            total_loss += loss.item() * Xb.size(0)\n",
    "            total_samples += Xb.size(0)\n",
    "    return total_loss / total_samples\n",
    "\n",
    "# Helper function for setting seed\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Run experiments with different noise levels\n",
    "noise_levels = [0.5, 1.0]  # Default and increased noise\n",
    "results = {}\n",
    "\n",
    "for noise in noise_levels:\n",
    "    results[noise] = run_experiment(noise)\n",
    "\n",
    "# Plot learning curves for σ=0.5\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results[0.5]['hist_adam'], label=f'Adam (lr={best_lr})')\n",
    "plt.plot(results[0.5]['hist_padam'], label='Polyak-Adam (lr_lb=1e-4)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train MSE')\n",
    "plt.title('1(e) Polyak-Adam vs Adam: σ=0.5')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Polyak step sizes for σ=0.5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results[0.5]['alpha_history'], label='Avg Polyak Step Size (αₖ)')\n",
    "plt.axhline(y=1e-4, color='r', linestyle='--', label='α_min=1e-4')\n",
    "plt.axhline(y=1.0, color='g', linestyle='--', label='α_max=1.0')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Step Size (αₖ)')\n",
    "plt.title('Polyak Step Sizes: σ=0.5')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create results summary table\n",
    "results_table = [\n",
    "    [\"Method\", \"Noise (σ)\", \"Train MSE\", \"Val MSE\", \"Overfitting\"],\n",
    "]\n",
    "\n",
    "for noise in noise_levels:\n",
    "    results_table.append([\n",
    "        \"Adam\", \n",
    "        f\"{noise}\", \n",
    "        f\"{results[noise]['hist_adam'][-1]:.4f}\", \n",
    "        f\"{results[noise]['val_adam']:.4f}\", \n",
    "        f\"{results[noise]['overfit_adam']:.4f}\"\n",
    "    ])\n",
    "    results_table.append([\n",
    "        \"Polyak-Adam\", \n",
    "        f\"{noise}\", \n",
    "        f\"{results[noise]['hist_padam'][-1]:.4f}\", \n",
    "        f\"{results[noise]['val_padam']:.4f}\", \n",
    "        f\"{results[noise]['overfit_padam']:.4f}\"\n",
    "    ])\n",
    "\n",
    "# Print results table\n",
    "print(\"\\n1(e) Results Summary:\")\n",
    "# Print formatted table without tabulate dependency\n",
    "print(\"+\" + \"-\"*15 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\")\n",
    "print(f\"| {'Method':<13} | {'Noise (σ)':<10} | {'Train MSE':<10} | {'Val MSE':<10} | {'Overfitting':<10} |\")\n",
    "print(\"+\" + \"-\"*15 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\")\n",
    "for row in results_table[1:]:\n",
    "    print(f\"| {row[0]:<13} | {row[1]:<10} | {row[2]:<10} | {row[3]:<10} | {row[4]:<10} |\")\n",
    "print(\"+\" + \"-\"*15 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\" + \"-\"*12 + \"+\")\n",
    "\n",
    "# Log stability diagnostics\n",
    "for noise in noise_levels:\n",
    "    print(f\"\\nStability Diagnostics (σ={noise}):\")\n",
    "    print(f\"  α_min hit: {results[noise]['metrics_padam']['alpha_min_pct']:.1f}% of iterations\")\n",
    "    print(f\"  α_max hit: {results[noise]['metrics_padam']['alpha_max_pct']:.1f}% of iterations\")\n",
    "\n",
    "# Test assertions\n",
    "sigma = 0.5\n",
    "assert results[sigma]['hist_adam'][-1] < sigma**2 * 2, f\"Adam MSE {results[sigma]['hist_adam'][-1]} too high\"\n",
    "assert results[sigma]['hist_padam'][-1] < sigma**2 * 2, f\"Polyak-Adam MSE {results[sigma]['hist_padam'][-1]} too high\"\n",
    "print(\"Tests passed: MSE within expected range\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Adam LR: 0.05\n",
      "1(e) Adam learning rate tuning:\n",
      "  lr=1.0e-03: Avg MSE=89.292359\n",
      "  lr=1.0e-02: Avg MSE=6.319579\n",
      "  lr=5.0e-02: Avg MSE=0.215285\n",
      "1(e) σ=0.5 Trial 1: Adam - Train=0.2155, Val=0.2980\n",
      "1(e) σ=0.5 Trial 1: Polyak-Adam - Train=0.2270, Val=0.2863, {'final_loss': 0.22696729749441147, 'convergence_epoch': 26, 'alpha_min_pct': 0.0, 'alpha_max_pct': 18.5}\n",
      "1(e) σ=0.5 Trial 2: Adam - Train=0.2153, Val=0.2966\n",
      "1(e) σ=0.5 Trial 2: Polyak-Adam - Train=0.2338, Val=0.3185, {'final_loss': 0.23379979841411114, 'convergence_epoch': 26, 'alpha_min_pct': 0.0, 'alpha_max_pct': 18.75}\n",
      "1(e) σ=0.5 Trial 3: Adam - Train=0.2151, Val=0.2973\n",
      "1(e) σ=0.5 Trial 3: Polyak-Adam - Train=0.2233, Val=0.3165, {'final_loss': 0.22332636453211308, 'convergence_epoch': 27, 'alpha_min_pct': 0.0, 'alpha_max_pct': 19.5}\n",
      "1(e) σ=1.0 Trial 1: Adam - Train=0.8619, Val=1.1920\n",
      "1(e) σ=1.0 Trial 1: Polyak-Adam - Train=0.9070, Val=1.1451, {'final_loss': 0.9070238508284092, 'convergence_epoch': 50, 'alpha_min_pct': 0.0, 'alpha_max_pct': 23.75}\n",
      "1(e) σ=1.0 Trial 2: Adam - Train=0.8611, Val=1.1863\n",
      "1(e) σ=1.0 Trial 2: Polyak-Adam - Train=0.9342, Val=1.2705, {'final_loss': 0.9341998025774956, 'convergence_epoch': 50, 'alpha_min_pct': 0.0, 'alpha_max_pct': 23.25}\n",
      "1(e) σ=1.0 Trial 3: Adam - Train=0.8603, Val=1.1891\n",
      "1(e) σ=1.0 Trial 3: Polyak-Adam - Train=0.8931, Val=1.2648, {'final_loss': 0.8931494653224945, 'convergence_epoch': 50, 'alpha_min_pct': 0.0, 'alpha_max_pct': 23.75}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1(e) Results Summary:\n",
      "+---------------+------------+------------+------------+------------+\n",
      "| Method        | Noise (σ)  | Train MSE  | Val MSE    | Overfitting |\n",
      "+---------------+------------+------------+------------+------------+\n",
      "| Adam          | 0.5        | 0.2153     | 0.2973     | 0.0820     |\n",
      "| Polyak-Adam   | 0.5        | 0.2280     | 0.3071     | 0.0791     |\n",
      "| Adam          | 1.0        | 0.8611     | 1.1892     | 0.3280     |\n",
      "| Polyak-Adam   | 1.0        | 0.9115     | 1.2268     | 0.3153     |\n",
      "+---------------+------------+------------+------------+------------+\n",
      "\n",
      "Stability Diagnostics (σ=0.5):\n",
      "  α_min hit: 0.0% of iterations\n",
      "  α_max hit: 19.5% of iterations\n",
      "\n",
      "Stability Diagnostics (σ=1.0):\n",
      "  α_min hit: 0.0% of iterations\n",
      "  α_max hit: 23.8% of iterations\n",
      "Tests passed: MSE within expected range\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb41dc346de3bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T15:14:01.794555Z",
     "start_time": "2025-04-28T15:13:59.476917Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "id": "7cf84565346951c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:49:55.614128Z",
     "start_time": "2025-04-28T18:49:55.612573Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:49:56.011628Z",
     "start_time": "2025-04-28T18:49:56.010051Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6a3d597dbd2be4d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:49:56.246106Z",
     "start_time": "2025-04-28T18:49:56.244651Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4e4e8e5a7cf590b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:49:56.460132Z",
     "start_time": "2025-04-28T18:49:56.458624Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a1cbcafb93fdba73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:49:56.677263Z",
     "start_time": "2025-04-28T18:49:56.675938Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "14370b65e1e58ba8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T18:49:58.613200Z",
     "start_time": "2025-04-28T18:49:58.219363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 12: Linear Regression for 1(a) with Tests\n",
    "'''\n",
    "1a: Implementation and Testing of Polyak Step Size\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "\n",
    "# Assuming LinearModel and set_seed are defined elsewhere\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate synthetic linear data y = 4x - 2 + noise\n",
    "torch.manual_seed(0)\n",
    "N = 200\n",
    "X = torch.linspace(-5, 5, N).unsqueeze(1)\n",
    "y = 4 * X - 2 + 0.5 * torch.randn_like(X)\n",
    "\n",
    "# Wrap in DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "mse = nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Run multiple trials\n",
    "num_trials = 3\n",
    "hist_const_all = []\n",
    "hist_poly_all = []\n",
    "polyak_metrics_all = []  # Store metrics for alpha_log\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    set_seed(42 + trial)\n",
    "    \n",
    "    # Train with constant LR\n",
    "    model_const = LinearModel().to(device)\n",
    "    hist_const, metrics_const = train_sgd(\n",
    "        model_const, loader, mse,\n",
    "        num_epochs=50,\n",
    "        step_method='constant',\n",
    "        alpha=1e-2,\n",
    "        alpha_max=1.0\n",
    "    )\n",
    "    \n",
    "    # Train with Polyak step\n",
    "    model_poly = LinearModel().to(device)\n",
    "    hist_poly, metrics_poly = train_sgd(\n",
    "        model_poly, loader, mse,\n",
    "        num_epochs=50,\n",
    "        step_method='polyak',\n",
    "        alpha=1e-4,  # Lower bound\n",
    "        f_star=0.0,\n",
    "        alpha_max=1.0\n",
    "    )\n",
    "    \n",
    "    hist_const_all.append(hist_const)\n",
    "    hist_poly_all.append(hist_poly)\n",
    "    polyak_metrics_all.append(metrics_poly)  # Store metrics to access alpha_log later\n",
    "    \n",
    "    # Log trial metrics\n",
    "    logging.info(f\"Linear Trial {trial+1}: Constant - {metrics_const}\")\n",
    "    logging.info(f\"Linear Trial {trial+1}: Polyak - {metrics_poly}\")\n",
    "\n",
    "# Average results\n",
    "hist_const = np.mean(hist_const_all, axis=0)\n",
    "hist_poly = np.mean(hist_poly_all, axis=0)\n",
    "\n",
    "# Plot losses (Figure 1)\n",
    "plt.figure()\n",
    "plt.plot(hist_const, label='Constant LR')\n",
    "plt.plot(hist_poly, label='Polyak Step')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train MSE Loss')\n",
    "plt.title('Linear Regression: Constant vs Polyak')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot αₖ vs. epoch (Figure X for Step-Size Instrumentation)\n",
    "# Extract alpha_log from each trial's metrics\n",
    "alpha_logs = [metrics['alpha_log'] for metrics in polyak_metrics_all]\n",
    "\n",
    "batches_per_epoch = 10  # 200 samples, B=20\n",
    "epochs = 50\n",
    "alpha_per_epoch = []\n",
    "for trial_log in alpha_logs:\n",
    "    trial_avg = []\n",
    "    for epoch in range(epochs):\n",
    "        start = epoch * batches_per_epoch\n",
    "        end = (epoch + 1) * batches_per_epoch\n",
    "        epoch_alpha = np.mean(trial_log[start:end])\n",
    "        trial_avg.append(epoch_alpha)\n",
    "    alpha_per_epoch.append(trial_avg)\n",
    "\n",
    "# Average across trials\n",
    "alpha_per_epoch_avg = np.mean(alpha_per_epoch, axis=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(range(1, epochs + 1), alpha_per_epoch_avg, label='Polyak Step')\n",
    "plt.axhline(y=1e-2, color='r', linestyle='--', label='Constant LR (α=1e-2)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('αₖ (Averaged per Epoch)')\n",
    "plt.title('Step Size vs. Epoch for Linear Regression (T1)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print and log final loss values\n",
    "print(f\"Final Constant LR Loss: {hist_const[-1]:.4f}\")\n",
    "print(f\"Final Polyak Loss: {hist_poly[-1]:.4f}\")\n",
    "logging.info(f\"Linear Final: Constant LR Loss: {hist_const[-1]:.4f}\")\n",
    "logging.info(f\"Linear Final: Polyak Loss: {hist_poly[-1]:.4f}\")\n",
    "\n",
    "# Test assertions\n",
    "sigma = 0.5\n",
    "assert hist_const[-1] < sigma**2 * 2, f\"Constant LR MSE {hist_const[-1]} > {sigma**2 * 2}\"\n",
    "assert hist_poly[-1] < sigma**2 * 2, f\"Polyak MSE {hist_poly[-1]} > {sigma**2 * 2}\"\n",
    "print(\"Tests passed: Final MSE within expected range\")\n",
    "\n",
    "# Edge-case test: Tiny gradients\n",
    "X_edge = torch.linspace(-0.01, 0.01, N).unsqueeze(1)  # Small inputs\n",
    "y_edge = 4 * X_edge - 2 + 0.01 * torch.randn_like(X_edge)  # Low noise\n",
    "edge_loader = DataLoader(TensorDataset(X_edge, y_edge), batch_size=20, shuffle=True)\n",
    "\n",
    "model_edge = LinearModel().to(device)\n",
    "hist_edge, metrics_edge = train_sgd(\n",
    "    model_edge, edge_loader, mse,\n",
    "    num_epochs=50,\n",
    "    step_method='polyak',\n",
    "    alpha=1e-4,\n",
    "    f_star=0.0,\n",
    "    alpha_max=1.0\n",
    ")\n",
    "assert hist_edge[-1] < 0.01, f\"Edge-case MSE {hist_edge[-1]} too high\"\n",
    "logging.info(f\"Edge-case Test: Polyak MSE={hist_edge[-1]:.4f}, {metrics_edge}\")\n",
    "print(f\"Edge-case Test Passed: Polyak MSE={hist_edge[-1]:.4f}\")\n",
    "\n",
    "def train_sgd(model, dataloader, loss_fn,\n",
    "              num_epochs=20,\n",
    "              step_method='constant',\n",
    "              alpha=1e-2,\n",
    "              f_star=0.0,\n",
    "              alpha_max=1.0):\n",
    "    model.to(device).train()\n",
    "    history = []\n",
    "    convergence_epoch = None\n",
    "    threshold = 0.05  # Loss threshold for convergence\n",
    "\n",
    "    # NEW: Log alpha values for Polyak\n",
    "    alpha_log = []  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for Xb, yb in dataloader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            out = model(Xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            grads = torch.cat([p.grad.view(-1) for p in model.parameters() if p.grad is not None])\n",
    "\n",
    "            if step_method == 'constant':\n",
    "                lr = alpha\n",
    "            else:  # 'polyak'\n",
    "                loss_val = loss.item()\n",
    "                denom = grads.dot(grads).item() + 1e-12\n",
    "                lr = (loss_val - f_star) / denom\n",
    "                if lr < 0:\n",
    "                    lr = alpha\n",
    "                lr = max(alpha, min(lr, alpha_max))\n",
    "                \n",
    "                # NEW: Log alpha_k\n",
    "                alpha_log.append(lr)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        p.data -= lr * p.grad\n",
    "            running_loss += loss.item() * Xb.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        history.append(epoch_loss)\n",
    "\n",
    "        if convergence_epoch is None and epoch_loss < threshold:\n",
    "            convergence_epoch = epoch + 1\n",
    "\n",
    "    metrics = {\n",
    "        'final_loss': history[-1],\n",
    "        'convergence_epoch': convergence_epoch or num_epochs\n",
    "    }\n",
    "\n",
    "    # NEW: If Polyak, store alpha_log inside metrics\n",
    "    if step_method == 'polyak':\n",
    "        metrics['alpha_log'] = alpha_log\n",
    "\n",
    "    logging.info(f\"train_sgd: step_method={step_method}, final_loss={history[-1]:.4f}, \"\n",
    "                 f\"convergence_epoch={metrics['convergence_epoch']}\")\n",
    "    \n",
    "    return history, metrics"
   ],
   "id": "91a614868766a54a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Constant LR Loss: 0.2329\n",
      "Final Polyak Loss: 0.6698\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Polyak MSE 0.6697766457994779 > 0.5",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 117\u001B[39m\n\u001B[32m    115\u001B[39m sigma = \u001B[32m0.5\u001B[39m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m hist_const[-\u001B[32m1\u001B[39m] < sigma**\u001B[32m2\u001B[39m * \u001B[32m2\u001B[39m, \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mConstant LR MSE \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhist_const[-\u001B[32m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m > \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msigma**\u001B[32m2\u001B[39m\u001B[38;5;250m \u001B[39m*\u001B[38;5;250m \u001B[39m\u001B[32m2\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m hist_poly[-\u001B[32m1\u001B[39m] < sigma**\u001B[32m2\u001B[39m * \u001B[32m2\u001B[39m, \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPolyak MSE \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhist_poly[-\u001B[32m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m > \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msigma**\u001B[32m2\u001B[39m\u001B[38;5;250m \u001B[39m*\u001B[38;5;250m \u001B[39m\u001B[32m2\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    118\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTests passed: Final MSE within expected range\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    120\u001B[39m \u001B[38;5;66;03m# Edge-case test: Tiny gradients\u001B[39;00m\n",
      "\u001B[31mAssertionError\u001B[39m: Polyak MSE 0.6697766457994779 > 0.5"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:54:59.375370Z",
     "start_time": "2025-04-28T19:54:59.077446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Paste your logged alpha values here\n",
    "\n",
    "\n",
    "epochs = np.arange(1, len(alpha_values) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, alpha_values, label=r'$\\alpha_k$ (Polyak Step Size)', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Step Size (αₖ)')\n",
    "plt.title('Step-Size Dynamics Across Training')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.yscale('log')  # Because the spikes can be huge\n",
    "plt.tight_layout()\n",
    "plt.savefig('step_size_dynamics.png', dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "b803546318186e93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAHqCAYAAACZcdjsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzsfQe8HFX1/9n3XgpJIJSQUBJ6DSJSQkdQmkRRUOwFsGtQ7IL4ExARK38Vg0hVUEGKSpUqPQFCLyGBVEIgvbeXV+b/ObNvdmfu3HLOnTuzs5v75fPI7uxtM3PL6acSBEEAHh4eHh4eHh4eHh4eGdCWpbKHh4eHh4eHh4eHhwfCMxYeHh4eHh4eHh4eHpnhGQsPDw8PDw8PDw8Pj8zwjIWHh4eHh4eHh4eHR2Z4xsLDw8PDw8PDw8PDIzM8Y+Hh4eHh4eHh4eHhkRmesfDw8PDw8PDw8PDwyAzPWHh4eHh4eHh4eHh4ZIZnLDw8PDw8PDw8PDw8MsMzFh4eHh4MPPTQQ1CpVMJ/PfLBUUcdFf55NCdOO+002GGHHazqnnfeeeH68vDwaE54xsLDwyM3vPTSS3DKKafA9ttvDwMHDoRtt90Wjj32WLjkkksS5X7+85/Df/7zH2gkent74dprr4WDDjoINt98c9h4441ht912g8997nPwxBNPNHRsf/nLX0JiK/rDZ7nNNtvA8ccfD3/4wx9g5cqVDR1fq2DZsmXhs8Vn/Oqrr0KrIT6HdH+eafbw8LBFJQiCwLq2h4eHhwITJkyA97znPbDddtvBqaeeCltttRXMmTMnJNKnT58O06ZNq5UdMmRIyIAgAd0onHHGGTB+/Hj40Ic+BO9973uho6MDpk6dCv/973/hU5/6VChJjRiQ9evXQ//+/aGtrRjZDD6X008/HX7605/CjjvuCF1dXTBv3ryQALzvvvvCZ3zbbbfBO9/5TmgF4PNF4DMuEldccQV885vfhE033RS+8IUvwM9+9jNoJfztb39LfEdGGufPddddl7iOzP+IESOs+8H5ietkwIAB7Lrd3d3hHzJ4Hh4ezQfPWHh4eOSC97///TBp0iR47bXXQkItjgULFsDw4cNLw1jMnz8ftt56a/jiF78Il19+eeI33CIXLlyYGG/RiBgLfJ4HHHBA4rf//e9/8IEPfCAcH0rZN9poo4aNs9lx5JFHwrBhw0ING2rQZsyY4aRdnEPr1q0r3buJmGkTGbBmzRoYNGhQYePy8PBoXnhTKA8Pj1yAWom99torxVQg4kQ6ml6sXr0a/vrXv9ZMMdBGO8LcuXPh85//fChBRQkotnn11VdL/R7++c9/wo9+9KNQOzJ48GD44Ac/GGpJTJg5c2ZIXB122GGp37Dd+HhFHwvRTCn+J/oJoMR4//33DwlMNLf6xCc+QRqfDqhd+b//+z+YPXt2TSJ9zTXXhP0/99xzqfJodtbe3h4+VwSO8R3veAdMnjw51DAhAYkma7/61a9SWoSf/OQn4fiHDh0aPt8jjjgCHnzwwUS5WbNmhX3/5je/CYnWnXbaKWzzuOOOC+8Vn/MFF1wAI0eODJ8DaoiWLFli9LFAwhy1RmiehtJsZAQ//OEPh/Mswg033BCOD83YNtlkE9h7773h97//Pek5vvHGG/Doo4+G7wT/cE6g1k0GfM4HHnhgeF+bbbYZvPvd74Z777239jv6FyCzd88994SMIN7nn//85/A3ZFY++tGPhu8f6x988MFw5513pvpAc0Gc61Ef2M4//vGP2u9o/vatb30r7AvXBc5R1DQ8++yzkAXRfHjmmWfC+8L+cU0hbr311lBggGZ42OfOO+8cvsuenh6tj0V8TiDjjvWw/pgxY0Jm2eRjgd+RCUJmD8cW7QN33313avy4LvFZ4RzBfvC5e78ND4/i0FFgXx4eHhsQUOo7ceJEePnll0NiQAU0w0BNARJqX/7yl8NrSBBEmgQkvCLCYssttwxNk9BMZcWKFSFhFceFF14Ylv3hD38YakV+97vfwTHHHAPPP/+8VlqMY0XcdNNNIdHHkc4i8SWakiCR/+Mf/zjBkODYkAH42Mc+Ft4vakGQeMT6yADIGDAqPvvZz4bEHxK3X/rSl0Ltz7hx4+Dvf/877LvvvomyeA2JR2QeIixduhTe9773hYQ6ju/mm28OnyES5ieccEJYBp/3lVdeCZ/85CfDPpCwveqqq0I/j6eeegre9a53pfpBZuQb3/hGyDggo4JtIyOExB+2j+Zw+Ay+973vpZjFOJBwRUL9gQceCIn+M888M+wfzXhwfuF8wc84tqOPPhp++ctfhvVQg/P444+H5U24/vrrQ2YJ+8G5gm3iPRx66KGJcueff35IqOJ1NE1Dc60nn3wy1Bwh8xQBzehwPF/5ylfC57X77ruH8xnroQYATa622GKLkKFGBhif+cknn5wwycL3iGNHpurFF18M+0GzPMRXv/rVsA6ui9GjR8PixYvhscceC+95v/32gyzAtvC947P+zGc+UzOLQiYatYvf+c53wn/xnpHZxLnx61//2tguMkb43vCZ4DrFOYFzDpmtfv36aevivf3rX/+Cr3/96yHjiL5FH/nIR0KGEJ8jAtcRzmNkOvE94bzBd4T7hoeHR0FAUygPDw8P17j33nuD9vb28O+QQw4JfvCDHwT33HNPsH79+lTZwYMHB6eeemrq+he+8IVg6623DhYtWpS4/olPfCIYOnRosGbNmvD7gw8+iLYcwbbbbhusWLGiVu7GG28Mr//+9783jvdzn/tcWHazzTYLTj755OA3v/lN8Oqrr6bKRX3hvzKsXbs22H///YNtttkmePvtt8Nrs2bNCp/DhRdemCj70ksvBR0dHanrIq655pqwz0mTJinL4PPYd999a98/+clPhmPo6empXXv22WfDdrC9CEceeWR47dprr61d6+zsDLbaaqvgIx/5SO1ad3d3eD2OpUuXBiNGjAg+//nP167NnDkzbG/LLbcMli1bVrt+9tlnh9f32WefoKurKzHO/v37B+vWrUuMCf8iXH311WHdiy++OHXfvb294b9nnnlmsMkmm4TjtMHee+8dfPrTn659/9GPfhQMGzYsMdbXX389aGtrC+dH/LnGx4HYfvvtw/HefffdiTLf+ta3wuuPPvpo7drKlSuDHXfcMdhhhx1qbX7oQx8K9tprL+148X2PGzcuyAKsL5IB0Xy47LLLUuWj9RbHV77ylWDQoEGJ94drGZ+BOCe22GKLYMmSJbXrt956a3j99ttvr10799xzU2PC7zhHpk2bVrv2wgsvhNcvueSS2rUTTzwxHMvcuXMT7wzXmCd3PDyKgTeF8vDwyAVoloEaC5TGvvDCC6F0EqXbKClHR2MTkJ645ZZb4MQTTww/L1q0qPaH7Sxfvjxl9oERnFCaGQElvii9vOuuu4z9ofnQH//4x9A5+t///ncoRd9zzz1DCXhkNkQBSlQxGhaOHU2yEChpRWdWlNjH7wN/33XXXVPmRDZACXI8OhQ+i7feeivRNkrgURqPkl6xLkqmI6AUHjVIcR8DNJ+KnKnxXlALgU62aHYiM79BzQ+aTEXAaFsI7Acd4+PXUbOhe8b4LNH3AbUfIiITF9T4oEkdai64QG0AvjPUMETAz/iO0JwpApri4L2jlF503BdNbXAe4TyNA+chPtfDDz888exRU4fmQmiOFt3Lm2++mTITigPLoAYD37FroKkR+vSIiGv9cK7h80FzONTATJkyxdjuxz/+8dCsKwLWRVB8WVDzGGkyERioAM3dorqonbj//vvhpJNOCk21Iuyyyy41rZuHh0f+8IyFh4dHbkAbaiSq0dQGzWXOPvvskCBBgj8iolRAUyEM/4k22WjKEP+LiB40d4oDiXSR2EPCAok2xKpVq8JoStEf9hEBCUU0H0LbciSY0J4cCRI090CTEArQnhsZFDTvQROuCK+//nrIHOH4xHtB0xXxPmyA9xZnqpCxQ6YKmQkEEsRo7oM+DfFyCPR3EAljJADxvcWBZjtI0KH9Opqf4PjRPwCZPBEYqSqOiMkYNWqU9LrYVxzoR4GmRHGGRMbQof8FvjO8H/TLkdngq3wm0AwK/UHQPAv/8B7RTyB6ftE4cJ6g6ZEJyFiIQBM5vA8RyMBGvyPQTAwZDmRCcM7gvESTrjiQUUczMHyeWA7Ns1w5myPzL4vI9corr4TmWvjOkKjH9x8xpLI5YJoTEZOhe/equlH9qC6uobVr14brXYTsmoeHRz7wPhYeHh65A4kUZDLwD4k/ZAzQn+Hcc89V1kFCGIGEC4arlYEbXhWdR9H2Ou5bETEdcSDRjJoW/EN/hIcffjgk+iJfDBmQcUJ7ePSfiHxF4veChDv6h6DkXwQSkVmA0m0k7OIEFPaD9vhor3/ppZeGhClKt+OaiXhZGeLRgpD4RqdclAh///vfD/1HsN5FF12UcKA2tUnpywY4HvSlQQ0DPmf8QyYPNTfIEKmA/SLDhdoOGcOABCsybdx3lCUCFDIa6KNxxx13hMwRamzwHaKmJJq/qP1CiT9q19C3Bn0c0LcEGfmsEnrZ2JHJx6hZyFCg3wJqD5D5Qm0VMkLRetUhy7vPa954eHi4hWcsPDw8CkUULvXtt9+uXZNFbEFpKErW0cQBzSAoQM2ASHSg9DliQJDIjJuhUIg/HC8yFjheFWOBmg/UwqADM0ZCEoFEGI4FpdjIWLlG5Dwumt7g/f72t7+F22+/PSS08ZmKZahAR2GU6CPhGn9fOubQFfD5odkP5kfQOfkiA4umc/iHhC5qMVCLhE7zKqk1vltkzJBYjjQHEVAajkwimkAhQ4bjwHZR2yY6q1OA8wcZBhGRGVF8fqEGBU2H8A9NxdDJGQMAoNYvyvGAGim8R/xDBgidtrFMHqY/6HCPTt34/jHgQASMnlUGIGOJzyWeHyeC7JqHh0c+8KZQHh4euQBt+2XSxMjfIW4SgkQUSkRFCSX6AqC0Fk0+RMTNmOIJv+J+BkgMI0MQEVpIGCOTEv1F4WXRLEpmmoUEHUYiQvMXFWGKjA+aSmFZHKvMhASJQrwflDaLzwS/I8FmCzTVwpCfyLR8+tOfTvyGDBX+YTQnHBuOU2dORJEYx8ePxD760eQNnAdonoY+MCKi8YjPEN9ZxFB2dnYazaBQC4PMYfwPozmhKVJkDoXaGmwXmRBRQk+RnI8dOzbUbMWfGWpK0NwPza4ijYl4Lzin8DfsA5krnHOi6RES1uhboLvXLJC9f5zzqEkpA3B8uKaRCYz7nSBTgUy1h4dHMfAaCw8Pj1yAjrbo1Ik22XvssUdIhGBeAMw1gURU3DkUcw+g4+XFF18cEkdIJKNT7y9+8YuQQcHPSOQhcYVOw2h+geXF/AeYGwA1Etg2hvbEcLPIEGBdHVBijXbqGAoVnbXRqRolwGgig47nGNYWnYdluOyyy0LiHsN/ik7YGKYTfR1Q0o1ZnFHajKZXSKCiNgalvWjKglJxdBY3AQkklG6j0zTeH/aLzsoo6UaHeFm2YtRaRG3LzKCowDCsKK3G94m5DHDseO/4TtBUKE/gPSDTiGFOkTBHEyAkyHEOoLQe/UbQBA3nA75D9LFA0zX0dUHNgqiJiIBEODJc+I5UmZ7RHA5zYeB8wLl0zjnnhIwcjgEZRnR0RidrnLdoFqbDWWedFc4pZHQxnCzOVzTTwmeJ44gcwjFsLc5BZHxxDqEfDjJV+Nxx3iATjveIzM8+++wTmmnhs8BxoIYqD2CYXPRpQLNEHDtqrVBTViZTJPQzQbMwfG5f+9rXQgYMnxuGu0YzOQ8PjwJQUPQpDw+PDQz//e9/wzCke+yxRzBkyJAwXOQuu+wSfOMb3wjmz5+fKDtlypTg3e9+d7DRRhuFYSHjoWexLIbFHDVqVNCvX78wDOrRRx8dXH755akQsNdff30Y1nT48OFhW+9///uD2bNnG8eKIWoxJO3xxx8fjBw5Muxn4403DsPkXnHFFYlQomK42Sg8puwvHjIVccsttwSHH354GF4X//DZ4L1NnTqVFG42+sNnic/h2GOPDccdD7ErAkPeYqjb3XbbTfo7jlEW2lQMGYrP4Oc//3l4bcCAAWFo2zvuuEMZWvTXv/51or3oud10003GULpiuNko1Ok555wThmaN5sEpp5wSTJ8+Pfz95ptvDo477rjw3ePz2W677cJQqFHIXxnwfWDfV111lbLMQw89lApZjOFv8f7xOWB4YhzrfffdV/sdnwfOPRlwvDjuTTfdNBg4cGBw4IEHhs8xjj//+c/hesDwrNjHzjvvHHz/+98Pli9fHv6OYX/xO4buxXmKcwk/X3rppYGLcLOqULePP/54cPDBB4drC0MZRyGkxfDL1DmBwOu4hkzhZmWhdbEPMUz1Aw88EL4bnAP43K688srgu9/9bvisPTw88kcF/1cEA+Ph4eGRF9D+G7NGo0M4SnE96kATIrTFR8df9DXw8NjQgBpCjGgl+mB5eHi4h/ex8PDw8GhhYLZkNAnB7NweHq0ODDkbBzIT6NeF0d08PDzyh/ex8PDw8GhBoP8FOqRjlCCU2KJfi4dHqwMDNGBYZPwX/Wz+9Kc/hc7vP/jBDxo9NA+PDQKesfDw8PBoQWDkInSWR0dWdGL28NgQ8L73vS90kMdIb+hYf8ghh8DPf/7zVPJMDw+PfOB9LDw8PDw8PDw8PDw8MsP7WHh4eHh4eHh4eHh4ZIZnLDw8PDw8PDw8PDw8MsP7WGiAmVUxgycmJMJkQB4eHh4eHh4eHh4bEoIggJUrV4aJQKNEnip4xkIDZCpGjRrV6GF4eHh4eHh4eHh4NBRz5syBkSNHast4xkID1FRED3KTTTYptO+uri6499574bjjjoN+/foV2rdHOeDngIefAx5+Dnj4OeDR1eA5sGLFilDQHtHFOnjGQoPI/AmZikYwFoMGDQr79RvJhgk/Bzz8HPDwc8DDzwGPrpLMAYpbgHfe9vDw8PDw8PDw8PDIDM9YeHh4eHh4eHh4eHhkhmcsPDw8PDw8PDw8PDwywzMWHh4eHh4eHh4eHh6Z4RkLDw8PDw8PDw8PD4/M8IyFh4eHh4eHh4eHh0dmeMbCw8PDw8PDw8PDwyMzPGPh4eHh4eHh4eHh4ZEZnrHw8PDw8PDw8PDw8MgMz1h4eHh4eHh4eHh4eGRGyzMWd9xxB+y+++6w6667wpVXXtno4Xh4eHh4eHh4eHi0JDqghdHd3Q3f+c534MEHH4ShQ4fC/vvvDyeffDJsscUWjR6ah4eHh4eHh4eHR0uhpTUWTz31FOy1116w7bbbwpAhQ+CEE06Ae++9t9HD8vDw8PDw8PDw8Gg5lJqxeOSRR+DEE0+EbbbZBiqVCvznP/9JlRk/fjzssMMOMHDgQDjooINCZiLCW2+9FTIVEfDz3LlzCxu/h4eHh4eHh4eHx4aCUjMWq1evhn322SdkHmT45z//GZo6nXvuufDss8+GZY8//nhYsGBB4WMtM95cugbOv/0VePi1hXDXS2/Dz+6YDAtXdmrrvDZ/JZx768vwzOylzsezaFUnXHjnZLjzxbelv788d3nYN/4bR2d3D/zu/tfgykdnQBAEsGzNerjov6/Cv597E/LEtAWr4LzbXoGnZi6R/v7g1AXh8527bG3t2gtzloX3MPmtFVA0cJw4Xhw3FQ+8Oh9+evtkeHt5/R4QOE9wvvz3pfq7Wru+By6+dypcO3FW+B3fRYRHX18Y9j1nyRooCj29AYx/cBpc+tA0mDqvOm+fnrUEbpw0B3559xRYua5LWffmZ96EX/x3CkxfuCq8T1wfWbB09Xq46K5X4T/P8QQY3T294T1c9vB06O2tP888gfsBzlvcH+JYsno9/PyuV+G2F95S1o3Wnu19/ukh+X3OX7EOLrhjMtz7yrzUbzc9XX2fKzTv0zWi+/zXs2/CxOmLw7k9c9Hq8Le7X67upQtWrmOvPd19RnNy+dqucG1d+dgsuOfNCnT19EKRwP7++L/XlXNy3vJ14Z5x/+T54X3gZ7wvBGXt6SDuMYtXdYZz8o4X03MS5yn+hvM2woRpi8L3MKvvXeG6xjMHz54IT86ovk9c+4h7XpkXvhP8jmO/8ek5iT0G9xecu/iZi/h5cMUjM+APD7wO67uLeZ/ifYq44ak34Nf3TIFVnd3adp59Y2l4D1PmrYA/PzwdLnngdXh9/sqwbVwb4vkue1fctYfv9avXPQO/vXdqOCdE4BrBd4V7Awc4n3EPwveJtE50vmM7uPZm9J0HunNPhnVdPfD/7nsNrn5sZuJc3NBQah8LNF3CPxUuvvhi+NKXvgSnn356+P2yyy6DO++8E66++mo466yzQk1HXEOBnw888EBle52dneFfhBUrqkRhV1dX+Fckov5c9PvFv0yCKfNXwTWP1xfE6wtWwpWf3U9Z52OXTYRla7vgrxNnw+sXHAcu8b0bn4eHXlsEADNh723eDVsPHZj4/QOXPBb+K/Z95aMz4Xf3vx5+3nqT/nD3K/Ph9herB/OeI4bAzlsOhjzw0csmwNI1XfCXCbNSz2J1Zzecfs2k2gZ+69cPCT9/aPzj0nsoYg587M8Tw39vfX4uPHX2e4zl8fD/wl+fDj8/M3sJ3PyVg2q/fffG5+CR1xfDlY/NhMe+/24YscnA8FD808Mzw9/vnzwPXnhzOXzr6F3go/uPhM9eVdUYPvLaQrjnzMPAFTq7emDeik7YfotBqd9umPQm/PqeqeHnX909tfbcIyxb3Qk//eDoVD1kQr530wvhZySeQjw2Ex75XnpOUvHj/7wEd75UnZOjtxoMOw6jzcnrnsDDvTr2zTfqgJP33cb5PiAegKdeXX1Xj7++CO78xqG1386+5QW4Z3JVOLPXVoNhu83Tz/z//vNSbe3tMWIwee39/cn6fW62UTt8ZL+6Rhlxxj+ehUmzlsJVj82EJ886CjYf3D+8/spbK+D7N78Yfl68ch1ceNJeUASQkLr1hSSz+d+X34Z/f/Vg+Orfng2/I6F1zan7s9bemdc/C0/MrN7nE2cdBVv03eerb9fn5OJV6+CQnTaHX97zGgC0w94TZsEXj9gJisK1T7wBv7kX+wYYNqgDPvSu6pyM8PW/PwPPvrEMrn68uhcgXnxzKZz9vt3hB7dU39XyNZ1w/onptWfCJQ9Mg0sfnhF+Hjl0AFw/6U2479UFtXU1arPqnJyzdA188/rnws8zF66CSz/1rpBo/NSVT4bXkOG54UsHwtf/Xn1XSERG597HL38i/BcJ4NvHHQJfue6Z8Du+kwi7DhsE79h2E/jn02/W9pahA9vh4weMZN1P/DyI0NEWwJcO35FUP8s+EN3nbS/MhSfPSs7J5+Ysg7P+9VL4ecXaLvjJ+/dQtvPhSyek7uG391XnR/xs/P5Nz8ODU/F8B9h76yGwzaYbgQ2unTATLn+kOgfufgXP/AHw0f2T+8XP7ngFbnqmSuPtNnwQ7DxsMAzo125s+5Zn54YMCSLaj/AeIkTnQeLcu/91+NMj1bkxatOBcPguaV/dSx+cDn/4X7XuNkP7w9F7DAdXyOss4PZPQSVoErYKTaH+/e9/w0knnRR+X79+PQwaNAhuvvnm2jXEqaeeCsuWLYNbb701dN7ec8894aGHHqo5b0+YMEHpvH3eeefB+eefn7r+j3/8I+yrWXHmRDn/+PtDukl1dOWyjufLe/TAXpsFpL5/8kw7LF9fCT+/a/NeeH5JXeH2iZ164JAR+Uxl3bNYtA7ggufSv+f5/Ezg9v3WGoBfviCvE2/ra3v2wB6bBvCdJ9qhJ6i+hzguPKAbznna/X2jgPCi59thwboKfGrnHjhoePI9X/xSO8xelR5PHLKxPPJ2BW6ZlT6EvrJHD4wW5iQV8ef16Z174EBhrCrg/c1bW72H3Yb2wrjR+Uozl3UCnPus+Z2fumsP7DcsfQ/xMpy196sX2mHumup97rJJL3xjr15lu99+RzfssHH18wNzK3DbG+2FrynV3nnG6B744+T0eKhrL17uzL26YadNqp8ffKsC/5ldb3f0pr0weVl1nxs+MIBz9k1LbfNCfE7uPrQXvi7MSdWzef+oHrhzTrZ39b0n26Grt9r3oSN6YcL8+l5/+m498K4tqvPt+cUVuOa1ZF+oCPjuk8kz5vIp+nf1jdHdcMnk9P2cuF0PHLNtAP/vpXaY1bfH7DAkgG/vzXsPsme1+YAAzt0v//epm5N3zWmDe95sY89ZGWTP9Uu798A7NrfbS2+c0QaPx977CSN74H2j1LQCor0SwMk79MIRW+n7/MPL7TB9pf7MiPD1PXtgd+HcO2xEL3xsp/QefdZT7bC2p1pmzJa98JlditUy5ok1a9bApz71KVi+fDlssknfhtWMGgsdFi1aBD09PTBixIjEdfw+ZUqVE+3o6IDf/va38J73vAd6e3vhBz/4gTYi1Nlnnx2aVsU1FqNGjYLjjjvO+CDz4A7vu+8+OPbYY6Ffv36Z2jpzotxhfezYsaQ6xx7/PujX7s5qLt72AQccAO/ZfUvl7/Ex/vrVR2D5+qqqfdjwEQBLFtZ+e+c794ax+5ulSKjGfnr2Uth9xMaw6SDac1WNBzF7yRq44LnHUr/r6uQ9B7h9o9nGL1+YIK0Tbwu1fSil+f6k+6FHosY/+phj4JynH5K2M2XeSvj2jS/C3ttuAr/88DtCQQEV+L4WPFHVCv1jejucf1pSA3TVnCcAVulNzmTPYdETb8Ats6p7RRxjxhwAR+6WnJNUxJ8XmmaO7dM8mPDH6Y/DvLVVs40th20JY8fu73wfiGPeinVw7rOPGN/5vvvuC2P33ipVP16GuvYQl82cCHPXrAw/4148duwYZbuHHnoovGvUpuHnNx+dCfDG69Lx5gnV3nnEYYfAHyc/Zb3u4+UOOvgQGLPDZuHn+RNmw39mVyWoiC2HDwdYVpX+Dho8CMaOPQKKQnxObjFsGIwdewDp2ey22+5w55xpmd7V2c88AF19pi94BsP8uYk5ecI7qnNywKsL4JrXnk/0hSZG333y/sQZc/mU51LjSb6Dg+GSyVWtbRy7774HjH33jnDNm08CrKqa5m662aYwdmxdq0uB7FkN2mgjGDv23aT6WfYB3Zx8/YFpcM+bM5S/q9qRAa1LcF+Pl9v/gP2tpfZP3DYZHp9fN3PeddfdYOx7d9aOCQn/m2e2w0Wf11sJXPfWUwArl5HGcfDBB8IhO20B33vqPujpqTIs22+/HYwdm9bE/d9z/4O1PVUGa+S228LYsXuDK+R1FlARWfBQ0LSMBRUf/OAHwz8KBgwYEP6JwJfYiBeZd9/UdruCNhiU0xg6Otq144j/1hFjbnohSZi2t+vbiXD5g9NC1Seadzz0vaOgrY1O4IrjCb93JJeQbAxZ31+WOUCpN6B/P1Kd6F2pnlh/8dnEvn/+2mdDf41pC1fDR8dsB4fuPAyoCARXMHF8lYqZ6ZXdU3ubvF57R4eTNWea20nUnyrOydQ8c7wP9O+XlJSq2qasK+raQ8QZSvysq4eCoeh37IMy3qIwaEDVdMnFuo8/vzZhTrbFnxfon5drtMXWFX6m349+vdL6ju8yyfba2+vzYuCA9DrprfSk1qFYRjTUEPfx9Luhz1syLNrJug+IddscrquOjn6p87Qj9q64qAhrAb9T28Jy6BOBpnK7jehTeyrmtgnRPoTrD0+jsD5hLBXGeDloFD3K6bPUzts6DBs2LFz08+fPT1zH71ttlZawedhD5jTVCLTHNi1bR8bInvKNJWtgmsKZjYNGGBLiofjKW8udvZfkIZ4P4sEC0OnTJfIfvR0KeKy5z1vX99AUdrcE9Otw92AK8tXPhKDgNxefd72aydpfokmnzO1meOYuUKSlu+492UBsjtM6+tm8/5JH4bj/9wj87Ym6T0hZz8dWQ9MyFv379w99Jh544IHaNTR3wu+HHFJ1oPVwg7Vd5WAs4uZYNpE58tioGzGKPz8yA97/h8fg5Esfd3JwuNo4/fZrjw2EzkmgKgGkoWyegB1MTacOZXVzjG8LnCG6uJ34ntST2nPr3/t1tBX+zIvc5zCK1Feuexpufd4uwlKRU8t9VyJnQe/hxbnLYcbCqhnfj//zcqZReMaCj1KbQq1atQqmTavbas6cOROef/552HzzzWG77bYL/SHQWRttKNH++3e/+10YojaKEuXhBmvWF+t8rEJHe32Bd/fZOtoQKVmAe1uj9xkMhxf5Lby5dC2MikXtsTkwufeT9f5dH3blJMvs5yTH/8QWjZ7DzQ7ZHLYlVss6f+MomveJ820pyXWgZ/DE8rJ12AzPHHHKZRNgzfoeuOeV+fBrdUBLJYImniO9glECp3mT4JGjgatNMUtGe0NEqRmLp59+OnS8jhA5ViMz8Ze//AU+/vGPw8KFC+EnP/kJzJs3D971rnfB3XffnXLo9mgNU6iOmM1ll7jrWMBmc0B1b1uJZfNFbHgqgjkvYnVD2MPLKrX2kEP2tmxfYdyEpEzToAgGlyIl1pnYyKTJFKIxxXw04F4p7xqZigidveXeV1yby4nt8W7F3Vi8EKbFGIujjjrKuDDOOOOM8M9jQzCFqjTeFArKDdd2rhyUiSgqAzgHUvzRObSy8ShSY2HZVjPY+3OIxm4HNxQn9MXmAifPnFezDK/IZn9ttMYikCQQxUSye22ziZGZS/tYNOYtcJjOMsyTMqBpfSw8ikNcatJIc5Gk83ZjlrB4IJVN0mwzGu4tqPbZRj2JlqDDg3LeT1EmhiY0iqjgEKa2e0EjhQE6xN88Z4guBFFx5lq358rfA7QkgpLPLVNXmJTz6IsfDhPg/vu5ueb2mO3nBVsfi0pJ9s5GwDMWHk1pCtVtGRUqK8p+aNlJtQLWRqnaLsvGZJlQptGWaSz5OmE3853mo7Eo68tPOG8X7JMXJ+Z0c8b2PbCFKdB4UBVBD05dEGaAx4zixTpv6zu755V5sGT1+vDzd258IdeoUC7R7m2h2PCMhQTjx4+H0aNHw5gxyeRNGyrKYgrVoTOFqhQjBS07XWRzT9QDy9R2Xo+m7M9cBVu77SLsvcvwTJv5vJatBSc+FiXiMmzfjwsNt855Ow4Z00FhXsv0nF3v06dfMwmemrkEzvhHPSlgVpCeqaEI13w5i4+Fy/0tWgcJDZ675lsSnrGQYNy4cTB58mSYNKma7XdDR56mUNYai4b5WJRrSxEPfyuNhaNduFG5EVoBRUvyyzaPm4ER0o0H4+bbPtNm8LHg3Nqazh7HPhYajQXxmoiUXKoJ9qQizFyztON6GpfHx6Ih3TY1PGPhYcTasoSbjYmxvCmUO1BvqWYKpdhpG2XmUtaN39YJu4jbaQYm0JXm5tHXF8JPb58Mc5asgbye3/qeXicai7KauHGIutUuTKHaaIyXPOyvuX3uXhW0uJmriznqev9Ptdegl7Ah+0q0ZFQoj3KgjKZQXcJpU9TSTzkSQmMhEl9FaCyUPhaQD5pNum7le5DrSBrfX6OADqOfveqp8PPj0xbBPd9+dy5zsrO7V5oFmtSWjnCG5kuQ58YUSu1jEf8qdewWx1px+1wbFYa3t5EaiwZo3lLO226bZ4wjKLUAq4zwGguPpjGFimfebpjGAsqNQvxGVFGhqL4arg+gsr8U5j0UcYCh6c6GgBVru2qfp85fmduc6+zusWaA44SzLhlc0YSNbXeuGQuOxgJNZG3yWOS5h7iS5NssWVe3RdKq5byvc56jqWSj/DU2FHjGwqNpokLFw802zMeiMfxMU8Q6z0uz0KwbeytIuFrgFnKBOCU7u7KYQpX/bQQFm87G105aS6x2dl/fTXsPIpGaxRztpqfnwAf/+BjcP3m+oi9oHGNRoP+cMbhHkFFjUaJzwMVYOrt7wpwerQjPWHg0jylUWxkS5JVod5OQGjYHZJEOfnmgFQj45LwqT1Qo0rOtbHjvTSTYQh8LaC0fi/gL4hCoqxuosQgZC0L7Yj3jeaK5/+/f/CK8+OZy+OK1T8urAjS9xoLGrDnqrNZeOcyO81iePb0BnPC7R+GwX/wPbn/hLWg1eMbCo2lMoeI+Ft0OEuTZbBhlsyBxYYfaW3Ifi2aFLd1cBMFdNgY5L8lrXncp7gNVjUW5xtg4jYXrcLNqHwu5Ez3BFApojIVsLVaayRTKmY9FUDiDXKS5mnYcOazQR19fCDMWrQ4/f+N6d2GBywLPWHhIMbBfW8IBMjdU7MLNdvU2KiqU+pBzgUWrOuHSh6bBi28uK2w87GRRJYsK1Qoo+tH5V5UVgcTHwrKlkr4Mex8Ltwny0g7aoDeFIjxfsc2eHN+BK2GU1YkXuHnHlHsIHAtMUnksGCvM5euM2koG48jWQWd3yW2qM8IzFh5SDOzXXj6NRUyM1aizWNxgXRAF8Ta+ef1z8Ku7p8IH//h4eEia64oXGieRyeudlJPsyk/zUEi42QL6aGWknbftfSzi2tdWeC8uCOlEHotehuZI8h5kcT5SZRga8KBhIV9t6rja24MGmELl2z55HJaFK5qNvImtQEnwjIWHUWJUFsYicBwC0EVc8Kx73fNzlsEhF/0Pvnzt0+HmPWH64tpvK9Z1ZR5fPhoLKBVKNpwYGOFmCz40G2XXb9tt2YT6gUwCaTnGPKXljQg36wJxUyhdiO+Ur0uosRC0ERKKXCyjCgZSaz7DpufOedvijMv4e71vSl/5mkI1CmXVKJYZPo+Fh3Ex5WkKxdkqy7C+XUtRPnH5RFjX1QvzJq+Dx6ctbsh4xDoYirRNkt3NdLZy+357+VoYsfFAaV8NRYPnWSE+FuR7rDR14qm89gwxXK+MoLVtqyxwZ/iRNY8FPTwv+liIg5Uy0ZQyyU6NY86hammiQtE84k1j4XYpmsC5m4WclvKY+wG0NrzGQoLx48fD6NGjYcyYMbChInBsM0vuV7N5lMHhNHXIScbE2QCRqYiweHVnQxL2pSR8ivHXbU1p7aj7A/jrhFmhpuYzVz0JrYqyaXaScLeWSn2bhWkseqwJyPJqLBr3ZnXhZrWJCrvSvi4yjYV4qUuRF4nrvC3b+xtpCmX0e9D8Nn/FOvjzw9Nh2oKVJA2n8wR5qbOWU9chE1LO5VlqeMZCgnHjxsHkyZNh0qRJsKEivpiKNIXihBbMCiuzoZQUpbEbUcrFwkG4WdtQvpyuz73tlfBfNP1auJLPUCX6ta2X83viaeOClpDk54Wg7D4WGBXKsq34eivteyl4YHGmJrXHxT9LNBaU/SxlCuUgymDYDoGJsUVPwa/ti399Gi767xQ49v89QlRYODaFEr83bG0EDTcNbDZ4xsKjoaZQun7z74tfRzwkZJKcIvccF7G+UxFSDKEXlVGhLPqW9Z9qt0l3cVuJbzHhZj2yICUpD52G7Z5qo3Ly2JhCvfLW8syCgKw+FnGIj47sYyFc6nYUZVAWcKOxmbcNe6vmt5fmLq+WCWg+WfnnsWjMOmnS46eh8IyFhxTxtdTVEyhVxXn2W3ScbFodisYicEKIUujLwEWCPOG7Kau50hSqQTtwWU1xWBoLKBbUeVJuc67GISUpzxButrSMhSChvfvlt+H9f3gM3vObh2ClRWAJZwnyAoPztqixIAh/8IxzAdk56SzcrJ2dqxNQtgvn4WZFU6gGLZNyrs5ywzMWHqTVVJQ5VFB2UygHG6wKNjSck3CzohbGFCFF1Q6/a4/SO2877pcViz67iVBeUk6X4WbLmnlbNMn76t+eDf9d1dkN/5w0J9e+tQnyYu9UfHKyDOiy/Uy81m0QnFHfkDREeM6MxeJVnfCnh6aHEQa5XVO3GorQyHUQgixHm8uRlHR5lho+KpSHFOJaQnOooRv1K6DfwOq3iibh3G/umQq7DB8CXzxip2R7DojwRvtYiG/KrmvhkFUcEGaTJcgFQdEEfKV4BqHow6sMh6UxypiDPvJSBkhNoSxHHF9vZQhQIQPFvMglEnksdM7bgdkkjeIcb9LSJsfGS3yWt/P2t298AR55bWH4ecoF70v27Upj4aYZ59YB6rq8tl2V9ajCMxYepMWUl8ZCNP+RhRaMytis7x//+2W4+5V54ef9t98ss6QwHYVJpma3NYUyXchHXSweWCqiITAMi7oBi9WN1TaAfT0+Zwpx3mY8VJTmTpyxGN657aYwdFD+woUILs7zvIiCtMaix3qeljXcbHwaFk1bJXwsejkmaWnNUS/Jx6IJTKEU1yOmIhKkufKxSPTtwMeCH27WfEVd192ErZ178WslXbJlgTeF8pBCXDdFhZzVEco2G3TEVCCeeyOpKnZhsiobk+2mY0NQBjmYd6nDzcq2WPVYNoSwmDqUdFisOYq38Jt7X4PPXvUUfPTPEwqW3mXvqygtGkaFsiUg47RoEUwlFY0kpJI+FjotttkUiRIVqktmwkRAysejp1zO266YGpoJsGNTqAxCM5fz1TMRfHjGwoO0mIqKDKXPbJ3vxmWlsXC466QIUQspkQuH9J4eW40Fv+9qu625c1sfhCXzsbjs4enhv6/NXwVL1+TrtNssB7q4D1Rt+4MWy2PRuL71CfIM74GgjUjlsTBQ4KpXJItKZSpjC0o7opDFpA2jvuJGRIUS++RYGDhlLFiakjr0+U6gpeEZCw9iCNJi+pWZQql+c9Bb9vERylCR4iso43HAevUSQy+amChOgjwPNYqg56zN9aA4uJgnuR3gsjwW1hqL3tIz2UHBTIcuQV5gMoUSRisNCZ4639xoLGSmUA1NkFegxiLvIASc5p2OJSi3ZryM8IyFB83EpiAWO23aEx+D277sNmpRYwG5HSQk9bOGESP3QziI432pw82yu25psDQWeQ6kRO/KVotju6byInbSzttZws1CKRE3yyragZVqCpXSWEhNodL1xBZNCfKUWlqC83beUaHiqOQ0/0kai7zDzXLqZvzdtqxHFZ6x8CCtpqL8C9OmRurfuBA3NiuzIcqVwNH4CHUenLoALr53KixZvd6+b4LpAKXt/OzZm3NrZx1escJFSMZsn2iRQjsXxGxuCgtRY5EhQV5pw8021BTKztxTnnlb5vfgJo+FTGMiQnV2cueLzRBdzS1KFDCpkC1D/yltfIM0FiVdnqWGjwrlQTSxyUnyZ9AAxDeIwPEhaLPppe0+Je1K+qERixX2+H5199Tw3ynzVsLlnzvACbOkkt5FV1W3YruZt+rGzZtfQbGmUMSxNVL978YUKp/JJc34bG0Kpa7YyKWhe/V5O5nrNBaJPBZSUyizoEScF7aZt1NO4JK905kG26JOscn5ArdRoVIai8aogJtVsNVIeI2FBOPHj4fRo0fDmDFjYENFahPITRpN76ccplD679VrdpIWG41FhHsnz++rk51ZUhE6UTkVUWHvvF0yBI3WWEDuoM59CmGeF/ORMIUq2dyS5U+wRXy96e6zkRqEopn/pI8Ffe8KTdJEpkFK7PM0Fqr7p2gsXD07isYirZV303kjnLez0CB5ayw8s6GHZywkGDduHEyePBkmTZoEGyoK4itSDevUn1kXcyYJiKoNmWOg4bttX0XVUYebzWczN0UuKZqoKdJHpnGgDc71LbCSBjoJN5uTplX4XiVo7drKO9mcEx+LggmpOLOqPROEYVUTFVI0FkIZhqNL0vck+dv6nnT0RHd+DsXUkbdjbsj1NNY57ZuQg+92iQJBlx+esfAgHchFOUFqiX/XQwjy2exsHapdbFyBgzpqQifIxRTKhNx8N4hSyAw9WJUsxhTKrlzWZ2NpHVZMfxnazZJ5uxnCzZY2QR6YTdJkTIP4rmwT5IntcDQWXE0fzXm70jgfC9fh4MXvjHsx3TcviEQ512eZ4RkLD+KizqmfoDjn7VTfLtqQchZ2/aSzkNtoVPh1xOeq9LHou1xxLLHKqglxTYi7mmb2Tvxlct62lxpmReBC6+doLKaWM4WbtXQcbmUk81io56A8KpSZaRCZFVmYWBWemrUExv7+Ufj7k7MlGguzdsQWvQF/X3B1ZlKacU4jWJ6j3LKNQQCtDM9YeEiR1hw0ph8XxEUEkV6z2XQpCfJso1nEpXTVdviwekYG5q5+3dAM8Ua52rCit2BX/fFU98Xepb3Gorhxuugqr+GmnLfDBHnQUhqLOIoeYdJ5WxyMWostT5BH0FgomLuKovzkt1fAOf9+WaoxMfVlC4rgJkskpTyiQmVBlntxuU+Vf3WWD56x8CAhP1MoOiGTu3OYRZ2AVCYoLhyuTR2iWYDpPqw1FqbfC97ZXR1K1hoLyB9kJtDwPU848bHILZpd8ntnV9pp2JWPUaOQ8HMo3HubFolPZooUEIhiCvNRbV8P8bnk6bxNYiyCfPx3aD4WMiFbY4SbTqerpK0mkAU0FJ6x8EhBeojktJDSam612jtnTSsJOlMtVbu2m5AdUcSvkz6MVJm39aY61s7bxnoFS/ML7a0xfdKjQum/5wkXfVlGEbVIkNd6Gov4Km/kCMW5Gj8zKGF/pVGhUoyFQphiuHGxmsykypnzNqFM0MAEedLxZOg+fY7SG3PJq9fWOlPiU9mAvb09Y+GRgpxYLkZjkbKrjH3XbW42i9hms0xLcGWHlt2zEh3vitJYiJuwytw4aNDhU7zGwlE7HE+GeNECDiTq2NLmCMW9jERPtnOrII0FEpS2j8bWcbiheSzynqNxrYROYyE1STNrYKmmUMb5I2FsDEXq15kThqaxUD+rorUlfVcz9KkWMha57vMxwapAK8MzFh4pyJZGcc7bmsEE+naemLEYjr34Ybj4vteInVPGpyesnGosHPhY2OXmCIgaC33j1poZQ8Xi6S43HZZUEF2FrcaimG77+g5K7GMhI3osNXaJPBYlnTRFM/exDnuZztsUbYSMIZGOw8hXCP3LIlA5eqcUH/+8fKJso0Jl0ljoaIEiNRa5zP0AWhmesfBIQbYZ5UfciYeG+rtOCoFD/sTlT8DrC1bBHx54HZauXk/ombBZajQoSrvSFDEW2MkwbJgEG1MocGMWYGvPa6pVdAx9dxoLTp9BYVmNw/4sy7k8ZE3Rr5rp6MXn0np5LOooeoRJTbX4W/xMACNjIROUBMR3wBWm5OljQWlHvI8i81i4tqBO8xX01ozvLcM4PMzwjIUHUWORz/IyEe7x75whrF7fnbomkjGU9sQiqUNOWses1aDARQI/O41FoB2P6zwWpTOFctWOrUlcEaZQZI2FnvHPFfG1b9tETsOVSYZtibi8AmM0s/O2zgQ28V34TZZPRJZVOx1iu9cuEp7wXaqxAChMY6F9Vhn2GoqvEkXIxgLBOoBYNRMoSXA9kvCMhUcKBfpuG6PO6NTeunqUXAA2NqupTUb2rAjMBy2PBbGi4zpq6V31X9WjtVXDZw0365oQbwSdV3SXLpnA/LJbZ2+3sMSeGcZbWh8Lh23hHOHMk4SmWiNskjlPU/aztI+Mfs9TQZ5HQ18mT+dt8VZdJYojaSyk1+zvPUULBA3ysdggvCLcwjMWHim4dEg29mUgRuMbmm4IeSWTMzM+5kPL9tkFJTOFii6rTHV0YSF1/Zk1FgVLS2EDCDdLLUcgVHITOjhouKixuTKFKqnyItNzXLGuCz74x8fhhN8/CksIJqpmjUW8XJrBA9J+FhDDzQasUMGyqFCu3qmdIMxN35TIZXKhg32ftibFrqPB5bEmg5Kuc1fwjIVH7huEti+D2VBC661th0+cBQ4IK7l2x3zYyfuyq2caD7eOKq4+18laVZ6dIK/gTdidjwVLxFYo6HksCIRKAWO3NifMafKkTSJRIm/bVjmpjLgmMMsQL773NXhp7nKYMm8l/OzOyaQ6AdG8RyakEMcqM3NKl9ELU1SQhbvNTWMR8BkAd5m3Sacl4QqjzwwmxW5XVDnXZ5nhGQuP0kn+kv0SpXmB2TwmsNJYpAYktCGpQygj74s/PmObhDZS9sZKHwtg+VgoNRbM2yrcedtVO7YaCwNXjITS3GVroREaC3lb9BtlmcOUWmORJnps52lZnbddrcHX5q+sfZ781gpaf0RNdfrRhSyecT9LmVCpRNx9navGQDGFKlJjwXXeppqRqkKQ52kmKb6SoEHMekn5/lLDMxYSjB8/HkaPHg1jxoyBDRHFaizohLxuk8rL0TmtsRC/E6Q0ZCqOWE7XhAVTE1AJncCNo69MyqhvF1o+8zZHq/XhP02Aw37xP7hx0hzboZE7pMynvEy+XLyF4vYtXl9tlWZgLNwY5bUlnMAhuymUJjwv/kTzsQicaCzEtlVhaxuVS8KVNpgyR7mO7iZwz4lMnRGaSgYzyNZmpcUdNjxjIcG4ceNg8uTJMGnSJNgQISMIc3OCNJgWxb9qFRaE4aWiQpHGJ9YRiWd7AtuVCVXWNqhRoaI5UCGbQqn6k7erHh/kgqBEEipquFk0J3nxzeXh5x/c8qJ1f9T1bGT8C9IK2ErMgyI5CwbiRErCxwJaD3EiymbeUSLxJZzEgWAKJZYxRMJrHudtNRMmQ+A03KyMgdP/zmuPXt843oI0p6s7u+Gax2fChGmLnLXZDOho9AA8yge530BBfWvGotdYJEHJBWBj5tGbpylURmJF7edgyBdAkPBRhkPxP7Fhfnqb1Xk7B2mWSrLKRXqOBtJIaqSEkAUzfkW3IUM6aZu9j4WjV1paH4v4vCIzFrFyOh8LWfK8lDaCEBVKFW424PpYMJy3gxw0FmmNuqEC8X24iAqVdZoHjUqQZznyClTgN/dOhWsenxV+f+Lso2GroQP72mxteI2FRwoUKbyzvgzSa+rGZDM8m6Q/NlIU6tBM2hFaGxYaC4JNcnx8qlC+ModWXTv1em6kalyoncvzbV9atsFMD5X4kZv+FbM3NKoNabuSfmx9TVSZ7kuVIC/gh/OO0G7BoAREc1SZEEfsgmIKpQo3G1Wl+lhIo0JBefNYBC4zb5uEDswHobNeaLSPBdXM95o+pgIxYfqiDUZj4RkLD7ZKs2FRoTRj0GXsVtehjE89HtWYzMyIXV95tZHWWOhDLypNoYiOg+wxFrwJO2MsLOsVkyCPSIAQDnfO8+I9k5hQwfJhFhYmO4NEVWeWX3So5Txgp7Gg/SjT0Ka0ETLGQvjuKtxsnqZQlGbE8XAzh2cpZ6IbuM8hTRsEjWEsZNeCjBpAaP51rYNnLDxoC6kwqWRglyBPI9VS16FIYfQMS2/Jw83aaGVUhI459CLxIKCWi343PAmK2RthGLH+HIFDcBPL2t5rqj/Ddyrjr6ublYFyQRvk5Red3gfSs5QcbrmkeSxcPbu4ozp5L9T8ltBYSPfawFm42bqW1jyW/KNC8ZO+cvdsrt9doo7hGvcxcHxrUnUdLvzoWSRNA7O1H5RonecBz1h4pFCoHXWQl7SfT1CTNupUCDy9lIbaT7Utu3r6VswQDxelaYZhQNRDjUrUmtrJCrWpliMJoz3JnUObdhoLE+OvKOQELlrNSyAi87WiJidLMfIJCbx6vMUHkokzPEHDfCzSo9IwYrI8FlKNhcB8WObuSUWFkjIW5nt+cOoCuOzh6bCqs1vdF+m80q9rqvDKhYBK7I87hbIk+8vbb8nmTKrEVnBZc9e4gnfe9kijSMZC+K5zjqaqx1VlRbtgEtFhkJpQJLjkqFABhWkxSPctmBqxiCmPhYrCSW2WAVVdbxhfQUxt7XrO7cv7LPagoc4Tm/nuCom1b92Io8FQzD6IXYvXyxoVSvf8K5YaCxfuJNzM2zJpO5Uw5Er810soWlNXby5dA6dfM6n2+Wcn7S0tRxmyKZcQ/kxxyuf6alTHp7937h7HFUAlyro0hZI0ZcMYVFo8xGwcXmPhUa5ws5rtQ/+bTd/Z7TwDyqZGHpyZ4DYS4anvfM5CzN4q9l3JKgUXvpsdDHOae+wfHLUvKxsU7GNBLZeSXJrLuEL8vdvuP3kR6jJTC6q2Na0hpEpPioWrkSTzWNgJWZSZtyXPXKwqc8zmjoNKgK/v7lG2ISJ6Kk/MWFK79rcn3lCOkSKFN81B6vnAji6lus8sU5u4nmQwmW6x9mYDw2SDoDzLPBd4xsIjhUKlkoa+E6EFtU6O6nrUOrR23UkurcbDbINm7iUcYMoIKQEzKpSbMRavsXDTYZkPD+p6oQRFKMRBOijXO5AzWGahg2xMZdVY6Ah4+zwWtDraNRgnViU/pZ+vfaQmcx4Lc7hZa6Y4cJF5Wy8YU92eeN+0BHmys9B+bmdZC07DzRLWOrtNKNNKdw/PWHikIJ3yBXEWOqJTq7GwsMe0i81t7seGuJf1ZaOCtQs3C6xws7aHmqo/s/N2PqCGw3XdvrxssaA4ZcuuZxY6MAo74Cty07TK2qUmiOTkZWgkqHtv0VGhtHksJL4u0v2Muicz9zy5doTWV6qeVR6LwOATqC9fK0cQoKXqGNrJmiCPUz2PcLO68MsUVBxm7i47PGPhkYJsA8jNFMogEU0QFzQhlvQ7pY60jMlmVb6dEsrI+jK3zTaFstDcmBgCqimUmljlMT9FhQytXYcGIHEAF9AdTXBJ87GwHG+FkyTNUgSZ16OUrRGRyKSu+zjhWyaCg7r3ckyhyBoLLWOhLhcmKgRzxCcXTuSy33EOpOYBUcBiGqONOZIpaSlVoEB6b4HbOUTNi5R7VChZ+xkXam+J1nke8IyFRwpmUtlhX4aNLhFVgtUOZRfmFyFJcImbd7ovM2Fu2tBkmWi5/dpGSKGYicnbzfa7azjTWDDaSb6D/G+Y7JyZmk8SZtft0OTDsK2Xm8bC3BdZY1FSUyhXzy7uvE29Q2pocbEUPkqdqZmsDf049L/L2hYjQ9nPXd5YSIyERnCna4cWbjZwq7EgalVlUPkJuhO08tupGNpsJXjGosVhM4FdSiWNfRn6SUo8NIcNZN+EKWVMG7dsLIFjKbK+EX2beOiZtDAqH4uaSpgYFSrIqCGplzc8CUtnZ7Uk0c1kd0VQFAGq9ka+N+Qz4CRRYtkGQMNMoVRIEb5x4UmJ6A0dAW+rsaAQqKb+kr4f6U0zLSjJ7mOhJsDT10Q/C/u5K+zThDqpvdxSY2ETFUrmA5nJx8JSQBeOxeE6kjVFGUtFcy6VaJnnAs9YtDhsNjW5VDJ/4kHWD5W4SBM3FAkLZYD6OpRNx2UuDnMEJfWFl+cuhwN/fj+8/w+PJaVqonTKkuCm2phzEwjmtQlTzQCcd2Ao2ghTKKUTZ4bDvQxRofKaPDJTC4rQQVYu6bwdlCZcpS5fBGcsNs7btj4Q4ZhTggtZPeKebCgmm5eixqJMztsiXEXuk9XJuq+ZaIMsplCssTDmj67dSkJlAS0Nz1i0OAJHlYpKUpZemHYSD5IlFKVMqo5547ZV4dJC2ZraUI/lC3+dBMvWdMHkt1fATc/MUfajkirWNBYKFQE13Gw6lCGPYXEFVbO23WXJnB4vqw9SYDEwaX98iaSsnssx5dFufs7b5r6ooTwTjEWJCI7kWIJiw81q+jM6bxPazyuPhVRjAVAYY2H0CaSeRb3Z+w6vKT7boGGmUH0jTzhfW7RT2YAS5HnGosVhZQolb8jFcIzNpgn5+Gc6wUUZLUUCYpTYSqUZ4Aayto1V1EzN/BWdtc+LV62XlqEkyHNtCmW6qfzs5BXtOpIwWgvZCzhzqOuF4jeQV/Qr6tqntuES8qRrRIGClrEoD8Hhaig20XR05RKmUGI9ch/Um+MLPdIJQC3nrtA3LfO20IaB2aWag9J8LGQX7ed2FkGNU+dtghCB3Sa0Njxj0eKwWV9y4iEfBAziNHAsgSVJYQztUiS4Lq04zP4I5jZMbao2Za5mQdUOJTdCEVCbalm2l2qfQXBriKU8QHW0p/CAtvNblQ9F1pctnZDbviUzUyRmldbZr5eIr3A2rmRUKFpDAZnhFH+TxYXSt6GD6Z1mMYkDY5Q9/XebvZx6PtgE4jDRDWxTKOMFNfJOumrlvF2BUq7zPOAZCwnGjx8Po0ePhjFjxkCzw2YBSTNv52aPYtj4Yt91Q7Ai5i02S0o/toSz7JA0lUn3bW7DJEWUOTtS+raNUW86vHMztXHcn01ELpf98/oQiVvaWIqUqCeZLUupb0NNoRRjguZIkKcT6nAea1ssLJRuD586byW8+OayvvYDYrjZ9DOnEeDmMtX29AUpEadMbZDnCZhhClErtqk2V01el+T9IzLb9muYaloog2m8nPkrF6ZkW6lBqVa6e3jGQoJx48bB5MmTYdKkSdDsCJpMYyFeSSxAnSmU+F1SVBSQUu7JGBXKFZMjZUhkbRsOKSJxk5CeKA5Lvuo6+d2FdLL6ezFMbf2yLRFLal5et+BDJ7VeFH2mCRFzGVfIIu2UteESMhtuqpRXRvxFZUtle60h4DnjTDpvy+vNWLgKjv/dI/DBPz4Oz8xeQibFZfs+SaZkKeyhRQcLrDRZxnaC7HswPdeQvh1pHXCssciwn7r025PSQxbLtKLpH9/LxfdOhbNueRGWr+2CZodnLDxSoBDLzvoybB4JtTenHcLBQdHCmFTHthsuqS+LDc1GsihL8iQfT6A1YTGp3VX9tYrztjOisAjaMs1ZyIul7kkiCpDUvfHpOXDghffDlY/OcDJGa1Oo3DQWgTVTLxuSas01Eq4Y3HgeC1WLF975au3zd258Qe9jkQhoZ34PmUyhrBgLoS/LMaSJezAinZxPPxb12PiMpOm84s4m6lhlyBzCPNFv37nHaN8cFSpZ/55X5sMf/jcNbpg0B37x3/paaFZ4xqLFYaexkGzWboZj7EtHGOvuxSYSk9U9WRD/tqENbZKRmZz1IiQiXCict1USM7UkVvweOJFEFcXUZu3PdIhT6wYFMC9p3yFiPdl8l4z4Bze/CAtWdsLPYgSj7YEefbNBkXOHHG5Z8oPLKDaukDA5ytAOJSpU/GqFsQZsNWhkYY+F83ZaK2C7oZj7So9HzxBQ9147AVqQs8aC3gDXF5FbNutyDYTvE6Yvqn2+6ek3odnhGYsWh5WPBWGzdIXUBqaJw03Nxir7TqlDKUMyhSKYNEn7IoyPvWGSGKxALj0ljEc3NqU0LNWu4Z4MN1GxZpbNRA4H1IgrxnZ08xzcIC1VJTKBhDKuQBUqaNvQ/DZ94Sr42GUT4cI7J7PbpTjtKrVAkmu25occPDN7KTw4ZQHZXy7h40IUGpidt819hUIP4hqwPau4wh6qWZs4vmoZUleSdsT9tEIwheX27dLHwjCHMuosOM9RHG8WUz5ZycxRoQIoTb6aPOAZixaHM+uMgiR/gSspA2ETo0m2DMSyVBVvLCLvy4EK30ZqLtZRmkL1tabWRNAIa66Ts4u5x5E62RJ3rky29GvATSdUAsQ2JwsFpvMzyCHJWBxf+uvT8NSsJXDFozPhuTeWOvCxEOc1nRiN1lxeZn/TFqyEj/xpApz+l0lwzyvzSHWSBDyf0OT4WOj61oeblT1LQvuO9mRK2GFbcYBsLpjCvhozZovrvtedtsCllkA6BlZd/b7FGgqBeaShYh05rNngGYsWh818pSR9c4WAQZRzCC6aNiLISWNhviLty9B39VqQuQ1qwi4VUalqk24KwhujC2KaIlnMCps5KG/H7rc8tCsUAiM3bWZ87Vt2oSN2ZixaXfv8xpI11mNTXaPO/ziBl9c+e/kjMxJmahTo9l6W83aMoKIT9MRxyZ4lZV8nPmcrHwtmcjl1uFk1A0pleFN+b2DH/FLyWJjfFa+NNFNEr5/yNTG1rUGQgxAwSDE69QuqBLTNBM9YtDhsJH3yzdrNeNJ90Tc+ToI8uXOl/ru0XVM/0nCDlgSmxXhMfdscspGPhYqoDBybQrmWdFH61LVrT8SKfVoexow+bEEmggn956bNjK996zaI5ZgdUKIBUddJnCDMQEtpYdOOzhSKQyTGnbeVfaV8LHR7fVCY87apmEziz5VGU/dJXfLS+njE/d8N82sTkSp9fpvbSNbVf9fB5O/HGUtUNql5I9SDgByJMTGe5ucrPGPR6rA6UBy1YwXNxqeVABi+28bFNobrsxyLDBRpEveQCiyS9ahNoaLyKimX+J12ahmZJcgOjhbOVc4Ee40FjajKAhPD7DKgQCM1FmRbeuZdkKIBEbVA8dwxulEULckMMoY+leWxoEIXojXetzyfCKH9wM38cakdSffN789IUKfGRuvb2nk7voaNLejHwNkDzE7r9MY4zCuVkQo076UF+AroaPQAPPLDktXr4a1la9n1OI6uWWGSjMS/6qVY+u+ytm2kMBSCzHZDpN2DurHPXPlk6JDKl94FPFMoRTtUqVD6GRagsaBwgBn7M80VcjuWv7H6IDLZFIdklvSPXlRr499oyOz4yRo7nSlUie4zPhSj7b4GNoSSrvXbXngL1qzvhm8ds5s8mavFnqcu5yCPhUNTKFFjYdJQm/zeqAnyKM9Uxgwm17CxiWRdC0FgbSyGuqw8FrXx8OoHDEFJ/HsrOG97xqJFsXhVJxz+ywdhbRclX2cSUhosp/POJKXXqeN19aSeDlYSZWFDMthuyq6qI+7wpUm6IT82bZGkD77ds8qRtHYfijZ1Eb10YzJrYbJPPrmaXtWfHZw55AUFhJslvgPKfLd9P8YDtG9Q67t77Zk0S6beXD6wz+MiczjuK5zfPhsDkXCJjzMllMnLNhYRBoXSt3//qwvgkdcWwUcPGJn6zYboU5cLLBhMoQ3LlxpY5KkwCc9SbRKFPxQfCxPdwN0ndIR59nwe9MZkAjVK7UDzsHX0Siv4WHjGokXxxwenWTEVail8PgeJicChLmbTBlu9xpeAmDUWMgZG/51aTi7ZdP8eApVZhkLyppZygRVhZRMrngu5kyWN8aEifVTYEhRaDtoJqJJL0RmUoqFzNSb89rcnZsNP75gcMhc2yCuTtc5PwqwFklwrMEEelWzRJaKzkfhqy1jc9/qeXlIeCetBGUyyqBoLVxpQmcbCpKEwmgQ5zfodsJhTc4Ni+4yqhrqcbOiBLBoZRSsG6meoq+41Fh6lxbou88rBxSHPomwmll1Bzben+2VFcpAyFuYypvGZneEksgiiVIhLcFNAukcFU6a6D+phRGWojJu84SYoGzEnKpTtc89Lm5Dsw0kXZFtrikO6q61BRgz8+NaXGfXT+1leGguSbX9Anye1gAm5eazwEdj4T0F2YUjovE2uYibspWOimkIZhR4EwZLD/cSkQRQl9ab1q/YDEtpxoLHgLjJuWHJ9VCj7dVULWsKlRQKxHeVPxfmwFgTvvL0Bg0r4hdcKGkN6o9OsRnkpRTsy9WiQeXwyPwTqwWKUCkmlccYhk/rWxZavaSx65W2p5w1tM6cSteTfLRhEXT1XJkwuJZX1LvJhXmpMo4GgkNIXtqYeqXUi/k5v943Fa+C9v304THgX125Q2+TegZygpBFCsiHkncfCBvH7yeK8bXNL1Hku1+q6G5NpCjqNCii2A3yNhdEMK8UgKvq2YSQNRbiPIct+YBJy2UXLDDIKJQNl/fj3FlBYeMZiQ4by4CvQpMCUgEvH5esqyukf/obPDYUaMPqxiTjFfQ+q4vHNSyzS09PHQKQoZROjpP9ev64/HEWYbpnyREQmqVpPQWRaznV32oSgeI2FRCpX7c/MjDviw8iEuQzf+udzMHPR6jDhHZpPqdqkZH62zrxN9bGQSaIVz98VsrZrFgCpQYsopO+P0zbNeZvaPv93myAhMtCEY7y9NfWcqbmNLLVAXGI8WZdXXjcWHZFPHUdSY0GpFyjHoOtfbkXSXPCMxQYMqnOt6poLmKT7ATUMp004SIvxGTdqiSkUuW2CRiWr1IcyEJUjqYn4oRCh0kEZD+/snAUndK/tXCfffxaNhaOFqDr0UkwmiWHn9Gseg027z89ZJk12Z2JeVOVMsA1zqupLFYktD1AJF9264yRM405ZHJ/tHhpeM9bBp0zuwYGPhbt9wPTcTYw0lUFM+Qc0xHmbxqjnnsci0tQn6tswy4Gxj1aBZyw2YKjpvuwErS10xHbAuRfpwa+X5mTVqETdmsqox5Nuy1THBMoGqMq8rdqM1fej/15rJ1XPcFhqf6Vtynmbbsjq2fIArHluCdUcNWrbpIScJeFkGlOGtuptmhkjdmdUgpJoApkgGEtEX+jmGkWCna1vWvtyYtZEfNP3A5vM21nmsa4duQ+F/gwxzUnV2ERzsj4lNnu8SWLc3IauPc4+w42eRRmHzpRJV48kVIl9bX59hWcsWhYUoZTaFIR6MTtMhExAlaARCAgr523DhiQjvtPRIGjPmSJVYW/OpDKBwpFU3hbV4U/VO9fkxaiwCNzYxHPak/ZBCkVcEh8LxRw1hRW1StpIfM5Ux1ITdMkf82LMOH3pCMaiTE5pdTS/MZqzyStBbV7OpBHqUBkXQzlZX6bcEVneWTqPhTiegGeep5qjxDPMNN54Na5JmEnwoIM5/w69sfq5p2lfU4+dhbwFOAvPWGzA4Bx8zmy7GYyE+LtuMVLsm7PYcNfrBObfTXtaplCAvFGrnplV5u2Adz9qHwvaGGu/W5gjpNqQzQfL/qiwpxGDxmkshL5Fht2UBIvSF4eRzgrd4Z4FMpMUahQbXajafNgKO+jWVdYIfTqEWxO1jvSsMu3RHMaFN4fjdWr9Wb5U+TmsJ/iNplCG9tCkcMW6LqOZrgzS0L9xYpp9dtmf104T5En2xiAjrdNLpBGaFZ6x8CCaQuVDbJk2OiqXTwkvl5Zi8AnS9Pf05pFmjuT9mLUf5kPLjMCcIA9UjIX8EFMfskLPxMM4qw+FzSZfvaYqC1awyZPC7yNfgts432VtMd4Pxy/RBaNB1iIw3xWNoAzoBKOjPBazF6+G8257BR6XJMtkQzMWVrhZi5uiE/6ydW1gLOgKC+M4pE78RAGLTd8pjUWqb/1YTIK9k8Y/Dh/4w2PG6FPyARv2WOZzMJ21OogCkCzhkqO9Id4GxQohEMeU+E39HlpAYeEZiw0ZHMLKmaTU0C5VVSsiJVkNzGprWoZW9ca8YOW61AZcLSO2oWjbNGYH74FSPh1uto/IUbVJtB3H+1vd2R1G69GNyXh4EwiFrNI0zniofbiUVNZ/c7MQqdoCSohmjkZJp4FMzwvG4U9kYk31qYn4SAk4Gfurac1R8Zt7X4O/TJgF37z+ueS7ixMu5MzbapjMjajtKOtQ35vkmmlfx3lFnxemuW0ek3qfoa8b5XpMnWn6M4WyxjD4wUtzlwv9aofa15b+GnceWJICisSetLUpHUdgeTOB2I58H6wWDVoqKpRPkCfB+PHjw7+eHrvM1c0CntTJTZ/mXAdqiYxWNU8hgCwkyiqH5NtfeAu+ecNzUuKIGs0ipW4mqH7ZztuK64m9i2wKJb9e/z2dGffYix+Gt5avk7YvtqtCRoVGXxkzA0juUFnN/P5I7RSyDuXfbeakUaOkmi+ptW5/+Cv7Jo8F4Nbn58IPb3kRPvDObeA3H91H366FpLzeV6Dcv7L6WLy9bG347+LV66Grpxfa29pTZahkizaKDUfiSyiakNhWGBG2CAxeJo2FoVxPwZm3b3pmLuy3wxYwoKOdZOJju7dSzlNSWwlimnl2cSVQmrpiVZt1Fq+TNfBLoKnXAnyF11jIMG7cOJg8eTJMmjQJmg33vjIPTvnTBLj75XnGshy6ypVZhzn8nfq7bi1TbBZNkSJkUEk6vnF9mqmo9htIrss7SkmXSH4i5jFzy6eJLzmREzDzU9z7yvwUUyFtNyvjQLhJimSR3B+jDxvoD2BHnaRalb9zUkABYtup66a1Dw5AFCZgsTNveB7WdfXCzc+8CcvXdmV2GlZrUTQEasablmlQbaFriudjwR8T9byhmKTJ23czDlqCPHcihn899xZc+ejMev8GQYDpPVHzulCigJnCo7PPLsN3HUzRs2yYlHgVWh4LELrUMFn5bOsNg2csWgxfvu4ZeHr2Uliyer2xLDWGdd9FJzCFu0t/jy1GzSBSG59UkqX/LoONpCNNqMvLmcZD8hMxjoVAdCvsdtX3rmKUkt/Xdck1fq6karXf9T8r++CYqtBgoJTtWtE+u+/e+AL8v/teyywNVEU7TTtrmwmp9O+069R1wwG1TW7cfln4TaqPjex51U2hst10nKDK+vxcJWq0GQdZoyCta56P1PUSzX9O6GBxzVizFYqKlz44LVZGP2/N5pm0vZzyvqXCm8D+Oag05hSYzIrtEuTRaBEbIWn8awsoLLwp1IYM1dKwkUqS+zQ0pJOQajUWBGczG+darmBBdmipn7N+zE4IKwvmKboHlbZGzSjpD7V6O7RyqnbT7enrq8swGGsCXEX60N2P+Nstz74Z/rvvdpvCUbsPt+6jbuYWsH2XjIwfkZi3MVU0wVYwaDJHIJlaEgUK4TWV+SHzGcQ1FgnfFlYrhP2WReQR9tm4jTlU6HsfgcGT/U5nXOTvpd6WeUiUiErUthE7bjlY3ZeBgCab6Vrcg5TJi13NquUKnGbeprcVFU04b5N8TgJln7p13Qo+Fl5j0ULgSy0V1yXXbDdH7iB0hIBuCBQzJzv1o574kZWWEeo2knvpRs18DbZEd5W5EK5pxkVl7mTXs84smq+MhNFUHA62DJ1OQsVqR/ub/NdnZi9l9aGSSKYPYMK6cmAKdf/k+fCXx2cZ+6IgHvGMmgWZ2xcpKpSydvoXU1JKKnpikzrISLhwBDnadiA/2JhCVfdo2qhqwhTl70Hm/U31OlRj3G34xsq+jCHMiXPUJPSi1BH748xl1ZlEhTmUO4fJsTTnC9R9ZtHGNAO8xqKFgPbBLAT5SAu0XRo2tjRRHpBGYZUgTzvSqH/993R5SR4LFTGjkWio6rkyhYoTF6pM66p7V92PyNzJYv3bmUKZftf+rCzjPiqUOAftWtLdr7tws/KVJ0aZoRzIZi2kvH706bX5K+GL1z4NeYBMSBnqUZhSUySaWl3JZVUoZy69EddYZCVWdPUpmZjr7bjt29S2WcPJ0Fgw9zxZWWuNnuq68gtBCy60RU3eqtrLTbCdgbJ6vMzb7kIAy/wmbUyiewMaLdL8+gqvsWgprOzUOxwCdVORXcuJoDFx7gFxDJQoFiZnaWm7TElHQPAjUbWteh9r1ncb23JNdIfmAgptTdb7oTipJ37X/0y0/6UTxLZz3VbazIuw42YhKplG4WmTNIGmNWGgkO588W1WPVYujFSbtHomwoEUDUjVtmTCqrPd8953/H0lGDobiavmtyxmLXKH5+T7pbZulQmeQVjWTKFY80D8ru+MqtmVXRf7MpkuUpkemwhtJi1D1n2Op/EwrEUmY0xlyMR6qgt6UyhoenjGooWwal2dAKUgcCgFIvfJJNTjP+sPOv132TXKHXH9Aapq0/Q1jtN0vN49b1Zg35/9Dy7676tW74F2IMiJJNV9qBgyqvqcK9knaYkMkDLLyrJBrtJxYz1NRdVv3LMofVDK26cQy7aMYdb3TgHZ/FGyzrnhVqnvUXZZFeKZKyjujqsSMj4//TxkEInCd1qEIVrbOn8VXdv0qFP68ZiIaQooU3KTgR1yYt2g9TaeMYq+TQyLvI5kTUg0lBTY7DOckOa8BHl8LVQVmmfP2PObEZ6xaCGs7uTl3VAfDvnNcg0Tb7QZ1avmzVumzeZgGq+sApV5ohBxd81pDzf5Pz88gzxmSt+aNBa1flQMgPowsjOFyuq87VwCZtkdh0HQdacjepxl806tBfm7pfoo6MqI5lWKIaR/BzvoJH5U8zeTuQyNoKUx1vFrOkaIIslMRIXKOFe0UfgYHA91X6D2nSgnKWYkgll5LBTvRXMvbM0lYSwYnEHWvkkjYfa5sDubpHUMF3kaB0oZdSHTfXPPFKuIkoFQJ7E2hbKJb82vsvCMxQZsCkWRlNSvuSFoTCpKHXGmjatOMNngxviWj0dfJ21ApH7QphCX8r547yHIYCakfDcBUcqlICjTtuj68cmIPk592dg4RCYVpoRvKnAIEUo0Elqfye+B0lyQEPXMRLxYPmcX2hnbbNihxoLRblSH0pes4WitcJhMc1QoeZcVJ3ks6GMSn4PcL0H/Xd22+T1Ahmdq2PJI0QdNfgwUwVN7wicuXka/XtN92+1JpKhQ0j1WNxZNWxkFg6b5xFpVGrPg6NlgMspUNVB/1+2hrWAK5Z23N2RTKCXBS7tmAw4jIZbXaiyEje/X90yFKx6tSvkj2PifUf0l6mMk2HcqxkMx1eLeg6p4fPNSMWEyYkvXJj0CD4/Bk82RxPgJs5PDo8nGPWPhKrjrpbfhxH22ge23qId7jIMosDaOTXtg0po095kyoQjkRKBEa2U6pE1rvH69r0/iGG1gS1BU57+6tM4B29y2OpqTnclFuh1ZW1zCRU+48YlzqkYBA0tQW5cua0P7+PypUa1MplCU8MumnlRDic+n9rYYYxG7blqvJmZXzfTo25HWCfcG9RnAmcomJsXUnmzfMrWvHIvmna5Y1wUnXvJYaC3yr68dqmf6AvnneHuIFuArPGPRSljVyfWxkEhbegPppuvMxyKVPEhPhFM3JnF8MxatTpchaQTEAejbkBWnElYuNCjm8griIl5GVk9G5ESmUET1uYqA4EvdTXMELO1/Fb1JfvjQHx+HlZ3dcO3E2fDUOceQ+6CA43OSl/N21KUxt4ROk0UyZ0x1SR+jBXT7ie56NXhBxozLyjEFDI1FhqhQkBU8xkpd1o30W16Of1aFZm605msDUSbIk80DBdOuYvAoAj6VICi19gxnHHXvpGiZZHV0QpL0b4Ey9LGUsZDUt8uHxVsZsvuK2r/kgddh9uI14efv3/xCsh6ox6tTJHmNhUepsJrLWAiTe+HKTvjYnyfCTAlR7oieSW/QBqIk8U0zBspBRwlJK4J70Mtss6kZeCmSa364WUIZRT+pyzXiM6uPBY/QSJvjYHm5aQCLsVARmZJryFQgFqzsdC4dZ2ksVL8xTyPlvDYQFHKNnGEeK34zTWX1rdLvlfpOZASXbnyy+UQ18dNpO7ihmFNjSJhC8esmJePqsqy2CUSqKvyxCZQgHbLhUIcftaUShkijQhlCNqfHYz4f4qZQycSHeibGpLGgMDWydlXj1b/F9Prg5PAwOapTs5/ztf4yU6jqv28tW1e7Nmvxavo+Duo9NJ6Hp1nhfSyaGGie8Y8n34Dla7sSxA8V4sQ/7/ZXpExFWNZ+mNo+VcSrrLxuc6NIVLhhAGVljNKw2P+NmzeHqTK0pRxPYOnY3Kt+V+RoT0TCysisMbRayjYkRZREpuVkN/keuDC3c+W8rZp7FPO8gNiW6vd4W6xBEpHUxtmtRdxPdM+aFnXOTDDWy8p/y6KxiFc2PcoX31wGB/38fvjUFU/UGP2s+62t83aFFQ7W3F+WqFCmchQfCxNU9xq/3KZg+Ezr1Zh5W6WJYZ570bh0wjKO4IWksdCuT/X4bSIrqp6rjiEINGPSDcFrLDwaBtycx/7h0TAp3pMzF8PvP7GvRbjZ5Ox+ff5KdVnNSli7vgf6d7QlpF3qPvXt6mQeuu3AhkmgEd3677Ly1A3UyOhICRAuqUFIkCclugPl5kxlGKgaC8oz5XyntKGrZx1ulth+Ft8MGz8hGVQEt1kCavaxSPWVOFDp1K6LW6USI5T7TLYrWZtUhko6F1VMSPrz7MWrYciADthiyABDVCgVkvb6a9b3wOeufgqWremCRasWw72T58H73rE1m/CjlpVJ+dNrmkj4E9qS722k5muSb2WyQ4IjutH0S/nuVRoLeZnqb+J3w9gUQxP3bpqPhUwTqiamRc2z2Jbpmu496/LvsIVzxLUtMgSBlsni0yLNBK+xaFIsW7O+lmn71uffcmIKpT9I5ddfnrscDrzwfjj24odhXZc53C3X/Ie6IdBsQEUmBjIzQunyEtaIKOGnSPJdaSySvivpQtU4/vK2VEMQ71PpY8GUhukOK914dHV09Ww3eVPMeBXMzHW8rKM8Foq5ZtI+YEGTFkCneVN9pozR5l6peSxka5G731BNYFS+bbI6yecVwFMzl8CRv34IDvvl/2Dxqs7UvSYYCyM9G8BHL5sI+11wX8hURFi4ar2xfpYIP0aNBSdBnqSgmZCnL/LausgQbc44z1XXYz8kZXYBw9TJQMBSGQsHPhbUBJK2DGMc6eHGn1l2jUU9DDF9XwoUn8Xn3QIKC89YNCtkmzPbFIpTVlH4C3+dFPaLztJolsXtk/M9a1x1ua1+DswIjZYxRlHiSNmVYyFclx7OshB7hkZNyZlU5bgEpo10Ux69xZKDIPZhq7FwRdDpoJJ0BkZbZZkkP7ASDlAky1lBX4tiPb0pFC3ztmL+9+oIWPXDw49fvu7p8DMKlS59aLpByhyrKxnHEzOWwNOzl0Jnt9yBgOMcq0N6XzDXoTavEoro6/CZftX9usi8rWZA5aZQyfbV7zyLKVQ6uhRlj9WvmdTZrt3nzC1w9sks22Z4T6l3mm5X9P3q0a1HRXuydpoRnrFoUnRJVh4/3CxDgqD4df6KzoQWxdyn/rtuQ9DHVadtfLq+s26GUQWTg7qqLRkRJ8JG2mK6rgqZqCLgs9rlcqNzmbVErqNCBYXmsSBOF+NvHKgklyatE4XZpdo3156P4nmTGTPNxkBdv1yNhZwZp/UVMJyEA+FzXCu9XmAIEv4VhLli0nBrCTdGZDexnW5J5fR7or18lVBEB5OZm6x99T5KYDANe7Apz4Uu3KzJ/NT0nlRj64lncKdmSzcIfdJMvm6BufaxUP9mhExL2/edI+wMguzBCpoF3seiSdElkTKtXu8mj4UMJJtUAqdtIr60mwdbwqEvQ7p/A9EkK07dAMVyJCJO27usD0XfhvFJo1vVDkJ5X6lDjkjEmSXX8nFQ66vKUKNCUWPe24yLormyatSA9LHWd1BqCIOonlGDpOkryPDeZdfP/teL8N+X5ynbsGV2jeFmZYwq8d3o6ur2Q5HZF7fbNCFjOVdqY1GDQmiqxmHaQzmmUDbvAX+mEpd1AhIYmbfFNlRty/fZejv1H9qUPhb6vk1nClUTQ2IkDcIbTi4JiumqbltOrQWN+ZgJAcFJnruv91q010zwjEWTYr0k02PWBHm6DdkRPZOWzpqIEi2Xn9XHwlyHYxda7SOLlNTcFzsGt+p68sGm8MKby+Hp2UsUxKeKURIPIwURx3wPxvCmhPfIsdlNS1jtGAsqOHPMmfO2eLD1AsxatBoeeX2hUZ1vFA5oTA5FQpkzRrG9+SvWwfVPzWG1oSYQeQny5AQljXDSaX30Qonkj6IYJ6WxSDzrwCJBXuDIxyIJydFlrqQqJinn0hRKZvJi7IsoKMDw7vtecJ/y3Fb5WAQsc1rD3qq4rjPj0bWlFToYzn7Tb5x7042Dn8dCnfjPlQN5r8X6LDM8Y9GkENXgdj4WgdOylPVgbEVDbOsWsV3oWGMVtj2/PPsobTwUiTeXeKUQN7Jn96Vrn1bWCbJKh5nSGhOjRnkmKnMveX/8Q1VWjh5uljYuXZvcw0gc68rOLjjud4+k9hWZls+ssdDN69h6NoxRqenr+xejGfGfbWDMWF0tZzBJcxxuVuXjktJYgB0hE0eFOGdcMbgp8zq5xzO572TbsrEZ9miWKVRUkPMuzWOMEHeaT7UN/MzbrvJY2DAWJqFDam4z/DHk9e3WAltjIXXeTo8hpUEMdOOlrdVmhWcsmhSd3elDNXNUKEZZW+LGKGFh1OVqLFILncJYiN+LDDeb2nz0znE84oxO4NX7T/4LBkmk6p1wQ+2ZTAtI71GmsSCaqJE1FtA8GgsR/31pnlRYIRub6YDXETMyyaHyllSMX2Deh95evhYuvvc1eOi1haS664XNwWSHT8lfwJFy1yXj6jbEaqKTp+i7kNX5XbffcqS+lDDUYj4A2xwwsv6y5bHQt0nRXFkHXIhVi5tC6YRC/DwW1Nwj4CDzNm19YE6VCdMXS9un1De+Ay5jQRQ2ha8o/m561XW8xsKjlJBF8uDnsTBdiP1E2BwpGSNNRHfasZc0vNxMobj2wbJNSP3s9IeCWA3vcX23xa4ou6wg9nSgSAIp5VWh+0RMnL4YfvTvl5RJGzngaCxShJDKWSQDg6AtqCPoqG2auiS/q/Sk1BHQUrMBCyaWW1bEN69/DibNWkouL9XUZGUsgJ9VWEdwmNapTe4BHbiMlbId4TvVEZjUtnRdU/Z1XvtKTZeGSay1QetK205cY6F79lyNhepBdAs3RnqmBqED5blgAJgP/vFxefsMSSjnGZkQFld0HW9KpH96tFoT9W+tkHnbMxZNitRB2BvAaoJpgH1UKDcw7WvpjYl2sJYlKpTU6Vk5HsP4hPLH/+4RmL6QR2RTWBryIW44YKimZmnHOjk+ecUT8nFYSAQ5eSzEX2RRbCjjojNs1HHx7YOV7QAxmaEkSlF6flcvLFrVCZ+58klYJWhOVRFZKGtJ95vu+FUxFaomuwSRLI5TT5zYh7PWMbnG/VEzJh0xKKtlfP6a30h+EpJxVOu6I/wpDJ6sbW4CvoCjCbXcB1LtxHpN+FgkGPlsFgHKPVp8Z5bMmk4wKHt2GAJZ2X7qO319ZlBYSBMq1ueQWtPQq3kXpvfW7PCMRYtoLLgRoRCcqUyZ9xQVntER1yRhUYAUpcEqjwWvH5M6ODkewyEg1OMyFbq+ddITXVv6kL/i92xO3spxSMaV9dDTXacQQrJ6dHML+v27On9S96iao7I5mT7hQ5x32yswZd5KSV/yA9XstK/+TfdKdHHgVX2mzIhMUaEkA0hF4FGNQUYM97WnY5pFQ0hjVChF/2RTi8ANg0tZTwmJbYWzdtIw+2zRfSxMztume6l+t1u08WrJqFCBWpgjCUKgG4s61wp/j5b5F8bfkE1OKF0F+6hQzPNGYTonO+v1Y9AxeGoGpRnh81i0iMZClBJSwImyQFmMJOdtA9GtIhpNmzNl46Me/LoRkcynCFekhLjoo+CAiiTlnCB3o4+Ukz7kAofvIRsBL5+/KsbHzsfCKpwxs57qXrnqc+q7ktkqq+y0X567XNpGYGAylAyecvRuiDWdoKaax4K3H1Il1VImN9avqh/TLadDi2bbP1yZk3Azb4d1iM3b5bHgaGkDnqlgFpNITTsJ521N2yZGQ3W+ihAFDdRAJ7r2aX3T55ytn4+NdoAQbyC1A/do9gOdwLIF+ArPWLQKY8F13EZw1ld2EpfWrkqqYdrYaKpa2sGfhVisq0jNdUwmPVxJvgykA4H4dk0HMpUgT2uOSN3HKuSssbAghKrt2REUKc0VQ8VvC7EZ1T3K7IS5ElmVSY7pVvQMlh1U9bpE5+1eUx4L2jX5GNTEaJpQjNdjSkgND7uoqFAmKXo2xkLG4JnrkCO9RWMP7OeBfcJNlfN2QDbvtYk8KIuSRvVh1LZPeC6cNc95qqbw6qa6qkhu2jXSqz7fdUJdn3nbo2EQJWwcm1erherMFIr7ve/ANUqhzAOkmupQxqMsz8ipaZI2OdFYKNpIHE7EuWMK00iPiqNnqIzjUEjMdZBKmDMySOk+9O1QoT1cXTEWQjuquSbTKHIPeBVxkWVN408256+qz+6Uj4XJFEvNHMR6k9eVrLdaVd3aEH4TtVQpHwvIBnd5LMyMevxKNbgOkZm3GJuJQaP2E/ZliHBVHQ+vL1mnVB8Lc5JV2t4rziWyj0VqvwjIZs+k9i3rx4uyw81qBFM6hqBHI2gKctrXywLPWDQp1gvhZm3Ue5wqJFMoi8zbJlOj2nlrGh+BOLZRUXMPCenmSiawhd8diKcpxDPnENdL080EhNi3DWwOmIDDdAnfbTUWVNKOE1RAtQ75RLadKZQ8WRS9J93hmqqnYywsyWZVLdF52xTaWfa8qIylbi6KvyV9LPSg5rGgQledszdRws0mUMFws7S2ZeVIezQRJsGWjODOQgAn6sXeQJsiKpRR681cq+p6BMbCwDyYzLJU1+q/6WkF7dgszrt4Xfk8S16sMDTzgeY9Nb++wjMWUowfPx5Gjx4NY8aMgWbJvG3FWDAkCHlJSo2bcEDr30ZjQdlfZBJbbXnGBmjSoLjQWKgGnCDwGIe4lugl+ohk11iYr6RKZNBYkBkLQzvqsdHG5RJUjQXFttjI9Me1Y8kTvu8fOtGWmVgj57HQ35g8spO47lV11UyJzkbedM+iA7ro+E31xwkI/bH2Jov1RG3dJipU+DSI4685bxt+p5rAcBB/ne1xU6hYmTQjyvNnUw1NfEdUvxjdXkYPw65uX9m4sW58D2J122eFIGcgE1dNwRSC+HpMIlG0BTgLz1hIMG7cOJg8eTJMmjQJyorOrnSmWC54Z0PgxhTKMIb03hGQNmebPBa0MKX6NmR9BM40FlCI8zbnENczn7SDw1aKpu7HXEdrfiJetww3a01AMKSDLrRYLEd7CZGUnt+BVmOZlNQlzTkfeW0hrO6Uh8l2KXmO1ZJe7Uo5b5t8LMwErXJ8UmatelH1bGXVjFGhAnfR+0Rw9qaUKZRUym9HdMqKmfdovj8MNcJdtSytbXPf8neWcERmRhZMn7cqpt4i6qJkzajWfrVvHkxMFLUuOwqhYr6I0cVEE74eDVOnW+etAB9utknhQmORNg3SEDSE5imRaUx92qqRKfdvYypgQ8SSBSuGzYUqKdeORUU8JzZ8ej+cOUI1heJnE9d/p9TR1iPeR6qa5dzl3L2r40ccm+oWZfNffF93vPg2LFn9hDKRocoMYe6ytfC5q59SjlFHAFQPe/7TcJXHIsiUx0JNjOrei9ieuNuKvkBZMm+bHm2WzNsmwk5IYGxoO13StF4DZsCKsI6iuCmLeLWu3XtI2O+Hf9gyxuJVM5tmUyjxd5rzts2Yq+NTj1XOFOoaF78yzq0MG2c4XxRrVndPvZqQvTrrhLijfrPCMxZNCmmm2Dw1Fq6kMAbiS+UHYeqfQvylbKBJJjS8One99DZMW7DKisDIx3lbcZ1QRtaWrqw4fpXTs3hfbNW0Yc5QxlZtR8H4gK3zth1hx2Fena1DaoK81PyXU30Tpi9W9sUJmaqqJ0LnAaE7luk+Fry5Lrumqi67ro4K5V5joapvGmNyXIYCFvuCq3luat6keZWNQ1Vctm7SJq60vlJ9xz6jNjBiuJLMpjge3pxU+o5YDFqa+yVwKVDizW9xbJzzYrNB/WDpmq5aP7o1G39HgaXzdm/svTU/W+FNoZoW6bjr+RGhfb8a69MYbdrhmz5w3ZtC0bQPotRBX/43974G/3n+LXPDBNOnPJ23dVIvdVu8EJxiZJF6OfsDIhoHtz4ntKErzRFd28bRCsl/4x5GVOZMFmUmy6zk1NU99irhb0P80Hws8J3rWqc67f723qnwoT8+Bi+9uVz/rCMCVsd0G243pbHQEHUmGE1Ps2gsZM+OPrRkPdmjNIytqu0i95D4x6Z/60ADcY1FpX6+JrQAJo2EIbS3amQ2CovA2H7GfZ84dhl0zJgM//jSwYmeVOeHzry6hxH+OftJXy54xqJFGIugBBoLJwnyFESnWQpl7tvk0yBDVimLrA31eATixgmzKG8jfpUT010bqYdIkNtkQE9W4NeX03LmZ7OuqyelHVTBNjGWaU3YtMm2e1earaWJpEwEAaOu1hTKds9T+VhITKH0c938/t9Yshou+d80eOHN5fCJyyeS6ur2Q9PtiuYr6mSKZpieLS/zdrLsirVdcOGdk+Gqx2ZCVujMytTjCf9PbN/kixA4OWdkiDeT8LGIO/Qb+ja+J8XPVL8ysW+dGVjWcNxicVsTXlqEy3g/NAYy1CgFxDwWmufUApZQ3hSqWSESO1bSO4YEgbsY1X3yxhB9zSOPBeVwSUt2jVXoBKxBauHGeVtxnSnBqbbFc2hVHU7ZNRb67/I+Je0oKkbXX5u/Ek750wRYsY6WfNI28R+HIXHm5Cc0o2JiZVI3rnZUGRWKUU+e5Ezxo87ER1EnzVhkl/K/vXxd7fPq9T0kh990eOtATcgIG66oIcyi8DQJUDhti3Pol3dPhVV9CV13H7ExHL7rMKFvzjhlY9O3wGGOo2euul+5iaX+OxXxamgIhVLgHuG6MQBI6oyh7TUKZbN+vCG3L/anF1Kk2tC8fRuLA9lACNGOE/6iKg2X2H/KNDEIrJJGUnxVyw6vsWhSdKbyWOSssSCUoSyIVJ8G4jqqYJbWURgL/saUlpKY69AJWOFQ6M1DYwHOokKFkhsNs0O1Lc4abjY9MEoRMwEgNnfmDc9LmYq3l6+FGQtXGYdB97EQv+sOV/l1rpSLGhUqLWXk6+wSxAWjno64qdIwFsIUFWPhQFAjjpfqtxIfl06Qwd0DE9oOBmNt+o3PICa/R0wF4pHXF5L6U8EmKpNKAq133pZXkGufUqVonWnaDtNYVMzv1RjqNjW/FHPUxsfCcF5SzlKOtpYzZ5K3o6+IztMJjYVCpGby1enVMA+6/FitoLHwjEXLOG/zNwKO2pTSPE1joZeY2Dpvk0LHBjZ10s8oc/ztWv/68biRTisORIt+TMQc9TCSOgQzYNL0UPqs1lMRC9XrMxelmQfE4b98EN7724fh2TeWCn3Y3RhHY+HCl0rWhzKPhURjwZ//cUKIQ5Dq9yM7O/CAnMeCrUlLaez08yvZX2BkulMSUgCYs2RNrb10VKhsPizaugzCUx/di0ldEopS5hh1HakSF+raMfk5UJGQYvc5b1fbV/dvCoxBJc7tnLf1e7Pl9qipb2sKpS8b4+Fq/aoYSN0YejSCTI4wqRnhGYsmhZsEeXQ4I2gMC0qpRg5y0FhoyqJU7ZOXPwHfv/nFVB12FCPieKjhWll9K5rQbXLqtgwSGuZhze2fYp+qAkqkz7/9FfjJrS+HfhO6fk2tRe/lG/94TlvR+u1JKqJd+tWPzYQX3lxm22qyC5Gp7aWvGe77ikt9OVW179VwqKvbBLqPBbP9lCmgGKKnVk4zPt06FX798yMz4IhfPQjn3z7ZqLGQ9qVj3ByaQnHzBbHmiEV/IaHIbJ9DgNtGh1P1HaFC0DabhFNU4tzu7NH7WKTntoQp4/TGKMw57+KO8vqAA/rzsEcT/jn93qCl4H0sWkVjYSO9c0UIOexTtfmYDgvKwuSEY/x/970GE2ekQ2ia4tvLQE8UJ0qfWN3I+1Zej29yVIbAJE2njUlmXpP3vL124qzQiRYxbMgA+ObRu6rrBbbmiOn3+cpby+E390yF4/baCj554HbWDO/5d0yB2158G1whIBISOqkbpzeMjnTdE7NZ9XXEjY0TeYT7J8+HV99eQXDe5rUr8hFKjYVuDogMn0ZjEeEvE2bBeR/cSx8VKtZw5JuRSWPhSPNk07epbdNZyA03y01YmdaM0/pKt6OKCiUvUx0Pj8kJXDpvh8E9ktd0Y7VZv9hGNH9tGVDTfBRNulXCFKUQVAFdTpr481YlGm0meMaiRaJCdVnpW+lLk7IJUBaEcWNTEFkumJ6UWYem7IsKyTAOjyvNCXJKJEXqW81ZmMtIqmgPWWJDttGTVOUp3UZMRZRrBBkLVcc2Ekb0ubh38vzU9Q9fOiFcqw9OXQgn7rMNDBnQYcVouWQqZH0oba0lEk62himAMDoSF+Zws+wmQ9Ohr/39WVIei6wCBFW4ZZ0pVFqSSR9DOioUZNBYmPriMBbkon19Z2NazM7bHGGKSUtrvuZC2R/a/ROYTVNkPioxbBukJGCcM3aMRYzBsmRujWsqbD/pvK1iLFxpTXodBGopEzxj0SIaix4L8TZnoWfJ4qrtM9WPvLwLG8SskvKwjs2GqNy89cSdE+dtgo8Fx4Qpi5RTddhlIeDQ3MTWTI8arUs9jrrZ3Am/fzQdAloQAKxa1y1lLNLRUvjgSrnSUlUaEVzVWvFGaMsfm8LNKt+7pr/Hpy+SXu9K+VjwdzxqIjid9FNnHmmalzqNhXy88usUpiqrViFTgwaCzNSUiVlItG/Q0lLCzdqenclIQTFSV0egGs709PkaOPaxUN972r8y3YfRfE/x2Ty45Dh1CJ811XnbUm8SaJ538+srPGPRMowFNTtwHJwaFI7aLo+FnsiMvrlgazhRoZQ0SxCwCX7V5pNWW4u/O2AsVExNrG3eIasuTM+HwSOARETlf37Xq6GJk0jQc9uhXlfhwSkLpGMQ21HR/nlIOE17BzWPhcxun/++shNWsjZVv+p6W9dFzUli70ui0iDUy8mIFPP+yE0SajYfVTOTZqYkcGPSpmGybNsmaSyowhRFcrTa75LfXJj8iPXCvaNilr6nnbfT6zfxXdG3FWNhEFa60VhgpT5TKEYDyaSCdO1QX2WF8zbtWUrzj2jeWwtYQnnGolkh2nfb2UTSJSsUztxFVCjVwerCedwU05sCijRPVkc+Hv2m70ZjYb5O7SUwFKabQqnHQgU+q8sfmWFR09wvd17YELi6kq60gzI89voi+OrfnkmE/OSERg2JB2aftvNYR9tMW7BKk4BRXTFy3Df3zb9TssZCdq2vri4MpVFjIWhddESdbLwRwmYCd+/UVnBAgeweTGOrat0caSxkPhap/gIHjEUsKhSjryKjQplyepi0JxQk7p3RQGIchp6r/iwxUyjNvbHGAHGGMLugr8zwjEWraCxsTKE4ZR1N9pTqtjeAecvXwVZDB+rH5KB/F2EAQ+dtRz4WgUni6ERjoSBuLDiL+cvXwT0SPwK+87ZeqmYCFhcJYw5qzn+GZ5N1zssys+alsaASL5+56knpdarGU2buYIJtdDPdvPj45U8of9P1RtVu2WlmgLQn6wiwtGlLjOAw9K+LQsMRAFW1QWbiPK+oUBzotD+63sjjNwiSaFGhwKkplC5poimEuS44QFYLCJlkX2sKZbHRJRkEOuK3ww83K18NHAbVlJMmzgy3AF/hw822SrhZK1MoBkFD2QNIZYTv/315Hhx80QNw3cRZ0jZUTo02cGH7WtVY8OvIr+sJbEW0SidISk9oNzRvxTr4v/+8nL3vlNiMWR8CWL62K/s4mNe55cjtOGEsIBOoggl+ENZ8GAsddNWojIVJUi2vI65fAlMv1OWYkojQR4Uyjzd+3WX4S310r75/A4dtEzQWZL8yAxMif5f671TEqyWiQmnmhMncl5p52zZ8vS44C0VjYdorOCZNyXHQz7tQO5TwsVCYvIWclLwPGZLPQr1XtEJOC89YNCk6BVthVcx0HWwTzKjLENpRLJr/u/UVaT+upMdcybBuw+VHhQqsxlNYHgsoFi40FivW2mss6v2q2+eNR9UQdRwO3nPG+tSoclz1f5b7y4Mh6WSYQmW9TzHSFCVbdJbcEqmoUAamRJ27xC1x4yrvD7VtU38cptHcloTgTBH32TkLNISSJ8gTxmPws6EKMeyct9NrJv41JU+yeHcckyblOEgaC3NUKK5Jb6DzT/SmUB5lQKdwaHVZqi41X5O/EZqnbKDGxacYkxsfC5q0RgeV9EJbJ1Z8yer18N0bn4fNBvWHfbfbVDs+Fz4WJDOEgncyjhO9DFjchcZC3bGb52ErHbR5H1nXB7V6dZvh9WVjplnviw8VQY9Yx9JY8AYgEupqxkJyra8vbY+G4aQzb+uloDqNhcsdQTc3ZVZZrDCiNs7bDBsWm3CzLjSQYb3YINtQYxEbk6yMrC/zXquYA7bO2+K1BHOb+jXdL+Nh8TQWunEIEBLkye7LJipUoGH042s3Tx+7ouAZiyYELoy0j4WNxsJtWRfmUqqIGi6WWnrDt5PK8KNC1fGbe6eGOQ0QL79Vz60gjyqT/a5VQ+XYbrtGVid6fAe5mkIxiA797/JnvmzN+jCvw67Dh8AnMGkeU/JF6Ssv4LviTkt7k6bAOWNB1VjY+FiI9ymaq9bLyfpL/isdk6H/dFQofV2dNtPlfNJG96r9G9hFnNJof3R9UvccE4OZR7jZKAlcrzIqVHJ8uvGYSHnVo7YxrZZruuLMbbp8qg2TKVTi3jlEPf28w6hQYl1VIkRX5li98fE1P1/BZyxmzpwJjz76KMyePRvWrFkDW265Jey7775wyCGHwMCBVQdcj3whO7BcRHHQS2bM7dMWOo8Yq0ny8tBYaJpU+TdiHfajjnWEoUkjvDZ/lXZ8eZrI6NTpRcPGVGKFC8ZCRVgJ/7pqP7rP82+fDP9+bm74ee+RQ904b1uPkt8Rd3y2vkK2jLVOQ0L2sehFBirIxlgo+lLFxJe1kahnGE/ax0JOyER0k1Jj0asPscoFV/rNee/SqEwEhp8TuEM3HLnGJN0fB1ge31GyXjwqVJwI1Z8ZJmGaOjKYndBNx7hQmBozU2hHgAeM+Rj3Z4nqyhlznkSoN/4sRIYwobHYgBiLv//97/D73/8enn76aRgxYgRss802sNFGG8GSJUtg+vTpIVPx6U9/Gn74wx/C9ttvn++oN3DIDiwXztu6KU3SRlj1qW+DIsmjIh01hT4OV1GhBksSpFE2G1vUwljqpGgNFpHYdL9inT1jITugk+OhDYg77GjeREwFYtLMJbDz8CFCu3aHehGoHrK8vmxCYWcyhep14WNh40sCJMZCq7HQtB9Y7G8vvbkcbnx6DrwYyzwfkaiqx2TjuG77HqVmYSwpsKw/A2PBDEGga4+S+ZvtQ1b7N8kM1qNCmYUXprZV37OaQumuac+gqF+mFpg8NoYgrZogL8lZSH1pmGdAkPgszpH452DDYCxQI9G/f3847bTT4JZbboFRo0Ylfu/s7ISJEyfCDTfcAAcccABceuml8NGPfjSvMW/wkDIWDkyhsqjhXYUVVCdtIgzA1HdqE65eQLOaoRv1o7Vh6WwcQZZ5WRyP6rsNAoIUrdHbmI3PihNTKEW3rp6Hyf45ei8uNEa6R7hyXRcM7NcO/dqzx+pwYSKUd72ubhc+FtmZO5V2ROfvoN2D2XksAjjxj49ZBJXgv2MdSNru2KPK2xSKq3ULNFNG2pdmryX1F1aoJOq1qaJCpfYY+7Mpq6DyzhfflrSvlsTbMYX6+up68XHoK8ZzhtTKK8bKStIXxMaQRYLQKozFL37xCzj++OOVvw8YMACOOuqo8O/CCy+EWbOqoUM98kGnM42F3aJQlzG3w7GhjPfrwqEpZX8aAFz60DT41d1T4dMHbQcXnry3sQ0cPjsqVKxfDmPhJNxsoJIWqfstGjaysTwZC1c+BCpGVrzmIlOvagwvz10OH/vzRNh0o35w33eOhKwICjSFsp2XOg0J3cciu8aC57yd/qQuw48KxYuKZhNUWDcuAmMRGyzHDMc68za5B75ZULx/FALyTeqq/8b3hWRUKDWByomqJLblAiJzoWMEpIyFkT6gMwiJduPMGEljkSwvq4LLjfP0AuIYWoCvoIWb1TEVIrbYYgvYf//9s4zJw0pjYUOe6b8nfisoKpSqXxf7nywqFDIViL8/+QZ5QFnCJw4e0K4sJ9IhbjQWgVlj0eCdzC7cbAZTqEjyp2yfNx7V2qOsLxkBa/M6VHW+d9MLsGZ9D7y1fB1c9vB0yAqbaEkiwUvuy5Ih6XLhY2FBXNOdtwPlnNObDWmYjiDQ5rHgtOdKi0YdR6r/jAIVM0PFkzRzBUlR8dfnrwxzNN3/at2vjgJZhLC4KVRSKMQda3YhBgfJc0Y8f2VMoaE9RdvOtAW1Z50MNys3hVIL67JoTRot6HOBTLrxxYsXJ77PmDEj63g8CJAdWNQ49AlwFiahMGX/5R520TcXSy3Vt+Ggll7PqG4e1F/nYxHkFhVKpzJv9D5WdLjZOrOa7caj6iptIVVjkTX8rq4OJjWM8ObStfyGxX44WYuzZPHNYgqlUZFQx4JSaj5BTHXeltXt+83abIzuQ1Zz3lY8Cy7h7ULiH3/XWQkrip0+pwtbJ/5x/3g2DC8ODoRp4SuTON1nNX3Ke+tPMAKGsXB9LPIysUwnyJPvBdx5FBCFeY0+jxvOWJx66qmwevXq8DM6cJ9++umuxuXBSI6H6LHQWHDi59M0FoQypqhQijbzcE61oXfCBHnczZzILOTpY6FTQzd6H+P2j2N3ksdC0z4HamLVvL7kTsL8N6JaHxsPrDOyK9dlTypoZaZVMGNhmzcjDpX5gxvnbbX0U9en7nFgm+I8ND0/rfO2w02BFlGQV14Hc/Zm3ru1db4Wo/5xkTCFqlRqxFoWbXP6fM1399cJsGwYi0TMEdY46H3EIvvW6sq1jHrndNsQ762QxyITY3HeeefBZz/7WXj11VdDpuK6665zNzIPJdb39OQUFUoNSvOkDdhQRBYD/LqJs+CUyyaCa9hF3+Gr6uMbjk6ammceC11s9aIT5IlgS90ggBUZiGSZE6TYft8HElSBE9IaC8hPY6G4PmRAv4QTtwsUpbGwnf9W2lsHIVep4WbljqAESaah75TGgrnX5rUf6O+pj6GKFcrqW0bRirOS8HGJd0cal1QeC+F3mzOsaI1Fsi/1GZR3uFlO3qZ4zpCofOAg4W5AZG4cbF/NmSAPQ8widtppJ/jkJz8J73//++Ff//oXDBmSDJvoUaTztoOoULqyhFVMWeecxYdY19UL/3frK5AHrKSvCukFFUpiQ+a87URjER3cwvUMUi/XYEvdXEWFUsxG7nhUBHBAtK3nrEOuVsC9xoLve2DLINjOSx3zToVNyFUXPhYmPwpl370yBs7uAcqY3byR3I/cEObKvpjMAlfj5spHJFA5FCcIVGbbqTCneWss5J9l3ynjSf4c5DK/qo7ySR8LVVQozibVW6Izt5SMxUc+8pFQNRe9IMxb8e1vfzu89r///c/1GD0IjIXN4e1eMkVgPowSiSTWEaO42CDt0FzNeKpDmIUzg12ritgQy7mSXKg1FvHPjd3lessWbjbgzXeVs3AqGVUvLVutVUZ4xfVN4oxFZ/ZnZpffwZ7AbZQplI2fQToqFJ1xrZt8ZtFYJCeY6UhQJshzbArF3Z/y9rFQRA916LydbfyBwhSqnsfCTmovK5/3e9ZpGKRaAIIZm6o9V/XEBHlY20UeCyDFfmu8BUHDGIsHH3zQ/Ug8Mkm9dZFQVEjJt4KMOSpc+FhIiP28IAvzaeArwjLscLOxe2ZpLFyYQtXaVvfV6Mzb3Fe8rrtH+xwzR4Xq+4WqBVSaQinaNeUMCBw+w40H9nOrsbBgQ61NoSzXvguNhVW+DuJ9qkziwn61Y9K1GUjyWMjLyhKtiW0VvSUEDvc9ijkNKyoU21QzG2Q5TcI8FpL2bf0/VN9dQ2f+o/I507dHI8519UgJ8mLf0VdGFjWSu0cERO6m+dkKCx+La665pmYKlSUsrYc9ZBI5qwR5tosiV+ZDIK4L5N5pCf4sws3Gyut9LAQJt4t7l+nVhe+NlpBwyZis2gqVFif+OxI31NCHVMJZrvrXhwKmQnUvcVOoVY6ct9mSfFvn7RzCzRZpChVHB1KGBNty66hQvbKoUPq2dPO/6D0hixRehGnsTAsWi8hLgaMtW2hHFhWK3bYDKQarP81vkC1BHmdfYZlChVGh6uv1jSVr4H9T0iGDuft2L5G56d0QNRaYXXvcuHFw0EEHwcknnwyHHnpomBDv9ttvh4ULF+YzSg8jsd3lwBRKa99Lac9BGZGYcCG1V44lSD/Xjhx8LOKl1xftvF1rWyQ65J8bAe5tLl/jKCJUoL7MkXqrNBsp0zaZOl3qvO3ujQzoaMusOYgDW+gte4I8Jz4WNhnG1b+1xRkLSbnoku0bkkeFkpet9aXRWBStxbTNvC1ty0jg8YQZ3OmU+dnVmEwwRoXKyoXlrZvSmkIF2RLosjQWwNBYxLRDprFwnl+gGI+s3Q2OsfjnP/8JZ5xxBvzjH/+Ahx9+GAYOHAjr1q2DUaNGwUMPPZTPKD2Mi88m3CxrYRJmOy2soEkiETiXPlLHQtm0QkKQnXm7/rmrW7eh0AgDJwoL2x06DzD7X7aGHxueYwqFP/AYC4Xkl2gCk3o35J5jdTTSZ5eotldQgjzLwa93pbFw+PDaYxLQQJsgz67PHmlUqEAfdUiTx6KRm0LWfc/sAMxjGov2sZCZxcX4UsG8iNd2WthhNUSr/ig7XV5RoXgaC7NJtI3woTcxBnW5Rh/HDTGFwvCys2fPhrvvvjs0iVqxYgXccccdsMMOO8D3vve9fEbpYdzorKJCiRIERll5oexlUlqEPDUWlqZcfJvbwEpj4SSPhYJgCUqketVFyZFhrSSPiw3USRDT9uqKgvrM2wTTNikBa/E6dDkJ3IIvybc1Z7Qdu81eSInWlQXtMcpQ62MRZNFY9NKilSmEDfHxFb0lxNdGVvNXigMwjyC0FyTZIJD0G8YpqsiIa/uzyKY+FwGTqeHlsQjsxmEoK0aFUraZ2rbNDC2JsWgBlUWHjeP2M888A7vvvnvt2tixY+GYY46Bbbfd1vX4PKimUFYaCzpBEzj0UdD/noQL8w1lX8J4KUwM91Cq9gMlcN4WD5R0mUZB1n8RNqiqVgKmxoL6nuTOimbnbVJGe8XduDZ1QNqV26K1KVRvo6NCQS6QvRMTsW9sUxJUwpQRXpfHoug9IUs2aRFG2pRpzmqbedsW9dDDcvOcZDJBbtvCd+tRUjtU9yU1haLzFazBc6KOiXkslGPJsEf06szOG30gN0JjMXz4cJgwYULq+pQpU6B///6uxuXBNYVykCAvyGwKxe/T9LsLe2lqX7Txp00OOGCFm3WisYg+qNtu9EYml+QHuRGOkTRK1UVgeE8iVEwIZX7JiBxZtDITlPb0jt9tNaIOlNwUylUeC3cPz+S4Wfvdskvck1I+FkrGItBruXqL12LGu8seFcpUn/dmsxLvXMj6i9O6tpGRZGPL+zXrtCsqQYu2PUuBWNIUSl82kTOEEeDBtM31Eh92C/AVfI3FT37yE/jiF78IN954IxxyyCEhM/H666/DzTffDOecc04+o/QwaixcJMhzyTRMW7AqPCB232pjZRl5GzSpmwukJShmYtvKeTtWPg9JuLZvVQSLEplCySVXgXNClSzll4TutEuQZ2YQqhmexXpCGcIYlEyS43eLzXHbtKXzbee/K+dtlye8iZE3aREo7ZM1FrU+g8K1NRQmJ7uPhalP3uFXfB6LSGNRb6fSVjeFStrqM8+i+OcCXrKOoA8stJS2+U44511bLGcIJ2wxJRpZBF3RRp/HDWEsTjvtNBg2bBhcfvnlcP3118OAAQNg5513Dj+jSZRH/pAtPhspLi8qlLn9aEEgU3HMxQ+Hn2874zB458hNY+0YxgRFaiySvdFMoTImyGPkX3DjvB0Yzd4avY/JuteNyRWzqdNYUBjAgJkITeVjoTNTi8qYoSISwSmYNFnfGOwGYW9mYFdP7Nvls4vzwnLn7b5/LdvH+uIZYPKxKEOCvPqY3HVo8tGozuEgt/k7YfpiOPq3GQLZ1N5P/ZJonRMlc+U+NqqdvyskGJnUWLKZQrHGz7nv0HnbzFqkojiytC0BqdwGlSDvAx/4QPjXqhg/fnz419OTX9Zn987bNoxFPmV/esfk2uezbnkJ7jrziFg7psVHk7q5QEoyTOgqNBNg8jrxZgvXWEgOqeqYyrN76QgtGVzlNlFL+Xk+SyoNioqxwDMrTtylJXmB1byUjgHy0FgU40zdSMldNVqXu/5Nkta687Zdn7hXUDUWtahQyvnv9t51iAg4l72ZniE3nK6Nr8/0havZdWr9RUxmnLEQ6Fz8Da9lWSNFvGGdRF+VMJTcHmMcCS2PoSbRxUIimDSNIYjVpbfbsj4Wq1fzFgm3fNmAeTomT54MkyZNgjJCRvPYmUKJC11TNqBHvYkTWrpoRPIxuXfEVEFn264TWLCjQkVEZG/AIlid+Fj0PVHxcEyqZZvMFCrHOcHSWPSNkZpDJrolMfSoyWQqi/O2c42FBdFpbQrVaMaiSFMo4V83UaH0GeHV5p6NMIVyycSZ+uTteXlGJtSaQsVmg2j3bztfkhL/oKEaC9ngWT4WHFOoOGNv2I8o2opoLJzxBLHP3nkbAHbZZRf4xS9+AW+//bayDD7U++67D0444QT4wx/+4HKMHrmZQnHKpgun46ab2zX1Kf7uInQktS9KnOnqoctkLPq2lC7mvbjUWOiuF50MK7PztjNTKPXDYeWxUJRVZVJHG976Nb3/S1gGyuS8zadmrDNvN3Beio6ZTtrTaAqyh5vlR4VS+1jYjYNIj8n7dPiyKQQep7dPXfkkFAnZOSra/VvPl4L3/STh7SDcbLxtzjgY9ejO20khi9kUKiCNoQX4CpopFCa++9GPfgTnnXce7LPPPnDAAQfANttsEybHW7p0aSjdnzhxInR0dMDZZ58NX/nKV/If+QYMebjZ7HkstGWd5V3QlxHbyDNBnjhcioQUi3AfddQsx78i73CzCQlqg7cyrq1t1udiSpBX1Vhkl2imn3k0gGSZ1P3baCxE5iQIYGVndw7O2/zZYssIuiQ2uaiGknT97PrM4KQmINkkyLLM2yrNnklLYGsGhpq4bst7cM3EafsqUCNjg5o2ImELJfpYVP9l+/tBsft+kqAXBJGS/k1na+J2GcNPBgfQV6yG9qXlsUi0a8qfEsS/6MqVeHK6ZCwwZ8Utt9wCb7zxBtx0003w6KOPhiFn165dGzpy77vvvnDFFVeE2or29vb8R72Bo8dVuFnxO1PTwDVzMveR3mpyTZAnEn6Evqr2udzN3I5JcmEKUj+A5GOKl2kUAubmmrvzNjOPheq9qjIhx7PoSp23LXwsxDqf/8skePT1RYnEbC4QSntLHm62Ufk6KOu5DRQOtxmJXZuoUDotl81Y2nCuMddmnYiGQlFm4i06h+IjjCfIC8vYMnAaDUIu0JggUcyrU7/HngpLu5EwRdTXC7VDRI2FKYx0HPjzM7OXws/vehVmLFK7C5R3ZubkvL3ddtvBd7/73fDPo2SmUBbEVtYkQSofBV2zRkl0kF0TkwdjFa/D3tT7yjdGYxG9E/FdlccUSh7PPH9Jti6pHMUELzC8J5WpYNrHQmg3o8ZizpI18ODUhbkEP6hK6bgMMjShKZRjWyiDg3ZW0yucruK7Vs1hisbChnDNwsMWyUTm8GpzQUJhEUrR8UIl015Y9H0HzN94PhaWYwryEbKYxt4bBPCRP6VzwMnKbXAJ8jxKmsfC5vROSbED5iYglLGQrIr3Jf6er8Yi3X/tN2WdLBqL3oaEzZS3ldBZQCNB0YbF4YpY1kls13eb+4iGqHqvPSQfi7TJjdgzjbGoF+pkMrDsw5RZx1XekSJRJfTdM2VR266I+Wx5LNRt2YwkzjBzUSQtZZPksSE+FrG3IDJtNSbVsm3EwpWd0MiM6lyBUlhH8dkEjiANnbcpU1mkBUy0SkAccJnnZq7hZj1aQ2PBOT4oxJ/sYBQjLJjMrVLEfo6MBWX8Ln0suASf23tXayyCJnPedpYgT8NYuAgLLDYRFUuaNMgidgnzkjCGZBP5rhl+grzmOylDQt8xPxQ9h8Ch+VHCx0KYcCppds3URqmxyGAKZYkipbRVM7fyzklZOGDRFCqrj8Xv7n8Nfnf/6y6Gq+8vfs6kxpKG2U9BzajowPEprIabpeWxoIaQ5Yy3vDOTDq+x2JAzb7M0DhLiTxUViqn5iB+8KeftHKWdunCzujq2mznXFMoFVJFoyqOvUBNaKrgz71FLdDnrSRVuVmSA6nksBI2FIxM9TnlrWJiR5GjNmBtspfamNuP/pvvLorFIM3DKxI2xOtLfw/HxxxLXxJVbY8HPRVQkAqUpVPaoUFH5IpiKeH/i5+oFC1MofXVSPbPGghYVKjQL7bUbe7P6/1DhGYsmhOygtgo3a/geh1x9by5TYSya0BRK1FjkGBVKPIgpBFD1UGIyFkH+/iKmvnXamUbbdMpV4uoxuRqvWmMRQBfBFMqkQaE4b1d9FgztUxgLghmfC4TEL7ODRs8vG2SN0qRqM4RGU2YLnGupqFBKziEaj1pjYcO72wQKoAijXMMmslmRqPkqCqNMRIUSypLbhmKRZASE/VARHc2aUSHXM2gsBCaOqr3lZN7WoQm3yxQ8Y9GE6M1JY8ENgWbjvK0djyQSCzX5mA3Eg5dqCsUdUlR8fSMYi+jfwM0GnQdk/Qd5hpuN+tCUoWjKooNRxdSrfJDiBJjUx0KsZxxJce8zNNlhkieuHcibVmPRa9BYZOgQ51BKUKJkHKJxKMZpORYbS6haAs+CNRZllgpHzyI+RDFSEZ6V4b/cxgu+70RiOlEQ2WsTCTEmQGHcSyLMrqFa9VkTws2KfpmBGwFLABsoY4HhZj/zmc/AIYccAnPnzg2vXXfddfDYY4+5Hp8HkbCy0VhkTUaTMoWSlBXXp5ZglGksSsZYhE6SgaXGoiGmUObQwI0+ZMXel6/tgrnL1irLu5oTqvsOfSwY70pFOKfnF0hMoSgHUnni09v4AjQyH4Ut8sh1oDNfyeosLtVYGEyh9D4W/LFQMxYnxmLQnuSC0k/HaJ4ESil6nSErt8aCKzAxhpuNC1As+zbmsZBYWsjbRLNouzG0moY3M2OB+SyOP/542GijjeC5556Dzs5qZIHly5fDz3/+8zzG6EFhLFzksdAsVRlxkDaFSpdJMRa6qFBhg8nf8zQfEhkE0nq2OHSje+5siMZCTswkneCgoYjPm+VruuDwX/wPPnrZxPzzWCivY+btIHPmbRXjmrCV7pXb1uskfQ3VWFhE1GlK5+3wlbodd/QYZPtkVkZGtiZMmbdVBIytqVDFkXQ+b+DdlZl4q5mHBepna/vcir7tZH8C49vbC2fd8iJ87W/PhPs+wqQoTgrEOONgnHeMPBa2DuQ6lHhq5sdY/OxnP4PLLrssTIjXr1+/2vXDDjsMnn32Wdfj85DA2aaYVWNhQZjrylQdowuMCtXL70sW1tGIEmgsdM+90ftYfGx/fmR6mDFaB3caC/V1TtAAusYiSDm5ogmjrHpSwmYeQ1GEkk2+haZkLHLQWOBz+NYNz8G9k+crTMzsITOzVPn+mBx/bUPf2vluu89wbuwxYwSuvFFjGmLXcM9I+mb1vUNm21hPJQjJAzoTpBuffhNumDQH/vvyPLjov6+y/RRso1qSNBYUUyhLX6QNAWzGYurUqfDud787dX3o0KGwbNkyV+PyKIKwEr/rmg0YpkSBPWMhHjJW+TmIUOUZCKGS5lnYA5fBx0LHBDbaFCo+trVdPcbyeZtC9RKdt6Pqqjmq0ojFfSxwSkgl2InxlMzHIig+g3zRWLJ6PTw/x+15dtdLb8N/nn9L+lvWPBYyoYVt5u2QH7EYik1UKOyraOIsazLCvCE1c0rlsYj+5d0Jll5XoIBLZ7r01Mwltc93vzKvr3zgzFciWU8+Jhmo01gMN2scQ0Au2vAzuXDGYquttoJp06alrqN/xU477eRqXB4a5BUVh8lXkOqL8aD1+QnSMog8w82qbODN8e2Zm3nfPTciKlT0kgJtXO/Ggjud8zeF4gVDUJUV50k93GxSqiy7/4QpFDGmeq1u/nGhWl5j8fTspfDnR2Y4bXPawlXK3/AJZdnW5RoLlUrOzFgXZwrVAI1FA7QkLkyhZD4W3BeFba4jCG9cIUnQi4KWjFGhGOPg5rEI/60QfCwYe1svy2wKNizG4ktf+hKceeaZ8OSTT4bqorfeegv+/ve/w/e+9z342te+ls8oPQqR2Oo2W0oSM1n9l+Yuh89e9ST8+7k3q2VMiYtEZqXABUaPCsVlLKBxeSwIGosy2RtTEhM5I1Q1nAVHu6RidMTrsgR5S9d0wSOvLdQOjXa7dpK8IsxImpGxyAOymd3Rp70K12CGxyQTWqg1FhKJuANTIRvnbdvQtq1sCiUbm5gN2uQno9VYFMlYaARY8hD2AUMza6ctMM23SPNWcTyPAnrRUjO+uWTePuuss6C3txeOPvpoWLNmTWgWNWDAgJCx+MY3vpHPKD0ScCX4puShiCCb5ypTD1Ei8Ojri8K/k/cdqV1deYR45CAufVDSnJiIymIzR6zP0axL2bdCOmmrUs4D8cOEQptk1VgEBI0FJ4+FMtysMo9F/SZV5jZcH4ui3qFc/6WHZyzUczvMVo2JQTNmg5YJLVTPvW5Go16PGJmtCFQj6xStsWi8llaHug9MfZQ4TSoGrQYV67qK9LEwa3A5e0WRplAho2wQtnJogYBTFmDDYizwYZ9zzjnw/e9/PzSJWrVqFYwePRqGDBmSzwg98jOFin9m2Daq6pilDfrsstVws+Z7Qxv1PIiVhDlJ4C4qTmM1FtEhJR9TtUxjUXT4UtnBnRwPhu6k5LGoQlVW6WNBipFer0uZNUUlPJQFWKDU8ZBr4+JzIcu0lgktVKaX0bxXvRc0A8M/Ltosgtc3ZG40wPzKBvH5gHNHlnmb7e8XBIVqLOKHizhUuQlo+lof7536ncWI99336s5ugvM2UWPBDdkfkIvWTK6rWcDtM9o3Cuyt4Nprr4VXX30V+vfvHzIUBx54YMhUrFu3LvzNozlNoUxNUjJvm6QooQ+FTmNh+D0ChTCzAYU4w3vmPv9oA4wf9Bv1a4cioJJOJr6WSWMB+WssotegM0En+cMEeo1F2hSq+p0yfdkai4I0BDZmJM2YIC8PyN575Mif1ddA5rytmgcmjV2eZoxZHWBdoEoQum/X1bEk0/xXCcx6mTtffBteeWu5lZars7sxPhbiUOWZtwNyUADOtFmzvgeO/PWDcPBFD8CTMadxKWoaC7dCloBeFBas6ISjfvMQHHPxw4VpD12CzVicdtppITOB+SziwDwWp59+usuxeRQYZcW4uQfZnZ9DjYTxd/O95cXAJxkLd6p7mcZi8IBiGAuVLW6SqWws4Rcw360qjCa5v0hjoZhr+DM1j0VVu6FgQi2CA0jbCXhl8qTjsWkuMdOMCfLygIxAigKEZTUD5Thvq8wjs8JmX3ZF5HOyflfdWdzPSVcCL1M4YMSFd70K7//DY7Bo1XoL5+0io0IFLI2FbM7G103Sx4I+jn889QbMX9EZ7usynzap87aBUeYmkuxllD3nPy/DG0vWwPSFq+F3978GG0Tm7fPPPx8++9nPwnnnned+RB6FHdQcB17ciJFzvvLRGfDcG0sVdfRtyJyzOb9HiIfrdIkEcaaR9tk+/7gUfPAAthXiBmEKRVH7ZpWAmw7uaoI82uGr0w6owhlTRs89QItiFG0SuTVjuNmiEO1lWR2KOeFmq/1V/TpcopFRoThnQnwOuzxKQn8ZB5BplJC4lrV+x4tvMdsu1hQqfg9pP786KlrH9XgdNaNi0lhQUeuP4L2dR+ZtxOS3lkOEOUvWQrPBirr5zGc+A4ceeiicfPLJ8PLLL8N1113nfmQeBeSxiC1Sk8IiAPjJrS/DrX1x2F85//hUHVliH45GgpqUKjdTqNhz1UVMsbFrTWks+hfDWKhCS+o2/KIR779SwPw3zdOqxoJGdemJN/G7oeN42Vghrs9/3hoCbut55qJpdtRNobKtQ05UqPo+5va92OSxqGqAs/ddFUjQGqo+6+rnjrY2Z/mFXJ1LNY2qENRC1ryNv1+xGovY59RvfFMojjDUFtSoUOw8FkAvG6cV+ndsAD4WkUTx4IMPDkPOogM3MhmzZs3KY3weErhaUPEN3USsYZ8RU4GYsXC1MkGe6oA0+Vjg75R7K8YUKnAmMY9Kxw+wIYVpLJL/1q5LzL5yUgQZwZ3N2RkLPYFPNYXCg4IzFwJmWGOxXlYzPheoStZ5HfioUBSiKZtxTqeEONYxmDZO+EZY7B9VLbUDjQXjUAjiEdqsbDbkcLV/9hLyWNgCmyxWY6E+UwOidjORcdzUgAPUo0Lpy1EFoRE4GsI4rYDMb7OBPeL4JrDddtvBhAkTYIcddoBjjz3W9dg8cnfe5phCpaHLjaB0ztb9TrQzLsIUSs8A9Wb2sRhUmI9FQN7U83quJugyzErLZw03W+Mr1O2QTaEY0vho3GxnbEr5AiR5tm17Uyh1VLh6HousplABT2ORh4+FRR0ucebMFCqDlsXFGPSIBHSQ1Fi4aBk1FkU6b2vOVNl7l03ZuIlZ0mcjn32lHhWq0jCByfrYXtHRvgFoLM4999xEaNlBgwbBv//9b/j2t78d5rTwyB+uLAuS0gRDWeF33OjSeTD6NkTLcLLUJFH5MRZx8xOXGougcT4WgSmCV/0Hl4csB1zCIncfC3TIpmgsULPBYDLrEboobccO0BIxFja+AF5jUYXM5CYimqpCFfvnJGOEdQKQsphCuYoKxe066tKlWa2rc0km+BAT5Fm3HfpYlMMUSl5ep9Xj5aPIW2PBPYd6WY7e9c/925tPY9Fhw1ioHLo9mtd5W0Xw63JGcA8EUzhZPBsph2tbA30sEOxwsxKNxeD+BWksamMQtUtp4roMGosiMm9H9VWt4HWq3TXHf4AS7SU+hlo9Rtu5h5vt+48Dz1io50rceTtTHguu83YO8l67bdmNSRbnTKjKr6qdtjuUBrvaP2Uhwp3pQkIfiwLzWCQ6tzUXtGdUssD0zG3pAC6aUWNBYixuu+02OOGEE6Bfv37hZxWQqz7xxBNdjs9DgryTw4mLWrYNhRoLQiZNrsaCcmu5MRZxqa+GmrP1sYjb7RetsUipoSXvLC+neCMElX/eGos3l66Fi+97TRuOk+68TZf+ybREprI2plN5Wh7ZaCx8uFk18R+tucx5LBjhZl0wMi7zWLhgcVimUDFmxuWe5yqRmUzoE2osnLQO0FmkjwXTdEm27SbPe54mN8t7NL1P23xWXPRrVY3FSSedBPPmzYPhw4eHn1XAF9HT0yBueAOCK5tlSohKXbQNqwR5mvF87uqnSNmp8xKsx5+B7pDn2NVX26r+29ndCOftSDovaiyi392HS+QiPoZKQYTqHx54XTMemimUrcaCNHwuY1GYxqJ407UyI54V2Ib4j9acDcMWh0zDppubNk74JtjQ1SvWdsH9kxcUn8cicL/nOYsKVfu3/n5wmE5MoTADNeGMdQWqwCMi4mVzMk7gF2HyGU0J1xqLXsvhtqwpVG9MKhf/7NEYuJIAJpNqydtU7bsoPxHrmBa6KU8FhakIx9RkPhZQBh+LXnN+i1KYQhWgsTABhxP1gc61OwwbDNMWrEqXY47lov9OgbteehvmrVjn1P8pGnO9fI6MhUUit1Z23kZpKvV5i8Q/zvVoyWUl8mV7p1ZjEUrtXTMW/P3j6dlLw7+s4Gixq4+leu8utzx3plA5O28XqrGof9bNtwrVFCretpMRqsdieuDeFEqN5mOFPBxqLOSfqVIYblQoU+ZtKopw3u7NOSpUUT4WpnfV24TO23nb7IcMQx8ROLBfO9x95hHw1SN3zmwKhXjhzeW0MWQwhcrfeZspqWtpjUXFmvjHxxjVzxwVipnHInSaLkGCPFdg+Vjgf4H7UJ7OziWJySQK8po93GxgubfHn2tijeS0z1EZZK6Aq8dywTWjKRR5xBMnToQ77rgjce3aa6+FHXfcMTSR+vKXvwydnZ15jNEjL+dtTYxp04Ydt1PVmdeIC6vomOUcxNe9bpxsH4u+4otXV9dHv/YKDBlYlMYiMoWSjykO7v61+eD+sMvweoS4opy3ucR8Jo1FewU62ttgs0H9Ck3+lmAUypTHwptCJcDZinR+O1X/MvvnJMu7olsn1cR0jTeFcgWuKVRvifNY1E2hYnBmCtXABHlMzas8QR6PUbEBNfE2l1HottwHW5qx+OlPfwqvvPJK7ftLL70EX/jCF+CYY46Bs846C26//Xa46KKL8hqnR84aC9WcV21muKbSuREiFa68MUcJTnM7wHrIGgsmYwFBSFTMXLQ6/L7TsCEwsKOxUaFk6nauxuLMo3eFP392f2djpCJ/OrX6vuISTXnWW16CPNYIgjKbQrUuo5CnlFoabjayLc9IKElNoXQ+FjkQZg1lLJhUfZCDkMqVia7MV9Ed0xIUrLHgQbZ3xV9RseFm9Q+du/d3WwqhGmVJkAVksenzzz8PF1xwQe37DTfcAAcddBBcccUV4fdRo0aFoWjPO++8fEbq4ZxAT0hFFYtEdXAiEZ6OChWNT8VYuEmGlNdCozrA2thWzl68uiZV3GXEkMKkEHWTJ8XvsVnAVeW7svvlRoXKG6HGou9doXZJN+ds1dsmvLVsHZx5w/Ow/eYbQW9QaWrn7VYGZy+SJbGLqocMW4YHK2NadAKowFHG67IQQLxws/VzyFUkJ5dMSj3IQ1KT6+LEuPmZN8OoeEWBGgkvmqvyzNtyUyjX87eOKCqUW8uRHst9uRkFOWTGYunSpTBixIja94cffjgMQRthzJgxMGfOHPcj9MjPZjkRBYm3YctCw0YLXZf3oujQgq6d2e00FgCvz687/+42fGPo11EQYyFx0o5/j98Km7FwdDAnD9DGI4iZkegc5wKFCYoLfPem5+G1+avgMQDYdhCBsYh/ztMUipbDcoNBVlOomsYi43OV+lhoNRZu8keUx8fCzk/IJS/kLEGe8G9tnA6aL5KpqEJ+/qjmqkxOk3Tezt8UihoViq2x6LUTQjWjIIdM3SBTMXPmzPDz+vXr4dlnn4WDDz649vvKlSvDPBcezWMKRSGkVYQjMjfpqFBmjUXRyZDsw82CUx8LJBIj7DpiSGEh5GRO2iqzNa7EzVVs9aQtceNZC3wmEcPQr2YKpdJY5LPrx+fL3DUUjUX+Y6r240br2Crg7EXxcNP1+uAkj4U8QV6xmbcbuXY5ZkhxrZvLEbs6l2rzIK7JLYnQxdX5o9K4mTJvc8Nw52kKVVS42d4m9FEjUzdjx44NfSkeffRROPvss2HQoEFwxBFH1H5/8cUXYeed5ZFTPMrvvK1iVlT7dTWqiECs9h2OqraqCfKgvBqLvDJvQwCvL1hZ+74bMhYdxRwTMulX+D19drFthENTKAe3UUqNRd9BF8011bioifTyRtJ5O2dTKK+zqIGzZKRzJceoULptCn9qJY0FRyhS1ZznYArl0MdizpI1MGXeilwS5BUJ1fkjItpvZXNS7WORzz4UPWnXeSxs0YR8Bd0UCv0rPvzhD8ORRx4JQ4YMgb/+9a/Qv3//2u9XX301HHfccXmN0yP3cLNMHwuJ9gFDab7rp/fB8rVdSsLdBVGSX4I82qbBVmkGdVMotNnffovB8Orb9UMjT+BrvfHpOfDDW14Uh1T9N3YrbI2FoyCIZZOA43i6alGh2rRzrqjDpTzO2+V7X40Eh5iUm0LVP2d5rNQcQBGyakhkaFAaHAsfi5gplMsxOHoAc5ethS9d+3RCM+5KiFM0ZFnEZcBbxb1URtsow81C3hqLsjAWAbQsYzFs2DB45JFHYPny5SFj0d6ejGpz0003hdc98kdPLHlXlqg08Zq9Vj4W6UoqpsKp83YBeSx04+RuKPiOZiyqMhY7DhscOm4X5byNTuPXP/VG6notKlQ8u2uDNBbhOILAqfQwC8JoT31EYOS8LRtbyIDk5GPBRXHhZr2+Ig7OnJUR//H9NQuhsp45D/NgECvNZAoV1XPqvO2mnd/d/1rqXC/HzphFYxGQGG+TKVTCxyJnUyjTUy8qjHZvKzMWEYYOHSq9vvnmm7sYjwfDZAeJ0+7enlylnMpwsxZx0F1pWvLKYxFqYXqD0MFNd2/cDWVVZ3eNAN1q6Ebhv0UxFhOmL5b/EJlCxW6Fy6+5fAs4DnytZdhCq/bnUGPeEaopl3dODRvk62NRQhVTA8HZi2RMaMXRe+Oa5OWRebuRYO1dsbPL5VHiikmRzpOKm6hQRUMWOlfnZyE3hVJEhYKcTaG8xsIaxWTp8nCKiEAPI9aoFQS8BHmKc0lvCsV0XnIU4jC/cLMAZ1z/LNz10jynG0o8bviAvmhQRTlvc2xfIyK6IRqL6N8SbKJdscUQmUIV7bzNRTFhGPt8AXJrvfnAWTK6PBZZCQiuKdSv7p4Kr7xVjDlmEeCYpOXFG7vSpMu2mrDpJlRbcPairu5e6X4af6xUqwInplCGckUJlXqbcMNtRiZ4g0d0PmUlTikaC2Xm7dB5m9efK+dtl9lS48BnYGIqbBLdSBmLgsLNmjb8+Hvnm0JVnJk/yGK3NwpxiWHNFIpQtpGYs3QNXDtxFixYuS5fU6gWjwrFnc5Zicl4f0VqLDCfQSuBsw/FQ/u62r+wGVcWurJmmpSviAmMzGVxL+01+VhAEVGhqBoLKAS9TbjhesaiCRFNtKzmNHFpgmruqhaXytFKh6rzdnmjQlGlflwCIB5mckBfxu2IYC3Ths/VBLk87GR5NRqF+PvVZd6uli2HKdSa9T3wk1tfga9e90zuWpQyaJXywqB+Sd/BvLWn8epZCIii7L3LClYeC9z9+h6XS2Yg17hNTRoVKnrOlLld9bFIX08wf4WYQkX/msLNFrP3B024tD1j0czO2xmJ02jC3j95Pnzj+melZZQZhy2iipQ98/bq9TR/Fa4KNKGx6FddckUlyFNB5rzdbqWxcDuesklnTJm3y6KxiPDsG8vyjwoFrYuN+vOsg7MSpknn7WxtbcjghZut7zOuzDlDHwhHW7pMixIyLk3IWchMbnWMhUwoEidzinTeNj3vwpy3ezcQH4upU6fCJZdcAq+++mr4fc8994RvfOMbsPvuu7sen4dmomU2heqTPn7x2qfZGzbW4873kBlxEm42nx12zfpuUjnufa/r6i2fj4XEqY4fbta9lK5sm2hNY6H4vSw+FnHkm8fCfcbmMmFQ/8ZpLMo4l5oF7HCzfZ+jgNlZn3zF4bkkawXbbkK+IhZuNrA2hUpEhSpgidQ1FnoUJQTrKZmwjQI2dXPLLbfAO97xDnjmmWdgn332Cf8wCzdew988iptoWU2CsBmTlF5tAmKRedKZxgJyAUZvykNj0dndIzGFKh9jwZW4uQ03W/23bLRVey3cbLkT5MWR55DiOQBaEVwTxaw+FkmiqXWfa97g7EOheMtxVKjqXpgf6R+2D62tsfjBLS8qstMrokLllSCv5mNhCDdbkLY6aMJtga2x+MEPfhBm3v7pT3+auH7uueeGv33kIx9xOT4PCSKCHhkL/LOVdC1e3QmLVnZqy+jyWLBNoRwlZcrLx2JNJ80UqoftvF3fLCOn7bzugYtEHguuxsLhYVdaU6hauFn5ncoOwkZjVWeGUHEGlO39uAZ3DWRdxvF51YySybKAtZ/GwknXAlBkfPahKZRDJiV1rfY/N9h+i0Ewe/EaKFO42RfmLJNeV2fehlxA3QJ8uFk12GLTt99+Gz73uc+lrn/mM58Jf/MobqK1ZdzMbn3+LTjqNw9ZbdjVPBbA97GAMvtYUDUWvLtYK4kKVcaMqFxmJ3wPzsPNQqlQCzer+H2hgTFvBJatyY+xqJpPQsuCz1hkNIWC1jGFaqTGhWUKFfvs6iRxG7VJ4mMRmkK5e76H7LQFFIHaiR+41+rlFhWKmMeCauGQFRsEY3HUUUfBo48+mrr+2GOPwRFHHOFqXB5EjUVeRLZJ1R9GheKaQjkKVZmbxoLovM2973j50jAWkk3LJvGgK8vf0mosNJm3ERjetWxYvjY/xqIaUKdc78gluEsgq/mLKka/hxw/fN8e8NmDt8+WeTv2nF3JRqrhZpvHFKooU9y6iav93I6b6CY0FnlFhSI6b0+dvxKKQO+GYAr1wQ9+EH74wx+GPhYHH3xweO2JJ56Am266Cc4//3y47bbbEmU93CLuNI2bad4mNarm45E1OLbfZXbeXk32sbC/hwHMcJa5R4VK+Fg0LiqUiwOoEc7b85ZvWIyF11iI5d31V0J3HRby9DGIsMvwIbDlxgPguidmJ65zXGPizzl0inYRFQr/y5OxcBwVKmtEyTx8LFRQ+1hAQ8PNFrUPBk244bIZi69//evhv5deemn4J/sNgYusp4cmAfagIxnFJ18pidbHwsIR25nzdltjTaGymCyURmMhuQW7qFCuBlRO6Ux0AKsc2+evqJpCbTqoX64mSKVhLMLIbq0LrqAmq2DHR4XSAx9v/LGo9hzOORgXXjgzhXKZIK/SOhqLmiVUFo1F4oHEws0CNERjsfOWg2H6wtVQFHqbUODAnl29vb2kP89U5IO4g1/VFCrf/lTEJm7OXGfDbmc+FtBQ521uVCgVY/HHT+0L7xw5FEpjCsXWWLiTUtZMoUpGXEUHsEp6NW9FVWMxfOMBcObRu+a+HhvNWODraUIBGhnc95c93Gy9ftm0dWWAuCepItFxhE0JxsLRenWl+dCGm3WpsShoo4pO/Gwai1h7BXhv13wsFL/vufUmUCR6mnBfyMS2rltXPjOAVkdcqoWbTd6mUMpws6FJlk1UqOxjsvEFoIDqjOVKY/GBd24Dt51xeEiUliPcLFdj4S62uozRKQOiA1iXgR4xYpOB8O1jd4OXzz8+jLjSuqZQbnLRlBWVwk2h6p+9xsLMuKl8GTjvIamxcLOLYQu5+lg4bq9oH4ss23rCFCredoZxaftr0+8FRTMWvSU7Eylgzy7URFxwwQWw7bbbwpAhQ2DGjBnh9f/7v/+Dq666Ko8xesQgSpjz97FQaSzAzhTKhY9FTvdMDR2ayceiL49FHI3JqJqWJHHNbvE1uBr7HS++Feb76C1rVCjDjaLdN2JQ/w4rIsDlHMjXFMprLNxGhSq/xqKRGZ9TGovQlyHbe4j7WITVnHhvO0yQJ2sGI0CCO0Rhz4vzsbCf2/G9N75E8lsvJo3FxlAkgnJuC1qwZ9eFF14If/nLX+BXv/oV9O/fv3YdE+RdeeWVrsfnISAu1cJNN2+HOWW4WYuoUGF5Fz4WDTY3ybLQZT4WjcipWpckJTVgHFTtft2M/Se3vgJXPTazvBoLQznUWESwWZNPn3MMnLL/SHCBPB9hyV5P4523sybIi20HZZv7RZvNkLTTiqE0PCqUy2R7snCz4BaFmUJJgoRkMoWK+1jk5bxd996WYsdhQwr1lewt6b6gA/vpXHvttXD55ZfDpz/9aWhvr0tfMQP3lClTXI/PQ0DcvD80hcqZsdCZgHAnfBhu1sGYypJczgYD+kkYiwbcTj28a3wcXDtetx6Fv7p7aukI18h52/Rc4uZsNo9kiyEDGs4wt3KUkmZIkFdUJt9m2m9lfIXcFMrWeduVliFfIZ9L7XBcE1uYxiJwM/8SUaEgHxj4ipAp22xQXaieN3rLuS1owZ5dc+fOhV122SV1HR22u7rKERVlQ3LebpwplF2CPBfOuXlHwsoTUlOoBoxDFluDK71TOVK2knSmX59I2TTnhm88MPMLbYTmKku461YEdz67TJBXsqmfCrlcDudtlSkUvc2kMCXL6JL9uzqKZXtgNUGe+/w8eaNqOhnAzEWr3ftY5KyxUDGKeFkmIMwLvWXdGDRgP53Ro0dLE+TdfPPNsO+++7oalwfReTtvGludx4LPJITO207GVH4CjGUK1YD7kTnVcR0QXZknx9FbVo2FodyITbJpLMJ6TTCtWz8qFFdjUXGXx6KkD7aRGou0j4WcAedo7uPnlrM8FugD4WgBy3z4XJpaFc0sXvK/afDJK56wrh+/77i2NLcEeQYfC3zXRZpCBSXdF5zmsfjJT34Cp556aqi5QC3Fv/71L5g6dWpoInXHHXfkM0oPhfN2/pu+0sfCIkGeqzwWzWwKVZTTHNn2NXatrXY40l5Svbw7lC3cbN15G8gaC1tGsRkYi1aOCoUmDlx6K8s7+9C7tkl8/+ntk6GMaKSPhbiWVPkiOGsuzsC58hNzSfh3SzIlut4bitJYIC6+77VM9ZVnTE7bUDS/VN1WFJYHeaGnZGciBWwq50Mf+hDcfvvtcP/998PgwYNDRuPVV18Nrx177LH5jNJD6bydv49FReNjAQ3JvN0MBFjZo0LJwrtyI6RsGKZQkY+F/kaHbVy3ubV/JOWf2K0cFQpDcHIZZRshx+aD+8P4T+0HF568d6K/tV3lzP3UUI1FhRYViuMykAed5jKQhWx8oUYEmjBBngOo8ljkdVZEe73qfeKaLVJA2NuE+y1bY4E44ogj4L777nM/Go/MplD43eV6U5pC9bZeHovGmUIVP46V67rh639/Bo7YdUvrcbg8TMvKWNQ0FoZy/WMHte37bIZpjW+nXG/IHfYeOdQijwX/paGj//vfuXXTvPNGaizErqvPS2IKJRmj6ixMClNc3Zu7ZLViAtY85khk4tkMSPpY5B8VyoTQx6JgU6jVnd2wcu16aBawn85OO+0EixcvTl1ftmxZ+JtHY/NYuCa6taZQTFbalZlLy0WFapCk+q6X5sHZ/3qpPg5uuFmF9DALJFYADUVkMmB6Ni7mZDPM6nD7KRnz5wKjNt8IfvvRfSzyWPD7is+VZnDYb28gESqGka36gaXLydanMvBIQjjnhnBXJe5zYfoStdqsPhZZoc5jkVd/yX9lvxcbbhbg/lfnw8G/eAh++UI7PDh1IZQd7Kcza9asMEmeiM7OztDvoow4+eSTYbPNNoNTTjkFWsoUSmLj7proVppC2USFCjUW5U2QVwTiku0IZZFasoeRw7gxSV6ZEK0n3a1WiZPsxGJZ5oEeredhcc1pY+Dh770HRm0+qBDn7bgGoBm2sjJFhQojXEueuUygphKyrY8lQmVaf+qjQjl6TKLzdt00xx36dzTBxOtD/MiMyIdvXv8cXP34zFz6M+3f+HuRPha9QQATplWF+W+tqcDAAiNS5W4Kddttt9U+33PPPTB06NDad2Q0HnjgAdhhhx2gjDjzzDPh85//PPz1r3+FVtJYtMk0Fo5PKtXBWQ07aeG87WRM0JRAgkIWP7wst8M2hSLUGdy/HVavpzMLK3LMGp2FqNIRDemQmNCyaMWoUKG0ue8dcvcWm/02XqcZItw1UkMsPp/Q/FdaLn1N9WgfmLLAef6J6qjcPCdxfZmciVvfxyJuCgXw8tzlcNsLb+XWHyXcbNE+FhNmLAo/d1QC2HfUptAyjMVJJ51Ue9gYFSqOfv36hUzFb3/7WygjjjrqKHjooYegFRA3FSlCY9HmMEEe1tmQfSxU6tNGhJuVgStprx7yFWNG6hmMGObo+1FKUyjNfboiDhttFoOSsHVdels0FCi0ms4i/v6K8LGIawBKsvTLm3lbZgrVls0USmzPBVTRqooIf7phmUIFsHRNvr4GNdMzxe84r4o0hXpj8WqYs2Rt+HnHjQMY2K84bYktyE8HQ8vi33bbbQcLFiyofcc/NIPCkLMf+MAH2AN45JFH4MQTT4RtttkmnED/+c9/UmXGjx8fMi4DBw6Egw46CJ566inYUJFw3g7DI2ZjLIZu1A++ePiObCK+agplk8diwzWFUkk5ynI34WMN3EaF2jKWkZqC5WXTWESSvQqd8GrWcLMbEQ6sVnSxiD937tZSyepj0eiX3mxRoRTCDNkYKeN29fyrvh+5cRbxf5ou3KzTqFBF9FeLBCj/HS8XmSBv1uI1tc+7Dg1aMyrUzJlu7dpWr14N++yzT2iq9OEPfzj1+z//+U/4zne+A5dddlnIVPzud7+D448/PmRkhg8fHpZ517veBd3daUnnvffeGzIsLZ3HopJNuoQbjG6RqDbeafNXwePTq+o5KqqRpCAzmsF8QAalXWZJbsfKeRvcMhYr1nWVMtysbs7JHExt0OhpgIzFUugyh5uF1kL83RbiYxHbtF3T7Niea6fWxuaxkHyvZDOFEsu4uLtqgjzIBXm0KzPJLSsSayzIX7Nb01hUdBqLxmgNdms1xmLixIlhNKi4VgKT4p177rkhc4CmUpdccgkMGMAjJE444YTwT4WLL74YvvSlL8Hpp58efkcG484774Srr74azjrrrPDa888/Dy6Amhf8i7BixYrw366urvCvSET9if12rq9/rwRBaolxzYSwvH6J9BrtVKlY19UDqzodmLoEJQsdxHCYk86jkuwVqH3kaJR6erqlDH0cWwzuxxpDV0/AYoo55a0Q9IbvDO9VR3gl3yt/TFi/0aF2Kep9NEWQBe9oZvT29NTeH64BFizeGT7lqL/A8V6GBGPcOblIwtYUmAM1CNxkX2LfPd3dEEjmn6xv0lmI9RzQqRXga/DpbVfni0t5WiVoojUcWyPdPd3avdgFcA8I16fidXZ3d0EjfN/7t7fBdoPTNGFR4PRLZix++tOfhr4KEWPx0ksvwRe+8AU47bTTYM8994Rf//rXoXbgvPPOA1dYv349PPPMM3D22WfXrrW1tcExxxwTMjqucdFFF8H5558v1XwMGjQIGgExX8islfXX9sbs2bBsdVKE09m5jiX7XN+5DqZPw8yYcvbirTfftAkeJsU/n8a2suO116Yqx1tmrF+7Bu66667U9dWr20sgrwaYPXsW9PbSnRDRJHHJlEC7jSydOzO/dxUeOPk+t0lPToSFkwGmLKso76Ora33iva5YwX+fWH/2LFxnjZMkrl+72jju9V1dMH36jIaO0zWefPLJvnkM8PY83jt4++257GexeNGC2nyZM9vtO6/09jhfEyuWLSO1uXTpUn05i/W6MhTu1es88cRE6OxJr8VZM9JzEtelqb/58+dBd1d2x+u1a9fCG7Nm5bIuunt6QjrApaR+4uOP2aYxKxxI50TP9cUXX4I3B0Ku5/+MGTPgrrumwfLl8n0c38Xst9XnQV7oqPSEViqNyiG3Zk3dJMsE8sxCrcAFF1xQ+37DDTeEpklXXHFF+H3UqFGh9sIlY7Fo0aJQOjZixIjEdfw+ZcoUcjvIiLzwwguhZmXkyJFw0003wSGHHJIqhwwMml3FNRZ4X8cddxxssskmUDR3iBMIs5mjc3yEZ2YvBXh5Uvh55512hM63V8D0lbihVzFk0EawbD0yFzRsPGQQvGP0KLhzDjIXaaBPzcQFbhgCVxi95x5wxxuvQ7Nh2GZDYezYg1PXL5n2OMwPibrGYqcdd4QJC96AHqIW4OCDDoKDdtwMvv1EeqMbsckAOGmfbcL8ALe9MTmH0aKdaz/ocqEB0+Ddhx8O79h2E9h0+mL406vPSMsMHjgQxo49svb9itlPwJurq9pOCr58xA4w9rjd4Nm7psAj896ARmHEsM1g7htIRKrR3tEBO+w4EuAtPOxbA4ccchAcuMPm4efblz4HsIQeJ37UyJEwaSEvQs3WW20FY8e+K/z89B2vwqPz54ArDBzQDzrXul0Tw7fcAqavXGIshyHdZ65Uz5/+He2w1hAcIN3mUJgTW0uHHnJoGGVOXIu77rIz3P9W0kx7wID+sLq7y/guZq1ZAmszSsFR8LjjjlvCw3ms30obHHvse+Gpf9zvrMn3HnUk/OKFx6EZsNNOO8LD86r7zTv23htGbbYRXDpZvhe7MBvEfGxjj98NrnrjicTci3D88cfBoifnwN1vFkuDDOzfH1nYFE1YFCILHqeMBUoj4gT+ww8/nDBhGjNmDMyZ426DdIn776ctSDTjkply4UtsxIuU9V1pq3PJ/Traw7842pm2k6g6H9Cvo6lsMft3NIekRQRGc5DNo7I4o7czI4X069cB/cPNLol3jhwKt51xePj5n5PyI5QpDohZM9EjoRauQc2cQxOP+HvlvM9vvncX+OpRO4fPsj22thuBQf0J6yqoao1bCf06Ynss097EZn9EAjvqr73d7TvPI4wo9R7NSSSxnd5M0Ys6cM8J0v2I5yDV/0U8LzHABgpF0CJu7rJqJB4KqoFU8lm/SPzifHF5Smw0IL1vNwq7Dh8Cry9YRZp/+IzjNFAW4L7dKxGitbW3VZ+3Yp/r368fDBpQPD3Y3neuNIoe5fRJ3oWQqYgct9FE6dlnn4WDD65LX1euXOn8ZocNGxZuvPPnz09cx+9bbbUVbIjAyErxzUw8SLgRPNA+vJ/GtrqMjtLNEEmFFxWqHPdTdWSkj6VCuJ7nvVEInn4ZieCa42rFfWbiTx20HXznuN1rBH2jp/XADTQqVCLqDPPebPbHPPNY5BFG1FVUqKw5PxAqoyU5M08QPOB/sXfwgXduDY/+4L3wuUO2Z40TW8jLxyLyS3E5U+IBBBqNq08bA/8Zdxgxj0UA3Vw/KMv5WNGMp8g8FmUTQFJAfjpjx44NnaUfffTR0GQIVX9HHHFE7fcXX3wRdt55Z6eDQ2no/vvvHybfizvW4HeZKdOGgPiaQuc0MWIH23m7ra0W+Ub+e/kmc4n2REd5LKAUwA2T47xdywgrjj+RFwByg27eujpAI+ZFRwCmQmIS25YkFW4oKBldWzEqVPz1cYlDm8M+vme7Xh/9cvAqdRUVyuYsEYVI4XdpVKj0RdKzFUNmB2kBHnWcJuf1MqFMCfKQSN9m09BxgpDHAn1O3Dxnk9BJGW62Qgt04RrNlL+LbFOC/hUYDvbII4+EIUOGhFms42YQGKUJfRG4WLVqFUybNq32HbUi6M+x+eabh/b96POACfkOOOAAOPDAA8Nws+grEUWJ2tDgWmOB5iS6TaaMc7mMzA4FjQpRR0bFMkOpEAcpobHIcQJRNAVZiaKofoUzH4n3LBJDpmqD+rfDGkYW83zyWLhJclkmxOcol2axmV5JjQU4RVYNXZ5aECvtjoRpl7UjOxMovakyeWNodA5Ck0vIFy7nSpkYC3x3un1azGPR7Sie8h5bbwyLVq2HmYoErrrHPaABSera2lqQsUCzJExmt3z58pCxEG1D0SEar3Px9NNPw3ve857a98h5GpmJv/zlL/Dxj38cFi5cCD/5yU9g3rx5Yc6Ku+++O+XQvaEgvuGFGguBuOIS3eGi1hBo3hTKHVT5QspyP1yzpUp8/DFqM0vCMddEVNYDNKqve0cycw07xkJfE6VkuTIW/c2HJW4/rZx5myt1bs+cx8KxKVQO6lx3plD8OuISV+WdqNjmsVBc5xKvoXCliZZFI3OTyPZxnaYtMf+CALp63JhC4d5+6xmHwbQFq+DDl05I/a7aj4vOvN3SGosIQ4cOlV5HDYMNMIStaTM/44wzwj+PZOZt3KhTzm0WPhYYH7mZtAONGtPmg/vDktUYwrCKTQf1g2VrurKbQkE5gI+VczjWTKHE64kykBsoRFR2UyiFuZfG6d32nk3V8mZAKT4WyFM0EwFFQfypck2hbN5JYv9yrbHIQRLtjLGweFYp5lvwiVCVi8qaIFYLrDUWldzz0Lhc/mU619uNVhNxHwt3plD4DDYZ2A/2224zeb9QMlOoNmgaNNFQPeSmUJVM6rKqxqK5TKEatSduOSQZMWzzQf2dmEKV5RlXbE2hUpZA+UlkuSYaWc04Iq2Ibs6Jy8dWY6Gr+OOxu0M5nLdbjKsQ3gPXL9SGQIvPSfcaizxMoRwxFhZMfkobWJGvRVtpbmgKJdFYfWT/kax2sAXXGc9lfThrqyRnTjS/yKZQ6GPhyHnbNK91mbcb4bzdXiJm0ATPWLSaKRRzx0BJQb8mM4Vq1Ji23HgAX8JLiQpVkkdsbQol1CvqdijaCEpIWlofFYbGgupjYX7+nzpwJHx65x749IGjcn+uZOftFlNZxF8X12m3fD4W7meJK/MqG+JfVodq9kQ1hZIV236LwXDjVw6BH43dgzTOqjVozhoLh22V6VxHAl+3Z4qmil0ONRY252GlQf6S7SV6ZyZ4xkKC8ePHw+jRo8PcHGVD/OAL4+cLEiobjYVODVlGJrlRnLvIWHDRCPVpvhqLSGWhbidfjQXFFKotf1MouuJBu1ZlfRy+8zA4cHgQ3kfe5wrNebu1o0JxiUOb+Z2ICuWYXSyzKZSN86l8jVBNoQhQmEIhDtxxczhp321p4wyjQiWvHbbLFmFyzTKiTIyFaX6Jzttxc/A8+1VNoEaZQrWVkRhToNyUToMwbtw4mDx5MkyaVM1wXSbEFxVuDuJBYuNjoWcsyjeZy8JYcB+N0hSqJF4WXHtxlSw/fj/5+lhQTKEqjkyhKmRzK+o9p5+bpP+EU2Ol4YwF2pG3mMIiMV+5NEtWHwvXW1mZnbdt1qIopcV1KDWFkkWFIrybqs8GvX8d4j4WGw/ogL9/8WA4ZT+eSZUOLudKWWhUk7ZCFm7WlfO2yUxWHNW+220KV3zugHA8FO3uhmwK1ZwpjDdgxDcvWZg211GhyjiVGxVFSTR9YjMWyqhQUApwh6HysYg3lCdjSjFzyiLBDe25CeFmbd040jH69Ydf7hoLQlSoah6L1uIs4u+PncfC4p0k81hUSh/tp5HhZqVRoaTO27ZjSn4XXz/1PK06byfHidh+2GAoI8oSiZDyfJMJ8tyFmzWaQgk/X/LJfWHkZoPCz/2FqKhFoK0k74wCr7FoMsSZdSR6RKktd/JhfV3YzrJsQGWwNcxqv6w69MvyhLmx2KO5pvOxyPPeSM7bGSS48XWhjwol3r87H4usPiIcUH2GvMbCnjip1smPWcS2nWtBGpggTxoVSlZOqrEwty+aVgWW5idYKs5wR/WO2m1LOGX/kbD3tvJomhy4mitlEnxT5lacxEFTxW5nGgvD2hXPtdgLUAkJ80R7mV6cAZ6xaHLnbZHYbbcxhdLEkC4hX9GwsGsjNqlnBx2+8QC2CZNSel6Sh2wrEUlHhYp/rjT0UMqSMIyab0B03qZOC0qCvPicyXuWUEyhoIDoN0UjPo1swoxy52iueSwIpiV5RXP6/vG7OyeM5FGhZBoLS8bCkSlU1Xk73m50vQK/+eg+cPs3Die1o+1D+K4LE98skm+KOWtbyhSqMRqLePGG+FhUoGngGYsmQ9J5O70wuZOvak6l0ViURp7eeC3Kh/bdBnYZPiQkwMZ/ej92fZX0i3M3e269CXx0/5G5SC+sTaFS7cRNPaBp81hQJbW2ztuUaLNJsxkoicaiuTmLNLFayc0USra3xvt3/UqRCXDepmbiYaS7v37+QLj28wfCQTtu7t4UKqWxUGiBXOWxEN4/3RQqWTcP4l1s0VZqXirGgm0KFbgLN9vOZSxiGotGRIVqK897M8H7WDQZ0s7b6kPSRYK8Eu1BDTeFws3k3m+9G9Z29cDgAR3sZ6M0hWK0857dt4QfvG8PeGnucpgybyW4BDdkYnRw63wF8nxTFP+JLJtxQlugNYWydN7WqXpkY8iZyac6JLqKytIo4DqM30P8sXNvrc1GYxF33m7LQ2NRHPGHvxy525aZ28mi1VNdp5lC6VcVlQhvS/lY5MBYVNLn0UroZrfjyGXGCSiCn1QeC0caC9O71Zn4NiSPRaWExJgCJZpiHi6ct7lTDzUeusVdRia5kRsjEgLIVCC4j0ZF5FZKIm1iR4VSaSwKc95uy9V5m24KBY58LPRjyBtUjUWT8xWpPVOMk8+BaX+UmRElNBaOXy+27dwh3FHUQKtws6kqGBVKz4DXSwJfYyH8TtZYCGdzHtue2OTAltBYEEyh2vJx3uYmyIuvq0ZoD9rLSIwp4BmLJkNc0tbuwnnbEG62jM7bZdoY3WgsGIdzjrfOfay18hqVcZ6virLRZnE8pUZksg03S/OxKM4Uimo33HKmULHPXFOoUOLNmBvVa3FTqBw0FgWuM86ctEqQJ/GxkEZPa7cNN6u/B/L2gXksbOplMYWylJqX6fykCE7EcLOunLdTvnEGNJqub2v0ABjwjEWLmUJxpfnVBHnNM2GbjXN3rbHIk9HjkiT1qFDmMs0YbjZB1GvukpLoToaUpkcaFaoY5+3dR2wMg/rTLGO5xHfZoBPG2Jh56d6LbPrFCRrXWxm2nYcWRIVK3lGhJEyglLFoc6OxEFUW1P0WS+XtYyHekK2df4n4CtKcaBd8LLpcaSyMPhZqzWYj0F6mF2eA97FoelMoUVrqVmPR6MVU6jExx6FkLCplMYVilq/VEwnr2PdK84abjROgeql0xc4UisCQuAz1KSOax+ywGXzl3TuHyZ+oe0ezm0LJpOARuDxTVYIuhASSJHSLP7OEj0UOUaHyaFOFSsGmUNif3BRK0jahO2wrvl5tc7TgkOI+xXlse20brPM25KSxMDAW4vcGP7b2JhKoeo1FM+exqKST29nksdAmyCvhXC7LxlhxlseiJKZQ3PIVc3SjPN8VKSpUlnCzxAzJVAdTm6hQSQfyDEyS4gbw+RwzegRsMWQAeW7FI9M1I1KMYOwrPyqU2flXJAhy97GAYp23XbRDldKGGgtZOVkeC0L7+L5dvIOq83bcxyL/M2qgpcaiTPQpycdCeJaunLez+Fg0Am0lem8meMZCgvHjx8Po0aNhzJgxUHaNhaht4E6+UGOhDTdbPpSFc+fuM0oim6OxyPHecXxMC/PY/2NXi1FYEJ23s2gs4nXV7dhqFShMlyszRdUY4xJa6sHZcj4WsfvmamNUpjn1ttPPNT6vKk2Rx8JN1EC7zNtpJlDuiyQxhSL0t747Kf22ndrYU7xqLpZQlaD1NBYkH4vk3uPKFIpLRzSa7Ghv9AAY8IyFBOPGjYPJkyfDpEmToNzO2/oIJ9TJqiNWTe3tvOVgGLX5RrAhJMjLaxPlvLFKCZ23U6ZQBR1iJOftTHksaKZQtsweRdVOHYPts4oTUtTbuOuledDMSO+ZWZy3Df43qLEQXlz8XTj3scA8Fq7b1DTIYWLcaCzkjJOsbUpvnd29ToQfDcljYem83WjJe6Y8FgHSQL1OzgiTz0J6f260xqICzYImJdE2XKSdt+0i0tiqA0Wc8/49YZ+Rm0KRaPQCdx2Foiw+FlxUCBtwnsMVs87L8L69trZvnxhuNm1aQ7vptAlVJT+NhYIbD5pUIpaX8zbbxwJnv47pDDNFqwmaPMyWCjWFYnTmKvN2hTi/KWPr6nGlsUjmsSgmKlRrmUKpxhU/NvERczJv6+ZuO9t5GxqK9kYPgAHvvN1kEE2hRHCPFV2MciqxXDSxWxbimjsKlYQkbx8LlGyhZM7cNlc13GcKJVRLGBDl+Kp0c/c7x+4WatL2296e6U1oCyzDl+pAeTau7PGVh1JQvnUlw6Wf3g8Wr+qE/7v1lcxt6XL/2Gks1Kg6b2s0Fo6JhWpUKMfmVTofPEY7Vs7bsjOOyIBT9lU0hUqEM7V03gZBY1FEgrxWyGMRXwv4uVfCNIiMP8d5G9d6p+Y3HSole25tTcRYeI1FkyFhCoXO223ZfSx0MG2QeWR6bZawa5UmiQpFzRIqNv3pg7aDG79yCLx/760N5dNSxfov+b0rHcHzzaN3hZP3HZnNeZuYQ0IXZUgHk9N3//YkkZjlWfYj+VhAafHePYbD6G2G5u5j8YF3bu00jwW+MpEgSMwrgByiQhWZx4LemY3yLR0VSn7GyX0szO1TBC4UiJG/cvGxaIDGIu89Ib4WVPMsvk/+v/tfg0dfX8Rov806j0XaeRsaivZGD4ABz1g0GeIRWfDASuWxcETs1tsz1y9cY9FqPhasjZ7/rKkHUEXCkBy44+aa/BtyjUW8pTyFLLqgAy5MieLEij7zthuNhcg4uMy6rVL7J30syn1wuRqeLirUt47ZDT5+wCjYeuhA2pgMDF8UblZF0Dh3tM4j87ZmnXHWt43EVSSmxPCwWde56LxtCxxTMkFe/mvJWmPBeA+2kads1qJqP09FhWI4b+vmhdm/Q9zXG7s/tnuNhUde6DVl3ma+Ua46ULZwi57uzbTA4jAR6HmaQlGwpqtHamOsMg+pOW8rrlc/Vxo6D/II0WrjK0Gr50YTIoPq0I6/2zIzFqawrhyIzzV+34MHdMAvT3knnHviXqS2VFGK6m2nn2veeSwqha6zfJ230zlyGAnyKFGhXPlYpJy37dox9eFGY1HJnXmhIk6/qIQfWZ6l1nmb6V/aaLKjvYmo9SYaqocsj4XIkbMT5Blmq6m9PGx6TciTAGKpiV35WJTEFGrRyvWJ7xGJYjprdT4WuWoscs4YTzWF0tnss8w8HLXLigqlGU+ZUCUo3QwwlVQ0g/CiYliTocZCaCuZmwScYmC/dudtlsp5O4wKlS4n0+5Reqv6WNS/2wYyxTY+fdD2te9fPGIny5Y0fVju6yI4r8GWebGZW8o8TxkmtM58iJsgT1znR+w6jDSGNkfr0ZtCeTRNHousplCN8LHIk7HAg5kK7jBcaFpsbh1t9SlYsHIdK29BRDDpJO+Nct52H7FEQzxaahZk0lidDXCmA5YQbrbM0dZwZEVoLOplaG3hu9fK8yWmUCM32yg3Zm6LIf1zyGOhYSxy3relUaEk7cg1Fua1mGIsMkSFOn6vEXDRh/eGCz60F3xkv5F2DWn7cBNulvMe8rYOiLf/8TGjpGVsh2DK6cLVWIjff/vRfeAzB28Huw4fUsg51VZmyY8Az1g0vfO2+ZDMpJ42aizSB+fGAzrgyN22hLyQ52bHkQLxI3Bll8jY3Dk1kdLClfL4GarDtqIYU/K55Peu8pbg2GosqPds8rFwyTep5l6zpLoLTaEqOT2Liv2aDEsxTKHQtGSrTer+G66ZgGFDBpQ23KydKZTwXfG4ka+wcbZFUygXxmMRw/PJA7eDzx6yQy5nVCq4Q0cbnHrI9rARQxgma8dVWRvEBaNnvGdXOO3QHcKgIXHYPksTIW42AxeZ2uT34ZsMhJ+dtDecsr+eiexwNBe8xsKjIOftdDQM187bFVIUkmSp6798cBiVJy/ka16T35JQh5vlgyNZ02ks4urcb7w3+c6i4SoZi4pZ8m56VwfusDkcN3oEfO6QuhlBaQ69RLhZe8mXrY8FxWQnswNuk2TRDgk3RySzLkEe+xA3+H6E2exjj3iHLQYnCB7XUzhkLBw3qpvfnPPGhfN2GGVLobHQCzjk6HLlvF0A0Sf2gM/h/A+9A1467zjYZyQ9YhrnneV9W/G5tVH/djjvg3vB94/f3YmFAs4dXVWuxsK2XLsjgqWZNBY+j0XTO29XciWkKYtG5nTKXUxXfu4AuP/V+XDDpDkNNYWiJF2zhYtws661MMfsOSKUsqEm7NCdt0j8FhFESudtxcHNcd6+7osHhna8F/33VeAibzV9/uFmi7s/ldSsOdgKQlhXxfOLa3jr183CGI6PhW6OYzPzVtRNDHfacrCx7yzYcsgA521qM287aofnYyF7p31mL8JeVTHMcdRYJE1V7FZEI0i+iNAMx89h8Fga8nzvTJp/xCBwoUIm9Ez+no32oT6jfo6Elc2ksfCMRZMhHsUCJ2w6j4VbjYWpvVBSJJTBOtxFcMzoEfD8nGUN59xZ9pDMYag2sry3C9094bMcq8hTYTpq6xoL8Xr9gtHSru/uXRAdrkF1shXHTh1V+gCtkMOiOjOFagLOwva+VYwF5bnSmUM9wyO+0x2HCYyFYwXpJht1ON9PdIwTK4+Fq6hQAS0aFjWPxeABbqJC5Q2xj/jjrDSpKZRsToiXbBnl0P8pi8bC0qSV208rhtlvoqEWh/Hjx8Po0aNhzJgxUDas6eypfR7Uv12Sx6LYcLPo2CeWQXrMZhFQGYY8OXeOPWTFkRMkS41uQ4Brquhu12gKpRhShbE5R3Vt7FDzDo8aHxPHFIpDlCbqib+n2rW/XxVTa51puECo5pntPiGuQ6nGgupjYcy8nfy+47AhuUmEo4SKZXXCtzOFommIZJpzkzap5rwNDqJCWdbL0kd8jnJeOU9joU9ayU0oSdmXxPFZm0IZAiuY/Utp/ZjmWD/vY+GBGDduHEyePBkmTZoEZQPmGogwqH+HJI+FW42FaS7L1I2hxsKFPW0DOHfOuF0ROnlvF9pY3qSbUOWxqBid3EzNV4hZUBtjCkXUWEjMNSgwSVhdOf1Ro0KVFdH+wg6WoLhnynN1Zwql11i4pBU2G9zPeZum9jh92cxnWZZ0WZ9VjYXIWZj72zYWoasZfSxUv+nA2Wp19/Xh/baFP35qP9h4oL3hi2xOpOmJfJJFZvUvJZu0trvSWHjGwiMnrF3fndBYZDWXiCQGN331ENhjq42lG7kOMkmRjSlUtR61XH4LrL0FfSxMcfZNUCU6VZpCScqY+rfxFcp7n41rA1mZt6nzuM0gkXSYx6KjiRkL1TyzPYgp4WYrzjQWyV93Ek2hHC7+zftseooUbLKct23OBMkaqKjeqUxjoWl7yIAOuOST+7LHJB1nAc88dc7GNaockzRJWXT633LjAbCLEDpV12otz1GGPURGdOtMa4t13rYTEHF9OahwKWjKG56xaDKs7jOFQqIHbcCzRoWKCLoxO2wOd3/r3fDd43ZjLpq0xiIMQWtj2kKs06yMhcuNgWPCoo3qogsl2ffvZoP6a3/XSd5N7yr62YroyJmCij83XU/OTKEk68gVVIybyjG/TKhpxpiPQ/X8KMIYssbC6GMBcNTuW9Y+bzY4uZZcTuHNI42FYx3o5sKYC9VYSHwsZOeNLF+IbmznnTganvzR0bDXNkMT606Vs8eMRmgs7HqXEcxfPXIneOpHR6f97QxzO+seEo+8J7abdR/MagpVKSisrSt/1zLBO283Gdb2mUKhGZRc+sZrz5QHg6KxkC209t78CMU8GXeejwVvIOoNiNMnH7qNj3K7Z52wRxixq6unF9as75EQfGrJutEUKtJYODCTaJjztq0plIRo0rab4XZdSc0agYrtelP5WFCiQrGklRrmvAJw4cl7w7+ffROOHb0VeYw2iAQArpbF+9+5Nbx39+FKwQKC05WVsCm1t6S9t6N1kjLJ1KyZAf3aYfCA6hkaL5Il83bRSAg+GP2rXgPuR92x4DDhNU070W+ZNBYEUyjbZ1vNbVLJP9yso3OqQxFsgttOGeAZiybD6s7umhmUDFmjQqWkzwSCRWYTabPXqIIX4YLr1iw4l+Acfnyzs0pDDiUdkUTZrFBF/sTZR0N3by/sfd69BI1FhT0fbTbNvG1OE87bumeoswXTwLTWvI9FNlMo1fOjBLwgExUSSXmy7Qpsu+lGcIaQI6ZWH9xhiz7Ngit7//Gf2i/8d/maLmWZSsGmUFW+oiJ9zxwTmsTccJJ5O3+kNTKx/QmyvYdI64DO7Ko+REQ/ZQkAIQs3awpqQUWVNtH8bjC/JfdbcRNutn97WxilrBUYi+YVY22gWNsnMVYxFtypZwpXaxJ0SvNYhEmh+JuNWsJYKYwYYmksKo58LCBfuEhwhcmLNh5YNbWIUKuqoau1Eq/Yj3Y+FjkzFkSNhTj2iu34Uwy6/rubPBZBEzlvu5n3FD8ynvO2+nezKWDy93PG7gnXnDYmm48FZEc8p43uDOD0ZaeVFPpDxkK4VtdYpMemIriz5n8qhcYi3ilLY5EuHCkqUCtNNSWKOs2msZBFhQI3GgtFMkVd38l+K26ct9to7fTT5Jui9FMmeMaiiYDE+hrBFEoEV1qVIopSRGKF4GPhRpKs2gSwj91G1J3KIhV2q/hY2GycWSVUlN+MY1ARfBWixCvjOFSamKP3GA4uEJem6UZnHWfdoOhwSfw0tcYi+pf5OFTlnUaFMmQEN41Z7GbMjpvDdlsMgkw+FhmmzTF7Dod3jhwKv/noPrEx2t+fe1ModZADTmI116aBeSWSGzKgA/7Q52DuzsdCvdd1xSwDwkhbmoZrGosMe4hsLabPDLtni/NCjMJm6tumV3OCvAqpHdN4mklj4U2hmgioJots8FCCLAPbedsgvTMejIpwszZ7jWrd4IK66tQx8NcJs+A9ewzXZpJuyqhQHB+Lio3ztt1v1LGkfSxiBLn2YKr/aJXHQhj794/fPZS4ffbg7cEF4sSHPoOrWQJu52PRlnuSRHEWXXDSO+CP/3sdlqxeD1095eA66s++4mS9UQhc6nQMGQuHGgtZojcqtt9icGZTqCtPHcPaEzl9uchVIztbaqZQ4thCuyme0MB2xufhwoRM3p8/e0D4/Lu6ugxRofjP9K+fPxBOvfop2HRQP/jkgduF17pipjhhXhQd0+xA60kRnthOZ9w/37P7cPjvy/PsaCViv6Zp3UGcHCaTKe+87ZGrGRRisJKxALc+Flppt7ySbSIXZRSX9jYYtfkg+PEHRlu1yxtDW05aA/Wz5DwuG+kQPtdPHbQd/OPJNyR9Z9BYCP/W2wSatFMYIxfiPEPzjX232wxcIX7oaYlHy7CwJu1gKjlYHqZQwoRCpgz/zr31ZfjrxNlQCtQY2HyiQmU5xJFo0GuzTP3wYu/LsNmgfuG8P2LXYeF31+QHdQ0b29E8DJxzby1bCw9MWWAMYKDSWHAcBOPEnouoUHloLHBcush0yTwWjLOo79aP3G1LeOT774FNB/erWQHETaHQNIciGMri/khZi7ZPFun0I/sistn5WFAFRPrfO4gaC5Nmo5k0Ft4UqomwOpHDosOJujntY5H8XX9oVpR+GaM2G8RmclRjLzJ+c14+FjqpRd6CCHw/P/3gXmGuko/sN5LMBBoFOpEplGbO5OpjYYhoFkGnDtchKdWssEJiUtBWqMZCxVjIy5cpe7OdvkKTeZvwXKmHOGpPOQnyTL+b7drTePrHx8LVp42xDstrgl5jwWhHUxg1ZSftu23qevo8SifCi/ZWGV9RoQgNwAFyWC6pfUX8PTaNbTQWCDS72yTmOxfXUpoI3ejX4RtXfXtcalIT/dgKKisVGLHJwPyjQhkKtlN9LFpIY+EZiybVWCidt5lzL+1jQSeSaqYwkgWAB+693z4SfnXKO0OJmkvn7TyRV196B8hK7veEGzjmKhnYT8zU7oLgU88ZqimUzaZJTQ6JRNf79toKPn3Qdsz2ieFmrZ23QVsvdX+wYSRXUjGQXAKjCI0FmovoYByzVGNB6jpRJ08CRPe4OHuXiYaUDVvGfKcZcPn80D0GaqQexLePSeZ2kiGP1WUKN52ICsUYgG5Oro9rLNAUiiB4uuwz+xvXQZbztpJx31CZxrrzsXBjCtVheIZZzJaLRhMN1SOeQ8BVuNl0Hovk73obS7nGItosMIvnxw4YBQP7ycfK8bFwCd0mmBcBpt1cWIcCv++Eyrzi0nlb3mZ8zlDNKHTP53cff1foP2GbPwI1Fpd9dn84/bAdwD7cLDA0FhVHGgs7TYi0L2VUqPLDWmNBjAqVRfNbJb407VR4c6DqY1EuJpBCXFLA1d6o3pVYTBluFv9zcKZ8/T07w9WnHaDN0p2Hhs/UZHzf4ZrlqhA3har6WJjHt8+oTeGJHx0NPzvpHZCLKZTlo43aPuf9e4bM4ZffvVNDNBYdVO2n0RSqecj15hmpR8IUaiOVKVRGaVc65CWfsBQPCGpmzuEKtaVrYn+Axvk7L42F1pwA8oXOFjfL/aoIBfqGTBsHmki8X8wIa0F48yOmxTQWmrdkrVkwEQ4uGQtFZdXaLJPWvRZuNuveJrSnrUvsDM1F9IyFiZjOrrFopBkby3nbIm+AVGOh2MPE+nqNhVxoIFsOyDy+d48RsPVQtVlNHseGyf9RJzDStqspnPCxMDDN8feA2dkHD6AJENmmUJYnZPR8ULB55jG7win7j2RpEqj9tjnzsWjT/u44QnKu8IxFizlvcxeh0ceCxFjomREhmacSR+22ZRgqFG02h8RCyrom9nXxonk+FoxD1VFklQgcH0OdLa4LswmdCYJO8hufqzbSI27ISLY2z9J5mx5NRCQc0tJr3e8cqGoqfSxKJDWvm1xWnLxvyhKnTq3Qx0IzLlM7Mn+BrEuySKs3TlfmCFmSOpI1oNLsUZgQ2d5xwjvqQguMOsgZX31c7mHy3bIVYOvOm67umI9FR/JZp+ZVaturlEpjYTIRdKex0P/eQVyQJgbEO283OcaPHw+jR4+GMWPsEhXlhdUxxkIVbpaLdHIvw+5BSFwlLgCqxgI3vKtOGxNmecZY6jb2sKR++ggCGfAg+/0n3tUXxUnfzmGxJFIm6AnsfBHv22XSNZWPTfyKrvV412Z710pmiT53X+5nG27WVtKVIhz474YrpVeFiiyTxiJ6MEVqLKjrwijVtdJYZHv4Rb47U1IvHmFUIRHXKfOxvjNMZ5KpO/e+dtTOYdS8Uw/ZHj53iC5UtVvhkAnpMN46jQW9f91r+MqRdXOhM96zi1b4k9q+KuXy/RLHa/oOlvdj2ivaLcPNps3US7Upa+HDzUowbty48G/FihUwdGidwG001hKiQvUww+WluXqO415URtz8k9+j3BtUIEEVr+OaU8fxDWhvg/WxmN3xxfyhd20Lh+8yDGYtXg0f+dNEZTunH7YjTFuwChat6oQHpy7MoLGAXKFTfetN3WjEfloLEv9MY6jsNBY8gp6tzaMmyLM0WZIl/0r0b9EuPkfZelPVHTJAHlihTEeY7fJXzXvScyS+xKrGQg0jKS3xsejLgWoN7jzHIYzYeCB8NUZUUmGyC+cQkbKfRZqsIhVkRYXo6yVOxKGpzM9P3ls7NtX44uNyjVTW8dTvtP1JhG5Pfu8ew0OfNhQGHr/XVvCnh6bX+wjrBU6FVPnmsRAYCabGwtVb7WdpCoXfu3t7kns7NAc8Y9Gkztsqe0YuEW8iXvSOezRJYq9FoOtuIQOoS+B4Q41FZ/q36JDaYsgAeGPJGm072MavP7oPrFjXBe88795CfCxs1M2JA0giIbWFyscm/p0qzbV5x1yNBTeRk8oOW4Tt/ExL/LgHXxr92iqwXtaX0Db6GeFe8dtYhuVkeSgNVPPMBJWgkKSxYDhvZ8lsn1qP6LNRlx9ZgTttTtlvZLiP2YCjTeYmC1QFRqA6b6uuxetwoBW05LBeTPtb/GfO2tDGEalUkmF/Yw2bTLPsNBb5+ViI4xW7ckVXmDUWFSsGBL+v7Yr100SmUJ6xaFLGYiNFpCUuEZ/evPRS1GTZ6r+m+c7VooiMBYfAwqKmR6AzhUr6I9D6pZRylb3WtfN2ls2qbgqn1hxQz2LTOGTtcBkLLn+rSqLF1TyoYFprJtMDXnb3JNDcEENLquK8lymPRd15u+LIFIrSJ62PcB9xGBUKiaHM/i3M55QlwRmHsTBqJYnJJ1UMuIlRj8PGvFY3+jzMVIwa48S+7q7dRFlGFD4XIcOlY2hyH4t+xLkmOrKL9ZrJedszFk2ENTFTqChTZlYi3kTcUAhD00ZF9bGIo6e318pJFzeL3liSH63GwkRMOtyoGxkVSqexyJJVt/7+hetxSZqulUo26RHXBpWbVZcazSOdC4bWvilQgiliG+cQE6tuNri/th3ZPmCZlDgzKtYR79R+VOa6dCljFoJT9s6z0qjc6jb7s5XGwmQK1cYPyRyXTKd9LNTPwkYbWLDCIi1x1/zOIeo5ZXXaZ9nzziWPhSPGQvzuLo9FxU2yzfY27bnSTBoL77zdRFjdaXbetjE7ioNjN1lPXKVvM8YjkNEdYw44WZkpmyZuBKpcFjoiPAu0m5hFP5y3rLPFpdqSa01UxOuxz7rbjr8rs/Qo/TtXoh/kRDSlM+TSnqnJN8MmjK3OYfnKzx0A795tS/jbFw4yNyQ04zqAAgc1k0vmQlFtGxRBACdBHsVcVN0PSPJYZIPYpi5MalbGon8HfbSmvYYWoCFdpua8zfCx4JwpuvFR+rJFWrDgJtwshz5N7uUmIaSFcIiwr9hqg9oK0li0uYoK1SYyPqLGonkYC6+xaLHM2zZmR3Gk5i6BHjYtfJsx2TpvkxiLCsAAIQO1rC/qEU8j+NQbKIeUsNlb4s/E1tE4AkbM+vF/XoZPjBlVP0gkdtCyzyLiP5ljiqch1jH6WGQMbEAtR1ahp77rJVTbbDoQnp9jqbEAgGNGjwj/aGOrpAhoWbADDtAxFE2vUPjxz6cNNxIfi3ya5WwKxYgKpW1HX19q1mOso9ceiW2edcIesHBlJyxb0wV/fHBaw0yhjARWxe5d1TNvp5tT7T/xiG9uNBbuiT6TRiB5C27PyPoYYmeHgTawEaiTTKHADiZnbWpwEhNMj7ODrPlOzknRqsKHm/XIBWti4UIGq6JCZTv7WfbidUmiHiM326gw521KUSziUmNBKac7f22YBU6VhN9I6jde5xgx64WfHAfnvH80aSxURY0x3r+kHXR0VbeYxnabD2bNJSrxYc1YGMw8xLGee+JesMXg/mEyKupYbNXoqbE4MPDdbvNBcNGH94YT9t7KaizcEagIB1KCvDZGVKhKBlMo8TvBx4JrwrHJRv3gi0fsBIfvOqzcplCSZ0V5VzXnbeG6jnB0MZ+TfYFzpDWhbjQWrLKJeiYJf7miQonrRCfxj693XTAA+fjchJvtLzyL1Pg9Y+GRB9Z0xjNvq6JC0TkLTEYngmODH81z0+Y//lP7KTUsKnTH7oOTyp5CQOFGoPax4C9eimRDr7Ggw4YGSLwfxvtVtmeMJBbvrkKMCsXPgsoNx4rv/J5vvxt+Q4yAQyU+0hs+rZ5pqolzBqX9E85+L0w8+73KOq4inVQyJiN06YRdm6PMW1MVt3He3nhgh/Rgr5ouafZIw2OTrT/T4zERGKk8DwrCu4YSO29TiKm6xkLyLF06b1cKZiwM+5ttuFmOSU28qDkQAbBB21ccmUJpurrmtGrOsoH92uCLh+/E6tVUroMRYU73vZlMoTxj0azhZjNoLI7afUv455cPhvGf3o8tRd0nlrguWlKm+f6ObYfCU+ccA//6+qFARZw/4hBLdEljeyFOfa7zWKDUlwudeZcL0/k0WR3rT6epUYxRWrbi5l3tvOUQOGX/kbkyFnTbXLGeWcI2oKM9/KOORdWXCWJxaix2HdpsNQ+1f7PdQ/06TfgQx4Sz3hv+xddf5F+hJzj5UnozoWJM500mvBEqs1DXeSxMxLxseDSNhdr8T13HLZHWmAR58bKQjylUIsIffy7n6bx90ru2gad/fEwiqV+ibYZPBfqe3fvtd8MjP3hPLbAF9Z2a7rtDWCO/OuWdinIiYyEyRs3DWHgfiyaMCoXzGDnrCDjfIsshilobN+KDdpJnjTZN3ZGbD4IX3lxe67c6HvOEHzKgQxki16ixcO28jVGhFIfc6K03qX0eNmRA7bOqPBU6aYOJYDrrfbvBf1+eD3tuvQkctssW2Zy3ORoL4mPXMaO6JuLlrKSZQr+uoxaRTaEY5oOJchWDlsCCmFeGm2VL+yvOnbdVwR5wzSFx+9wby+RjqWk6Mg+hOg7UWKLPSE9vmM+Dgo0H9gv/4mNQaT2Tfel/l92T2bSCt1YiwlvWLN7DD47fA2zBmRemZy133s7gY4HfFY/KedJVKD5Bni6MuA4s5SNxL6cVSEMlsLj6tAPgzw/PgNMP20HZbFdvkDijRYh0g4k22G3ExmADo49FW7LAB/fZBnYdPiS0Onnf7x5Vm0I1scbCMxZNqLFAAj1++MRDrHIT5IkQFx86/akcYLkWChwpUY+1j0WFVEY85I7YdRjssMVgODbm3LrV0IHw/eN3h/9NWQDnnlj3KbDzsbDXWOyy5WC49YzDwRYJ520DY/HFw3eEKx+bGX5+7+5pUzmaxkLdvqqk7B3jPP/ph/aSdoLNpiU4bjkLe1MoGkyhNO20ZyqNBbcd9xqL+n6RbOuuM4+AecvXwcEXPWCox8O3j90N7nllvvRZ3PnNw+Hfz82Fk+PJwAioSJ4JJ8cJibEwjMHoY1GRz2Ox2l9OHxNqk3XEmQn9iIwZhbGQ3RbHxyJdVrfn8uez3mEenCN1PxVXPhYcjYVmPDlqLN67x4jwDzFz0WppmW6DeUYqXG9Gra1tuQ6Bk8NM7/tutxnBFErUWEDTwDMWTchYDBLMoKoLJkg5PdtAXCSH7TKsmkq+NwhVeA9OWSD0SydaKKHlInT12CfII2ksYocc+n9cpwjBOe49u4R/2vYIJI+OSDVtTFlVoHpTqEqKGEPp8dZDN1JqtURofSwqdhqLM96zC3zrmF1rc0Yct4y4cq6xaM/beVv4TrhHE1RV2GZEwncnGgvNfkFRnHGJwT222gT+/fVDw33z01c+mRjHriM2hh+8jy+pj48heia6UdkQY3n5WIgjxaAaVKbij5/aFy6+97XwlIkTepx5gQSVFpWsPhZCc+YAW86QR1QoUyAV29DorHCzOftYUOaPqlmTENWUx8LcL618Vg0jPUGe11h45IDOvnCPouQnPnGz57GAlMPoreMOg/kr1oXhIh94db71ga8ilGTVbTUWJJtNwbSpK2MoLcrtUyX3MmTdUOL1dUxAlHjx+0zTiHT8eDUjk6ynS3aX3GhdSPO5oM47U0ZaW42FlWO9og67KZE4dehjIXs8uvHVM7zzIZMMZllO8ao1xoLIPFN/zxwVSrFW0muf/iA+8M5twr+rHpsJF9wx2crHwmQ6JrtvksailsdCbK845EHzmfL0JE1OOcK3fHwsbJirLD4WBxsEX+nM27yxkQVEht9XxxIb6yBqKFJnovex8MgDkd+BOAHjhGPWPBayZYLqcvxDxJuvsDUWFSsfC46mg+ZjkYwKFdeO2IByV1lMobIS0fENSSf1skZK8s4jGMPPKam//lCVOWy6Tgydex4LzXOzJeaVGguuGUDquwvGoqJsi5J8zKWPhTViVSMBjz57vb4vaV3TfmCYF2nNl3ycNgILsQZVY4FEkokhkppCZYgKFWosHFL81L3MFcRb1wpXGN27yrydbhdyymORLvOR/UbC5w7ZQVuPm7fCFqbnuWTVelI74lpqZh+LJrLa8ujpI4BTKr72/DQWIuKt1+Y5ccIrHUuBr7FA3wdksNJcvXkcFaLjJRW0TL768eTKWGgofRcHYqoF4mEU/80Us1uaSCxHU6gwjKjlvK60oI+FC+gk/Lrx6RgSG2SZ81yNhdF5W3bNUMcYmrlCk+jbPAexikgMqcKKVxMJGhgiyXgoxFRUxsQM456/85aD4Rcf3hts4NrU0mXeD86b5PljyPuTl62wGQiK0FBs9uKP7QO//dg+xjM869lGfd+mbtbHLCJ0fkbiWhIDxvg8Fh65IPKfEA+Wg3esqwR328ouskEEEzEVd95m+1iowgJK+oz7isgWFPo9vHz+8aFPAHszEXwssoJy+1k2OdnhyskirYt37oSxSEne6xf00lz5GKv11GVl5RGBQ50FR1uQigpFfKbpW8guUVb7WDTeZjwiOuXENG3dukCW8znhY9EhJ2gTfRnNlirOBQ1im6qEXzbOoBWD8/ZfP3+g1NEfr+lCT8vGRx1jLd+LRMIfb/MD79waHvjuUfCJA7eDpjCFYvjncKTx9qZQhrI6BlvFWFgsRqoFQ1a3MOp5Ynqepx26Q+0+rzm9mi+DcuakLFOaiLHwplBNhEiKL07AC056B8xfuS4Mh4jh2X7x3ym13zBTL5pHLVvTlSkXQoQ4PVuPS19xTqzF+1FtPrJ4/hRirOLIGbXWXiVfU6istpXJqFDuN6u0j0X8N029WEGR6TTlU8lbY0ENNasaCwU292h7yGXNvO0CUdAJGRGk1ehFEmlHY3JlElHbQyoZ1naFPz5u5u1oHolzw05jIWgBhD19zA6bw5M/OgaWrlkPR//24Xq5jnZ2FvLMPhbihaC5nbfTv8f757Sbj8ZC97vqXCaZQlVsfd80SWmZr0ureTfU3WbTjeDB7x0V+lpgQAkV+ptMoTxj0dwYP358+NfTU09IVwZEfgfiwtpy4wHw768fJq1z1gl7hAnB5q1YBx//8xNh/otvH7ursg/T1I3nyWD7WDBMoWw3CJLgs1KBfg4XKc0USie5z9l5WxM9xImLhaZNKvEivmJj0qG8TaEYTLCJQbCN+mLjY6Hqm9tSmiYL3GksZMS0ZoR1AYYbZJnz8RCXESGga87GfMS8H9IJTp2m2EUyUJmAZvPB/Ws5l+IMiNEszNYUSuVjkQu5L0dnt3tawSQMsI8KZSsIsf8dQ7fLwsbSnLcrdoyFZED/+NJB8O9n58Jph+n9M3imUOb3NIogzDVpLJqIr/CMhQzjxo0L/1asWAFDh8YzTTcO6DsRWQexw6ZVKmH40Ie+d1Rmh7aEjwVzo1KbQunruYihn+iP6RCe9yFh1lhk7Fsj2XIRaUJnCkX3sWgzmEJVCjaFsn/o5DCF4vccJMqxH5jtgHNgQqiwbVl/ugztEWPhaFBZzP/igR4ic8osPhZyjYVjU6havg3IPr+E7yrNrzgGNJmyyelB2Z9qpl6mwToGCvSiPE+LiQ66HJiOvWT0PTo4ez6nD12zGK745EsnwOD+7bA0Zj1BWdPUOSdCVuzQnYeFf1zoRmlcr5WKnfN2W/NqLLyPRZMg7nNgct7TbSjGkHGGuZuMChV9oPVvY09pNCOyOD1w3K6ZFRN03RkllBkJqsTh4IC4YJlCEc1ExFdsSg6Vt8Yii0bLmcbCYp2rhs133ra//zu+cTjc8rVDEskmEYNrplCy8ek0FgrC0RJZGPV4aOrc8lhkDTerKC+2azXFRYZBxVjInLdNhLLkGmWMynC6OVtC/eOLB4UWAAhxrhevsaC/TM57l/lUqqAbw17bDIWnfnQ0TDjraHrntXZtE5dmI2+TyYB1NIgbRq5/yhQqu6CpUfCMRZMgHiWJw7m63kzlplC08agWWBZTIJu1huO1ZXJsoTWFIqhSs6Cii67kgrEwHOjqcakPRpOJW/RMPn/YjrVNedcRQ8AVitBoUe/RSR6LAqP6bzqoH+y//eYpAibSWMhmiFYiGP3r6BaoRNhJ79om/Pf4vUZIBTw1U6gMa1teJ9vcVEVQE+ksK1Mo4bsqyo3MZMouWSBHYyHuIfTIbjbAJIu3n3E4/O0LB8GH9+NlcG+sj0XF0qeSx9CK2HRQ/9geQAdFWy2Dyy1cKzxwREt0CIxEM0eF8qZQTYJkXof8JhjPVtNVp/Zx222HUCZTKMg93KxbxizdhoFCVkB3W6YQrtHvP3jf7rDPqKHwzpGbSp35bZFljVFrpnN30N/7FZ87AH59z5Tw4H99wara9S8cviNMmL7YgcaCV17ahvAkdD4WWqKlprFQl0ENJB7Eq9f3OCOsfv3RfeBTB20fzi+pxiIyhdL2RRfUUGESBqQkvH0chS6RpUsfC1lfaDZm6k3ue0OXTBs1FjnEi0XmAv/yACvDOuNVct57UpCoL5uXRJ0651yPhzpbXN11h8D5i/fpNRYezmGbido1EhtN35LKY8OOQ3e/4lqjjCR03i7cFMqeuOcwJRd8aC/42UnvEOrHPiukmVkgtkDWWBDMX1RlI2JiYL92+NC7toUdhw0Gl8iyxqj7f/q5VchjQNOLe799JJx6aNIJ8b17DA/jvH9izCirManGYgNRQj44MoWS9afprua8rSizx1Ybw0vnHQ8/J+YnoL5aPNgP3HHzBMPaHfOxiPYQ/dj1ncnSDpk1FjzJscrHwk5jkayj2kfFvqrO26ZxyzQW5jHVw+mmGXWXu7zLY+5zh2wPpx6yvbaM2T+nYpl5O5uFAmU8LpGaz2SNRUbGQmb2LYErrVj/vvDVqnVesCw0E5poqBs24k6DWW0HdTBtULLF5j7XN93W3MrHIoOfSi7O2w6jQr13zxHwmYO3J/tY5GIKZUlYc/JY5M1cZ5kf9DwW+pukRUxJ9/3h/UbCx1OMBe95pYeWnQitOW9LTV507YBxT0IGsxjnbVlUKB2DTBcYZcrWrfk9WuP/v73zAJOiyvb4aSYnZoAhDHEIQ87RERAJCwIiYg6LGDGgi/GtuCqmXcPu6qoPMazprbuy6i66KpjT0xXBNSvyRFFRQMBEHmag33cudE91dYVbVbe6q7r/v+/rmQ4VbtW9VXXOPSk5jom8zx6buUK5ibEwtFhE5GMsbNqQ4vp2lhzcoyVdPb1vkruLk35ORVYoo/TyZvg2oW5jsTAbIyqfEVbjUNVx5+mOS/8ZlbdBxlosEhULNZqFlxk6N9car+OnO5kR1nn61c28RBxWaFXj8qL/7F2z8KPGgxO8WLRk17Q7BJljNK1boT9/km0yW95Vxq2IgwJ5lsK5vWVAu5xtszwMnYQYC4msUE4Kjsq2zzZ4W69AxJePeFaw9Ls2FYwNhEE3iUOkLBYmFhk/4ytSgZO0wk6O1FnwtmYfihSLvu321XLo09a8poOlJVfy3uw19kFVgTy3k1n661xVLEcqQIxFCGMsrGIOPBNx4grlXOi4e+YQ+suyr+jL77fT2h92Sq3jSIiUaArfB1QWyPOe2coaJ4dvdJNLeAD58PB1W8fB2mIRsX64+G2x8DI+ZI/fRviXsliYbtvbw0/Fs1K/z6I8qxgL+7bYKayybVYlCMSzQkXc78vodmXXOltXqIg/Y8LYFUouxkLmfuvWFcpM8VR9h0i1nuIkC1MkBU3ZX+QAADnfSURBVDEWXgrkabnv5GH00sqNNK5XK6nlk2OGEr8wkz+8zvAn6PxW9yfyZzIrDxYLkEqLhcribk7RXmsxBduJ7+nEPm3oL6eNoFHdKpWkV3QjGOcELiuU9bqGqVUdbMutL657i4WKGAv9sqm1WHiKsZCtY2Gj5LmpSmseoyLVJCXjxKxwXGzGzViAtFK85SwWsm1Wrlh4cJPo2rKUOu4vnnXJpB7717G7H9pkhTJpj5L4qiRLhPE29Idglj3Kap19u3OfFYo/qpTFfAkltFJKHVmqI/64Qmn34cLiZESrskI6bnhH8V+GiEs3Vc8xFhZtSI0rVCThMywWQDlaE7y/MRYOcjvHgre97jPiofK2i/2xP7aZb3AQ0806e8AYbd/4vSqcxFiwO8yO/Zl7nFgs9PjvCuUlxkJuOTt3JZl+Nxs76bBYsGsQB5W3alroaOZ8X/vs22LXJNkhoWrocECyvcWCbMfxE3NG0qcbttKIzs33bY+8Fsgz+d5h22S2IV/Hwnxnd80csr897iwW5nUsdDPbHh9UJQXqss7J4GSGOvl6N04MEPvNncUiXVmhIi7rWKhrjxerpCx6mSTVXhUqgWIRErTZSIITY+E+ZaKTVaweSm6uaQ4izUtx8LbVPcJWkHBk57axWPhy89fPjpvvo6wwt1GxcDBbl3KLRRpiLOxM/k72ZVW00Ov9x4wP5k9MCKJ28sCVKaRotkzMFULaBU/RNSBXedt+X81K8qm2awtN+7zGWMgpm+7SzUYMz4HdckZCEhdS5PHSrVWphcXCQYxFUhtktyBHl5alNG1AW3pl1Ua67bhBSrZpPbnirV921Te6T7u9d2qVE1WKvV/KrPJ0symO9s/TncBUp8RXCRSLkJCqGAvbYEODZd1cgAkKisHvh/avoqc+WC/eD+xQYbod/boyTWFf75QHb1tOyXpYVyrGQqNYkHqc3L/LCvPouy11+9sScZAVKhKirFCyy+ktFhEXWaGMl9E333lWqIht9iI9+sxMERMB1OnzXnnlbUUbypVyhfIuvDu2WJh+7/3AXVssDBSQ8qI86rDfDcxoHdnzF7do21gAVciJtx8/iBr27E2J0OfIUh1J7hczxcLRmHQQY+HP08X9pJL3+YNk7ww/LRaRJGU8fRPIXoFiERKCkhWqpKBxyJTtf+/dFSr5eK6Z3pfaNSuivm3LqVOLEul1ZWpqsGLRq01jRgqehUprHQuF6WaNH87G71VhPFNobrEgiaB/uxgBP90BU3VTt3Mhk8sKJWmxcNo2C1fMWNvslI2oQeC2+N7hDaPRYmG9nOx21WVxiUhYLJTsKnG/NmPTdEwoaEuyACtnHTHKHmVnlRTfSbSpMcYiuQ1+3O9SNZPs5b7PMS1bSYUrlHY9d+POK0mJAFIVYyGZalfVGIu4VKCCSHhtLVmG9sFuN5ta26XRrN67Si6lWwy7oXzNYX3Ew4Rf18YKsblxhbJRR5qX5NO8yb1shX43F3Vhfg6VF+fRw2ccIAIm+ZiCHLztbObKyGKh+Z3UY+fbrLdYxNheZ14l2as7iB0HdW9p+buXm7rbIOKIwmNMrlng1GJhbjHlOIqPrppEZx/c1XIbdQ17DBULpzQGb8tbU1OhWJi5AanYl9Vq9jEWcsK+G4xmxuWyQiXvPFkRSPymtCBX6t4nH2MRpEoW9jiZO5FR5OLbdTAQEtycbVbzLb2vbrPS6WYVukI5KebqFv1WwlRpWw8sFmG0WNhcWLccO5CuX7pS5Inu265caTuqK0vo35eOF+9blhWI/25u1w7uV5a4WTcm5LBfs9a3OX0F8qwxFDAdBOYlpiVUf7Ny4s8fs3Ix2+oaXAvdXt0BbzqyP81d9C61KS+kJ95b52vwttnsflJfubDKmMlKblMAmzSF9mgLdEYiIk7JTk7buT+WRlsczw2ylgpZuVHVJRCb4HGS3UwWXs3scA4b0I4WLV8rJpuui03u6NY13KaCA5dOIxuxV8LsrnF2lSIPFguz78KCoxglfb9YKL2u61jYLOubxcK1YuFtv7Jp9FXdT5roNhRigwUUi7CgrfhqN2PFwtKtLoPLZC6SmEIRw1WMRcJOna/fuG7iyrVdK+nL77+2XMXL7KkvrlB2FouIt9nyxOBt+W1J7zNpdlzOFWq7E8VC99lrTm++Rv5+Zq14b6RYeEs3m8jvj+pP1z71ibhuN2/bLV2rQ+YYZdMOOz0a/fnu3LKE1v28S7zv0Lxo/76tL/yd9XsM4y+cu0JFglnQyiRoWIUwL9YzOVHNS/Lo+QvHiFpAB9W0lK5voEJQkXXXkLFsJCcviLhSLBotFtbKdBDtFV4sU24tFk7GZEJ6eVtXKH8k4WRLrtykj1dFWlapilhkPvTmGhtezQKuUCEhKDEWRrgxMftlsbh0ck8a070lTerT2nQdL7OnbrGO3Xbn2mCyMct9+2KxsHFBMFMs9H771tukEBXIiyRZ+d6cNz5u6ZNcTcoqY5aRzU4xs0O/+PUz+lOrsgLqUllCcyd0l9rGTk3waFGe+/Mp29US8eX7tqfoqRev1xHxwRXK4jcWrDpXltDYHq0MhU/tLnN8rmFjhn5PhpYNmwkJeYtFE9MsayGWzRxaLOTd9BzVsdCml0/TuUyaVJK8ISgtKGf1/Nbtpzhffr7+rDH73ElnHtAp6fqEKxQIVIyFF9w8fPwI3pZfN/lh9OCpw8X76kuflspek3ZXKJX3P0PFQitc+D9mSjXKg1WMhRV6eTnVgW1egreNlCCjMackxsLMFUr/2eHh6Bfv2KKY/n3pONHm+Fi2ufB3mbhCmVkWerYpE/Uc7NridYIjlRYL165QHmayIyb3HRWHLXvuktKfGlosrC12McWC7+d/efNLOmVkZ2uLRfKoD7zJwmm2oT8e1Y/ufG0NnblfII1vR7eslSunW1eopPbozqdfBdzcbtWrmCQ7XPSnpbQghzZvk1uXJ0JPG9VZeIH8vLM+4bcQZ5uFYhEWtD7OqU6Vaoc7V6io7zfm387oSzc9s0rMtH62cVt6XaE8xFg4wa7IlC+TILptVpUXSlksnIwPt0WS3GKnvLO5m5WF383oJ+EaZtxWO1cQGSHO7DpK9td1qllEPFtxtK5QMlmhHjp9BC374nv6vw1b6baXVmuaoravlaWbjc+UOxMO5e9rUZdZoSLGFgsFxy27jWSLRcRx5reK4n2KBVug+WV7bzWwWIQZo2fGYQOq6MihHW3XVWWx0FpEebXLp/ai655eKQThodXNddslX3Dbj6lyhWriwWKhdS23ex6ECSgWoay87d+AczOWXakIBoX2VHPiiE50wvCOtPjdb+nCR96Pf1+Un/qpAEuzrMITYLSlRIuF+pOt32Lbin0++EaUaoK3vSir6bZYXHlobzp2WAfDm79sPQo7wUrGYmEevK1vkzMiCq57sxgLMypLC+jQ/m3p7p8+T/hetqvlg7cjSseIL5e2B4uFVtlMzAjn/bhlt5Hk85+b49iq5jjGwmD7qXT/8oq+WrazAnnJ6WZll5VXLCJ06sjOItNkt9alScqL2bn2eqt224ees0Jp61hYxUgqqtAe8ToZFCBCbGzJLrTpHjMhxkKLl6Oxs3zwxaq/PtNhsbAUPlxsj6v1Wu3n6CHt49+xP7ZMO9yivyFaWyzkhAW7au5+ugOK7dsoFvv8t42XkVUQ7IR/mRgL2eBtp6dLxThJcIXSWizs9m3n0hKQ4O1GdxP1FgurW7zd2Nfk+UhQQlJrsYi4sFgkfm7qNCuUjcVClZVcBfk5OUlt1NdrcpRmXDZbl9MYi4T19rXpwG6V1Kos+R5v1lyv15vb1VWKSU7iqApdyhf6XQRMzHMEFAsDFixYQL1796Zhw4ZREIO3fbVYuFjHc1YoD8gEbOpvukUOTZW+u0K5OOk3HdWfCvOaJFkAYje5+Yf1EW46S+eOTrjR+TF0IjbKwxGD2on/hw9sS00Lw2GxsBPeLKst6z6bCQgRHy0Wdv7rdsgsbzehkOAKpY2xsFnPyeyr2F58u5Tiytv+WSyszr+d0qtVyhMUC3dN0bWLXFos7IO39dt2brFIHvNBmvRdcMJg0Z7urUvpQIM05x01VcgdF8jTn28LxcLJvdMyxiKpDXKW2VTheb8u01cXulQsPLuvBgi4QhkwZ84c8dqyZQuVl6utA+GWhoQYi2Dpg25mghKzTUR8tZYEwWJhHWPh/Pi7tiylt+ZNELPafec/m/Q7KxwnjEj2xfXbNYCVHT2/P3qACLzs3bYpfbzuZyUWC/8VC3uLhSzyAoLRg8WuurVsVijJJrhc3s5908s1x8UylWaFUmyxiPiwL61SVl6USz/vbJAem9prR7t/FcftdhNy6WaNYyzcKnhJFos0Gyym9q+iYdXjhbXZaLKhddMCZVmhVE1kJbhC2bXBzGLhUVxxb7Hw6gqlaYOD9QrdWixsxm+YCJaECtKfbtbFpr3esL0czV5ZaSLtrlBqLRYMVw/XWyxst+XD0PlxR2Nthqry5PgKfsj1a18u/oclxsJOeXfSn7Jt9TXGwuEgk7mmnVz3CXUsbJbVt9XM7S+5PaktaGXmgqN6Xz1alzkaT9pnheqsULI3EP2+jBUL6xlaxwXyDCwgQZPNWjUtTDgXEUWTAUnLOnDdUVF92mq7nl2hXMdYeNqt9OSnXkks8pBaO2G7IdYsoFiEhFQFb7vBVeXtVLpC6S7QoAVvq+xN25oYpJ4N+wunMW2amsdXMLKKhX2MRXqDty3rkuj6Wlqx0H2WWc9MmE6KsXB4uvYont5NdIWyXlbf1OaSM9eyLVZvsYj4uq8erUsduekluEL5nLjBDP2+jFyhIspcoYz7QX8dpttiYcQ5Y7vF33PiAi369KNWJLlVWizrNiuU3T0kcDEWCp8REQe/FSpyhQqanOcEKBYhIVXB224ePpzOVVawNJ4JIdfYCaBi+wGoY5Eq7zW7oeFHCrsNWzSKhUXgtpOATLte9fumy25bylyhpC0W1oKRo+BtA39z1ZZAuyXOG9coNP2it3nBSj36w7azWMSVq3S5QimaHTajuweLRUKMRRO1zyFnFouI/bnRffRqsdj3XbCFM65hcMmkHnTDEf1oVE0lXTalp/i+JD+HBnSokN5OksHCaobdwSlx8pw2u8d47QK3q3u2WEjvR2+xyHG1P6+TQUECMRYhQRtjETRNdvrAdrRo+Vpa8/12uvfkoS624P54ZGahghBjYSVgqHz22T1I/Rg5W3c1SCsWrNTxw/Of73xLv5nay7Vbi1/K9T/OrqVd9XuptktycKV08LZbVyhyjnnwtvVnO6yqosty9sFdhXDYpWWJiAlqxCZ4W/e5WXG+4qxQlLLgbRX70lss7NBmhdIaNyKKn0NW6PdlFEycLEglflFRJNfv5skRgpsVSns/nKOxWnAsWsfmJdStVam0dVdgY/1J+M2RK5RxvI7hdpv4ZbFw6wrlbb+ySlVSsok8NVmhwgwUi5CgnYWySiXnFTfXIgtOj5xVK9ooK0Spc4WSsVjoXaGCplikxmQr2uGz5cQq1WyM2Qd1FS9vMRbqDuSg7i3ptf/bRNMGtKUhnRKLPplh+aDR17GQ7F83w8BMAbObDVZisbBZhAtFnT66i7MdG1wPssHbqahjwfe32L24+X6Fx1rJ9H5txwpoyWLqCqWgLfVarcUCmUrQyVY1d8U0Y8eY7A4UPlGNz9Mhfds4Xi/ZDUyNsptYV8PdpJXXiVC7tYdXN6e7Xv3C5xiFiO8Tl01C4LonC1yhQkKqYiy0M4sju1nP2npLY5dY0dMtUTfp4AyKNflNqqxM6bD8H9q/Kv7ebqZfFju5VqVufceJg+n+k4fR74/qr2R7STOx0hYL550nXcfC4cCQsVi4nQG2jbGwcIViJZDp267RVS1W/VzedYFc84+zD6ShnZrRb6b0amyXDxaLY4buq0Mz+6Aujq9ps+BtFbcg7batiLjKChVxdd3E5hjsskyFWVCzw2oeIdl6GXE3cWezmnnwNnmCN9uv3b7snCfVdkr6fVzPVvSr8TUJtZus2iOL7HDR37cLXQZvJ1vYwgssFiFhT4piLHh27K6ZQ+jtL3+gM1zMNroh4rPFIunGmgZXMr8zxzRuy3pjfjxcrzi0t5hdHNihgmp0/uBusU83q06zYJeDsT1bKduevgdkr1d3FguzbVnPBqu4rtyiLdhopIjq2xqzDDB3zxxC7379Ew2tbkYr12+h3Q17aUinZg7rWLi/4HiMP3b2gZbtVbGvG4/sTxdN7EGtmxbSV5u2OFrXz+DtelnFQrcr4+Btb2NUf4716/PnEV2aC7dLpu9+4TQTiVicWz737N4Zw8mt04nFwux3r5YyXv8vpw2nd77+kQ7sWmn4+4W/6C7eP/qfbxrb4/ERITv5qcpiEckgiwUUi5BQn8IYi0l92oiXn6i6ZuQuvvSbxC1doVLYPj9uViz8XH9EfyUC55rN28X79s2S09ZqCVr1eRUxFm4wUwC8ppuV8aV3m4ChRWmBUBBWmE1eJGVxy0nwSa/dX1ysf/sKV8qQ6jSOVttzK9xwf/F1FXvvBNPgbQWH3eDSFcooxkLVZRFXLAzStV4xtTd9++NOMYbY+pOpWPUtn3utYuFsPMlnhTLbrIp+rijOp3E95RNAqEjSIlvHQlXlbeba6X3okbe/oV8fsi+IP6xAsQhlHYsM8GBTlRVKYgYt1e5BnVoUU33DXlqnScOquvK2W/ycifbKPScNoV89/B71rCqjMftdXswIWgIDFYKsynGQXCPA33SzTps+sU8b8VKxLZlqwwnbV3z7tJ7NTP04NSuQRykM3taTl5vcDreuT6YxFgbbZ3e1v59ZS5mOfnJKGwsjgok1CTacVPTWPl7t1jJXLFJ3Ddx63EC66JH3xeRDYsIIb1jH1BkndXDDzNpq8WLe+uJ7CitQLEKC1ufZy8ANIl5m7KXqWFBque/kYdSlsoQ6z1siWXnbnIhiT8vgqhVE3VqV0ZK5o6WWDbTFQtejuX7GWEhmhXL6bHdTeDLdHDawLd307Ke0edtuuvOXg02XUy3o+OEK5a3vNPtXrETVS6ab1SOTcIQziJ05pgs9/cH6eNyMGaeMrKb73/iSqlsUU9OifWKMx3wFoUY/YVTXsNdU4XZfx8KdKxT3VSozVLJba5mTjFpmuHStjCq6dYbvDtwIFIuQkKoYi1ShKvWfVFaoFM8aRvbv87cz+tJvFn9ElaX5dHCPlmmPvwj93SokFgur4O1Fsw+ge19fQyeM6OjrdWRXLEy5xULhQN1e1zi76gR2QXj54oNp49Y6y9lK1SNHVc0A0+17qDWhjUVScc24tlhIWpPmTe4lXnawqwjH5wzsWKE5/9mrWeiTLXDsUYwCXTCx+zoW1ivqf73y0N60Y3cDnXxgZ0olTQvl6p84ubdaTvpE/PEKiIb4WQ3FIiQEufK2G1JZIE82k4kqYjMYJwzvSD1al1F1ZQkVWGSisqzcq7htQczl7oawXgMHdGkhXv4Hb+s+O9yuzDVjV2vELT/ukK84rKesME+8rFCtrFtbHFVYLJxtQ9t1WuM2C/fzJu+rIXP5ofbCu5cYCz0F+4O3r5neh6558hM6ZlgH8gIrkXpXuuQxH857hAqFr65hj7mLoIPT4qzyduICp45KrUKRLhlFf16qyhvjA3lSMRuf1VAsQnjjyIgYCw1ebv8yioX2JpsKYjchFgiGVtvXRUilxSKEHi6hUyzczt67Uiwkt+XUJcepMq6yN37asdtz6kYrlAucEeM6Fz5s3rP7ypljuoqXW9hCEGNqv8Y007IWi5Nqq+noIR18qSWUHGNBWUOilSqSaLHQZeRyMv4dVd7OovNtdb8f3rk5HTu0A73/zU90y7ED3W84xM9qKBYhIZMtFl6QeYbvqk+tYuHVr5q7N3ZcTbIoeNsJQVauIykUeKtbNKZu7V3V1HQMeqmFYLauXyPpR41iIVt1O51o+42FuB279ygVtlTVsVDB2B6tREV1zrR01WF9pNfTPrP8KlCazTEWCTGYrFhog7d11nIn48lR5e0M0yxkS3gYTSTcqKgmUlgJ7tMZmMZY5GVA8HaC/6KHG5KMoKxNtRdE9MefqzFdq75XZ4heEWjl2m2fuVlvUp/WdPjAtqKAFBf6M88KFVFSC8EMleO0XUVx/P3gjvtqVHjl9P1uGb2qmiq3gmjvzfpUkypOixc3NicZgKTaEomI+Ibbjh8kXRE9VTipPp1p7EnwaIhQneaZp68h4uS0JGSFyqYT6kBG4UBxTiLAzDIo4OcW9nZoVVYg3l8+1Z3rYrqAxSIkZJrFQhkBtFg4vf/qF8/jGSeT37zil198qkl3AgOrB417i4W7dvzpuEHK2mBXC0GLX0PpvHHd6JVVG4Vywz75Kvj15J40untLGtheG+yrhjoLtxM1moUHpTCLnhXZHGNRr3eFSrBY6BQLl1mhskyvkIatgpzS+J2vflRaaDU/twk9/avRtHrjNhrRuTnt2eMuqUU6gGIREjKtjoU6V6jgWSyczg5bFVRTbrGgzCAnwFa7VMZYUApiLGSEU5VCHNceeOHCMeIYVCkB7ONvVxvFLdqJi2SLhff2O03966crVJDJZsFX2+c81rUxFkkWCyeuUJr3WTSUHMsoXMxysoOYI1lalhWIF7MntfOjngi/hJqFwdtBFqpkmdC7sYrmkYPb+RtjkabgbenlLVIzKs8KlSGahWoXD5W4bZrKvjGqQqzancbPrCUsEIfF9cLKYqHiEJym/jXLCpXpeB3zGZPcJSeSWMfCgyuUkxiLTCOh8nZ2HbpnYLEIYdaHdLuBqOCowe3pq++3i0DHc8Z2c70dGVPt8cM60sJXPhfvbzjCuvBSeiwW+hgLHy0WGaJZZMI1kMqMXY4tFlqBQuZcZ153qFEsFGzfi8Uiq1yhbD5nMtpK2+zRsL2uXpErlGY9yi5UpcTPRqBYhNIVKvyjnIWVSyb1VHrxmwlPHVsU02Nn1YrCWZN0uc/9wGnv6JutdXWT2VZZYS5t3SXnf4l0s/7jtmV+1ltx+mA8tH+VqHfAnDHaOB/9uWNr6OHla0W77545hLIVbTrr5Aw83sdpcX6ue1eoLJKIks51Fh17gnwgLBZ71LhCJUzcZc/53IdkgTyQBBSLUKaTgweb0wI+MvUklOHZFcqZxWLxOSPp0bfX0rQBbbMn3WyQfTxcPoD97BunehinFb36sD708856OuOgLobLtCkvpOcuOEgsM6hDY32DbEMbw+WlyrEZFcV5NKXDHvqivoKumd43NMHb5UV5Ymxo0yD7SVZbLHQTj/Ua16j8HPeZyrRzHdmkpDKwWLgHikVISDBvB1moSjGJrlDBOC+Ob8BW6WYlVu/WqpTmTQlXOjqv5KRZubbqF7ej0F+lz7l73qwDq22X69qylLIda4uFmn1Mah+lKVNqKS/Puqp40mRLGhWLJ+aMpBdWfic14aGCLDZYJFRE10886pXdICSXCF2MRRrbEUYw9R1KH0oM8xgTeze6Npm5bKQax65Qus/a/lXd1RlisAj0NeD2ARwkVyggj7ZmgF6IS4dIonlUpHWWubqyhE4f3UVkzEmPxSKSnR4NuonHfM1EFeP2rAT4lusL2e0G5g1YLEJCtgbk2TG6plK4bGzeVkfnHOw+CDxI6WYTgrdJLX5m8kklQb4G3Ao0/rpCBfd8hR1tulk/skJ5KdgXZAVcNdmdFUrT5zl2Fgt3+4BwDWSBxSKUMRa4wPUuGxdN7EFF+YluCOnCebrZiHnwtuKuRvC2/7jtMz/7JrhnK7OyQunrWKSDeZMb3SJ/eYC6SsBBJ5tjLPTJXYo047CFrkK624mPbDqfzGmjGmPLzp9Qk9a2hA1YLEJ242CBCjMHwcZp/+gX1wZvq9b8MyV4O9CKhcv1Kors/efdAotFeNPNOuWIwe1EbEXTwlwa0qkZZQ1ZHGOhDdZmxeL+U4bRrPuWU7tmRTRjUHv69T8+jP8Oi4UcI7u1oAUnDBYxVNP6pyZOKFOAYmHAggULxGtPAEodbq9roBufXEkffPNz4AUqoKZAnp8WiwzRKwJttXPbZzWty+jYoR3o9dWb6ZZjBwaiTcAZSZW303Di2RXmqCHtKdtIsvzqXIKypc4VF1g9oEsLWnH5BCrJz/UkM3B8xu79blaFSfFDyZw7thvd+erndOHE7hR2+Nqd2l99Ne1sAIqFAXPmzBGvLVu2UHl5eVrb8sfnPxO54sMgUAE1dSy0ST3Q28YEWcH2orzdeFR/ETSoWiCFYpEagmCxyFb0YzybMpZpXaVj98amhXmerZf3nTyMfnnvW1RWkEunjrJPjnLxpB507rhugXAJBOkDikXA+ctbjUpF0AUq4DJ4Wyd+aCaf1AdvZ4jJIt21XKy62GushB+z3NnmxpAu/Eo3C+zRPxp7timjbIELvy585XPxfmyPlpbLOrl1jqqppFcvOVjUJDFTVPRAqQDZYyvMENjMCYKNY2Ei4q7onxu0qR87Ni+msBJkBXtPAJW3AJ+u0HPFob3j7w8b2DZrU56mG/25bt+siLKF88Z1o2OGtqeTajvZBuw7HZOdWpRQRXFiADgAVsBiEWCMZj6DLFABNYGyflb8nDGoHT32n2/o6x920D0nDaWwEuTrIIhWIQi4/jHzgE7UvCRPKOrtKhKFWVgsUsfWuvr4e3bdySYrXXF+Lt101ACpZbPotIA0AcUiwGzelfwdYiwyj4hVNXHF++KAxr+fWUt790bTWpU3kxULPrdBI8CnK/Tk5zYRmXdAelm1YWv8fY8scoNyCm4FwG+gWASY73ZGQiVQAbcF8iIpUyxihFmpCLqCrcn8GByCe7oyGswOp47PN233pFhwFsj6+karR5DgduXm5tKuXbtcZasc07WcVm/cJt43KySxHRAu6j2OATtycnLE9lVY+qBYBJj1O8IlUAE16Wa1gikEE2OCrGAH0RUKdSzSA1zQUsew6ma04ssfxfsDu1Y6Wnfbtm30zTffBPLaZbhdbdq0obVr17oS/C4+sBlt21VGBXk5tGndN7TJl1aCII8BGYqLi6mqqory873F1ECxCDAbDCwW2ZSbO1vTzWofbhBLjEm3gm0lMAaxCCHGUXoIsgKcaVx1WB8672/vUp925TSlXxvp9Xj2l5UKFqpatmwZyNiMvXv3CuWntLSUmqQ5Ix7IvDEQjUZp9+7dtGnTJlqzZg3V1NR42gcUiwCzYYeBYoEHVeBxOjusd8n3MytUppBugS1K5srD/npSgSKIwlKmcvnUXnTd0yupa8sSqmmVPbUU0k2ftuX00sUHu3IxYcGKlYqioqLACpUs+BUWFkKxyFL2+jwGeOzn5eXRV199Fd+PW6BYBJjOZVGqbF5B7++vuh0EgQrY41SGa9BJon7WscgU0iEoX3d4X7r88Y+oojiPJvRqHSqLBW4bqeO0UZ1F5eMuLUtCH8uUTUD5BtlOE0UKCxSLAHN0l700ZcoIqr3xVdq8rU58B4tFMBnXsxW99OlGKnWR5rBer1hoXaEiwRNSs5UThncUwmKXylLLIlBBzAoFX//Uwdd/33bl6W4GAACkBSgWIaC0IIc270voAItFQLn5mAG09KMNNNJh0CCzW5dGCK5QwYRnn2WCQoNYIC8C7wkAAAApAIpFCCjKb+wmKBbBhCuTHj+8o6t19RaLPZoZb/R2+AigwQLjCAAAQErAPFYIKMpr7Ka6hgBGhgJP1Ov6NKHyduqbE1huOXYAtS0vpCsO7U1BJogpK5FuFgBgx/fff0+tWrWiL7/8Utk2Dz74YDr//POVbc+vbQaNgxUf43HHHUd//OMfKRVAsQgBxRqLxY7d6gujgPRiFWMBA1UjXN343/PGi+DYIKO1OAUF6BUAADt++9vf0vTp06m6ulp8Pvnkk0XMEL+4tkG3bt3ommuuoYaGBgoTnEb17LPPpo4dO1JBQYGoBzFp0iR644030qKsbJJozz//+U+69tprle3z8ssvF/3788+NyYD8Aq5QIaAovzFQdCcUi4wjOcai8T0EwvARQL0CFgsAgCU7duyge++9l5599tmE7w855BC6//77qa6ujpYsWUJz5swRaUnnzZtHYeHII48UKVQffPBB6tKlC3333Xf04osvCgtNUNvTvHlzpfvs27cvde3alR566CHRh34Ci0UIKNJkoNlZD8Ui07DMCpWG9oDMc4UCAGQmy5cvF7PtXIegZ8+e9Pbbb9Pdd99Nhx12mKPtsNLAs+cHHHBAwvexGfVOnTqJWfYJEybQv/71r/jvrHD86le/Ei5UXPtg1KhRtGLFCsN9/M///A+1aNFCrKPl8MMPp5kzZ4r3zzzzjNhGRUWFWPbQQw+lzz//3LLtTz/9NJWXl9Nf//rXpN9++ukn+t///V+68cYbaezYseI4hg8fLhSj2Dliy8yrr75Kt956a9xCE3MH4/oR119/PXXu3Fmc4wEDBtBjjz0W3z6f+3PPPVe8uA2VlZV0xRVXmD4HZNqjt6BwW3JycqhZs2bif6yNvIxMG2NMmzaNFi1aRH4DxSIEFGssFjt2h8sECZzVseB0woixCDdBdIWCxQKAzGPZsmU0ZswYmjp1Kn3wwQfUq1cv4arEQuvVV1/taFss7A4ZMsR2ORZcebY9xn/913/RP/7xDzH7/s477wh3KXbr+eGHH5LWPfroo0Wlc61isnHjRqEYnHrqqeLz9u3b6cILLxQKEs/ic22FGTNmCOHZiL/97W90/PHHC6XixBNPTPqdK1Xz6/HHH09SaGKwQlFbW0tnnHEGrV+/Xrw6dOggfmOBnRWiO++8kz7++GO64IIL6Je//KVQRGLwsefm5gol79Zbb6Wbb76Z/vznPxvuS6Y9ergt3377LX366afi/7vvviuUroMOOki6jQwrMNxG2f26Ba5QIXOF2lWP4O1MdoXKy2miq2ORpkYB1wRQr8A4AsAB025/nTZt9Vf4MqJlWQE9ed4o6eVZAGdh/ZJLLhGfWcDmF8dJDBo0SHz3xBNP0Msvv0x/+tOfLLfFFZfbtm1r+jvPwLOgz65S5513XlwJWLhwIT3wwAM0efJk8d0999xDzz//vHCrirVLq5SccMIJwrWK282waw7HGsRm39lNSMt9990nqqJ/8sknwp1Hy4IFC+g3v/kNPfnkk0LBMoIFfm4fKw0seA8ePFgsy8HM/fv3F8uwpYFjSIqLi4V1JgYL4L/73e/ohRdeEIoHw65Lr7/+Ot11113xfbLgf8sttwgrQo8ePejDDz8Un3mfbtqjh60U3C5uH7fziCOOEO256qqrpNvIcP+yUrhhwwZhKfELKBYhs1iAzKO6RXH8/dDqZjSkUzP60wufic8DmgdQSgWWBDH4HhYLAORhpWLDll0UZL755ht688036Q9/+EOC0MoKgNZawZYMdo2xY+fOncKVSc9TTz0lZtjr6+uF1YAVAxZoGXZR4u9HjhwZX57jL3hmfOXKlYb7YYF62LBhYua9Xbt2QsiOBYkzn332GV155ZX01ltv0ebNm+OWiq+//jpBsWBXH7Z2cMAzb88KVlbYqsNWGbbyLF26lG666SZhVeB9m7F69WoRe/KLX/wi4XsWzmOKG8PuY9rCuLW1tSIDE1tnWClQ1R7mtNNOo61btwrlja05sm2MKXYML+8nUCxClhUKZB6zDqymFz/dSD/vqKebjupPzYrzRZB+YW6EOu9Yle7mAQ+uUEGpOxOMVgAQDthyEPT9xgR3nvGOsWrVKiHU9+vXL0GxYCF2y5YtIo6B38+ePTtpexwb8OOPPyZ9z3EAbJXgmXKe8WblxQss7LKiw647EydOFK477AqljQPg2XS2fPD+WLFghULrfhXbDrtesUVj6NChCYK9Eaw0sfDNL46BOP3002n+/PmWgvy2bfsqE3P7WAnSx554odBFe1iJfO6554Q7U1lZmeM2xtzT2ALkJ5BYQ0ChJngbZGb/PnLmPhNmjHlTeomZoCVLoFiE22IRDJE+IM0AIBQ4cUdKF5w2NBbIGxMaWfDUWydYAeGZanZVYkuDfmZbK6izW5KekpISETdhBGcZYoWDrQYx1xp+bnHwtlXqVhai2TWLrRYcDB6LZ+CsSKwcsVIxevRo8R279Jjtm60C7ELF5+G///u/yQm9e/cWcQ4x+DjYwqBfhoVztpaYuVoxbF3RsmzZMqqpqTG0Vsi2Rw/HsbBVgxUIPnanbWQ++ugjat++vVAi/QSKRQho32yf+YppUZKf1rYAAORjLIKjWASjHQAANQwcOFAIwixscrzC3LlzRf0JjkXgeAkW9Nm9iYV3dl9ipaFPnz6m2+OAa85MxFYLzj4kAysdnCmKYyk4PSrHSnB72NWGXXbM4PZcfPHFQoFgy0UM3i8HJXNWq6qqKiEsX3rppabb6d69u4gfYeWCLSlGcSSsrPD54eBwjmHgmX4ODOd2cixKDD53rCBwBiZ2/eLj4WW5nRwMzZYTzlbFCh0rUk2bNqVZs2aJdbmdHO9y5plnCivK7bffblqMTrY9eoWALRncx9yHHCMRU4a4nTJtZNj1iq1EfgPFIgT8oldrGtWtkj5Zv4XumTU03c0BAFhwcI+W9PDyr8X7E0d0TFs7ygpzaesuZJEDIBOJFavjLEQcvMvBv5whiQVHrj3BlgoWSNnfn5ULOxcmdp9it6pHHnlECMiy3HDDDUKgZTcr9v1ntyQO8LZSTjhYmuMMePadU83G4JgBTofK6WvZ/YkDoW+77bZ4YLcRvMxLL70Ut1zoBXpWEkaMGCGCqWMxIWwh4ViPyy67LL4cC+cshLMFgBWyNWvWCGWDi9Sx6xBnXvriiy9EGlw+T9p1TzrpJLEOu6Hl5OQIBcDI3cxJe7Sw4sHKGluktDE1bKF45ZVXpNq4a9cuYRHhdL5+E4ki6bop7JPIFwBrf6z5pZJ9bjBLaMqUKSIYKua7HRSfbZCeMQCCD99SF7y8mn7YXk8XTexOJQW5aRkDn323le57Yw1N6VdFo2v89akF/oH7gL+wwMVCJNcAMApeDgIsuLM8wnIIC9+ycGYmDnBmawQLrpx+lAVbM1jQZ+sDKyRO9uOG8ePHi9l3VhzCDCs0bD2yy7qVrjEQg+NkFi9eLGI03FwLTuRhWCxCBJQKAIIPux2dO64m3c2gmtZldP0RxukLAQCZDwducwwDz16fc845wv2GLRJmcGA3Z2ViC0cs7kE17GrFs+z8uuOOO3zZB0iGJyXYRSsVQLEAAAAAAMgw2E0qBsc8WMU9xLAKulYBB4mzcsFF/NiNCaQGDphPFVAsAAAAAACA73BwdCbBlheQiL9OdAAAAAAAAICsAIoFAAAAAAAAwDNQLAAAAAAAAACegWIBAAAAAAAA8AwUCwAAAABkNSjpBbKdqKJrAIoFAAAAALISrpTM7N69O91NASCtcHVvxmshTqSbBQAAAEBWkpubS8XFxbRp0yYhUPldcdpt1WVWfLgychDbB8I9BqLRqFAquEp7RUVFXNl2CxQLAAAAAGQlkUiEqqqqaM2aNfTVV19REGHBb+fOnVRUVCTaC7KPaArGACsVbdq08bwdKBYAAAAAyFry8/OppqYmsO5Q9fX19Nprr9FBBx3k2U0FhJN6n8cAb9OrpSIGFAsAAAAAZDXsXlJYWEhBhAW+hoYG0T4oFtlJTojGAJz1AAAAAAAAAJ6BYmHAggULqHfv3jRs2LB0NwUAAAAAAIBQAMXCgDlz5tAnn3xCK1asSHdTAAAAAAAACAWIsZAoFrJly5a0BOpw+i/ed9D96YA/YAwAjAGAMQAwBkB9msdATA6WKaIHxcKCrVu3iv8dOnRId1MAAAAAAABIq1xcXl5uuUwkijr2lgVJ1q1bR2VlZSnPHc3aISs0a9eupaZNm6Z03yAYYAwAjAGAMQAwBsCWNI8BVhVYqWjbtq1tgT5YLCzgk9e+ffu0toEHEG4k2Q3GAMAYABgDAGMANE3jGLCzVMRA8DYAAAAAAADAM1AsAAAAAAAAAJ6BYhFQCgoKaP78+eI/yE4wBgDGAMAYABgDoCBEYwDB2wAAAAAAAADPwGIBAAAAAAAA8AwUCwAAAAAAAIBnoFgAAAAAAAAAPAPFIqAsWLCAqqurqbCwkEaMGEHLly9Pd5OAIl577TWaNm2aKDTDhRcff/zxhN857OnKK6+kqqoqKioqogkTJtBnn32WsMwPP/xAJ554oshnXVFRQaeddhpt27YtxUcC3HD99dfTsGHDROHNVq1a0eGHH06rVq1KWGbXrl00Z84catGiBZWWltKRRx5J3333XcIyX3/9NU2dOpWKi4vFdi655BJqaGhI8dEANyxcuJD69+8fz0lfW1tLS5cujf+O/s8+brjhBvE8OP/88+PfYRxkNldddZXoc+2rZ8+eoe9/KBYB5O9//ztdeOGFIgPAO++8QwMGDKBJkybRxo0b0900oIDt27eLPmXl0YibbrqJbrvtNrrzzjvprbfeopKSEtH/fJOJwUrFxx9/TM8//zw99dRTQlmZPXt2Co8CuOXVV18VD4tly5aJ/quvr6eJEyeKcRHjggsuoCeffJIeffRRsfy6devoiCOOiP++Z88e8TDZvXs3/fvf/6YHH3yQHnjgAaGQguDDhVdZkPzPf/5Db7/9No0bN46mT58urmkG/Z9drFixgu666y6hbGrBOMh8+vTpQ+vXr4+/Xn/99fD3P2eFAsFi+PDh0Tlz5sQ/79mzJ9q2bdvo9ddfn9Z2AfXwJbh48eL4571790bbtGkT/f3vfx//7qeffooWFBREH374YfH5k08+EeutWLEivszSpUujkUgk+u2336b4CIBXNm7cKPrz1Vdfjfd3Xl5e9NFHH40vs3LlSrHMm2++KT4vWbIk2qRJk+iGDRviyyxcuDDatGnTaF1dXRqOAnilWbNm0T//+c/o/yxj69at0Zqamujzzz8fHTNmTHTu3Lnie4yDzGf+/PnRAQMGGP4W5v6HxSJgsObJs1js/hKjSZMm4vObb76Z1rYB/1mzZg1t2LAhof/Ly8uFO1ys//k/uz8NHTo0vgwvz+OELRwgXPz888/if/PmzcV/vv7ZiqEdA2we79ixY8IY6NevH7Vu3Tq+DFu1tmzZEp/1BuGAZx0XLVokLFbsEoX+zy7Yesmzztr+ZjAOsoPPPvtMuEV36dJFeCKwa1PY+z83bXsGhmzevFk8aLQDheHPn376adraBVIDKxWMUf/HfuP/7EupJTc3VwimsWVAONi7d6/wqR45ciT17dtXfMd9mJ+fL5RHqzFgNEZiv4Hg8+GHHwpFgl0c2X968eLF1Lt3b3rvvffQ/1kCK5Ts7syuUHpwH8h8RowYIVyXevToIdygrr76aho9ejR99NFHoe5/KBYAAJDG2Up+iGj9akF2wMIEKxFssXrsscdo1qxZwo8aZAdr166luXPnijgrTtICso/JkyfH33N8DSsanTp1okceeUQkbgkrcIUKGJWVlZSTk5MU+c+f27Rpk7Z2gdQQ62Or/uf/+kB+zgLBmaIwRsLDueeeKwLvX375ZRHMG4P7kF0if/rpJ8sxYDRGYr+B4MOzkd26daMhQ4aITGGc0OHWW29F/2cJ7OrC9/HBgwcLizO/WLHkxB38nmeeMQ6yi4qKCurevTutXr061PcBKBYBfNjwg+bFF19McJfgz2w2B5lN586dxQ1B2//sL8mxE7H+5/98s+EHU4yXXnpJjBOe8QDBhmP2Walg1xfuN+5zLXz95+XlJYwBTkfLvrfaMcCuNFoFk2c+OXUpu9OA8MHXb11dHfo/Sxg/frzoQ7ZaxV4cN8d+9rH3GAfZxbZt2+jzzz8XqeZDfR9IW9g4MGXRokUiC9ADDzwgMgDNnj07WlFRkRD5D8KdBeTdd98VL74Eb775ZvH+q6++Er/fcMMNor+feOKJ6AcffBCdPn16tHPnztGdO3fGt3HIIYdEBw0aFH3rrbeir7/+usgqcvzxx6fxqIAsZ599drS8vDz6yiuvRNevXx9/7dixI77MWWedFe3YsWP0pZdeir799tvR2tpa8YrR0NAQ7du3b3TixInR9957L/rMM89EW7ZsGZ03b16ajgo44dJLLxVZwNasWSOucf7MWd2ee+458Tv6PzvRZoViMA4ym4suukg8B/g+8MYbb0QnTJgQraysFJkCw9z/UCwCyu233y4GVH5+vkg/u2zZsnQ3CSji5ZdfFgqF/jVr1qx4ytkrrrgi2rp1a6Fgjh8/Prpq1aqEbXz//fdCkSgtLRWp5U455RShsIDgY9T3/Lr//vvjy7ASec4554gUpMXFxdEZM2YI5UPLl19+GZ08eXK0qKhIPIz4IVVfX5+GIwJOOfXUU6OdOnUS93cWBPgajykVDPo/O9ErFhgHmc2xxx4braqqEveBdu3aic+rV68Off9H+E/67CUAAAAAAACATAAxFgAAAAAAAADPQLEAAAAAAAAAeAaKBQAAAAAAAMAzUCwAAAAAAAAAnoFiAQAAAAAAAPAMFAsAAAAAAACAZ6BYAAAAAAAAADwDxQIAAAAAAADgGSgWAAAAMo5IJEKPP/54upsBAABZBRQLAAAASjn55JOFYK9/HXLIIeluGgAAAB/J9XPjAAAAshNWIu6///6E7woKCtLWHgAAAP4DiwUAAADlsBLRpk2bhFezZs3Eb2y9WLhwIU2ePJmKioqoS5cu9NhjjyWs/+GHH9K4cePE7y1atKDZs2fTtm3bEpa57777qE+fPmJfVVVVdO655yb8vnnzZpoxYwYVFxdTTU0N/etf/0rBkQMAQPYCxQIAAEDKueKKK+jII4+k999/n0488UQ67rjjaOXKleK37du306RJk4QismLFCnr00UfphRdeSFAcWDGZM2eOUDhYCWGloVu3bgn7uPrqq+mYY46hDz74gKZMmSL288MPP6T8WAEAIFuIRKPRaLobAQAAILNiLB566CEqLCxM+P6yyy4TL7ZYnHXWWUI5iHHAAQfQ4MGD6Y477qB77rmHfv3rX9PatWuppKRE/L5kyRKaNm0arVu3jlq3bk3t2rWjU045ha677jrDNvA+Lr/8crr22mvjykppaSktXboUsR4AAOATiLEAAACgnLFjxyYoDkzz5s3j72traxN+48/vvfeeeM+WiwEDBsSVCmbkyJG0d+9eWrVqlVAaWMEYP368ZRv69+8ff8/batq0KW3cuNHzsQEAADAGigUAAADlsCCvd01SBcddyJCXl5fwmRUSVk4AAAD4A2IsAAAApJxly5Ylfe7Vq5d4z/859oLdl2K88cYb1KRJE+rRoweVlZVRdXU1vfjiiylvNwAAAHNgsQAAAKCcuro62rBhQ8J3ubm5VFlZKd5zQPbQoUNp1KhR9Ne//pWWL19O9957r/iNg6znz59Ps2bNoquuuoo2bdpE5513Hs2cOVPEVzD8PcdptGrVSmSX2rp1q1A+eDkAAADpAYoFAAAA5TzzzDMiBawWtjZ8+umn8YxNixYtonPOOUcs9/DDD1Pv3r3Fb5we9tlnn6W5c+fSsGHDxGfOIHXzzTfHt8VKx65du+iWW26hiy++WCgsRx11VIqPEgAAgBZkhQIAAJBSONZh8eLFdPjhh6e7KQAAABSCGAsAAAAAAACAZ6BYAAAAAAAAADyDGAsAAAApBR64AACQmcBiAQAAAAAAAPAMFAsAAAAAAACAZ6BYAAAAAAAAADwDxQIAAAAAAADgGSgWAAAAAAAAAM9AsQAAAAAAAAB4BooFAAAAAAAAwDNQLAAAAAAAAACegWIBAAAAAAAAIK/8P+MSWt2b1iwJAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:56:49.404979Z",
     "start_time": "2025-04-28T19:56:49.380169Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2372a010a0853e50",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:56:50.606226Z",
     "start_time": "2025-04-28T19:56:50.353520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_sizes = [16, 64, 256]\n",
    "alpha_arrays = [\n",
    "    alpha_B16,  # Polyak step sizes when batch size = 16\n",
    "    alpha_B64,  # batch size = 64\n",
    "    alpha_B256  # batch size = 256\n",
    "]\n",
    "\n",
    "variances = [np.var(a) for a in alpha_arrays]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(batch_sizes, variances, marker='o', linestyle='-')\n",
    "plt.xlabel('Batch Size (B)')\n",
    "plt.ylabel(r'Variance of Step Size ($Var(\\alpha_k)$)')\n",
    "plt.title('Variance of Step Size vs. Batch Size')\n",
    "plt.grid(True)\n",
    "plt.xscale('log')  # Because batch size often plotted in log-scale\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('step_size_variance_vs_batch_size.png', dpi=300)\n",
    "plt.show()\n"
   ],
   "id": "28f10f850d4dd4e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-28T19:45:59.779008Z",
     "start_time": "2025-04-28T19:45:59.777646Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "43dad80225a677cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c94831462b1ac1fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
